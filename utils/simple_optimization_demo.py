#!/usr/bin/env python3
"""
MiniGPTç®€åŒ–ä¼˜åŒ–æ¼”ç¤º
ä¸“æ³¨äºæ ¸å¿ƒä¼˜åŒ–åŠŸèƒ½ï¼Œé¿å…å¤æ‚ä¾èµ–
"""
import json
import time

import torch
import torch.nn as nn
import torch.optim as optim


class SimpleTransformer(nn.Module):
    """ç®€åŒ–çš„Transformeræ¨¡å‹ç”¨äºæ¼”ç¤º"""

    def __init__(self, vocab_size=10000, d_model=256, n_heads=4, n_layers=6, d_ff=1024):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = nn.Parameter(torch.randn(2048, d_model))

        # Transformerå±‚
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=n_heads,
            dim_feedforward=d_ff,
            dropout=0.1,
            activation='relu',
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)
        self.output_proj = nn.Linear(d_model, vocab_size)

    def forward(self, x):
        seq_len = x.size(1)
        x = self.embedding(x) + self.pos_encoding[:seq_len].unsqueeze(0)
        x = self.transformer(x)
        return self.output_proj(x)


class SimpleMemoryOptimizer:
    """ç®€åŒ–çš„å†…å­˜ä¼˜åŒ–å™¨"""

    def __init__(self, model, enable_amp=True, gradient_accumulation_steps=1):
        self.model = model
        self.enable_amp = enable_amp and torch.cuda.is_available()
        self.gradient_accumulation_steps = gradient_accumulation_steps
        self.step_count = 0

        if self.enable_amp:
            from torch.cuda.amp import GradScaler
            self.scaler = GradScaler()
            print("âœ… æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨")
        else:
            self.scaler = None
            print("âŒ æ··åˆç²¾åº¦è®­ç»ƒæœªå¯ç”¨")

    def should_update(self):
        """æ˜¯å¦åº”è¯¥æ›´æ–°å‚æ•°"""
        return (self.step_count + 1) % self.gradient_accumulation_steps == 0

    def scale_loss(self, loss):
        """ç¼©æ”¾æŸå¤±"""
        return loss / self.gradient_accumulation_steps

    def backward(self, loss):
        """åå‘ä¼ æ’­"""
        scaled_loss = self.scale_loss(loss)
        if self.enable_amp and self.scaler:
            self.scaler.scale(scaled_loss).backward()
        else:
            scaled_loss.backward()

    def step_optimizer(self, optimizer):
        """ä¼˜åŒ–å™¨æ­¥éª¤"""
        if self.should_update():
            if self.enable_amp and self.scaler:
                self.scaler.step(optimizer)
                self.scaler.update()
            else:
                optimizer.step()

            optimizer.zero_grad()
            return True

        self.step_count += 1
        return False


def get_memory_usage(device):
    """è·å–å†…å­˜ä½¿ç”¨é‡(MB)"""
    if device.type == 'cuda':
        return torch.cuda.memory_allocated() / 1024 / 1024
    else:
        import psutil
        return psutil.Process().memory_info().rss / 1024 / 1024


def benchmark_training(model, device, config_name, enable_amp=False, gradient_accumulation_steps=1):
    """åŸºå‡†æµ‹è¯•è®­ç»ƒæ€§èƒ½"""
    print(f"\nğŸ”¬ æµ‹è¯•é…ç½®: {config_name}")
    print(f"   æ··åˆç²¾åº¦: {'å¯ç”¨' if enable_amp else 'ç¦ç”¨'}")
    print(f"   æ¢¯åº¦ç´¯ç§¯: {gradient_accumulation_steps} æ­¥")

    # é‡ç½®æ¨¡å‹
    model.train()
    optimizer = optim.AdamW(model.parameters(), lr=1e-4)

    # åˆ›å»ºå†…å­˜ä¼˜åŒ–å™¨
    memory_optimizer = SimpleMemoryOptimizer(
        model, enable_amp, gradient_accumulation_steps
    )

    # æµ‹è¯•å‚æ•°
    batch_size = 16
    seq_len = 512
    vocab_size = 10000
    test_steps = 20

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    dummy_input = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)
    dummy_target = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)

    # è®°å½•åˆå§‹å†…å­˜
    initial_memory = get_memory_usage(device)

    # æ¸…ç†ç¼“å­˜
    if device.type == 'cuda':
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()

    try:
        # é¢„çƒ­
        for _ in range(3):
            optimizer.zero_grad()
            if enable_amp and device.type == 'cuda':
                from torch.cuda.amp import autocast
                with autocast():
                    output = model(dummy_input)
                    loss = nn.CrossEntropyLoss()(
                        output.reshape(-1, vocab_size),
                        dummy_target.reshape(-1)
                    )
            else:
                output = model(dummy_input)
                loss = nn.CrossEntropyLoss()(
                    output.reshape(-1, vocab_size),
                    dummy_target.reshape(-1)
                )

            memory_optimizer.backward(loss)
            memory_optimizer.step_optimizer(optimizer)

        # å®é™…æµ‹è¯•
        start_time = time.time()

        for _ in range(test_steps):
            if enable_amp and device.type == 'cuda':
                from torch.cuda.amp import autocast
                with autocast():
                    output = model(dummy_input)
                    loss = nn.CrossEntropyLoss()(
                        output.reshape(-1, vocab_size),
                        dummy_target.reshape(-1)
                    )
            else:
                output = model(dummy_input)
                loss = nn.CrossEntropyLoss()(
                    output.reshape(-1, vocab_size),
                    dummy_target.reshape(-1)
                )

            memory_optimizer.backward(loss)
            memory_optimizer.step_optimizer(optimizer)

        # åŒæ­¥GPU
        if device.type == 'cuda':
            torch.cuda.synchronize()

        end_time = time.time()
        final_memory = get_memory_usage(device)

        # è®¡ç®—æŒ‡æ ‡
        elapsed_time = end_time - start_time
        total_samples = test_steps * batch_size
        samples_per_sec = total_samples / elapsed_time
        memory_used = final_memory - initial_memory

        results = {
            'samples_per_sec': samples_per_sec,
            'elapsed_time': elapsed_time,
            'memory_used_mb': memory_used,
            'final_loss': loss.item()
        }

        print(f"   â±ï¸  è®­ç»ƒæ—¶é—´: {elapsed_time:.2f}s")
        print(f"   ğŸ“ˆ è®­ç»ƒé€Ÿåº¦: {samples_per_sec:.1f} samples/sec")
        print(f"   ğŸ’¾ å†…å­˜ä½¿ç”¨: {memory_used:.1f}MB")
        print(f"   ğŸ“‰ æœ€ç»ˆæŸå¤±: {loss.item():.4f}")

        return results

    except RuntimeError as e:
        if "out of memory" in str(e).lower():
            print(f"   âŒ å†…å­˜ä¸è¶³: {e}")
            if device.type == 'cuda':
                torch.cuda.empty_cache()
            return {'error': 'OOM'}
        else:
            raise e


def benchmark_inference(model, device):
    """åŸºå‡†æµ‹è¯•æ¨ç†æ€§èƒ½"""
    print("\nğŸš€ æ¨ç†æ€§èƒ½æµ‹è¯•")

    model.eval()
    vocab_size = 10000
    seq_len = 512
    test_batches = [1, 4, 8, 16, 32]

    results = {}

    with torch.no_grad():
        for batch_size in test_batches:
            try:
                # åˆ›å»ºæµ‹è¯•æ•°æ®
                dummy_input = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)

                # é¢„çƒ­
                for _ in range(5):
                    _ = model(dummy_input)

                # æ¸…ç†å†…å­˜
                if device.type == 'cuda':
                    torch.cuda.empty_cache()

                # åŸºå‡†æµ‹è¯•
                steps = 50
                start_time = time.time()

                for _ in range(steps):
                    _ = model(dummy_input)

                if device.type == 'cuda':
                    torch.cuda.synchronize()

                end_time = time.time()
                elapsed_time = end_time - start_time

                samples_per_sec = (steps * batch_size) / elapsed_time
                tokens_per_sec = samples_per_sec * seq_len
                latency_ms = (elapsed_time / steps) * 1000

                results[batch_size] = {
                    'samples_per_sec': samples_per_sec,
                    'tokens_per_sec': tokens_per_sec,
                    'latency_ms': latency_ms
                }

                print(f"   æ‰¹å¤„ç†={batch_size}: {tokens_per_sec:.0f} tokens/sec, å»¶è¿Ÿ={latency_ms:.1f}ms")

            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    print(f"   æ‰¹å¤„ç†={batch_size}: OOM")
                    results[batch_size] = {'error': 'OOM'}
                    if device.type == 'cuda':
                        torch.cuda.empty_cache()
                else:
                    raise e

    return results


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸš€ MiniGPTç®€åŒ–ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 60)

    # è®¾å¤‡é€‰æ‹©
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"ğŸ¯ ä½¿ç”¨CUDA GPU: {torch.cuda.get_device_name()}")
    elif torch.backends.mps.is_available():
        device = torch.device("mps")
        print("ğŸ¯ ä½¿ç”¨Apple Silicon GPU (MPS)")
    else:
        device = torch.device("cpu")
        print("ğŸ¯ ä½¿ç”¨CPU")

    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ“Š åˆ›å»ºæµ‹è¯•æ¨¡å‹...")
    model = SimpleTransformer(vocab_size=10000, d_model=256, n_heads=4, n_layers=6)
    model.to(device)

    param_count = sum(p.numel() for p in model.parameters())
    print(f"   æ¨¡å‹å‚æ•°é‡: {param_count:,}")
    print(f"   æ¨¡å‹å¤§å°: ~{param_count * 4 / 1024 / 1024:.1f}MB")

    # è®­ç»ƒæ€§èƒ½æµ‹è¯•
    print("\n" + "="*60)
    print("ğŸ§  è®­ç»ƒæ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("="*60)

    training_results = {}

    # æµ‹è¯•é…ç½®
    configs = [
        ("åŸºç¡€é…ç½®", False, 1),
        ("æ··åˆç²¾åº¦", True, 1),
        ("æ¢¯åº¦ç´¯ç§¯", True, 4),
    ]

    for config_name, enable_amp, grad_accum in configs:
        result = benchmark_training(
            model, device, config_name, enable_amp, grad_accum
        )
        training_results[config_name] = result

    # æ¨ç†æ€§èƒ½æµ‹è¯•
    print("\n" + "="*60)
    print("ğŸš€ æ¨ç†æ€§èƒ½æµ‹è¯•")
    print("="*60)

    inference_results = benchmark_inference(model, device)

    # æ€§èƒ½æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ“Š æ€§èƒ½æ€»ç»“")
    print("="*60)

    # è®­ç»ƒæ€§èƒ½å¯¹æ¯”
    valid_training = {k: v for k, v in training_results.items() if 'error' not in v}
    if len(valid_training) >= 2:
        print("\nğŸ† è®­ç»ƒæ€§èƒ½å¯¹æ¯”:")
        baseline = None
        for name, result in valid_training.items():
            speed = result['samples_per_sec']
            memory = result['memory_used_mb']
            print(f"   {name}: {speed:.1f} samples/sec, {memory:.1f}MB")

            if baseline is None:
                baseline = speed
            else:
                speedup = speed / baseline
                print(f"      ç›¸å¯¹æå‡: {speedup:.2f}x")

    # æ¨ç†æ€§èƒ½æ€»ç»“
    valid_inference = {k: v for k, v in inference_results.items() if 'error' not in v}
    if valid_inference:
        print("\nğŸš€ æ¨ç†æ€§èƒ½æ€»ç»“:")
        best_batch = max(valid_inference.keys(), key=lambda k: valid_inference[k]['tokens_per_sec'])
        best_result = valid_inference[best_batch]
        print(f"   æœ€ä½³é…ç½®: æ‰¹å¤„ç†å¤§å°={best_batch}")
        print(f"   æœ€å¤§ååé‡: {best_result['tokens_per_sec']:.0f} tokens/sec")
        print(f"   æœ€ä½å»¶è¿Ÿ: {min(r['latency_ms'] for r in valid_inference.values()):.1f}ms")

    # ä¿å­˜ç»“æœ
    results = {
        'device': str(device),
        'model_params': param_count,
        'training_results': training_results,
        'inference_results': inference_results,
        'timestamp': time.time()
    }

    results_file = "simple_demo_results.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)

    print(f"\nğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    print("\nâœ… æ¼”ç¤ºå®Œæˆ!")


if __name__ == "__main__":
    main()
