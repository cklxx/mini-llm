#!/usr/bin/env python3
"""
æ¨ç†æµ‹è¯•è„šæœ¬
æµ‹è¯•æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å’Œæ¨ç†æ€§èƒ½
"""
import torch
import time
from typing import List

from config.training_config import get_config
from src.model.transformer import create_model
from src.model.config import MiniGPTConfig


def test_text_generation(model, config, test_prompts: List[str] = None):
    """æµ‹è¯•æ–‡æœ¬ç”ŸæˆåŠŸèƒ½"""
    print("=== æ–‡æœ¬ç”Ÿæˆæµ‹è¯• ===")

    if test_prompts is None:
        # åˆ›å»ºæµ‹è¯•æç¤º
        test_prompts = [
            [1, 2, 3, 4, 5],  # ç®€å•æ•°å­—åºåˆ—
            [10, 20, 30],     # å¦ä¸€ä¸ªåºåˆ—
            [100],            # å•ä¸ªtoken
            [1, 1, 1, 1]      # é‡å¤token
        ]

    model.eval()
    results = []

    for i, prompt_tokens in enumerate(test_prompts):
        print(f"\n--- æµ‹è¯• {i+1}: è¾“å…¥é•¿åº¦ {len(prompt_tokens)} ---")

        # è½¬æ¢ä¸ºtensor
        prompt = torch.tensor([prompt_tokens], dtype=torch.long)
        if config.device != "cpu":
            prompt = prompt.to(config.device)

        print(f"è¾“å…¥: {prompt_tokens}")

        # ç”Ÿæˆæ–‡æœ¬
        start_time = time.time()
        with torch.no_grad():
            generated = model.generate(
                prompt,
                max_length=20,
                temperature=0.8,
                top_k=50
            )
        end_time = time.time()

        # æå–ç”Ÿæˆçš„éƒ¨åˆ†
        generated_tokens = generated[0].tolist()
        new_tokens = generated_tokens[len(prompt_tokens):]

        generation_time = end_time - start_time
        tokens_per_second = len(new_tokens) / generation_time if generation_time > 0 else 0

        print(f"ç”Ÿæˆ: {new_tokens}")
        print(f"æ—¶é—´: {generation_time:.3f}s")
        print(f"é€Ÿåº¦: {tokens_per_second:.1f} tokens/s")

        results.append({
            'prompt': prompt_tokens,
            'generated': new_tokens,
            'time': generation_time,
            'speed': tokens_per_second
        })

    return results


def test_batch_generation(model, config):
    """æµ‹è¯•æ‰¹é‡ç”Ÿæˆ"""
    print("\n=== æ‰¹é‡ç”Ÿæˆæµ‹è¯• ===")

    batch_size = 4
    seq_len = 5

    # åˆ›å»ºæ‰¹é‡è¾“å…¥
    prompts = torch.randint(0, min(100, config.vocab_size), (batch_size, seq_len))
    if config.device != "cpu":
        prompts = prompts.to(config.device)

    print(f"æ‰¹é‡å¤§å°: {batch_size}")
    print(f"è¾“å…¥å½¢çŠ¶: {prompts.shape}")

    model.eval()
    start_time = time.time()

    with torch.no_grad():
        # æ³¨æ„ï¼šå½“å‰çš„generateæ–¹æ³•åªæ”¯æŒå•ä¸ªæ ·æœ¬ï¼Œæ‰€ä»¥æˆ‘ä»¬é€ä¸ªå¤„ç†
        generated_batch = []
        for i in range(batch_size):
            single_prompt = prompts[i:i+1]
            generated = model.generate(
                single_prompt,
                max_length=10,
                temperature=0.8,
                top_k=50
            )
            generated_batch.append(generated)

    end_time = time.time()
    total_time = end_time - start_time

    print(f"æ‰¹é‡ç”Ÿæˆæ—¶é—´: {total_time:.3f}s")
    print(f"å¹³å‡æ¯æ ·æœ¬æ—¶é—´: {total_time/batch_size:.3f}s")

    return total_time


def test_different_generation_params(model, config):
    """æµ‹è¯•ä¸åŒçš„ç”Ÿæˆå‚æ•°"""
    print("\n=== ç”Ÿæˆå‚æ•°æµ‹è¯• ===")

    # å›ºå®šè¾“å…¥
    prompt = torch.tensor([[1, 2, 3]], dtype=torch.long)
    if config.device != "cpu":
        prompt = prompt.to(config.device)

    test_params = [
        {'temperature': 0.1, 'top_k': 10, 'name': 'ä½æ¸©åº¦+å°top_k'},
        {'temperature': 1.0, 'top_k': 50, 'name': 'ä¸­ç­‰æ¸©åº¦+ä¸­ç­‰top_k'},
        {'temperature': 1.5, 'top_k': 100, 'name': 'é«˜æ¸©åº¦+å¤§top_k'},
    ]

    model.eval()
    results = []

    for params in test_params:
        print(f"\n--- {params['name']} ---")
        print(f"æ¸©åº¦: {params['temperature']}, top_k: {params['top_k']}")

        start_time = time.time()
        with torch.no_grad():
            generated = model.generate(
                prompt,
                max_length=15,
                temperature=params['temperature'],
                top_k=params['top_k']
            )
        end_time = time.time()

        generated_tokens = generated[0].tolist()[3:]  # å»æ‰è¾“å…¥éƒ¨åˆ†
        generation_time = end_time - start_time

        print(f"ç”Ÿæˆ: {generated_tokens}")
        print(f"æ—¶é—´: {generation_time:.3f}s")

        results.append({
            'params': params,
            'generated': generated_tokens,
            'time': generation_time
        })

    return results


def test_memory_usage(model, config):
    """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    print("\n=== å†…å­˜ä½¿ç”¨æµ‹è¯• ===")

    if config.device == "cuda":
        torch.cuda.empty_cache()
        before_allocated = torch.cuda.memory_allocated() / 1024**3
        before_reserved = torch.cuda.memory_reserved() / 1024**3

        print(f"ç”Ÿæˆå‰ - å·²åˆ†é…: {before_allocated:.1f}GB, å·²ä¿ç•™: {before_reserved:.1f}GB")

        # ç”Ÿæˆå¤§é‡æ–‡æœ¬
        prompt = torch.tensor([[1, 2, 3, 4, 5]], dtype=torch.long).to(config.device)

        model.eval()
        with torch.no_grad():
            for i in range(5):
                generated = model.generate(prompt, max_length=50)
                if i == 0:
                    mid_allocated = torch.cuda.memory_allocated() / 1024**3
                    mid_reserved = torch.cuda.memory_reserved() / 1024**3
                    print(f"ç”Ÿæˆä¸­ - å·²åˆ†é…: {mid_allocated:.1f}GB, å·²ä¿ç•™: {mid_reserved:.1f}GB")

        after_allocated = torch.cuda.memory_allocated() / 1024**3
        after_reserved = torch.cuda.memory_reserved() / 1024**3

        print(f"ç”Ÿæˆå - å·²åˆ†é…: {after_allocated:.1f}GB, å·²ä¿ç•™: {after_reserved:.1f}GB")

        return {
            'before': {'allocated': before_allocated, 'reserved': before_reserved},
            'after': {'allocated': after_allocated, 'reserved': after_reserved}
        }
    else:
        print("éCUDAè®¾å¤‡ï¼Œè·³è¿‡æ˜¾å­˜ç›‘æ§")
        return None


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ”¥ å¼€å§‹æ¨ç†æµ‹è¯•")

    # è·å–é…ç½®
    config = get_config("tiny")

    # åˆ›å»ºæ¨¡å‹
    model_config = MiniGPTConfig(
        vocab_size=config.vocab_size,
        hidden_size=config.d_model,
        num_hidden_layers=config.n_layers,
        num_attention_heads=config.n_heads,
        intermediate_size=config.d_ff,
        max_position_embeddings=config.max_seq_len,
        dropout=config.dropout,
        rms_norm_eps=1e-6
    )

    model = create_model(config=model_config)

    # ç§»åŠ¨åˆ°è®¾å¤‡
    if config.device != "cpu":
        model = model.to(config.device)

    print(f"è®¾å¤‡: {config.device}")
    print(f"æ¨¡å‹å‚æ•°: {model.get_num_params():,}")

    # è¿è¡Œå„ç§æ¨ç†æµ‹è¯•
    generation_results = test_text_generation(model, config)
    batch_time = test_batch_generation(model, config)
    param_results = test_different_generation_params(model, config)
    memory_results = test_memory_usage(model, config)

    # æ€»ç»“
    print(f"\nğŸ‰ æ¨ç†æµ‹è¯•å®Œæˆï¼")

    # è®¡ç®—å¹³å‡æ€§èƒ½
    avg_speed = sum(r['speed'] for r in generation_results) / len(generation_results)
    avg_time = sum(r['time'] for r in generation_results) / len(generation_results)

    print(f"âœ“ å¹³å‡ç”Ÿæˆé€Ÿåº¦: {avg_speed:.1f} tokens/ç§’")
    print(f"âœ“ å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_time:.3f} ç§’")
    print(f"âœ“ æ‰¹é‡å¤„ç†æ€§èƒ½: {batch_time:.3f} ç§’/4æ ·æœ¬")

    if memory_results:
        print(f"âœ“ æ˜¾å­˜ä½¿ç”¨ç¨³å®š")

    print("\næ‰€æœ‰æ¨ç†æµ‹è¯•é€šè¿‡ï¼")


if __name__ == "__main__":
    main()