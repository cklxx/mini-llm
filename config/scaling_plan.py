#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºScaling Lawsç ”ç©¶çš„æ™ºèƒ½æ¨¡å‹è®­ç»ƒè®¡åˆ’
"""

from dataclasses import dataclass
from typing import List, Dict
import math

@dataclass
class ScalingConfig:
    """æ¨¡å‹ç¼©æ”¾é…ç½®"""
    name: str
    d_model: int
    n_heads: int
    n_layers: int
    d_ff: int
    vocab_size: int
    max_seq_len: int

    # è®­ç»ƒé…ç½®
    learning_rate: float
    batch_size: int
    max_steps: int
    warmup_steps: int
    gradient_accumulation_steps: int

    # æ•°æ®é…ç½®
    min_training_tokens: int  # åŸºäºChinchillaæ¯”ä¾‹

    def get_params_estimate(self) -> int:
        """ä¼°ç®—å‚æ•°é‡"""
        # Transformerå‚æ•°ä¼°ç®—: 12 * n_layers * d_model^2
        transformer_params = 12 * self.n_layers * (self.d_model ** 2)
        # åŠ ä¸Šembeddingå’Œè¾“å‡ºå±‚
        embedding_params = self.vocab_size * self.d_model * 2
        return transformer_params + embedding_params

    def get_flops_estimate(self) -> int:
        """ä¼°ç®—è®­ç»ƒæ‰€éœ€FLOPs"""
        # 6 * N * D (Chinchillaå…¬å¼)
        return 6 * self.get_params_estimate() * self.min_training_tokens

def calculate_chinchilla_tokens(d_model: int, n_layers: int, vocab_size: int) -> int:
    """æ ¹æ®Chinchillaæ¯”ä¾‹è®¡ç®—æ‰€éœ€tokens"""
    # ä¼°ç®—å‚æ•°é‡
    transformer_params = 12 * n_layers * (d_model ** 2)
    embedding_params = vocab_size * d_model * 2
    total_params = transformer_params + embedding_params

    # Chinchillaæ¯”ä¾‹: 20 tokens per parameter
    return int(total_params * 20)

def get_intelligence_threshold_configs() -> Dict[str, ScalingConfig]:
    """
    åŸºäºç ”ç©¶çš„æ™ºèƒ½é˜ˆå€¼é…ç½®

    æ ¹æ®ä»¥ä¸‹ç ”ç©¶è®¾è®¡ï¼š
    - Anthropic: 22Bå‚æ•°å‡ºç°é“å¾·æ¨ç†
    - Chinchilla: å‚æ•°:æ•°æ® = 1:20æœ€ä¼˜æ¯”ä¾‹
    - OpenAI: æ¯8å€å‚æ•°å¢é•¿éœ€è¦çº¦5å€æ•°æ®å¢é•¿
    """

    configs = {}

    # Large: æ¥è¿‘æ™ºèƒ½é˜ˆå€¼ (ç®€åŒ–ç‰ˆ22B)
    configs["large"] = ScalingConfig(
        name="large",
        d_model=2048,          # å¤§å¹…å¢åŠ 
        n_heads=32,            # 64ä¸ªå¤´
        n_layers=24,           # 24å±‚
        d_ff=8192,             # 4x d_model
        vocab_size=50000,      # æ‰©å¤§è¯æ±‡è¡¨
        max_seq_len=2048,      # æ›´é•¿åºåˆ—

        # è®­ç»ƒé…ç½®
        learning_rate=1e-4,
        batch_size=1,          # å†…å­˜é™åˆ¶ä¸‹çš„æœ€å°batch
        max_steps=50000,       # å¤§å¹…å¢åŠ è®­ç»ƒæ­¥æ•°
        warmup_steps=2000,
        gradient_accumulation_steps=64,  # é€šè¿‡æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿå¤§batch

        # æ•°æ®é…ç½® (åŸºäºChinchilla 1:20æ¯”ä¾‹)
        min_training_tokens=calculate_chinchilla_tokens(2048, 24, 50000)
    )

    # Extra Large: æ›´æ¥è¿‘æ™ºèƒ½é˜ˆå€¼
    configs["xlarge"] = ScalingConfig(
        name="xlarge",
        d_model=2560,          # è¿›ä¸€æ­¥å¢åŠ 
        n_heads=40,            # 40ä¸ªå¤´
        n_layers=30,           # 30å±‚
        d_ff=10240,            # 4x d_model
        vocab_size=50000,
        max_seq_len=2048,

        # è®­ç»ƒé…ç½®
        learning_rate=8e-5,    # å¤§æ¨¡å‹éœ€è¦æ›´å°å­¦ä¹ ç‡
        batch_size=1,
        max_steps=100000,      # æ›´å¤šè®­ç»ƒæ­¥æ•°
        warmup_steps=5000,
        gradient_accumulation_steps=128,

        # æ•°æ®é…ç½®
        min_training_tokens=calculate_chinchilla_tokens(2560, 30, 50000)
    )

    # XXL: å‘22Bå‚æ•°ç›®æ ‡
    configs["xxl"] = ScalingConfig(
        name="xxl",
        d_model=4096,          # æ¥è¿‘GPTè§„æ¨¡
        n_heads=64,            # 64ä¸ªå¤´
        n_layers=32,           # 32å±‚
        d_ff=16384,            # 4x d_model
        vocab_size=50000,
        max_seq_len=2048,

        # è®­ç»ƒé…ç½®
        learning_rate=5e-5,    # æ›´å°å­¦ä¹ ç‡
        batch_size=1,
        max_steps=200000,      # å¤§é‡è®­ç»ƒæ­¥æ•°
        warmup_steps=10000,
        gradient_accumulation_steps=256,  # å¤§æ¢¯åº¦ç´¯ç§¯

        # æ•°æ®é…ç½®
        min_training_tokens=calculate_chinchilla_tokens(4096, 32, 50000)
    )

    return configs

def calculate_scaling_metrics(config: ScalingConfig) -> Dict[str, float]:
    """è®¡ç®—ç¼©æ”¾æŒ‡æ ‡"""
    params = config.get_params_estimate()
    flops = config.get_flops_estimate()

    # ç›¸å¯¹äºmediumæ¨¡å‹çš„å¢é•¿å€æ•°
    medium_params = 59484480  # 59.5M

    return {
        "estimated_params": params,
        "params_vs_medium": params / medium_params,
        "estimated_flops": flops,
        "training_tokens": config.min_training_tokens,
        "tokens_per_param": config.min_training_tokens / params if params > 0 else 0,
        "memory_estimate_gb": params * 4 / (1024**3),  # FP32ä¼°ç®—
        "training_days_estimate": flops / (1e15 * 86400),  # å‡è®¾1PF/s
    }

def print_scaling_analysis():
    """æ‰“å°ç¼©æ”¾åˆ†æ"""
    print("ğŸ§  åŸºäºScaling Lawsçš„æ™ºèƒ½æ¨¡å‹è®­ç»ƒè®¡åˆ’")
    print("=" * 60)

    configs = get_intelligence_threshold_configs()

    print(f"ğŸ“š ç ”ç©¶ä¾æ®:")
    print(f"  â€¢ Anthropic: é“å¾·æ¨ç†èƒ½åŠ› â‰¥ 22B å‚æ•°")
    print(f"  â€¢ Chinchilla: æœ€ä¼˜æ¯”ä¾‹ = 1å‚æ•°:20tokens")
    print(f"  â€¢ å½“å‰mediumæ¨¡å‹: 59M å‚æ•° (è·ç¦»é˜ˆå€¼ 370x)")
    print()

    for name, config in configs.items():
        metrics = calculate_scaling_metrics(config)

        print(f"ğŸš€ {name.upper()} æ¨¡å‹é…ç½®:")
        print(f"  æ¨¡å‹ç»“æ„: {config.d_model}dÃ—{config.n_layers}å±‚Ã—{config.n_heads}å¤´")
        print(f"  å‚æ•°é‡: {metrics['estimated_params']:,} ({metrics['estimated_params']/1e9:.1f}B)")
        print(f"  vs Medium: {metrics['params_vs_medium']:.1f}x å¢é•¿")
        print(f"  è®­ç»ƒæ•°æ®: {metrics['training_tokens']:,} tokens")
        tokens_per_param = metrics['tokens_per_param']
        print(f"  æ•°æ®æ¯”ä¾‹: 1:{tokens_per_param:.1f} (éœ€è¦{tokens_per_param:.1f}ä¸ªtokens/å‚æ•°)")
        print(f"  å†…å­˜éœ€æ±‚: ~{metrics['memory_estimate_gb']:.1f}GB")
        print(f"  è®­ç»ƒæ—¶é—´: ~{metrics['training_days_estimate']:.1f} å¤©")
        print()

def get_recommended_next_step() -> ScalingConfig:
    """æ¨èä¸‹ä¸€æ­¥é…ç½®"""
    configs = get_intelligence_threshold_configs()

    # åŸºäºç¡¬ä»¶é™åˆ¶ï¼Œæ¨èlargeé…ç½®
    return configs["large"]

if __name__ == "__main__":
    print_scaling_analysis()

    print("ğŸ’¡ æ¨èæ–¹æ¡ˆ:")
    print("  1. å…ˆè®­ç»ƒLargeæ¨¡å‹ (2.5Bå‚æ•°)")
    print("  2. ä½¿ç”¨æ›´å¤šé«˜è´¨é‡æ•°æ® (50M+ tokens)")
    print("  3. å¢åŠ è®­ç»ƒæ­¥æ•°å’Œæ—¶é—´")
    print("  4. è§‚å¯Ÿæ˜¯å¦å‡ºç°æ›´å¤æ‚çš„æ¨ç†èƒ½åŠ›")