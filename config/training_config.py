"""
è®­ç»ƒé…ç½®æ–‡ä»¶ - NVIDIA GPU ä¼˜åŒ–ç‰ˆæœ¬
æ”¯æŒ PyTorch 2.4 å’Œ NVIDIA GPU è‡ªåŠ¨æ£€æµ‹ä¼˜åŒ–
"""
import os
import torch
import subprocess


def get_gpu_info():
    """è·å– GPU è¯¦ç»†ä¿¡æ¯"""
    if not torch.cuda.is_available():
        return None

    gpu_info = {
        'count': torch.cuda.device_count(),
        'current_device': torch.cuda.current_device(),
        'devices': []
    }

    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        device_info = {
            'id': i,
            'name': props.name,
            'memory_total': props.total_memory / 1024**3,  # GB
            'memory_allocated': torch.cuda.memory_allocated(i) / 1024**3,  # GB
            'memory_reserved': torch.cuda.memory_reserved(i) / 1024**3,  # GB
            'compute_capability': f"{props.major}.{props.minor}",
            'multi_processor_count': props.multi_processor_count
        }
        gpu_info['devices'].append(device_info)

    return gpu_info


def setup_cuda_optimizations():
    """è®¾ç½® CUDA ä¼˜åŒ–"""
    if torch.cuda.is_available():
        # å¯ç”¨ TensorFloat-32 (é€‚ç”¨äº Ampere æ¶æ„)
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True

        # å¯ç”¨ CuDNN benchmark
        torch.backends.cudnn.benchmark = True

        # è®¾ç½® CUDA å†…å­˜åˆ†é…ç­–ç•¥
        torch.cuda.empty_cache()

        print("å·²å¯ç”¨ CUDA ä¼˜åŒ–:")
        print(f"  - TensorFloat-32: {torch.backends.cuda.matmul.allow_tf32}")
        print(f"  - CuDNN Benchmark: {torch.backends.cudnn.benchmark}")
        return True
    return False


def get_device():
    """è‡ªåŠ¨æ£€æµ‹æœ€ä½³è®¾å¤‡å¹¶ä¼˜åŒ–é…ç½®"""
    if torch.cuda.is_available():
        gpu_info = get_gpu_info()
        setup_cuda_optimizations()

        primary_gpu = gpu_info['devices'][0]
        print(f"æ£€æµ‹åˆ° NVIDIA GPU: {primary_gpu['name']}")
        print(f"æ˜¾å­˜æ€»é‡: {primary_gpu['memory_total']:.1f} GB")
        print(f"è®¡ç®—èƒ½åŠ›: {primary_gpu['compute_capability']}")
        print(f"å¤šå¤„ç†å™¨æ•°é‡: {primary_gpu['multi_processor_count']}")

        return "cuda", gpu_info
    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():
        print("æ£€æµ‹åˆ° Apple Silicon GPU (MPS)")
        return "mps", None
    else:
        print("ä½¿ç”¨ CPU")
        return "cpu", None


class BaseConfig:
    """åŸºç¡€é…ç½®ç±» - NVIDIA GPU ä¼˜åŒ–"""
    def __init__(self):
        # åŸºç¡€è·¯å¾„
        self.project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        self.data_dir = os.path.join(self.project_root, "data")
        self.checkpoint_dir = os.path.join(self.project_root, "checkpoints")
        self.log_dir = os.path.join(self.project_root, "logs")

        # TensorBoard é…ç½®
        # æ£€æµ‹æ˜¯å¦åœ¨äº‘GPUç¯å¢ƒï¼ˆOpenBayesï¼‰
        cloud_tb_dir = "/openbayes/home/tf_dir"
        if os.path.exists("/openbayes/home") and os.access("/openbayes/home", os.W_OK):
            # äº‘GPUç¯å¢ƒï¼šä½¿ç”¨å›ºå®šè·¯å¾„ä»¥ä¾¿å¹³å°è‡ªåŠ¨æ£€æµ‹
            self.tensorboard_dir = cloud_tb_dir
            os.makedirs(cloud_tb_dir, exist_ok=True)
            print(f"ğŸŒ æ£€æµ‹åˆ°äº‘GPUç¯å¢ƒï¼ŒTensorBoardæ—¥å¿—: {cloud_tb_dir}")
        else:
            # æœ¬åœ°ç¯å¢ƒï¼šä½¿ç”¨é¡¹ç›®å†…çš„runsç›®å½•
            self.tensorboard_dir = os.path.join(self.project_root, "runs")

        self.enable_tensorboard = True  # é»˜è®¤å¯ç”¨TensorBoard
        self.tensorboard_flush_secs = 30  # æ¯30ç§’åˆ·æ–°ä¸€æ¬¡

        # è®¾å¤‡é…ç½®
        self.device, self.gpu_info = get_device()

        # æ•°æ®é›†è·¯å¾„
        self.pretrain_data_path = os.path.join(self.data_dir, "pretrain_hq.jsonl")
        self.sft_data_path = os.path.join(self.data_dir, "sft_mini_512.jsonl")
        self.dpo_data_path = os.path.join(self.data_dir, "dpo.jsonl")

        # èº«ä»½è®¤åŒå’Œèƒ½åŠ›å¢å¼ºæ•°æ®é›†è·¯å¾„
        self.alex_identity_data_path = os.path.join(self.data_dir, "alex_identity.jsonl")
        self.ultra_think_data_path = os.path.join(self.data_dir, "ultra_think.jsonl")
        self.lora_identity_data_path = os.path.join(self.data_dir, "lora_identity.jsonl")

        # NVIDIA GPU ä¼˜åŒ–è®¾ç½®
        self.mixed_precision = self.device == "cuda"
        self.compile_model = self.device == "cuda"  # PyTorch 2.4 ç¼–è¯‘ä¼˜åŒ–
        self.gradient_checkpointing = True
        self.flash_attention = self.device == "cuda"

        # æ•°æ®åŠ è½½ä¼˜åŒ– - é’ˆå¯¹A6000å’Œ16æ ¸CPUä¼˜åŒ–
        if self.device == "cuda":
            self.num_workers = 8  # å¢åŠ åˆ°8ä¸ªworker (16æ ¸CPUçš„ä¸€åŠ)
            self.prefetch_factor = 4  # æ¯ä¸ªworkeré¢„å–4ä¸ªbatch
        else:
            self.num_workers = 2
            self.prefetch_factor = 2
        self.pin_memory = self.device == "cuda"
        self.persistent_workers = True if self.num_workers > 0 else False

        # åˆ›å»ºå¿…è¦ç›®å½•
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        os.makedirs(self.log_dir, exist_ok=True)
        os.makedirs(self.tensorboard_dir, exist_ok=True)


class MediumConfig(BaseConfig):
    """ä¸­ç­‰æ¨¡å‹é…ç½® (~80Må‚æ•°) - GPUä¼˜åŒ– + Flash Attention"""
    def __init__(self):
        super().__init__()
        
        # æ¨¡å‹æ ‡è¯†
        self.model_size = "medium"

        # æ¨¡å‹å‚æ•° (ä¸src/model/config.pyä¿æŒä¸€è‡´)
        self.vocab_size = 20000  # æ›´æ–°ä¸ºä¸æ¨¡å‹é…ç½®ä¸€è‡´
        self.d_model = 512       # å‡å°ç»´åº¦ä»¥é™ä½å†…å­˜
        self.n_heads = 16
        self.n_layers = 16
        self.d_ff = 1536         # å‡å°FFNä»¥é™ä½å†…å­˜
        self.max_seq_len = 2048
        self.dropout = 0.1

        # GPUä¼˜åŒ–è®­ç»ƒå‚æ•° - é’ˆå¯¹A6000ä¼˜åŒ– (ä¿å®ˆé…ç½®ä»¥é¿å…OOM)
        if self.device == "cuda":
            gpu_memory = self.gpu_info['devices'][0]['memory_total'] if self.gpu_info else 8
            if gpu_memory >= 40:  # A6000ç­‰é«˜ç«¯å¡ (48GB)
                # ä¿å®ˆé…ç½®ï¼šé™ä½batch_sizeï¼Œå¢åŠ æ¢¯åº¦ç´¯ç§¯
                # è¿™æ ·å¯ä»¥å‡å°‘å•æ¬¡å‰å‘ä¼ æ’­çš„å†…å­˜å³°å€¼
                self.batch_size = 16  # é™ä½åˆ°16 (ä¹‹å‰32)
                self.gradient_accumulation_steps = 8  # å¢åŠ åˆ°8ï¼Œæœ‰æ•ˆæ‰¹é‡ = 16 * 8 = 128
            elif gpu_memory >= 24:  # RTX 3090/4090
                self.batch_size = 8
                self.gradient_accumulation_steps = 16  # æœ‰æ•ˆæ‰¹é‡ = 128
            elif gpu_memory >= 12:  # RTX 3060Ti/4060Ti
                self.batch_size = 4
                self.gradient_accumulation_steps = 32
            else:
                self.batch_size = 2
                self.gradient_accumulation_steps = 64
        else:
            self.batch_size = 2
            self.gradient_accumulation_steps = 64

        self.learning_rate = 3e-4
        self.weight_decay = 0.01
        self.warmup_steps = 4000
        self.max_steps = 100000
        self.eval_steps = 2000
        self.save_steps = 5000

        # ä¼˜åŒ–å™¨
        self.optimizer = "adamw"
        self.beta1 = 0.9
        self.beta2 = 0.95
        self.eps = 1e-8

        # ç”Ÿæˆå‚æ•°
        self.max_generate_length = 1024
        self.temperature = 0.8
        self.top_k = 50
        self.top_p = 0.9


class LargeConfig(BaseConfig):
    """å¤§æ¨¡å‹é…ç½® (~500Må‚æ•°) - é«˜ç«¯ GPU ä¼˜åŒ–"""
    def __init__(self):
        super().__init__()
        
        # æ¨¡å‹æ ‡è¯†
        self.model_size = "large"

        # æ¨¡å‹å‚æ•°
        self.vocab_size = 50000
        self.d_model = 1280
        self.n_heads = 20
        self.n_layers = 24
        self.d_ff = 5120
        self.max_seq_len = 2048
        self.dropout = 0.1

        # åªåœ¨é«˜ç«¯ GPU ä¸Šè¿è¡Œ
        if self.device == "cuda":
            gpu_memory = self.gpu_info['devices'][0]['memory_total'] if self.gpu_info else 8
            if gpu_memory >= 24:  # RTX 3090/4090
                self.batch_size = 8
            elif gpu_memory >= 16:  # RTX 4080
                self.batch_size = 4
            else:
                self.batch_size = 2
                print("è­¦å‘Š: æ˜¾å­˜ä¸è¶³ï¼Œå»ºè®®ä½¿ç”¨æ›´å°çš„æ¨¡å‹é…ç½®")
        else:
            self.batch_size = 1
            print("è­¦å‘Š: å¤§æ¨¡å‹å»ºè®®ä½¿ç”¨ CUDA GPU")

        self.learning_rate = 2e-4
        self.weight_decay = 0.01
        self.warmup_steps = 8000
        self.max_steps = 200000
        self.eval_steps = 5000
        self.save_steps = 10000

        # æ¢¯åº¦ç´¯ç§¯
        self.gradient_accumulation_steps = max(1, 128 // self.batch_size)

        # å¼ºåˆ¶å¯ç”¨å†…å­˜ä¼˜åŒ–
        self.gradient_checkpointing = True

        # ä¼˜åŒ–å™¨
        self.optimizer = "adamw"
        self.beta1 = 0.9
        self.beta2 = 0.95
        self.eps = 1e-8

        # ç”Ÿæˆå‚æ•°
        self.max_generate_length = 1024
        self.temperature = 0.8
        self.top_k = 50
        self.top_p = 0.9


def get_config(model_size="medium"):
    """è·å–æŒ‡å®šå¤§å°çš„é…ç½®"""
    configs = {
        "medium": MediumConfig,
        "large": LargeConfig
    }

    if model_size not in configs:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹å¤§å°: {model_size}. æ”¯æŒçš„å¤§å°: {list(configs.keys())}")

    config = configs[model_size]()

    # æ‰“å°é…ç½®ä¿¡æ¯
    print(f"\n=== {model_size.upper()} æ¨¡å‹é…ç½® ===")
    print(f"è®¾å¤‡: {config.device}")
    if config.gpu_info:
        gpu = config.gpu_info['devices'][0]
        print(f"GPU: {gpu['name']} ({gpu['memory_total']:.1f} GB)")
    print(f"æ‰¹é‡å¤§å°: {config.batch_size}")
    print(f"æ¢¯åº¦ç´¯ç§¯: {config.gradient_accumulation_steps}")
    print(f"æœ‰æ•ˆæ‰¹é‡: {config.batch_size * config.gradient_accumulation_steps}")
    print(f"æ··åˆç²¾åº¦: {config.mixed_precision}")
    print(f"æ¨¡å‹ç¼–è¯‘: {config.compile_model}")
    print(f"Flash Attention: {config.flash_attention}")
    print(f"æ¢¯åº¦æ£€æŸ¥ç‚¹: {config.gradient_checkpointing}")

    return config


def get_medium_config():
    return get_config("medium")


def get_large_config():
    return get_config("large")


if __name__ == "__main__":
    # æµ‹è¯•æ‰€æœ‰é…ç½®
    configs = ["medium", "large"]

    for config_name in configs:
        print(f"\n{'='*50}")
        config = get_config(config_name)
        print(f"é…ç½®æµ‹è¯•å®Œæˆ: {config_name}")