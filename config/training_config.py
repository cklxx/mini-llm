"""
è®­ç»ƒé…ç½®æ–‡ä»¶ - NVIDIA GPU ä¼˜åŒ–ç‰ˆæœ¬
æ”¯æŒ PyTorch 2.4 å’Œ NVIDIA GPU è‡ªåŠ¨æ£€æµ‹ä¼˜åŒ–
"""
import os

import torch


def get_gpu_info():
    """è·å– GPU è¯¦ç»†ä¿¡æ¯"""
    if not torch.cuda.is_available():
        return None

    gpu_info = {
        'count': torch.cuda.device_count(),
        'current_device': torch.cuda.current_device(),
        'devices': []
    }

    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        device_info = {
            'id': i,
            'name': props.name,
            'memory_total': props.total_memory / 1024**3,  # GB
            'memory_allocated': torch.cuda.memory_allocated(i) / 1024**3,  # GB
            'memory_reserved': torch.cuda.memory_reserved(i) / 1024**3,  # GB
            'compute_capability': f"{props.major}.{props.minor}",
            'multi_processor_count': props.multi_processor_count
        }
        gpu_info['devices'].append(device_info)

    return gpu_info


def setup_cuda_optimizations():
    """è®¾ç½® CUDA ä¼˜åŒ–"""
    if torch.cuda.is_available():
        # å¯ç”¨ TensorFloat-32 (é€‚ç”¨äº Ampere æ¶æ„)
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True

        # å¯ç”¨ CuDNN benchmark
        torch.backends.cudnn.benchmark = True

        # è®¾ç½® CUDA å†…å­˜åˆ†é…ç­–ç•¥
        torch.cuda.empty_cache()

        print("å·²å¯ç”¨ CUDA ä¼˜åŒ–:")
        print(f"  - TensorFloat-32: {torch.backends.cuda.matmul.allow_tf32}")
        print(f"  - CuDNN Benchmark: {torch.backends.cudnn.benchmark}")
        return True
    return False


def get_device():
    """è‡ªåŠ¨æ£€æµ‹æœ€ä½³è®¾å¤‡å¹¶ä¼˜åŒ–é…ç½®"""
    if torch.cuda.is_available():
        gpu_info = get_gpu_info()
        setup_cuda_optimizations()

        primary_gpu = gpu_info['devices'][0]
        print(f"æ£€æµ‹åˆ° NVIDIA GPU: {primary_gpu['name']}")
        print(f"æ˜¾å­˜æ€»é‡: {primary_gpu['memory_total']:.1f} GB")
        print(f"è®¡ç®—èƒ½åŠ›: {primary_gpu['compute_capability']}")
        print(f"å¤šå¤„ç†å™¨æ•°é‡: {primary_gpu['multi_processor_count']}")

        return "cuda", gpu_info
    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():
        print("æ£€æµ‹åˆ° Apple Silicon GPU (MPS)")
        return "mps", None
    else:
        print("ä½¿ç”¨ CPU")
        return "cpu", None


class BaseConfig:
    """åŸºç¡€é…ç½®ç±» - NVIDIA GPU ä¼˜åŒ–"""
    def __init__(self):
        # åŸºç¡€è·¯å¾„
        self.project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        self.manifest_dir = os.path.join(self.project_root, "configs", "data")
        self.data_dir = os.path.join(self.project_root, "data")
        self.data_search_dirs = [
            self.data_dir,
            os.path.join(self.data_dir, "final")
        ]
        self.checkpoint_dir = os.path.join(self.project_root, "checkpoints")
        self.log_dir = os.path.join(self.project_root, "logs")

        # TensorBoard é…ç½®
        # æ£€æµ‹æ˜¯å¦åœ¨äº‘GPUç¯å¢ƒï¼ˆOpenBayesï¼‰
        cloud_tb_dir = "/openbayes/home/tf_dir"
        if os.path.exists("/openbayes/home") and os.access("/openbayes/home", os.W_OK):
            # äº‘GPUç¯å¢ƒï¼šä½¿ç”¨å›ºå®šè·¯å¾„ä»¥ä¾¿å¹³å°è‡ªåŠ¨æ£€æµ‹
            self.tensorboard_dir = cloud_tb_dir
            os.makedirs(cloud_tb_dir, exist_ok=True)
            print(f"ğŸŒ æ£€æµ‹åˆ°äº‘GPUç¯å¢ƒï¼ŒTensorBoardæ—¥å¿—: {cloud_tb_dir}")
        else:
            # æœ¬åœ°ç¯å¢ƒï¼šä½¿ç”¨é¡¹ç›®å†…çš„runsç›®å½•
            self.tensorboard_dir = os.path.join(self.project_root, "runs")

        self.enable_tensorboard = True  # é»˜è®¤å¯ç”¨TensorBoard
        self.tensorboard_flush_secs = 30  # æ¯30ç§’åˆ·æ–°ä¸€æ¬¡

        # è®¾å¤‡é…ç½®
        self.device, self.gpu_info = get_device()

        # é»˜è®¤å¯ç”¨å¯æ‰©å±•æ˜¾å­˜åˆ†æ®µï¼Œç¼“è§£CUDAå†…å­˜ç¢ç‰‡é—®é¢˜
        if self.device == "cuda":
            alloc_conf = os.environ.get("PYTORCH_CUDA_ALLOC_CONF")
            if alloc_conf is None:
                os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
                print("ğŸ§  å·²è®¾ç½® PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True ä»¥å‡å°‘æ˜¾å­˜ç¢ç‰‡")

        # æ•°æ®é›†è·¯å¾„
        self.pretrain_data_path = os.path.join(self.data_dir, "pretrain_hq.jsonl")
        self.sft_data_path = os.path.join(self.data_dir, "sft_mini_512.jsonl")
        self.dpo_data_path = os.path.join(self.data_dir, "dpo.jsonl")

        # èº«ä»½è®¤åŒå’Œèƒ½åŠ›å¢å¼ºæ•°æ®é›†è·¯å¾„
        self.alex_identity_data_path = os.path.join(self.data_dir, "alex_identity.jsonl")
        self.ultra_think_data_path = os.path.join(self.data_dir, "ultra_think.jsonl")
        self.lora_identity_data_path = os.path.join(self.data_dir, "lora_identity.jsonl")

        # è®­ç»ƒå¯é‡å¤æ€§ä¸éªŒè¯é›†è®¾ç½®
        self.random_seed = int(os.environ.get("MINIGPT_TRAIN_SEED", 42))
        self.validation_split = float(os.environ.get("MINIGPT_VAL_SPLIT", 0.05))
        self.validation_min_samples = 80
        self.early_stopping_patience = int(os.environ.get("MINIGPT_EARLY_STOP_PATIENCE", 4))
        self.early_stopping_delta = float(os.environ.get("MINIGPT_EARLY_STOP_DELTA", 0.0))
        self.label_smoothing = float(os.environ.get("MINIGPT_LABEL_SMOOTHING", 0.05))
        self.dpo_beta = float(os.environ.get("MINIGPT_DPO_BETA", 0.1))

        # å…¨å±€æ•°æ®é‡‡æ ·æ§åˆ¶
        global_ratio_env = os.environ.get("MINIGPT_GLOBAL_SAMPLE_RATIO")
        try:
            self.dataset_global_sample_ratio = (
                max(0.0, float(global_ratio_env)) if global_ratio_env is not None else 0.5
            )
        except ValueError:
            self.dataset_global_sample_ratio = 0.5

        # æ•°æ®é‡‡æ ·ä¸éªŒè¯åˆ’åˆ†ç­–ç•¥ï¼ˆæŒ‰æ–‡ä»¶ååŒ¹é…ï¼‰
        self.dataset_sampling = {
            "default": {
                "sample_ratio": 1.0,
                "max_samples": None,
                "val_split": self.validation_split
            },
            "sft_mini_512.cleaned.jsonl": {
                "sample_ratio": float(os.environ.get("MINIGPT_SFT_MAIN_RATIO", 0.2)),
                "max_samples": int(os.environ.get("MINIGPT_SFT_MAIN_MAX", 150000)),
                "val_split": self.validation_split
            },
            "sft_mini_512.jsonl": {
                "sample_ratio": float(os.environ.get("MINIGPT_SFT_MAIN_RATIO", 0.2)),
                "max_samples": int(os.environ.get("MINIGPT_SFT_MAIN_MAX", 150000)),
                "val_split": self.validation_split
            },
            "alex_identity.jsonl": {
                "sample_ratio": 0.25,
                "max_samples": 3000,
                "val_split": 0.1
            },
            "minigpt_identity.jsonl": {
                "sample_ratio": 0.25,
                "max_samples": 3000,
                "val_split": 0.1
            },
            "ultra_think.jsonl": {
                "sample_ratio": 0.5,
                "max_samples": 6000,
                "val_split": 0.05
            },
            "wiki_zh_full.simdedup.jsonl": {
                "sample_ratio": float(os.environ.get("MINIGPT_PRETRAIN_WIKI_RATIO", 0.01)),
                "max_samples": int(os.environ.get("MINIGPT_PRETRAIN_WIKI_MAX", 30000)),
                "val_split": self.validation_split
            },
            "chinacorpus_full.simdedup.jsonl": {
                "sample_ratio": float(os.environ.get("MINIGPT_PRETRAIN_CHINA_RATIO", 0.006)),
                "max_samples": int(os.environ.get("MINIGPT_PRETRAIN_CHINA_MAX", 50000)),
                "val_split": self.validation_split
            },
            "pretrain_hq.cleaned.jsonl": {
                "sample_ratio": float(os.environ.get("MINIGPT_PRETRAIN_HQ_RATIO", 0.03)),
                "max_samples": int(os.environ.get("MINIGPT_PRETRAIN_HQ_MAX", 40000)),
                "val_split": self.validation_split
            },
        }

        # å¯¹è¯è§’è‰²æ ‡è®°
        self.role_tokens = {
            "system": "<|system|>",
            "user": "<|user|>",
            "assistant": "<|assistant|>",
            "turn_separator": "<|endofturn|>"
        }

        # å¯¹è¯æ•°æ®å¢å¼ºé…ç½®
        self.conversation_augmentation = {
            "turn_truncate_prob": float(os.environ.get("MINIGPT_TURN_TRUNCATE_PROB", 0.1)),
            "max_turn_truncate": int(os.environ.get("MINIGPT_MAX_TURN_TRUNCATE", 1))
        }

        # æ•°æ®åŠ è½½æ€§èƒ½ä¼˜åŒ–
        self.use_high_performance_data_loading = (
            os.environ.get("MINIGPT_FAST_DATA_LOADING", "1") == "1"
        )
        cache_root = os.environ.get(
            "MINIGPT_DATA_CACHE_DIR",
            os.path.join(self.project_root, "cache", "data_loader"),
        )
        self.data_cache_dir = cache_root
        os.makedirs(self.data_cache_dir, exist_ok=True)
        self.data_cache_enabled = os.environ.get("MINIGPT_DATA_CACHE", "1") == "1"
        self.data_cache_force_rebuild = (
            os.environ.get("MINIGPT_DATA_CACHE_REBUILD", "0") == "1"
        )
        self.data_streaming_enabled = (
            os.environ.get("MINIGPT_DATA_STREAMING", "0") == "1"
        )
        default_chunk = 20000
        default_buffer = 100000
        self.data_chunk_size = int(
            os.environ.get("MINIGPT_DATA_CHUNK_SIZE", default_chunk)
        )
        self.data_buffer_size = int(
            os.environ.get("MINIGPT_DATA_BUFFER_SIZE", default_buffer)
        )
        cpu_count = os.cpu_count() or 4
        default_parallel = max(4, min(32, cpu_count))
        self.data_max_parallel_workers = int(
            os.environ.get("MINIGPT_DATA_MAX_WORKERS", default_parallel)
        )

        # è®­ç»ƒå†…å­˜ç›‘æ§ä¸ä¼˜åŒ–
        self.memory_monitor_enabled = os.environ.get("MINIGPT_MEMORY_MONITOR", "1") == "1"
        self.memory_pressure_threshold = float(os.environ.get("MINIGPT_MEMORY_THRESHOLD", 0.92))
        self.memory_cleanup_interval = int(os.environ.get("MINIGPT_MEMORY_CLEANUP_INTERVAL", 200))
        self.memory_log_interval = int(os.environ.get("MINIGPT_MEMORY_LOG_INTERVAL", 200))

        # æ•°æ®é¢„å¤„ç†é€‰é¡¹
        self.pretokenize_lm = os.environ.get("MINIGPT_PRETOKENIZE_LM", "1") == "1"
        default_workers = min(16, os.cpu_count() or 1)
        self.pretokenize_workers = int(
            os.environ.get("MINIGPT_PRETOKENIZE_WORKERS", default_workers)
        )
        if self.pretokenize_workers < 1:
            self.pretokenize_workers = 1

        # è®­ç»ƒåå›å½’è¯„ä¼°é…ç½®
        self.regression_eval_enabled = os.environ.get("MINIGPT_REGRESSION_EVAL", "1") == "1"
        self.regression_eval_prompts = os.path.join(self.data_dir, "eval", "regression_prompts.jsonl")
        self.regression_eval_interval = int(os.environ.get("MINIGPT_REGRESSION_INTERVAL", 500))
        self.regression_eval_max_new_tokens = int(os.environ.get("MINIGPT_REGRESSION_MAX_NEW", 96))
        self.regression_eval_temperature = float(os.environ.get("MINIGPT_REGRESSION_TEMPERATURE", 0.7))
        self.regression_eval_top_p = float(os.environ.get("MINIGPT_REGRESSION_TOP_P", 0.95))

        # NVIDIA GPU ä¼˜åŒ–è®¾ç½®
        self.mixed_precision = self.device == "cuda"
        self.compile_model = self.device == "cuda"  # PyTorch 2.4 ç¼–è¯‘ä¼˜åŒ–
        self.gradient_checkpointing = True
        self.flash_attention = self.device == "cuda"

        # æ•°æ®åŠ è½½ä¼˜åŒ– - é’ˆå¯¹RTX 4090å’Œå¤šæ ¸CPUä¼˜åŒ–
        if self.device == "cuda":
            # å¤§å¹…æå‡workersæ•°é‡ä»¥å……åˆ†åˆ©ç”¨CPUå¹¶è¡ŒåŠ è½½æ•°æ®
            self.num_workers = 16  # ä½¿ç”¨æ›´å¤šworkersåŠ é€Ÿæ•°æ®åŠ è½½
            self.prefetch_factor = 8  # æ¯ä¸ªworkeré¢„å–æ›´å¤šbatchï¼Œé¿å…GPUç­‰å¾…
        else:
            self.num_workers = 4
            self.prefetch_factor = 4
        self.pin_memory = self.device == "cuda"
        self.persistent_workers = True if self.num_workers > 0 else False

        # åˆ›å»ºå¿…è¦ç›®å½•
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        os.makedirs(self.log_dir, exist_ok=True)
        os.makedirs(self.tensorboard_dir, exist_ok=True)


class TinyConfig(BaseConfig):
    """è¶…å°å‹æ¨¡å‹é…ç½® (~1Må‚æ•°) - å¿«é€Ÿå®éªŒ"""
    def __init__(self):
        super().__init__()

        # æ¨¡å‹æ ‡è¯†
        self.model_size = "tiny"

        # æ¨¡å‹å‚æ•°
        self.vocab_size = 10000
        self.d_model = 128
        self.n_heads = 4
        self.n_layers = 8
        self.d_ff = 384
        self.max_seq_len = 512
        self.dropout = 0.1

        # è®­ç»ƒå‚æ•°
        self.batch_size = 32
        self.gradient_accumulation_steps = 2
        self.learning_rate = 3e-4
        self.weight_decay = 0.01
        self.warmup_steps = 500
        self.max_steps = 10000
        self.eval_steps = 500
        self.save_steps = 1000

        # ä¼˜åŒ–å™¨
        self.optimizer = "adamw"
        self.beta1 = 0.9
        self.beta2 = 0.95
        self.eps = 1e-8

        # ç”Ÿæˆå‚æ•°
        self.max_generate_length = 512
        self.temperature = 0.8
        self.top_k = 50
        self.top_p = 0.9


class SmallConfig(BaseConfig):
    """å°å‹æ¨¡å‹é…ç½® (~25Må‚æ•°) - ç˜¦é•¿æ¶æ„ä¼˜åŒ–å†…å­˜"""
    def __init__(self):
        super().__init__()

        # æ¨¡å‹æ ‡è¯†
        self.model_size = "small"

        # æ¨¡å‹å‚æ•° - ç˜¦é•¿æ¶æ„ï¼šæ›´çª„ä½†æ›´æ·±ï¼Œé™ä½å†…å­˜å³°å€¼
        self.vocab_size = 10000
        self.d_model = 512
        self.n_heads = 8
        self.n_layers = 8
        self.d_ff = 2048
        self.max_seq_len = 512
        self.dropout = 0.0

        # è®­ç»ƒå‚æ•° - ä¼˜åŒ–å†…å­˜ä½¿ç”¨å’ŒGPUåˆ©ç”¨ç‡
        if self.device == "cuda":
            gpu_memory = self.gpu_info['devices'][0]['memory_total'] if self.gpu_info else 8
            gpu_name = self.gpu_info['devices'][0]['name'].lower() if self.gpu_info else ""

            if gpu_memory >= 22 or "4090" in gpu_name or "ada" in gpu_name:
                # RTX 4090/A6000: é™ä½å•æ¬¡batchæ˜¾å­˜å³°å€¼
                self.batch_size = 32
                self.gradient_accumulation_steps = 6  # æœ‰æ•ˆbatch = 32*6 = 192
            elif gpu_memory >= 16:
                self.batch_size = 48
                self.gradient_accumulation_steps = 5
            elif gpu_memory >= 12:
                self.batch_size = 32
                self.gradient_accumulation_steps = 8
            else:
                self.batch_size = 32
                self.gradient_accumulation_steps = 8
        else:
            self.batch_size = 16
            self.gradient_accumulation_steps = 16

        self.learning_rate = 5e-4
        self.weight_decay = 0.01
        self.warmup_steps = 500
        self.max_steps = 2500
        self.eval_steps = 250
        self.save_steps = 500

        # ä¼˜åŒ–å™¨
        self.optimizer = "adamw"
        self.beta1 = 0.9
        self.beta2 = 0.95
        self.eps = 1e-8

        # ç”Ÿæˆå‚æ•°
        self.max_generate_length = 1024
        self.temperature = 0.8
        self.top_k = 50
        self.top_p = 0.9


class Small30MConfig(BaseConfig):
    """30Må‚æ•°å°å‹æ¨¡å‹é…ç½®"""
    def __init__(self):
        super().__init__()

        # æ¨¡å‹æ ‡è¯†
        self.model_size = "small_30m"

        # æ¨¡å‹å‚æ•°
        self.vocab_size = 12000
        self.d_model = 384
        self.n_heads = 12
        self.n_layers = 13
        self.d_ff = 1408
        self.max_seq_len = 2048
        self.dropout = 0.1

        # è®­ç»ƒå‚æ•°
        if self.device == "cuda":
            gpu_memory = self.gpu_info['devices'][0]['memory_total'] if self.gpu_info else 8
            if gpu_memory >= 24:
                self.batch_size = 24
                self.gradient_accumulation_steps = 6
            elif gpu_memory >= 12:
                self.batch_size = 12
                self.gradient_accumulation_steps = 12
            else:
                self.batch_size = 6
                self.gradient_accumulation_steps = 24
        else:
            self.batch_size = 6
            self.gradient_accumulation_steps = 24

        self.learning_rate = 3e-4
        self.weight_decay = 0.01
        self.warmup_steps = 1500
        self.max_steps = 60000
        self.eval_steps = 1500
        self.save_steps = 3000

        # ä¼˜åŒ–å™¨
        self.optimizer = "adamw"
        self.beta1 = 0.9
        self.beta2 = 0.95
        self.eps = 1e-8

        # ç”Ÿæˆå‚æ•°
        self.max_generate_length = 1024
        self.temperature = 0.8
        self.top_k = 50
        self.top_p = 0.9


class MediumConfig(BaseConfig):
    """ä¸­ç­‰æ¨¡å‹é…ç½® (~80Må‚æ•°) - GPUä¼˜åŒ– + Flash Attention"""
    def __init__(self):
        super().__init__()

        # æ¨¡å‹æ ‡è¯†
        self.model_size = "medium"

        # æ¨¡å‹å‚æ•° (ä¸ src/model/config.py ä¸­çš„ medium é¢„è®¾ä¿æŒä¸€è‡´)
        self.vocab_size = 20000
        self.d_model = 384       # ç˜¦é•¿æ¶æ„ï¼šé™ä½å®½åº¦
        self.n_heads = 12
        self.n_layers = 20       # æå‡æ·±åº¦ä»¥è¡¥è¶³è¡¨è¾¾èƒ½åŠ›
        self.d_ff = 1536         # 4 Ã— hidden çš„ FFN è®¾è®¡
        self.max_seq_len = 2048
        self.dropout = 0.1

        # GPUä¼˜åŒ–è®­ç»ƒå‚æ•° - é’ˆå¯¹A6000ä¼˜åŒ– (ä¿å®ˆé…ç½®ä»¥é¿å…OOM)
        if self.device == "cuda":
            gpu_memory = self.gpu_info['devices'][0]['memory_total'] if self.gpu_info else 8
            if gpu_memory >= 40:  # A6000ç­‰é«˜ç«¯å¡ (48GB)
                # ä¿å®ˆé…ç½®ï¼šé™ä½batch_sizeï¼Œå¢åŠ æ¢¯åº¦ç´¯ç§¯
                # è¿™æ ·å¯ä»¥å‡å°‘å•æ¬¡å‰å‘ä¼ æ’­çš„å†…å­˜å³°å€¼
                self.batch_size = 16  # é™ä½åˆ°16 (ä¹‹å‰32)
                self.gradient_accumulation_steps = 8  # å¢åŠ åˆ°8ï¼Œæœ‰æ•ˆæ‰¹é‡ = 16 * 8 = 128
            elif gpu_memory >= 24:  # RTX 3090/4090
                self.batch_size = 8
                self.gradient_accumulation_steps = 16  # æœ‰æ•ˆæ‰¹é‡ = 128
            elif gpu_memory >= 12:  # RTX 3060Ti/4060Ti
                self.batch_size = 4
                self.gradient_accumulation_steps = 32
            else:
                self.batch_size = 2
                self.gradient_accumulation_steps = 64
        else:
            self.batch_size = 2
            self.gradient_accumulation_steps = 64

        # é‡‡ç”¨ä¸ MiniMind ç›¸è¿‘çš„è¾ƒé«˜å­¦ä¹ ç‡ï¼ŒåŠ é€Ÿç˜¦é•¿æ¨¡å‹æ”¶æ•›
        self.learning_rate = 5e-4
        self.weight_decay = 0.01
        # ä½¿ç”¨ 3% warmup æ¯”ä¾‹ï¼Œå¹³è¡¡è®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦
        self.warmup_steps = 3000
        self.max_steps = 100000
        self.eval_steps = 2000
        self.save_steps = 5000

        # ä¼˜åŒ–å™¨
        self.optimizer = "adamw"
        self.beta1 = 0.9
        self.beta2 = 0.95
        self.eps = 1e-8

        # ç”Ÿæˆå‚æ•°
        self.max_generate_length = 1024
        self.temperature = 0.8
        self.top_k = 50
        self.top_p = 0.9


class FoundationConfig(BaseConfig):
    """åŸºç¡€æ¨¡å‹é…ç½® (~200Må‚æ•°) - ä¸­å‹è§„æ¨¡è®­ç»ƒ"""
    def __init__(self):
        super().__init__()

        # æ¨¡å‹æ ‡è¯†
        self.model_size = "foundation"

        # æ¨¡å‹å‚æ•°
        self.vocab_size = 32000
        self.d_model = 768
        self.n_heads = 16
        self.n_layers = 24
        self.d_ff = 2688
        self.max_seq_len = 4096
        self.dropout = 0.1

        # è®­ç»ƒå‚æ•° - GPUä¼˜åŒ–
        if self.device == "cuda":
            gpu_memory = self.gpu_info['devices'][0]['memory_total'] if self.gpu_info else 8
            if gpu_memory >= 40:  # A6000ç­‰é«˜ç«¯å¡
                self.batch_size = 12
                self.gradient_accumulation_steps = 12
            elif gpu_memory >= 24:  # RTX 3090/4090
                self.batch_size = 6
                self.gradient_accumulation_steps = 24
            elif gpu_memory >= 12:
                self.batch_size = 3
                self.gradient_accumulation_steps = 48
            else:
                self.batch_size = 2
                self.gradient_accumulation_steps = 64
        else:
            self.batch_size = 2
            self.gradient_accumulation_steps = 64

        self.learning_rate = 2e-4
        self.weight_decay = 0.01
        self.warmup_steps = 3000
        self.max_steps = 150000
        self.eval_steps = 3000
        self.save_steps = 8000

        # å¼ºåˆ¶å¯ç”¨å†…å­˜ä¼˜åŒ–
        self.gradient_checkpointing = True

        # ä¼˜åŒ–å™¨
        self.optimizer = "adamw"
        self.beta1 = 0.9
        self.beta2 = 0.95
        self.eps = 1e-8

        # ç”Ÿæˆå‚æ•°
        self.max_generate_length = 2048
        self.temperature = 0.8
        self.top_k = 50
        self.top_p = 0.9


class LargeConfig(BaseConfig):
    """å¤§æ¨¡å‹é…ç½® (~350Må‚æ•°) - é«˜ç«¯ GPU ä¼˜åŒ–"""
    def __init__(self):
        super().__init__()

        # æ¨¡å‹æ ‡è¯†
        self.model_size = "large"

        # æ¨¡å‹å‚æ•° (æ›´æ–°ä¸ºä¸src/model/config.pyä¸€è‡´)
        self.vocab_size = 32000
        self.d_model = 768
        self.n_heads = 24
        self.n_layers = 32
        self.d_ff = 3072
        self.max_seq_len = 4096
        self.dropout = 0.1

        # åªåœ¨é«˜ç«¯ GPU ä¸Šè¿è¡Œ
        if self.device == "cuda":
            gpu_memory = self.gpu_info['devices'][0]['memory_total'] if self.gpu_info else 8
            if gpu_memory >= 40:  # A6000ç­‰é«˜ç«¯å¡
                self.batch_size = 8
                self.gradient_accumulation_steps = 16
            elif gpu_memory >= 24:  # RTX 3090/4090
                self.batch_size = 4
                self.gradient_accumulation_steps = 32
            elif gpu_memory >= 16:  # RTX 4080
                self.batch_size = 2
                self.gradient_accumulation_steps = 64
            else:
                self.batch_size = 1
                self.gradient_accumulation_steps = 128
                print("è­¦å‘Š: æ˜¾å­˜ä¸è¶³ï¼Œå»ºè®®ä½¿ç”¨æ›´å°çš„æ¨¡å‹é…ç½®")
        else:
            self.batch_size = 1
            self.gradient_accumulation_steps = 128
            print("è­¦å‘Š: å¤§æ¨¡å‹å»ºè®®ä½¿ç”¨ CUDA GPU")

        self.learning_rate = 2e-4
        self.weight_decay = 0.01
        self.warmup_steps = 4000
        self.max_steps = 200000
        self.eval_steps = 5000
        self.save_steps = 10000

        # å¼ºåˆ¶å¯ç”¨å†…å­˜ä¼˜åŒ–
        self.gradient_checkpointing = True

        # ä¼˜åŒ–å™¨
        self.optimizer = "adamw"
        self.beta1 = 0.9
        self.beta2 = 0.95
        self.eps = 1e-8

        # ç”Ÿæˆå‚æ•°
        self.max_generate_length = 2048
        self.temperature = 0.8
        self.top_k = 50
        self.top_p = 0.9


class MOEConfig(BaseConfig):
    """MOE (Mixture of Experts) æ¨¡å‹é…ç½®"""
    def __init__(self):
        super().__init__()

        # æ¨¡å‹æ ‡è¯†
        self.model_size = "moe"

        # æ¨¡å‹å‚æ•°
        self.vocab_size = 10000
        self.d_model = 384
        self.n_heads = 12
        self.n_layers = 12
        self.d_ff = 1536
        self.max_seq_len = 1024
        self.dropout = 0.1

        # MOEç‰¹æœ‰å‚æ•°
        self.use_moe = True
        self.num_experts_per_tok = 2
        self.n_routed_experts = 4
        self.n_shared_experts = 1

        # è®­ç»ƒå‚æ•°
        if self.device == "cuda":
            gpu_memory = self.gpu_info['devices'][0]['memory_total'] if self.gpu_info else 8
            if gpu_memory >= 24:
                self.batch_size = 16
                self.gradient_accumulation_steps = 8
            elif gpu_memory >= 12:
                self.batch_size = 8
                self.gradient_accumulation_steps = 16
            else:
                self.batch_size = 4
                self.gradient_accumulation_steps = 32
        else:
            self.batch_size = 4
            self.gradient_accumulation_steps = 32

        self.learning_rate = 3e-4
        self.weight_decay = 0.01
        self.warmup_steps = 1000
        self.max_steps = 50000
        self.eval_steps = 1000
        self.save_steps = 2000

        # ä¼˜åŒ–å™¨
        self.optimizer = "adamw"
        self.beta1 = 0.9
        self.beta2 = 0.95
        self.eps = 1e-8

        # ç”Ÿæˆå‚æ•°
        self.max_generate_length = 1024
        self.temperature = 0.8
        self.top_k = 50
        self.top_p = 0.9


def get_config(model_size="medium"):
    """è·å–æŒ‡å®šå¤§å°çš„é…ç½®"""
    configs = {
        "tiny": TinyConfig,
        "small": SmallConfig,
        "small_30m": Small30MConfig,
        "medium": MediumConfig,
        "foundation": FoundationConfig,
        "large": LargeConfig,
        "moe": MOEConfig
    }

    if model_size not in configs:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹å¤§å°: {model_size}. æ”¯æŒçš„å¤§å°: {list(configs.keys())}")

    config = configs[model_size]()

    # æ‰“å°é…ç½®ä¿¡æ¯
    print(f"\n=== {model_size.upper()} æ¨¡å‹é…ç½® ===")
    print(f"è®¾å¤‡: {config.device}")
    if config.gpu_info:
        gpu = config.gpu_info['devices'][0]
        print(f"GPU: {gpu['name']} ({gpu['memory_total']:.1f} GB)")
    print(f"æ‰¹é‡å¤§å°: {config.batch_size}")
    print(f"æ¢¯åº¦ç´¯ç§¯: {config.gradient_accumulation_steps}")
    print(f"æœ‰æ•ˆæ‰¹é‡: {config.batch_size * config.gradient_accumulation_steps}")
    print(f"æ··åˆç²¾åº¦: {config.mixed_precision}")
    print(f"æ¨¡å‹ç¼–è¯‘: {config.compile_model}")
    print(f"Flash Attention: {config.flash_attention}")
    print(f"æ¢¯åº¦æ£€æŸ¥ç‚¹: {config.gradient_checkpointing}")

    return config


def get_medium_config():
    return get_config("medium")


def get_large_config():
    return get_config("large")


if __name__ == "__main__":
    # æµ‹è¯•æ‰€æœ‰é…ç½®
    configs = ["tiny", "small", "small_30m", "medium", "foundation", "large", "moe"]

    for config_name in configs:
        print(f"\n{'='*50}")
        config = get_config(config_name)
        print(f"é…ç½®æµ‹è¯•å®Œæˆ: {config_name}")
