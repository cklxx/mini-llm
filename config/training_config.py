"""
è®­ç»ƒé…ç½®æ–‡ä»¶ - NVIDIA GPU ä¼˜åŒ–ç‰ˆæœ¬
æ”¯æŒ PyTorch 2.4 å’Œ NVIDIA GPU è‡ªåŠ¨æ£€æµ‹ä¼˜åŒ–
"""
import os
import torch
import subprocess


def get_gpu_info():
    """è·å– GPU è¯¦ç»†ä¿¡æ¯"""
    if not torch.cuda.is_available():
        return None

    gpu_info = {
        'count': torch.cuda.device_count(),
        'current_device': torch.cuda.current_device(),
        'devices': []
    }

    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        device_info = {
            'id': i,
            'name': props.name,
            'memory_total': props.total_memory / 1024**3,  # GB
            'memory_allocated': torch.cuda.memory_allocated(i) / 1024**3,  # GB
            'memory_reserved': torch.cuda.memory_reserved(i) / 1024**3,  # GB
            'compute_capability': f"{props.major}.{props.minor}",
            'multi_processor_count': props.multi_processor_count
        }
        gpu_info['devices'].append(device_info)

    return gpu_info


def setup_cuda_optimizations():
    """è®¾ç½® CUDA ä¼˜åŒ–"""
    if torch.cuda.is_available():
        # å¯ç”¨ TensorFloat-32 (é€‚ç”¨äº Ampere æ¶æ„)
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True

        # å¯ç”¨ CuDNN benchmark
        torch.backends.cudnn.benchmark = True

        # è®¾ç½® CUDA å†…å­˜åˆ†é…ç­–ç•¥
        torch.cuda.empty_cache()

        print("å·²å¯ç”¨ CUDA ä¼˜åŒ–:")
        print(f"  - TensorFloat-32: {torch.backends.cuda.matmul.allow_tf32}")
        print(f"  - CuDNN Benchmark: {torch.backends.cudnn.benchmark}")
        return True
    return False


def get_device():
    """è‡ªåŠ¨æ£€æµ‹æœ€ä½³è®¾å¤‡å¹¶ä¼˜åŒ–é…ç½®"""
    if torch.cuda.is_available():
        gpu_info = get_gpu_info()
        setup_cuda_optimizations()

        primary_gpu = gpu_info['devices'][0]
        print(f"æ£€æµ‹åˆ° NVIDIA GPU: {primary_gpu['name']}")
        print(f"æ˜¾å­˜æ€»é‡: {primary_gpu['memory_total']:.1f} GB")
        print(f"è®¡ç®—èƒ½åŠ›: {primary_gpu['compute_capability']}")
        print(f"å¤šå¤„ç†å™¨æ•°é‡: {primary_gpu['multi_processor_count']}")

        return "cuda", gpu_info
    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():
        print("æ£€æµ‹åˆ° Apple Silicon GPU (MPS)")
        return "mps", None
    else:
        print("ä½¿ç”¨ CPU")
        return "cpu", None


class BaseConfig:
    """åŸºç¡€é…ç½®ç±» - NVIDIA GPU ä¼˜åŒ–"""
    def __init__(self):
        # åŸºç¡€è·¯å¾„
        self.project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        self.data_dir = os.path.join(self.project_root, "data")
        self.checkpoint_dir = os.path.join(self.project_root, "checkpoints")
        self.log_dir = os.path.join(self.project_root, "logs")

        # TensorBoard é…ç½®
        # æ£€æµ‹æ˜¯å¦åœ¨äº‘GPUç¯å¢ƒï¼ˆOpenBayesï¼‰
        cloud_tb_dir = "/openbayes/home/tf_dir"
        if os.path.exists("/openbayes/home") and os.access("/openbayes/home", os.W_OK):
            # äº‘GPUç¯å¢ƒï¼šä½¿ç”¨å›ºå®šè·¯å¾„ä»¥ä¾¿å¹³å°è‡ªåŠ¨æ£€æµ‹
            self.tensorboard_dir = cloud_tb_dir
            os.makedirs(cloud_tb_dir, exist_ok=True)
            print(f"ğŸŒ æ£€æµ‹åˆ°äº‘GPUç¯å¢ƒï¼ŒTensorBoardæ—¥å¿—: {cloud_tb_dir}")
        else:
            # æœ¬åœ°ç¯å¢ƒï¼šä½¿ç”¨é¡¹ç›®å†…çš„runsç›®å½•
            self.tensorboard_dir = os.path.join(self.project_root, "runs")

        self.enable_tensorboard = True  # é»˜è®¤å¯ç”¨TensorBoard
        self.tensorboard_flush_secs = 30  # æ¯30ç§’åˆ·æ–°ä¸€æ¬¡

        # è®¾å¤‡é…ç½®
        self.device, self.gpu_info = get_device()

        # æ•°æ®é›†è·¯å¾„
        self.pretrain_data_path = os.path.join(self.data_dir, "pretrain_hq.jsonl")
        self.sft_data_path = os.path.join(self.data_dir, "sft_mini_512.jsonl")
        self.dpo_data_path = os.path.join(self.data_dir, "dpo.jsonl")

        # èº«ä»½è®¤åŒå’Œèƒ½åŠ›å¢å¼ºæ•°æ®é›†è·¯å¾„
        self.alex_identity_data_path = os.path.join(self.data_dir, "alex_identity.jsonl")
        self.ultra_think_data_path = os.path.join(self.data_dir, "ultra_think.jsonl")
        self.lora_identity_data_path = os.path.join(self.data_dir, "lora_identity.jsonl")

        # è®­ç»ƒå¯é‡å¤æ€§ä¸éªŒè¯é›†è®¾ç½®
        self.random_seed = int(os.environ.get("MINIGPT_TRAIN_SEED", 42))
        self.validation_split = float(os.environ.get("MINIGPT_VAL_SPLIT", 0.05))
        self.validation_min_samples = 80
        self.early_stopping_patience = int(os.environ.get("MINIGPT_EARLY_STOP_PATIENCE", 4))
        self.early_stopping_delta = float(os.environ.get("MINIGPT_EARLY_STOP_DELTA", 0.0))
        self.label_smoothing = float(os.environ.get("MINIGPT_LABEL_SMOOTHING", 0.05))

        # æ•°æ®é‡‡æ ·ä¸éªŒè¯åˆ’åˆ†ç­–ç•¥ï¼ˆæŒ‰æ–‡ä»¶ååŒ¹é…ï¼‰
        self.dataset_sampling = {
            "default": {
                "sample_ratio": 1.0,
                "max_samples": None,
                "val_split": self.validation_split
            },
            "alex_identity.jsonl": {
                "sample_ratio": 0.25,
                "max_samples": 3000,
                "val_split": 0.1
            },
            "minigpt_identity.jsonl": {
                "sample_ratio": 0.25,
                "max_samples": 3000,
                "val_split": 0.1
            },
            "ultra_think.jsonl": {
                "sample_ratio": 0.5,
                "max_samples": 6000,
                "val_split": 0.05
            },
        }

        # å¯¹è¯è§’è‰²æ ‡è®°
        self.role_tokens = {
            "system": "<|system|>",
            "user": "<|user|>",
            "assistant": "<|assistant|>",
            "turn_separator": "<|endofturn|>"
        }

        # å¯¹è¯æ•°æ®å¢å¼ºé…ç½®
        self.conversation_augmentation = {
            "turn_truncate_prob": float(os.environ.get("MINIGPT_TURN_TRUNCATE_PROB", 0.1)),
            "max_turn_truncate": int(os.environ.get("MINIGPT_MAX_TURN_TRUNCATE", 1))
        }

        # è®­ç»ƒå†…å­˜ç›‘æ§ä¸ä¼˜åŒ–
        self.memory_monitor_enabled = os.environ.get("MINIGPT_MEMORY_MONITOR", "1") == "1"
        self.memory_pressure_threshold = float(os.environ.get("MINIGPT_MEMORY_THRESHOLD", 0.92))
        self.memory_cleanup_interval = int(os.environ.get("MINIGPT_MEMORY_CLEANUP_INTERVAL", 200))
        self.memory_log_interval = int(os.environ.get("MINIGPT_MEMORY_LOG_INTERVAL", 200))

        # è®­ç»ƒåå›å½’è¯„ä¼°é…ç½®
        self.regression_eval_enabled = os.environ.get("MINIGPT_REGRESSION_EVAL", "1") == "1"
        self.regression_eval_prompts = os.path.join(self.data_dir, "eval", "regression_prompts.jsonl")
        self.regression_eval_interval = int(os.environ.get("MINIGPT_REGRESSION_INTERVAL", 500))
        self.regression_eval_max_new_tokens = int(os.environ.get("MINIGPT_REGRESSION_MAX_NEW", 96))
        self.regression_eval_temperature = float(os.environ.get("MINIGPT_REGRESSION_TEMPERATURE", 0.7))
        self.regression_eval_top_p = float(os.environ.get("MINIGPT_REGRESSION_TOP_P", 0.95))

        # NVIDIA GPU ä¼˜åŒ–è®¾ç½®
        self.mixed_precision = self.device == "cuda"
        self.compile_model = self.device == "cuda"  # PyTorch 2.4 ç¼–è¯‘ä¼˜åŒ–
        self.gradient_checkpointing = True
        self.flash_attention = self.device == "cuda"

        # æ•°æ®åŠ è½½ä¼˜åŒ– - é’ˆå¯¹A6000å’Œ16æ ¸CPUä¼˜åŒ–
        if self.device == "cuda":
            self.num_workers = 8  # å¢åŠ åˆ°8ä¸ªworker (16æ ¸CPUçš„ä¸€åŠ)
            self.prefetch_factor = 4  # æ¯ä¸ªworkeré¢„å–4ä¸ªbatch
        else:
            self.num_workers = 2
            self.prefetch_factor = 2
        self.pin_memory = self.device == "cuda"
        self.persistent_workers = True if self.num_workers > 0 else False

        # åˆ›å»ºå¿…è¦ç›®å½•
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        os.makedirs(self.log_dir, exist_ok=True)
        os.makedirs(self.tensorboard_dir, exist_ok=True)


class TinyConfig(BaseConfig):
    """è¶…å°å‹æ¨¡å‹é…ç½® (~1Må‚æ•°) - å¿«é€Ÿå®éªŒ"""
    def __init__(self):
        super().__init__()
        
        # æ¨¡å‹æ ‡è¯†
        self.model_size = "tiny"

        # æ¨¡å‹å‚æ•°
        self.vocab_size = 10000
        self.d_model = 128
        self.n_heads = 4
        self.n_layers = 8
        self.d_ff = 384
        self.max_seq_len = 512
        self.dropout = 0.1

        # è®­ç»ƒå‚æ•°
        self.batch_size = 32
        self.gradient_accumulation_steps = 2
        self.learning_rate = 3e-4
        self.weight_decay = 0.01
        self.warmup_steps = 500
        self.max_steps = 10000
        self.eval_steps = 500
        self.save_steps = 1000

        # ä¼˜åŒ–å™¨
        self.optimizer = "adamw"
        self.beta1 = 0.9
        self.beta2 = 0.95
        self.eps = 1e-8

        # ç”Ÿæˆå‚æ•°
        self.max_generate_length = 512
        self.temperature = 0.8
        self.top_k = 50
        self.top_p = 0.9


class SmallConfig(BaseConfig):
    """å°å‹æ¨¡å‹é…ç½® (~25Må‚æ•°) - ç˜¦é•¿æ¶æ„ä¼˜åŒ–å†…å­˜"""
    def __init__(self):
        super().__init__()
        
        # æ¨¡å‹æ ‡è¯†
        self.model_size = "small"

        # æ¨¡å‹å‚æ•° - ç˜¦é•¿æ¶æ„ï¼šæ›´çª„ä½†æ›´æ·±ï¼Œé™ä½å†…å­˜å³°å€¼
        self.vocab_size = 10000
        self.d_model = 288        # 384 -> 288 (å‡å°‘25%)
        self.n_heads = 9          # 12 -> 9 (ä¿æŒ d_model/n_heads = 32)
        self.n_layers = 18        # 12 -> 18 (å¢åŠ 50%)
        self.d_ff = 1152          # 1536 -> 1152 (4å€d_model)
        self.max_seq_len = 1024
        self.dropout = 0.1

        # è®­ç»ƒå‚æ•° - ä¼˜åŒ–å†…å­˜ä½¿ç”¨
        if self.device == "cuda":
            gpu_memory = self.gpu_info['devices'][0]['memory_total'] if self.gpu_info else 8
            if gpu_memory >= 40:
                # é«˜ç«¯GPUï¼ˆA6000ç­‰ï¼‰ï¼šé™ä½batch sizeä»¥é€‚åº”é•¿åºåˆ—
                self.batch_size = 16
                self.gradient_accumulation_steps = 8
            elif gpu_memory >= 24:
                self.batch_size = 12
                self.gradient_accumulation_steps = 10
            elif gpu_memory >= 12:
                self.batch_size = 8
                self.gradient_accumulation_steps = 16
            else:
                self.batch_size = 4
                self.gradient_accumulation_steps = 32
        else:
            self.batch_size = 4
            self.gradient_accumulation_steps = 32

        self.learning_rate = 3e-4
        self.weight_decay = 0.01
        self.warmup_steps = 2000
        self.max_steps = 50000
        self.eval_steps = 1000
        self.save_steps = 2000

        # ä¼˜åŒ–å™¨
        self.optimizer = "adamw"
        self.beta1 = 0.9
        self.beta2 = 0.95
        self.eps = 1e-8

        # ç”Ÿæˆå‚æ•°
        self.max_generate_length = 1024
        self.temperature = 0.8
        self.top_k = 50
        self.top_p = 0.9


class Small30MConfig(BaseConfig):
    """30Må‚æ•°å°å‹æ¨¡å‹é…ç½®"""
    def __init__(self):
        super().__init__()
        
        # æ¨¡å‹æ ‡è¯†
        self.model_size = "small_30m"

        # æ¨¡å‹å‚æ•°
        self.vocab_size = 12000
        self.d_model = 384
        self.n_heads = 12
        self.n_layers = 13
        self.d_ff = 1408
        self.max_seq_len = 2048
        self.dropout = 0.1

        # è®­ç»ƒå‚æ•°
        if self.device == "cuda":
            gpu_memory = self.gpu_info['devices'][0]['memory_total'] if self.gpu_info else 8
            if gpu_memory >= 24:
                self.batch_size = 24
                self.gradient_accumulation_steps = 6
            elif gpu_memory >= 12:
                self.batch_size = 12
                self.gradient_accumulation_steps = 12
            else:
                self.batch_size = 6
                self.gradient_accumulation_steps = 24
        else:
            self.batch_size = 6
            self.gradient_accumulation_steps = 24

        self.learning_rate = 3e-4
        self.weight_decay = 0.01
        self.warmup_steps = 3000
        self.max_steps = 60000
        self.eval_steps = 1500
        self.save_steps = 3000

        # ä¼˜åŒ–å™¨
        self.optimizer = "adamw"
        self.beta1 = 0.9
        self.beta2 = 0.95
        self.eps = 1e-8

        # ç”Ÿæˆå‚æ•°
        self.max_generate_length = 1024
        self.temperature = 0.8
        self.top_k = 50
        self.top_p = 0.9


class MediumConfig(BaseConfig):
    """ä¸­ç­‰æ¨¡å‹é…ç½® (~80Må‚æ•°) - GPUä¼˜åŒ– + Flash Attention"""
    def __init__(self):
        super().__init__()
        
        # æ¨¡å‹æ ‡è¯†
        self.model_size = "medium"

        # æ¨¡å‹å‚æ•° (ä¸src/model/config.pyä¿æŒä¸€è‡´)
        self.vocab_size = 20000  # æ›´æ–°ä¸ºä¸æ¨¡å‹é…ç½®ä¸€è‡´
        self.d_model = 512       # å‡å°ç»´åº¦ä»¥é™ä½å†…å­˜
        self.n_heads = 16
        self.n_layers = 16
        self.d_ff = 1536         # å‡å°FFNä»¥é™ä½å†…å­˜
        self.max_seq_len = 2048
        self.dropout = 0.1

        # GPUä¼˜åŒ–è®­ç»ƒå‚æ•° - é’ˆå¯¹A6000ä¼˜åŒ– (ä¿å®ˆé…ç½®ä»¥é¿å…OOM)
        if self.device == "cuda":
            gpu_memory = self.gpu_info['devices'][0]['memory_total'] if self.gpu_info else 8
            if gpu_memory >= 40:  # A6000ç­‰é«˜ç«¯å¡ (48GB)
                # ä¿å®ˆé…ç½®ï¼šé™ä½batch_sizeï¼Œå¢åŠ æ¢¯åº¦ç´¯ç§¯
                # è¿™æ ·å¯ä»¥å‡å°‘å•æ¬¡å‰å‘ä¼ æ’­çš„å†…å­˜å³°å€¼
                self.batch_size = 16  # é™ä½åˆ°16 (ä¹‹å‰32)
                self.gradient_accumulation_steps = 8  # å¢åŠ åˆ°8ï¼Œæœ‰æ•ˆæ‰¹é‡ = 16 * 8 = 128
            elif gpu_memory >= 24:  # RTX 3090/4090
                self.batch_size = 8
                self.gradient_accumulation_steps = 16  # æœ‰æ•ˆæ‰¹é‡ = 128
            elif gpu_memory >= 12:  # RTX 3060Ti/4060Ti
                self.batch_size = 4
                self.gradient_accumulation_steps = 32
            else:
                self.batch_size = 2
                self.gradient_accumulation_steps = 64
        else:
            self.batch_size = 2
            self.gradient_accumulation_steps = 64

        self.learning_rate = 3e-4
        self.weight_decay = 0.01
        self.warmup_steps = 4000
        self.max_steps = 100000
        self.eval_steps = 2000
        self.save_steps = 5000

        # ä¼˜åŒ–å™¨
        self.optimizer = "adamw"
        self.beta1 = 0.9
        self.beta2 = 0.95
        self.eps = 1e-8

        # ç”Ÿæˆå‚æ•°
        self.max_generate_length = 1024
        self.temperature = 0.8
        self.top_k = 50
        self.top_p = 0.9


class FoundationConfig(BaseConfig):
    """åŸºç¡€æ¨¡å‹é…ç½® (~200Må‚æ•°) - ä¸­å‹è§„æ¨¡è®­ç»ƒ"""
    def __init__(self):
        super().__init__()
        
        # æ¨¡å‹æ ‡è¯†
        self.model_size = "foundation"

        # æ¨¡å‹å‚æ•°
        self.vocab_size = 32000
        self.d_model = 768
        self.n_heads = 16
        self.n_layers = 24
        self.d_ff = 2688
        self.max_seq_len = 4096
        self.dropout = 0.1

        # è®­ç»ƒå‚æ•° - GPUä¼˜åŒ–
        if self.device == "cuda":
            gpu_memory = self.gpu_info['devices'][0]['memory_total'] if self.gpu_info else 8
            if gpu_memory >= 40:  # A6000ç­‰é«˜ç«¯å¡
                self.batch_size = 12
                self.gradient_accumulation_steps = 12
            elif gpu_memory >= 24:  # RTX 3090/4090
                self.batch_size = 6
                self.gradient_accumulation_steps = 24
            elif gpu_memory >= 12:
                self.batch_size = 3
                self.gradient_accumulation_steps = 48
            else:
                self.batch_size = 2
                self.gradient_accumulation_steps = 64
        else:
            self.batch_size = 2
            self.gradient_accumulation_steps = 64

        self.learning_rate = 2e-4
        self.weight_decay = 0.01
        self.warmup_steps = 5000
        self.max_steps = 150000
        self.eval_steps = 3000
        self.save_steps = 8000

        # å¼ºåˆ¶å¯ç”¨å†…å­˜ä¼˜åŒ–
        self.gradient_checkpointing = True

        # ä¼˜åŒ–å™¨
        self.optimizer = "adamw"
        self.beta1 = 0.9
        self.beta2 = 0.95
        self.eps = 1e-8

        # ç”Ÿæˆå‚æ•°
        self.max_generate_length = 2048
        self.temperature = 0.8
        self.top_k = 50
        self.top_p = 0.9


class LargeConfig(BaseConfig):
    """å¤§æ¨¡å‹é…ç½® (~350Må‚æ•°) - é«˜ç«¯ GPU ä¼˜åŒ–"""
    def __init__(self):
        super().__init__()
        
        # æ¨¡å‹æ ‡è¯†
        self.model_size = "large"

        # æ¨¡å‹å‚æ•° (æ›´æ–°ä¸ºä¸src/model/config.pyä¸€è‡´)
        self.vocab_size = 32000
        self.d_model = 768
        self.n_heads = 24
        self.n_layers = 32
        self.d_ff = 3072
        self.max_seq_len = 4096
        self.dropout = 0.1

        # åªåœ¨é«˜ç«¯ GPU ä¸Šè¿è¡Œ
        if self.device == "cuda":
            gpu_memory = self.gpu_info['devices'][0]['memory_total'] if self.gpu_info else 8
            if gpu_memory >= 40:  # A6000ç­‰é«˜ç«¯å¡
                self.batch_size = 8
                self.gradient_accumulation_steps = 16
            elif gpu_memory >= 24:  # RTX 3090/4090
                self.batch_size = 4
                self.gradient_accumulation_steps = 32
            elif gpu_memory >= 16:  # RTX 4080
                self.batch_size = 2
                self.gradient_accumulation_steps = 64
            else:
                self.batch_size = 1
                self.gradient_accumulation_steps = 128
                print("è­¦å‘Š: æ˜¾å­˜ä¸è¶³ï¼Œå»ºè®®ä½¿ç”¨æ›´å°çš„æ¨¡å‹é…ç½®")
        else:
            self.batch_size = 1
            self.gradient_accumulation_steps = 128
            print("è­¦å‘Š: å¤§æ¨¡å‹å»ºè®®ä½¿ç”¨ CUDA GPU")

        self.learning_rate = 2e-4
        self.weight_decay = 0.01
        self.warmup_steps = 8000
        self.max_steps = 200000
        self.eval_steps = 5000
        self.save_steps = 10000

        # å¼ºåˆ¶å¯ç”¨å†…å­˜ä¼˜åŒ–
        self.gradient_checkpointing = True

        # ä¼˜åŒ–å™¨
        self.optimizer = "adamw"
        self.beta1 = 0.9
        self.beta2 = 0.95
        self.eps = 1e-8

        # ç”Ÿæˆå‚æ•°
        self.max_generate_length = 2048
        self.temperature = 0.8
        self.top_k = 50
        self.top_p = 0.9


class MOEConfig(BaseConfig):
    """MOE (Mixture of Experts) æ¨¡å‹é…ç½®"""
    def __init__(self):
        super().__init__()
        
        # æ¨¡å‹æ ‡è¯†
        self.model_size = "moe"

        # æ¨¡å‹å‚æ•°
        self.vocab_size = 10000
        self.d_model = 384
        self.n_heads = 12
        self.n_layers = 12
        self.d_ff = 1536
        self.max_seq_len = 1024
        self.dropout = 0.1

        # MOEç‰¹æœ‰å‚æ•°
        self.use_moe = True
        self.num_experts_per_tok = 2
        self.n_routed_experts = 4
        self.n_shared_experts = 1

        # è®­ç»ƒå‚æ•°
        if self.device == "cuda":
            gpu_memory = self.gpu_info['devices'][0]['memory_total'] if self.gpu_info else 8
            if gpu_memory >= 24:
                self.batch_size = 16
                self.gradient_accumulation_steps = 8
            elif gpu_memory >= 12:
                self.batch_size = 8
                self.gradient_accumulation_steps = 16
            else:
                self.batch_size = 4
                self.gradient_accumulation_steps = 32
        else:
            self.batch_size = 4
            self.gradient_accumulation_steps = 32

        self.learning_rate = 3e-4
        self.weight_decay = 0.01
        self.warmup_steps = 2000
        self.max_steps = 50000
        self.eval_steps = 1000
        self.save_steps = 2000

        # ä¼˜åŒ–å™¨
        self.optimizer = "adamw"
        self.beta1 = 0.9
        self.beta2 = 0.95
        self.eps = 1e-8

        # ç”Ÿæˆå‚æ•°
        self.max_generate_length = 1024
        self.temperature = 0.8
        self.top_k = 50
        self.top_p = 0.9


def get_config(model_size="medium"):
    """è·å–æŒ‡å®šå¤§å°çš„é…ç½®"""
    configs = {
        "tiny": TinyConfig,
        "small": SmallConfig,
        "small_30m": Small30MConfig,
        "medium": MediumConfig,
        "foundation": FoundationConfig,
        "large": LargeConfig,
        "moe": MOEConfig
    }

    if model_size not in configs:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹å¤§å°: {model_size}. æ”¯æŒçš„å¤§å°: {list(configs.keys())}")

    config = configs[model_size]()

    # æ‰“å°é…ç½®ä¿¡æ¯
    print(f"\n=== {model_size.upper()} æ¨¡å‹é…ç½® ===")
    print(f"è®¾å¤‡: {config.device}")
    if config.gpu_info:
        gpu = config.gpu_info['devices'][0]
        print(f"GPU: {gpu['name']} ({gpu['memory_total']:.1f} GB)")
    print(f"æ‰¹é‡å¤§å°: {config.batch_size}")
    print(f"æ¢¯åº¦ç´¯ç§¯: {config.gradient_accumulation_steps}")
    print(f"æœ‰æ•ˆæ‰¹é‡: {config.batch_size * config.gradient_accumulation_steps}")
    print(f"æ··åˆç²¾åº¦: {config.mixed_precision}")
    print(f"æ¨¡å‹ç¼–è¯‘: {config.compile_model}")
    print(f"Flash Attention: {config.flash_attention}")
    print(f"æ¢¯åº¦æ£€æŸ¥ç‚¹: {config.gradient_checkpointing}")

    return config


def get_medium_config():
    return get_config("medium")


def get_large_config():
    return get_config("large")


if __name__ == "__main__":
    # æµ‹è¯•æ‰€æœ‰é…ç½®
    configs = ["tiny", "small", "small_30m", "medium", "foundation", "large", "moe"]

    for config_name in configs:
        print(f"\n{'='*50}")
        config = get_config(config_name)
        print(f"é…ç½®æµ‹è¯•å®Œæˆ: {config_name}")