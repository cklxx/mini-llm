#!/usr/bin/env python3
"""
å®Œæ•´è®­ç»ƒæµ‹è¯•è„šæœ¬
éªŒè¯GPUä¼˜åŒ–é…ç½®å’Œè®­ç»ƒæµç¨‹
"""
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import time
from tqdm import tqdm

from config.training_config import get_config
from src.model.transformer import create_model


def create_dummy_data(config, num_samples=1000):
    """åˆ›å»ºè™šæ‹Ÿè®­ç»ƒæ•°æ®"""
    print("åˆ›å»ºè™šæ‹Ÿè®­ç»ƒæ•°æ®...")

    # ç”Ÿæˆéšæœºtokenåºåˆ—
    input_ids = torch.randint(0, config.vocab_size, (num_samples, config.max_seq_len))

    # æ ‡ç­¾æ˜¯è¾“å…¥å‘å³åç§»ä¸€ä½
    labels = torch.cat([input_ids[:, 1:], torch.zeros(num_samples, 1, dtype=torch.long)], dim=1)

    dataset = TensorDataset(input_ids, labels)
    dataloader = DataLoader(
        dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=config.num_workers,
        pin_memory=config.pin_memory
    )

    print(f"æ•°æ®é›†å¤§å°: {num_samples} æ ·æœ¬")
    print(f"æ‰¹é‡å¤§å°: {config.batch_size}")
    print(f"æ‰¹æ¬¡æ•°é‡: {len(dataloader)}")

    return dataloader


def test_training_step(model, dataloader, config):
    """æµ‹è¯•è®­ç»ƒæ­¥éª¤"""
    print("\n=== å¼€å§‹è®­ç»ƒæµ‹è¯• ===")

    # è®¾ç½®ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config.learning_rate,
        weight_decay=config.weight_decay,
        betas=(config.beta1, config.beta2),
        eps=config.eps
    )

    # æŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss(ignore_index=-100)

    # è®­ç»ƒæ¨¡å¼
    model.train()

    # è®°å½•æ—¶é—´å’ŒæŸå¤±
    start_time = time.time()
    total_loss = 0
    num_batches = min(10, len(dataloader))  # åªè®­ç»ƒ10ä¸ªæ‰¹æ¬¡ç”¨äºæµ‹è¯•

    print(f"è®­ç»ƒ {num_batches} ä¸ªæ‰¹æ¬¡...")

    for i, (input_ids, labels) in enumerate(tqdm(dataloader, total=num_batches)):
        if i >= num_batches:
            break

        # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
        if config.device != "cpu":
            input_ids = input_ids.to(config.device)
            labels = labels.to(config.device)

        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()

        outputs = model(input_ids)

        # è®¡ç®—æŸå¤±
        # é‡å¡‘è¾“å‡ºå’Œæ ‡ç­¾ç”¨äºæŸå¤±è®¡ç®—
        vocab_size = outputs.size(-1)
        shift_logits = outputs[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()

        loss = criterion(
            shift_logits.view(-1, vocab_size),
            shift_labels.view(-1)
        )

        # åå‘ä¼ æ’­
        loss.backward()

        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        # ä¼˜åŒ–å™¨æ­¥éª¤
        optimizer.step()

        total_loss += loss.item()

        # æ˜¾å­˜ç›‘æ§
        if config.device == "cuda":
            allocated = torch.cuda.memory_allocated() / 1024**3
            cached = torch.cuda.memory_reserved() / 1024**3
            print(f"æ‰¹æ¬¡ {i+1}: æŸå¤±={loss.item():.4f}, æ˜¾å­˜={allocated:.1f}GB/{cached:.1f}GB")
        elif config.device == "mps":
            print(f"æ‰¹æ¬¡ {i+1}: æŸå¤±={loss.item():.4f}")
        else:
            print(f"æ‰¹æ¬¡ {i+1}: æŸå¤±={loss.item():.4f}")

    end_time = time.time()
    avg_loss = total_loss / num_batches
    training_time = end_time - start_time

    print(f"\n=== è®­ç»ƒæµ‹è¯•å®Œæˆ ===")
    print(f"å¹³å‡æŸå¤±: {avg_loss:.4f}")
    print(f"è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
    print(f"æ¯æ‰¹æ¬¡æ—¶é—´: {training_time/num_batches:.2f} ç§’")

    return avg_loss


def test_inference(model, config):
    """æµ‹è¯•æ¨ç†"""
    print("\n=== å¼€å§‹æ¨ç†æµ‹è¯• ===")

    model.eval()

    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    prompt = torch.randint(0, config.vocab_size, (1, 10))
    if config.device != "cpu":
        prompt = prompt.to(config.device)

    start_time = time.time()

    with torch.no_grad():
        # æµ‹è¯•ç”Ÿæˆ
        generated = model.generate(
            prompt,
            max_length=50,
            temperature=config.temperature,
            top_k=config.top_k
        )

    end_time = time.time()
    inference_time = end_time - start_time

    print(f"æ¨ç†æ—¶é—´: {inference_time:.3f} ç§’")
    print(f"ç”Ÿæˆé•¿åº¦: {generated.size(1)} tokens")
    print(f"ç”Ÿæˆé€Ÿåº¦: {generated.size(1)/inference_time:.1f} tokens/ç§’")

    return inference_time


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å®Œæ•´çš„è®­ç»ƒå’Œæ¨ç†æµ‹è¯•")

    # è·å–é…ç½®
    config = get_config("tiny")  # ä½¿ç”¨tinyæ¨¡å‹å¿«é€Ÿæµ‹è¯•

    # åˆ›å»ºæ¨¡å‹
    print(f"\n=== åˆ›å»ºæ¨¡å‹ ===")

    # ä½¿ç”¨é…ç½®åˆ›å»ºæ¨¡å‹é…ç½®å¯¹è±¡
    from src.model.config import MiniGPTConfig
    model_config = MiniGPTConfig(
        vocab_size=config.vocab_size,
        hidden_size=config.d_model,
        num_hidden_layers=config.n_layers,
        num_attention_heads=config.n_heads,
        intermediate_size=config.d_ff,
        max_position_embeddings=config.max_seq_len,
        dropout=config.dropout,
        rms_norm_eps=1e-6
    )

    model = create_model(config=model_config)

    # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
    if config.device != "cpu":
        model = model.to(config.device)
        print(f"æ¨¡å‹å·²ç§»åŠ¨åˆ° {config.device}")

    # å¯é€‰ï¼šç¼–è¯‘æ¨¡å‹ (PyTorch 2.0+)
    if config.compile_model and hasattr(torch, 'compile'):
        try:
            model = torch.compile(model)
            print("æ¨¡å‹ç¼–è¯‘æˆåŠŸ")
        except Exception as e:
            print(f"æ¨¡å‹ç¼–è¯‘å¤±è´¥: {e}")

    # åˆ›å»ºè®­ç»ƒæ•°æ®
    dataloader = create_dummy_data(config, num_samples=500)

    # è®­ç»ƒæµ‹è¯•
    training_loss = test_training_step(model, dataloader, config)

    # æ¨ç†æµ‹è¯•
    inference_time = test_inference(model, config)

    # æ€»ç»“
    print(f"\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print(f"âœ“ è®¾å¤‡: {config.device}")
    print(f"âœ“ æ¨¡å‹å‚æ•°: {model.get_num_params():,}")
    print(f"âœ“ è®­ç»ƒæŸå¤±: {training_loss:.4f}")
    print(f"âœ“ æ¨ç†æ—¶é—´: {inference_time:.3f}s")

    # å†…å­˜ä½¿ç”¨æƒ…å†µ
    if config.device == "cuda":
        print(f"âœ“ GPUæ˜¾å­˜ä½¿ç”¨: {torch.cuda.memory_allocated()/1024**3:.1f}GB")
    elif config.device == "mps":
        print(f"âœ“ MPSå·²å¯ç”¨")

    print("\næ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼è®­ç»ƒå’Œæ¨ç†ç³»ç»Ÿå·¥ä½œæ­£å¸¸ã€‚")


if __name__ == "__main__":
    main()