# MiniGPTè®­ç»ƒæ¡†æ¶

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.4+-red.svg)](https://pytorch.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![GPU Optimized](https://img.shields.io/badge/GPU-Optimized-green.svg)](#gpuä¼˜åŒ–)

**å®Œæ•´çš„å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒæ¡†æ¶ï¼Œæ”¯æŒé¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒ(SFT)ã€DPOå’ŒRLHFå…¨æµç¨‹ï¼Œé’ˆå¯¹NVIDIA GPUå’ŒApple Siliconä¼˜åŒ–**

[ğŸ“š æŠ€æœ¯æ‰‹å†Œ](docs/MiniGPTè®­ç»ƒæ·±åº¦è§£æå°å†Œ/) â€¢
[ğŸš€ å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹) â€¢
[ğŸ Macä¼˜åŒ–ç‰ˆ](README_MAC_OPTIMIZED.md) â€¢
[ğŸ“– å¼€å‘ç¬”è®°](CLAUDE.md)

</div>

## âœ¨ é¡¹ç›®ç‰¹è‰²

### ğŸš€ å®Œæ•´è®­ç»ƒæµç¨‹
- **é¢„è®­ç»ƒï¼ˆPretrainï¼‰**: ä»é›¶å¼€å§‹è®­ç»ƒè¯­è¨€æ¨¡å‹
- **ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰**: æŒ‡ä»¤è·Ÿéšå’Œå¯¹è¯èƒ½åŠ›è®­ç»ƒ
- **DPOè®­ç»ƒ**: ç›´æ¥åå¥½ä¼˜åŒ–ï¼Œæ— éœ€å¥–åŠ±æ¨¡å‹
- **RLHFæµç¨‹**: åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ 

### ğŸš€ GPUè‡ªåŠ¨ä¼˜åŒ–
- **æ™ºèƒ½è®¾å¤‡æ£€æµ‹**: è‡ªåŠ¨æ£€æµ‹NVIDIA GPUã€Apple Silicon MPSæˆ–CPU
- **åŠ¨æ€é…ç½®è°ƒæ•´**: æ ¹æ®GPUæ˜¾å­˜è‡ªåŠ¨è°ƒæ•´æ‰¹é‡å¤§å°å’Œæ¢¯åº¦ç´¯ç§¯
- **PyTorch 2.4ä¼˜åŒ–**: å¯ç”¨TensorFloat-32ã€Flash Attentionã€æ¨¡å‹ç¼–è¯‘ç­‰ä¼˜åŒ–
- **æ··åˆç²¾åº¦è®­ç»ƒ**: æ”¯æŒFP16/BF16æ··åˆç²¾åº¦è®­ç»ƒï¼Œæå‡æ€§èƒ½å’ŒèŠ‚çœæ˜¾å­˜

### ğŸ—ï¸ æ¨¡å—åŒ–æ¶æ„
- **å¯æ‰©å±•è®¾è®¡**: æ¸…æ™°çš„æ¨¡å—åˆ’åˆ†ï¼Œæ˜“äºæ‰©å±•
- **é…ç½®é©±åŠ¨**: çµæ´»çš„é…ç½®ç³»ç»Ÿæ”¯æŒå¤šç§è®­ç»ƒåœºæ™¯
- **å®Œæ•´å·¥å…·é“¾**: æ•°æ®å¤„ç†ã€è®­ç»ƒã€æ¨ç†ã€è¯„ä¼°ä¸€ä½“åŒ–
- **ç°ä»£æ¶æ„**: æ”¯æŒSwiGLUã€GELUã€MoEç­‰ä¸šç•Œä¸»æµæŠ€æœ¯

### ğŸ¤– ç°ä»£AIæŠ€æœ¯
- **RMSNormä¼˜åŒ–**: æ›¿ä»£LayerNormï¼Œå‡å°‘è®¡ç®—é‡æå‡è®­ç»ƒæ•ˆç‡
- **SwiGLUæ¿€æ´»å‡½æ•°**: ç°ä»£å¤§æ¨¡å‹æ ‡å‡†æ¿€æ´»å‡½æ•°ï¼Œæ›¿ä»£ä¼ ç»ŸReLU
- **å…ˆè¿›æ¶æ„ç»„ä»¶**: æ”¯æŒTransformerã€å¤šå¤´æ³¨æ„åŠ›ã€ä½ç½®ç¼–ç ç­‰
- **é«˜æ•ˆè®­ç»ƒæŠ€æœ¯**: æ¢¯åº¦æ£€æŸ¥ç‚¹ã€æ¢¯åº¦ç´¯ç§¯ã€å­¦ä¹ ç‡è°ƒåº¦ç­‰

### ğŸ“š æ·±åº¦æŠ€æœ¯æ‰‹å†Œ
- **å®Œæ•´çŸ¥è¯†ä½“ç³»**: 8ç« æŠ€æœ¯æ‰‹å†Œè¦†ç›–ä»æ•°å­¦åŸºç¡€åˆ°å·¥ç¨‹å®è·µ
- **ç†è®ºä¸å®è·µç»“åˆ**: æ¯ç« åŒ…å«æ•°å­¦æ¨å¯¼ã€ä»£ç å®ç°å’Œåº”ç”¨æ¡ˆä¾‹
- **ç³»ç»Ÿæ€§å­¦ä¹ è·¯å¾„**: ä»åŸºç¡€æ¦‚å¿µåˆ°é«˜çº§æŠ€æœ¯çš„è¿›é˜¶è·¯çº¿
- **ç”Ÿäº§çº§æŒ‡å¯¼**: åŸºäºå®é™…é¡¹ç›®ç»éªŒçš„æœ€ä½³å®è·µ

## ğŸ¯ é€‚ç”¨åœºæ™¯

- **å­¦ä¹ ç ”ç©¶**: æ·±å…¥ç†è§£å¤§æ¨¡å‹è®­ç»ƒåŸç†
- **å¿«é€ŸåŸå‹**: éªŒè¯æƒ³æ³•å’Œç®—æ³•æ•ˆæœ
- **æ•™å­¦æ¼”ç¤º**: å®Œæ•´çš„è®­ç»ƒæµç¨‹å±•ç¤º
- **Macå¼€å‘**: åœ¨Macè®¾å¤‡ä¸Šè¿›è¡Œæ¨¡å‹è®­ç»ƒ

## ğŸ“š æ ¸å¿ƒæŠ€æœ¯æ‰‹å†Œ

> **æ·±åº¦è§£ææ‰‹å†Œæ˜¯æœ¬é¡¹ç›®çš„æ ¸å¿ƒæ–‡æ¡£ï¼Œä»æ•°å­¦åŸç†åˆ°å·¥ç¨‹å®è·µï¼Œå…¨é¢è§£æå¤§æ¨¡å‹è®­ç»ƒæŠ€æœ¯**

### ğŸ§® [ç¬¬01ç«  - æ•°å­¦åŸºç¡€ä¸ç†è®ºæ¡†æ¶](docs/MiniGPTè®­ç»ƒæ·±åº¦è§£æå°å†Œ/ç¬¬01ç« -æ•°å­¦åŸºç¡€ä¸ç†è®ºæ¡†æ¶/)
- **ä¿¡æ¯è®ºä¸æ¦‚ç‡åŸºç¡€**: è¯­è¨€å»ºæ¨¡çš„æ•°å­¦åŸºç¡€
- **çº¿æ€§ä»£æ•°ä¸çŸ©é˜µè¿ç®—**: Transformerçš„æ ¸å¿ƒæ•°å­¦å·¥å…·
- **ä¼˜åŒ–ç†è®ºä¸æ¢¯åº¦ä¸‹é™**: è®­ç»ƒç®—æ³•çš„ç†è®ºåŸºç¡€
- **ç»Ÿè®¡å­¦ä¹ ç†è®º**: æ³›åŒ–èƒ½åŠ›çš„ç†è®ºä¿è¯

### ğŸ—ï¸ [ç¬¬02ç«  - Transformeræ ¸å¿ƒæ¶æ„](docs/MiniGPTè®­ç»ƒæ·±åº¦è§£æå°å†Œ/ç¬¬02ç« -Transformeræ ¸å¿ƒæ¶æ„/)
- **æ³¨æ„åŠ›æœºåˆ¶æ•°å­¦åŸç†**: è‡ªæ³¨æ„åŠ›çš„å®Œæ•´æ¨å¯¼
- **å¤šå¤´æ³¨æ„åŠ›å­ç©ºé—´åˆ†è§£**: å¹¶è¡Œå¤„ç†çš„æ•°å­¦åŸç†
- **ä½ç½®ç¼–ç å‡ ä½•å­¦**: åºåˆ—ä½ç½®çš„å·§å¦™ç¼–ç 
- **æ®‹å·®è¿æ¥ä¸å±‚å½’ä¸€åŒ–**: æ·±åº¦ç½‘ç»œè®­ç»ƒç¨³å®šæ€§

### ğŸ“– [ç¬¬03ç«  - é¢„è®­ç»ƒç†è®ºä¸å®ç°](docs/MiniGPTè®­ç»ƒæ·±åº¦è§£æå°å†Œ/ç¬¬03ç« -é¢„è®­ç»ƒç†è®ºä¸å®ç°/)
- **è¯­è¨€å»ºæ¨¡æ¦‚ç‡åŸºç¡€**: è‡ªå›å½’æ¨¡å‹çš„æ•°å­¦åŸºç¡€
- **è‡ªå›å½’å»ºæ¨¡ä¸å› æœæ©ç **: å¹¶è¡Œè®­ç»ƒçš„æŠ€æœ¯å®ç°
- **åˆ†è¯ç­–ç•¥ä¸ä¿¡æ¯å‹ç¼©**: BPEç®—æ³•çš„æ·±åº¦åˆ†æ
- **ä¼˜åŒ–ç®—æ³•æ·±åº¦è§£æ**: AdamWä¸å­¦ä¹ ç‡è°ƒåº¦

### ğŸ¯ [ç¬¬04ç«  - ç›‘ç£å¾®è°ƒæ·±åº¦è§£æ](docs/MiniGPTè®­ç»ƒæ·±åº¦è§£æå°å†Œ/ç¬¬04ç« -ç›‘ç£å¾®è°ƒæ·±åº¦è§£æ/)
- **ä»»åŠ¡é€‚åº”ç†è®ºæ¡†æ¶**: ä»é¢„è®­ç»ƒåˆ°ä»»åŠ¡ç‰¹åŒ–
- **æŒ‡ä»¤è·Ÿéšä¸å¯¹è¯å»ºæ¨¡**: äººæœºäº¤äº’çš„æ•°å­¦å»ºæ¨¡
- **æŸå¤±å‡½æ•°è®¾è®¡ä¸ä¼˜åŒ–**: SFTè®­ç»ƒçš„æ ¸å¿ƒæŠ€æœ¯
- **è¯„ä¼°æŒ‡æ ‡ä¸æ•ˆæœåˆ†æ**: å¾®è°ƒæ•ˆæœçš„é‡åŒ–è¯„ä¼°

### ğŸ”„ [ç¬¬05ç«  - å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆ](docs/MiniGPTè®­ç»ƒæ·±åº¦è§£æå°å†Œ/ç¬¬05ç« -å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆ/)
- **RLHFç†è®ºä¸æ•°å­¦åŸºç¡€**: äººç±»åå¥½å­¦ä¹ çš„ç†è®º
- **å¥–åŠ±å»ºæ¨¡ä¸åå¥½å­¦ä¹ **: äººç±»åé¦ˆçš„æ•°å­¦å»ºæ¨¡
- **PPOç®—æ³•è¯­è¨€æ¨¡å‹å¾®è°ƒ**: ç­–ç•¥ä¼˜åŒ–çš„å…·ä½“å®ç°
- **DPOä¸æ›¿ä»£RLHFæ–¹æ³•**: ç›´æ¥åå¥½ä¼˜åŒ–æŠ€æœ¯

### ğŸ² [ç¬¬06ç«  - ç”Ÿæˆä¸è§£ç ç­–ç•¥](docs/MiniGPTè®­ç»ƒæ·±åº¦è§£æå°å†Œ/ç¬¬06ç« -ç”Ÿæˆä¸è§£ç ç­–ç•¥/)
- **è‡ªå›å½’ç”Ÿæˆæ•°å­¦åŸç†**: åºåˆ—ç”Ÿæˆçš„æ¦‚ç‡å»ºæ¨¡
- **ç»å…¸è§£ç ç®—æ³•æ·±åº¦è§£æ**: Greedyã€Beam Searchç­‰ç®—æ³•
- **é«˜çº§é‡‡æ ·ç­–ç•¥ä¸æ§åˆ¶**: Top-kã€Top-pã€Temperatureé‡‡æ ·
- **ç”Ÿæˆè´¨é‡æ§åˆ¶ä¸ä¼˜åŒ–**: ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡è¯„ä¼°ä¸æ§åˆ¶

### ğŸ“Š [ç¬¬07ç«  - è¯„ä¼°ä¸åˆ†ææ–¹æ³•](docs/MiniGPTè®­ç»ƒæ·±åº¦è§£æå°å†Œ/ç¬¬07ç« -è¯„ä¼°ä¸åˆ†ææ–¹æ³•/)
- **è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡æ·±åº¦è§£æ**: BLEUã€ROUGEã€å›°æƒ‘åº¦ç­‰æŒ‡æ ‡
- **äººç±»è¯„ä¼°æ¡†æ¶è®¾è®¡**: ä¸»è§‚è¯„ä¼°çš„æ ‡å‡†åŒ–æ–¹æ³•
- **é”™è¯¯åˆ†æä¸è¯Šæ–­æŠ€æœ¯**: æ¨¡å‹å¤±è´¥æ¡ˆä¾‹çš„ç³»ç»Ÿåˆ†æ
- **åŸºå‡†æµ‹è¯•ä¸æ¯”è¾ƒåˆ†æ**: æ ‡å‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½å¯¹æ¯”

### ğŸ”§ [ç¬¬08ç«  - å·¥ç¨‹å®è·µä¸ä¼˜åŒ–](docs/MiniGPTè®­ç»ƒæ·±åº¦è§£æå°å†Œ/ç¬¬08ç« -å·¥ç¨‹å®è·µä¸ä¼˜åŒ–/)
- **è®­ç»ƒåŸºç¡€è®¾æ–½ä¸å¯æ‰©å±•æ€§**: åˆ†å¸ƒå¼è®­ç»ƒçš„å·¥ç¨‹å®ç°
- **æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯**: å†…å­˜ä¼˜åŒ–ã€è®¡ç®—åŠ é€Ÿã€æ··åˆç²¾åº¦
- **éƒ¨ç½²ä¸ç”Ÿäº§ç³»ç»Ÿ**: æ¨¡å‹æœåŠ¡åŒ–çš„å®Œæ•´æ–¹æ¡ˆ
- **ç›‘æ§ä¸ç»´æŠ¤**: ç”Ÿäº§ç¯å¢ƒçš„æ¨¡å‹ç›‘æ§ä¸æ›´æ–°

> **ğŸ’¡ æç¤º**: æŠ€æœ¯æ‰‹å†Œé‡‡ç”¨ç†è®ºä¸å®è·µç›¸ç»“åˆçš„æ–¹å¼ï¼Œæ¯ç« éƒ½åŒ…å«è¯¦ç»†çš„æ•°å­¦æ¨å¯¼ã€ä»£ç å®ç°å’Œå®é™…åº”ç”¨æ¡ˆä¾‹ã€‚å¼ºçƒˆå»ºè®®æŒ‰ç« èŠ‚é¡ºåºå­¦ä¹ ï¼Œå»ºç«‹å®Œæ•´çš„çŸ¥è¯†ä½“ç³»ã€‚

## ğŸ“‹ ç›®å½•

- [æ ¸å¿ƒæŠ€æœ¯æ‰‹å†Œ](#-æ ¸å¿ƒæŠ€æœ¯æ‰‹å†Œ)
- [å®‰è£…é…ç½®](#-å®‰è£…é…ç½®)
- [å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
- [è®­ç»ƒæµç¨‹](#-è®­ç»ƒæµç¨‹)
- [é¡¹ç›®æ¶æ„](#-é¡¹ç›®æ¶æ„)
- [é…ç½®è¯´æ˜](#-é…ç½®è¯´æ˜)
- [é«˜çº§åŠŸèƒ½](#-é«˜çº§åŠŸèƒ½)
- [æ–‡æ¡£èµ„æº](#-æ–‡æ¡£èµ„æº)
- [è´¡çŒ®æŒ‡å—](#-è´¡çŒ®æŒ‡å—)

## ğŸ”§ å®‰è£…é…ç½®

### ç¯å¢ƒè¦æ±‚

- **Python**: 3.11+
- **PyTorch**: 2.4+
- **ç³»ç»Ÿ**: macOS/Linux/Windows
- **æ˜¾å¡**: NVIDIA GPU (æ¨è) / Apple Silicon / CPU
- **å†…å­˜**: æœ€ä½4GBï¼Œæ¨è8GB+ (GPUæ˜¾å­˜æ ¹æ®æ¨¡å‹å¤§å°è°ƒæ•´)

### æ–¹å¼ä¸€ï¼šä½¿ç”¨UVï¼ˆæ¨èï¼‰

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/your-repo/minigpt-training.git
cd minigpt-training

# ä¸€é”®è®¾ç½®UVç¯å¢ƒ
uv sync

# æ¿€æ´»ç¯å¢ƒ
source .venv/bin/activate
```

### æ–¹å¼äºŒï¼šä¼ ç»Ÿpipå®‰è£…

```bash
# å®‰è£…ä¾èµ– (PyTorch 2.4 + NVIDIA GPUä¼˜åŒ–)
pip install -e .

# æ‰‹åŠ¨å®‰è£…PyTorch (CUDAç‰ˆæœ¬)
pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121

# æˆ–å®‰è£…CPUç‰ˆæœ¬
pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cpu
```

### GPUä¼˜åŒ–åº“å®‰è£… (å¯é€‰)

```bash
# Flash Attention (NVIDIA GPU)
pip install flash-attn>=2.6.0

# XFormers (å†…å­˜ä¼˜åŒ–)
pip install xformers>=0.0.27

# DeepSpeed (åˆ†å¸ƒå¼è®­ç»ƒ)
pip install deepspeed>=0.14.0
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1ï¸âƒ£ ç¯å¢ƒæµ‹è¯•

```bash
# æµ‹è¯•GPUé…ç½®å’Œæ¨¡å‹åˆ›å»º
python -c "
from config.training_config import get_config
from src.model.transformer import create_model

config = get_config('tiny')
model = create_model(vocab_size=config.vocab_size, model_size='tiny')
print('âœ… ç¯å¢ƒé…ç½®æ­£å¸¸ï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒï¼')
"
```

### 2ï¸âƒ£ å¿«é€Ÿè®­ç»ƒæµ‹è¯•

```bash
# è¿è¡Œå®Œæ•´çš„è®­ç»ƒå’Œæ¨ç†æµ‹è¯•
python test_training.py   # è®­ç»ƒæµ‹è¯•
python test_inference.py  # æ¨ç†æµ‹è¯•
```

### 3ï¸âƒ£ æ ‡å‡†è®­ç»ƒæµç¨‹

#### è®­ç»ƒåˆ†è¯å™¨å’Œæ¨¡å‹

```bash
# æ–¹æ³•1: ä½¿ç”¨è®­ç»ƒè„šæœ¬
python scripts/train.py --mode sft --config small --retrain-tokenizer

# æ–¹æ³•2: åˆ†æ­¥è®­ç»ƒ
# 1) è®­ç»ƒåˆ†è¯å™¨
python scripts/train_tokenizer.py --vocab_size 10000 --data_path data/dataset/minimind_dataset/sft_mini_512.jsonl

# 2) è®­ç»ƒæ¨¡å‹
python scripts/train.py --mode sft --config small
```

#### ç›‘ç£å¾®è°ƒ (SFT)

```bash
# ä½¿ç”¨tinyé…ç½®å¿«é€ŸéªŒè¯
python scripts/train.py --mode sft --config tiny

# ä½¿ç”¨smallé…ç½®æ ‡å‡†è®­ç»ƒ
python scripts/train.py --mode sft --config small

# ä½¿ç”¨mediumé…ç½® (éœ€è¦è¾ƒå¤§æ˜¾å­˜)
python scripts/train.py --mode sft --config medium

# ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
python scripts/train.py --mode sft --config small --resume checkpoints/checkpoint.pt
```

#### é¢„è®­ç»ƒ

```bash
# é¢„è®­ç»ƒæ¨¡å‹
python scripts/train.py --mode pretrain --config small

# ä½¿ç”¨å¤§æ•°æ®é›†é¢„è®­ç»ƒ
python scripts/train.py --mode pretrain --config medium --data_path data/dataset/minimind_dataset/pretrain_hq.jsonl
```

### 4ï¸âƒ£ æ¨ç†å’Œç”Ÿæˆ

#### äº¤äº’å¼å¯¹è¯

```bash
# å¯åŠ¨èŠå¤©æ¨¡å¼
python scripts/generate.py \
    --model-path checkpoints/best_model.pt \
    --tokenizer-path checkpoints/tokenizer.pkl \
    --mode chat

# ç¤ºä¾‹å¯¹è¯:
# ç”¨æˆ·: ä½ å¥½
# åŠ©æ‰‹: ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ
```

#### å•æ¬¡æ¨ç†

```bash
# å•ä¸ªé—®é¢˜æ¨ç†
python scripts/generate.py \
    --model-path checkpoints/best_model.pt \
    --tokenizer-path checkpoints/tokenizer.pkl \
    --mode single \
    --prompt "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†å²"
```

#### æ‰¹é‡æµ‹è¯•

```bash
# æ‰¹é‡æµ‹è¯•ç”Ÿæˆè´¨é‡
python scripts/generate.py \
    --model-path checkpoints/best_model.pt \
    --tokenizer-path checkpoints/tokenizer.pkl \
    --mode batch \
    --output results.jsonl
```

### 5ï¸âƒ£ GPUä¼˜åŒ–é…ç½®

ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹æ‚¨çš„ç¡¬ä»¶å¹¶ä¼˜åŒ–é…ç½®ï¼š

```bash
# è‡ªåŠ¨æ£€æµ‹å¹¶æ˜¾ç¤ºä¼˜åŒ–ä¿¡æ¯
python -c "
from config.training_config import get_config
config = get_config('small')  # ä¼šæ˜¾ç¤ºGPUä¿¡æ¯å’Œä¼˜åŒ–é…ç½®
"
```

#### ä¸åŒGPUçš„æ¨èé…ç½®

| GPUå‹å· | æ˜¾å­˜ | æ¨èé…ç½® | æ‰¹é‡å¤§å° |
|---------|------|----------|----------|
| RTX 3090/4090 | 24GB | medium/large | 16-32 |
| RTX 3080/4080 | 16GB | small/medium | 8-16 |
| RTX 3060Ti/4060Ti | 12GB | tiny/small | 4-8 |
| Apple M1/M2 Pro | ç»Ÿä¸€å†…å­˜ | tiny/small | 4-16 |
| CPU | ç³»ç»Ÿå†…å­˜ | tiny | 2-4 |

### 6ï¸âƒ£ è‡ªå®šä¹‰é…ç½®

```bash
# åˆ›å»ºè‡ªå®šä¹‰é…ç½®
python -c "
from src.model.config import MiniGPTConfig
from src.model.transformer import create_model

config = MiniGPTConfig(
    vocab_size=32000,
    hidden_size=768,
    num_hidden_layers=12,
    num_attention_heads=12,
    intermediate_size=3072,
    max_position_embeddings=2048
)

model = create_model(config=config)
print(f'è‡ªå®šä¹‰æ¨¡å‹å‚æ•°é‡: {model.get_num_params():,}')
"
```

## ğŸ”„ è®­ç»ƒæµç¨‹

### è®­ç»ƒé˜¶æ®µæ¦‚è§ˆ

1. **é¢„è®­ç»ƒï¼ˆPretrainï¼‰**
   - **ç›®æ ‡**: å­¦ä¹ è¯­è¨€åŸºç¡€èƒ½åŠ›
   - **æ•°æ®**: å¤§è§„æ¨¡æ— æ ‡æ³¨æ–‡æœ¬
   - **æŸå¤±**: ä¸‹ä¸€ä¸ªtokené¢„æµ‹

2. **ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰**
   - **ç›®æ ‡**: å­¦ä¹ æŒ‡ä»¤è·Ÿéšèƒ½åŠ›
   - **æ•°æ®**: æŒ‡ä»¤-å›ç­”å¯¹
   - **æŸå¤±**: äº¤å‰ç†µæŸå¤±ï¼ˆä»…å›ç­”éƒ¨åˆ†ï¼‰

3. **DPOè®­ç»ƒ**
   - **ç›®æ ‡**: ä¼˜åŒ–ç”Ÿæˆè´¨é‡å’Œåå¥½å¯¹é½
   - **æ•°æ®**: åå¥½å¯¹æ¯”æ•°æ®
   - **æŸå¤±**: DPOæŸå¤±å‡½æ•°

4. **RLHFè®­ç»ƒ**
   - **ç›®æ ‡**: åŸºäºäººç±»åé¦ˆæŒç»­ä¼˜åŒ–
   - **æ–¹æ³•**: PPOç®—æ³•
   - **ç»„ä»¶**: å¥–åŠ±æ¨¡å‹ã€ä»·å€¼æ¨¡å‹ã€ç­–ç•¥æ¨¡å‹

## ğŸ—ï¸ é¡¹ç›®æ¶æ„

```
minigpt-training/
â”œâ”€â”€ src/                    # æ ¸å¿ƒä»£ç 
â”‚   â”œâ”€â”€ model/             # æ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ training/          # è®­ç»ƒé€»è¾‘
â”‚   â”œâ”€â”€ data/              # æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ tokenizer/         # åˆ†è¯å™¨
â”‚   â”œâ”€â”€ inference/         # æ¨ç†ç”Ÿæˆ
â”‚   â”œâ”€â”€ rl/                # å¼ºåŒ–å­¦ä¹ 
â”‚   â””â”€â”€ utils/             # å·¥å…·å‡½æ•°
â”œâ”€â”€ config/                # é…ç½®æ–‡ä»¶
â”œâ”€â”€ scripts/               # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ data/                  # æ•°æ®é›†
â”œâ”€â”€ docs/                  # æ–‡æ¡£
â”œâ”€â”€ tests/                 # æµ‹è¯•ä»£ç 
â””â”€â”€ checkpoints/           # æ¨¡å‹æ£€æŸ¥ç‚¹
```

### æ ¸å¿ƒæ¨¡å—è¯´æ˜

| æ¨¡å— | åŠŸèƒ½ | ä¸»è¦æ–‡ä»¶ |
|------|------|----------|
| `src.model` | Transformeræ¨¡å‹å®ç° | `transformer.py` |
| `src.model` | ç°ä»£æ¿€æ´»å‡½æ•° | `activation_functions.py` |
| `src.model` | ç°ä»£ä¼˜åŒ–å™¨ | `optimizers.py` |
| `src.model` | MoEæ¶æ„ | `moe.py` |
| `src.training` | è®­ç»ƒæµç¨‹æ§åˆ¶ | `trainer.py` |
| `src.data` | æ•°æ®åŠ è½½å’Œå¤„ç† | `dataset_loader.py` |
| `src.tokenizer` | BPEåˆ†è¯å™¨ | `bpe_tokenizer.py` |
| `src.rl` | å¼ºåŒ–å­¦ä¹ è®­ç»ƒ | `rlhf_pipeline.py` |
| `src.inference` | æ–‡æœ¬ç”Ÿæˆ | `generator.py` |

## âš™ï¸ é…ç½®è¯´æ˜

### æ¨¡å‹é…ç½®

```python
# è¶…å°æ¨¡å‹ï¼ˆMacä¼˜åŒ–ï¼‰
tiny_config = {
    "d_model": 128,
    "n_heads": 2,
    "n_layers": 4,
    "vocab_size": 5000
}

# å°æ¨¡å‹ï¼ˆæ¨èå­¦ä¹ ï¼‰
small_config = {
    "d_model": 512,
    "n_heads": 8,
    "n_layers": 6,
    "vocab_size": 10000
}
```

### è®­ç»ƒé…ç½®

```python
# é¢„è®­ç»ƒé…ç½®
pretrain_config = {
    "learning_rate": 1e-4,
    "batch_size": 32,
    "max_steps": 50000,
    "warmup_steps": 1000
}

# SFTé…ç½®
sft_config = {
    "learning_rate": 5e-5,
    "batch_size": 16,
    "max_epochs": 10,
    "gradient_accumulation_steps": 2
}
```

## ğŸš€ é«˜çº§åŠŸèƒ½

### ç°ä»£æ¿€æ´»å‡½æ•°

```python
# é€‰æ‹©æ¿€æ´»å‡½æ•°
from src.model.activation_functions import get_feedforward_layer

# SwiGLUå‰é¦ˆç½‘ç»œï¼ˆæ¨èï¼‰
feed_forward = get_feedforward_layer(d_model=512, hidden_dim=2048, feedforward_type="swiglu")

# GEGLUå‰é¦ˆç½‘ç»œ
feed_forward = get_feedforward_layer(d_model=512, hidden_dim=2048, feedforward_type="geglu")

# æ ‡å‡†å‰é¦ˆç½‘ç»œ + ç°ä»£æ¿€æ´»å‡½æ•°
feed_forward = get_feedforward_layer(d_model=512, hidden_dim=2048, feedforward_type="standard", activation="gelu")
```

### ç°ä»£ä¼˜åŒ–å™¨

```python
from src.model.optimizers import get_optimizer

# Lionä¼˜åŒ–å™¨ï¼ˆæ¨èï¼‰
optimizer = get_optimizer("lion", model.parameters(), lr=1e-4, weight_decay=0.01)

# Sophiaä¼˜åŒ–å™¨ï¼ˆå¤§æ¨¡å‹è®­ç»ƒæ¨èï¼‰
optimizer = get_optimizer("sophia", model.parameters(), lr=1e-4, weight_decay=0.1)

# Schedule-Free AdamW
optimizer = get_optimizer("adamw_schedule_free", model.parameters(), lr=1e-3)
```

### MoEæ¶æ„

```python
from src.model.moe import create_moe_model

# åˆ›å»ºMoEæ¨¡å‹
model = create_moe_model(
    vocab_size=10000,
    d_model=512,
    n_layers=6,
    num_experts=8,      # ä¸“å®¶æ•°é‡
    top_k=2,           # æ¿€æ´»çš„ä¸“å®¶æ•°é‡
    moe_type="sparse", # ç¨€ç–MoEæˆ–å…±äº«ä¸“å®¶MoE
)

# ä½¿ç”¨MoE Transformerå—
from src.model.moe import MoETransformerBlock
moe_block = MoETransformerBlock(
    d_model=512,
    n_heads=8,
    d_ff=2048,
    num_experts=8,
    top_k=2,
    moe_type="sparse"
)
```

### LoRAå¾®è°ƒ

```python
# å¯ç”¨LoRA
config.sft.use_lora = True
config.sft.lora_rank = 16
config.sft.lora_alpha = 32
```

### æ··åˆç²¾åº¦è®­ç»ƒ

```python
# å¯ç”¨FP16
config.optimization.use_fp16 = True
```

### åˆ†å¸ƒå¼è®­ç»ƒ

```bash
# å¤šGPUè®­ç»ƒ
python -m torch.distributed.launch --nproc_per_node=4 scripts/train.py
```

### æ•°æ®é›†æ¦‚è§ˆ

æœ¬é¡¹ç›®åŒ…å«å®Œæ•´çš„minimindæ•°æ®é›†ï¼Œä½äº `data/dataset/minimind_dataset/` ç›®å½•ï¼š

| æ–‡ä»¶å | å¤§å° | ç”¨é€” | æè¿° |
|--------|------|------|------|
| `pretrain_hq.jsonl` | 1.6GB | é¢„è®­ç»ƒ | é«˜è´¨é‡ä¸­æ–‡é¢„è®­ç»ƒæ•°æ® |
| `sft_mini_512.jsonl` | 1.1GB | SFTè®­ç»ƒ | ç²¾é€‰SFTå¯¹è¯æ•°æ®ï¼ˆæ¨èï¼‰ |
| `sft_512.jsonl` | 7.0GB | SFTè®­ç»ƒ | å®Œæ•´SFTæ•°æ®é›†ï¼ˆå­—ç¬¦é•¿åº¦<512ï¼‰ |
| `sft_1024.jsonl` | 5.2GB | SFTè®­ç»ƒ | Qwen2.5è’¸é¦å¯¹è¯æ•°æ®ï¼ˆå­—ç¬¦é•¿åº¦<1024ï¼‰ |
| `sft_2048.jsonl` | 8.3GB | SFTè®­ç»ƒ | æ‰©å±•Qwen2.5å¯¹è¯æ•°æ®ï¼ˆå­—ç¬¦é•¿åº¦<2048ï¼‰ |
| `dpo.jsonl` | 867MB | DPOè®­ç»ƒ | RLHFåå¥½æ•°æ® |
| `r1_mix_1024.jsonl` | 351MB | æ¨ç†è®­ç»ƒ | DeepSeek-R1è’¸é¦æ¨ç†æ•°æ® |
| `lora_medical.jsonl` | 33MB | é¢†åŸŸå¾®è°ƒ | åŒ»å­¦é¢†åŸŸQ&Aæ•°æ® |
| `lora_identity.jsonl` | 22KB | èº«ä»½è®­ç»ƒ | è‡ªæˆ‘è®¤çŸ¥æ•°æ® |

#### å¿«é€Ÿå¼€å§‹æ¨è
- **å¿«é€ŸéªŒè¯**: `pretrain_minimal.jsonl` + `pretrain_test.jsonl`
- **æ ‡å‡†è®­ç»ƒ**: `pretrain_hq.jsonl` + `sft_mini_512.jsonl`
- **å®Œæ•´è®­ç»ƒ**: ä½¿ç”¨æ‰€æœ‰æ•°æ®æ–‡ä»¶ï¼ˆ~20GBï¼Œ4B tokensï¼‰

### è‡ªå®šä¹‰æ•°æ®é›†

```jsonl
# é¢„è®­ç»ƒæ•°æ®æ ¼å¼
{"text": "è¿™æ˜¯ä¸€æ®µç”¨äºé¢„è®­ç»ƒçš„æ–‡æœ¬..."}

# SFTæ•°æ®æ ¼å¼
{"conversations": [
    {"role": "user", "content": "é—®é¢˜"},
    {"role": "assistant", "content": "å›ç­”"}
]}

# DPOæ•°æ®æ ¼å¼
{"prompt": "æç¤º", "chosen": "æ›´å¥½çš„å›ç­”", "rejected": "è¾ƒå·®çš„å›ç­”"}
```

## ğŸ“š æ–‡æ¡£èµ„æº

### å®ç”¨æŒ‡å—
- [Macä¼˜åŒ–æŒ‡å—](README_MAC_OPTIMIZED.md) - Macè®¾å¤‡è®­ç»ƒå®Œæ•´æŒ‡å—
- [å¼€å‘ç¬”è®°](CLAUDE.md) - é¡¹ç›®å¼€å‘è¿‡ç¨‹ä¸æ€è€ƒè®°å½•
- [é¡¹ç›®ç»“æ„è¯´æ˜](PROJECT_STRUCTURE.md) - é¡¹ç›®æ–‡ä»¶ç»„ç»‡å’Œæ¨¡å—è¯´æ˜

## ğŸ” æ€§èƒ½å¯¹æ¯”

### æ ‡å‡†Transformeræ¨¡å‹

| é…ç½® | å‚æ•°é‡ | å†…å­˜éœ€æ±‚ | è®­ç»ƒæ—¶é—´ | æ¨èç”¨é€” |
|------|--------|----------|----------|----------|
| Tiny | ~13K | ~0.2MB | 10-20åˆ†é’Ÿ | å¿«é€ŸéªŒè¯/Macä¼˜åŒ– |
| Small | ~66K | ~0.8MB | 30-45åˆ†é’Ÿ | å­¦ä¹ ç ”ç©¶/å°è§„æ¨¡å®éªŒ |
| Medium | ~2.5M | ~30MB | 2-4å°æ—¶ | ä¸­ç­‰è§„æ¨¡è®­ç»ƒ |
| Large | ~25M | ~300MB | æ•°å°æ—¶ | å®Œæ•´æ¨¡å‹è®­ç»ƒ |

### MoEæ¨¡å‹

| é…ç½® | æ€»å‚æ•°é‡ | æ¿€æ´»å‚æ•°é‡ | ä¸“å®¶æ•°é‡ | å†…å­˜éœ€æ±‚ | æ¨èç”¨é€” |
|------|----------|------------|----------|----------|----------|
| MoE-Small | ~200K | ~50K | 8 | ~2MB | MoEæ¶æ„éªŒè¯ |
| MoE-Medium | ~10M | ~2.5M | 16 | ~120MB | ä¸­å‹MoEè®­ç»ƒ |
| MoE-Large | ~100M | ~25M | 32 | ~1.2GB | å¤§å‹MoEè®­ç»ƒ |

### ä¼˜åŒ–å™¨æ€§èƒ½å¯¹æ¯”

| ä¼˜åŒ–å™¨ | å†…å­˜å¼€é”€ | æ”¶æ•›é€Ÿåº¦ | æ¨èåœºæ™¯ |
|--------|----------|----------|----------|
| AdamW | 2xå‚æ•°é‡ | æ ‡å‡† | é€šç”¨è®­ç»ƒ |
| Lion | 1xå‚æ•°é‡ | å¿«é€Ÿ | èµ„æºå—é™ç¯å¢ƒ |
| Sophia | 2xå‚æ•°é‡ | è¶…å¿« | å¤§æ¨¡å‹é¢„è®­ç»ƒ |

## ğŸ§ª æµ‹è¯•å’Œè¯„ä¼°

### è¿è¡Œæµ‹è¯•

```bash
# æ¨¡å‹ç»“æ„æµ‹è¯•
python tests/test_correct_small.py

# ä¸­ç­‰æ¨¡å‹æµ‹è¯•
python tests/test_medium_model.py

# æ¨¡å‹æ£€æŸ¥
python tests/inspect_model.py
```

### æ€§èƒ½è¯„ä¼°

```bash
# ç”Ÿæˆè´¨é‡è¯„ä¼°
python scripts/evaluate.py --model checkpoints/model.pt --dataset test

# æ€§èƒ½åŸºå‡†æµ‹è¯•
python calculate_model_comparison.py
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

### å¼€å‘ç¯å¢ƒè®¾ç½®

```bash
# 1. Forkå¹¶å…‹éš†ä»“åº“
git clone https://github.com/your-username/minigpt-training.git

# 2. åˆ›å»ºå¼€å‘åˆ†æ”¯
git checkout -b feature/your-feature

# 3. å®‰è£…å¼€å‘ä¾èµ–
pip install -e ".[dev]"

# 4. è¿è¡Œæµ‹è¯•
python -m pytest tests/
```

### æäº¤è§„èŒƒ

```bash
# æäº¤ä¿¡æ¯æ ¼å¼
git commit -m "feat: æ·»åŠ æ–°åŠŸèƒ½"
git commit -m "fix: ä¿®å¤bug"
git commit -m "docs: æ›´æ–°æ–‡æ¡£"
```

### è´¡çŒ®ç±»å‹

- ğŸ› **Bugä¿®å¤**: ä¿®å¤å·²çŸ¥é—®é¢˜
- âœ¨ **æ–°åŠŸèƒ½**: æ·»åŠ æ–°çš„è®­ç»ƒæ–¹æ³•æˆ–å·¥å…·
- ğŸ“š **æ–‡æ¡£**: æ”¹è¿›æ–‡æ¡£å’Œç¤ºä¾‹
- ğŸ¨ **ä¼˜åŒ–**: æ€§èƒ½ä¼˜åŒ–å’Œä»£ç é‡æ„
- ğŸ§ª **æµ‹è¯•**: æ·»åŠ æˆ–æ”¹è¿›æµ‹è¯•ç”¨ä¾‹

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT License](LICENSE) å¼€æºåè®®ã€‚

## ğŸ™ è‡´è°¢

æ„Ÿè°¢æ‰€æœ‰ä¸ºæ­¤é¡¹ç›®åšå‡ºè´¡çŒ®çš„å¼€å‘è€…å’Œç ”ç©¶è€…ã€‚

## ğŸ“ æ”¯æŒä¸åé¦ˆ

- **é—®é¢˜åé¦ˆ**: [GitHub Issues](https://github.com/your-repo/minigpt-training/issues)
- **åŠŸèƒ½å»ºè®®**: [GitHub Discussions](https://github.com/your-repo/minigpt-training/discussions)
- **æ–‡æ¡£é—®é¢˜**: æŸ¥çœ‹ [docs/](docs/) ç›®å½•æˆ–æäº¤Issue

---

<div align="center">

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ªStaræ”¯æŒä¸€ä¸‹ï¼ â­**

</div> 