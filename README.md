# Mini-LLM è®­ç»ƒæ¡†æ¶

Mini-LLM æ˜¯ä¸€ä¸ªé¢å‘æ•™å­¦ä¸åŸå‹éªŒè¯çš„å°å‹è¯­è¨€æ¨¡å‹è®­ç»ƒæ¡†æ¶ã€‚ä»£ç å…¨éƒ¨é‡‡ç”¨ PyTorch å®ç°ï¼Œèšç„¦äºè®©å¼€å‘è€…å¿«é€Ÿç†è§£å¹¶å®éªŒ Transformer æ¶æ„ã€æ•°æ®æµæ°´çº¿ã€è®­ç»ƒå¾ªç¯ä»¥åŠ RLHF ç›¸å…³ç»„ä»¶ã€‚

## âœ¨ é¡¹ç›®äº®ç‚¹
- **æ¨¡å—åŒ– Transformer å®ç°**ï¼š`MiniGPTConfig` ä¸ `MiniGPT` æä¾›å¯é…ç½®çš„éšè—å±‚æ•°ã€æ³¨æ„åŠ›å¤´æ•°ã€RoPEã€GQAã€SwiGLUã€MoE ç­‰ç°ä»£ç»„ä»¶ï¼Œä¾¿äºæŒ‰éœ€è£å‰ªæ¨¡å‹è§„æ¨¡ä¸ç‰¹æ€§ã€‚
- **æ•°æ®ä¸åˆ†è¯æ”¯æŒ**ï¼š`ConversationDataLoader`/`PretrainDataLoader`/`DPODataLoader` è¦†ç›–ç›‘ç£å¾®è°ƒã€é¢„è®­ç»ƒä¸åå¥½æ•°æ®ä¸‰ç§å¸¸è§æ ¼å¼ï¼›`TokenizerManager` ä¸ `BPETokenizer` æ”¯æŒä¸­æ–‡å‹å¥½çš„ BPE è®­ç»ƒå’Œç¼“å­˜å¤ç”¨ã€‚
- **è®­ç»ƒå·¥å…·é“¾**ï¼š`PreTrainer` ä¸ `SFTTrainer` ç­‰å°è£…äº†æŸå¤±è®¡ç®—ã€æ¢¯åº¦è£å‰ªã€Cosine è°ƒåº¦å™¨ã€æ··åˆç²¾åº¦ã€æ¢¯åº¦ç´¯ç§¯ç­‰è®­ç»ƒå¸¸è§é€»è¾‘ï¼Œ`MemoryConfig`/`MemoryMonitor` èšç„¦æ˜¾å­˜ä¸å†…å­˜ä¼˜åŒ–ã€‚
- **æ¨ç†ä¸è¯„ä¼°**ï¼š`TextGenerator` æä¾›è´ªå¿ƒã€Top-kã€Top-pã€Beam Search ç­‰ç”Ÿæˆç­–ç•¥ï¼›`benchmarks/performance_benchmark.py` å¯ç”¨äºå¿«é€Ÿè¯„ä¼°ä¸åŒé…ç½®çš„æ€§èƒ½ã€‚
- **RLHF ç®¡é“é›å½¢**ï¼š`RLHFPipeline` ä¸²è”ç›‘ç£å¾®è°ƒã€å¥–åŠ±æ¨¡å‹è®­ç»ƒä¸ PPO ç­–ç•¥ä¼˜åŒ–ï¼Œå±•ç¤º RLHF ç«¯åˆ°ç«¯æµç¨‹çš„å…³é”®æ­¥éª¤ã€‚

## ğŸ“ ä»“åº“ç»“æ„
```
mini-llm/
â”œâ”€â”€ data/                    # ç¤ºä¾‹æ•°æ®åŠé…ç½®
â”œâ”€â”€ docs/                    # é¡¹ç›®æ–‡æ¡£ï¼ˆè§ä¸‹æ–‡ï¼‰
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ benchmarks/          # æ€§èƒ½åŸºå‡†è„šæœ¬
â”‚   â”œâ”€â”€ data/                # æ•°æ®åŠ è½½ä¸åˆ‡åˆ†
â”‚   â”œâ”€â”€ inference/           # æ–‡æœ¬ç”Ÿæˆå·¥å…·
â”‚   â”œâ”€â”€ model/               # æ¨¡å‹ä¸é…ç½®å®ç°
â”‚   â”œâ”€â”€ rl/                  # å¥–åŠ±æ¨¡å‹ä¸ PPO
â”‚   â”œâ”€â”€ tokenizer/           # BPE åˆ†è¯å™¨ä¸ç®¡ç†å™¨
â”‚   â””â”€â”€ training/            # è®­ç»ƒå¾ªç¯ä¸å†…å­˜ä¼˜åŒ–
â”œâ”€â”€ tokenizers/              # å·²è®­ç»ƒåˆ†è¯å™¨ç¼“å­˜
â”œâ”€â”€ utils/                   # é¢„ç•™å·¥å…·æ¨¡å—
â””â”€â”€ test_lightweight_monitor.py
```

## ğŸš€ å¿«é€Ÿå¼€å§‹
1. **å®‰è£…ä¾èµ–**
   ```bash
   git clone https://github.com/your-org/mini-llm.git
   cd mini-llm
   pip install -e .
   ```

2. **è®­ç»ƒæœ€å°ç¤ºä¾‹**ï¼ˆCPU ä¸Šè¿è¡Œä¸€ä¸ª batch æ¼”ç¤ºå®Œæ•´æµç¨‹ï¼‰
   ```python
   from torch.utils.data import DataLoader

   from src.model.config import get_tiny_config
   from src.model.transformer import MiniGPT
   from src.tokenizer.bpe_tokenizer import BPETokenizer
   from src.training.trainer import LanguageModelingDataset, PreTrainer

   texts = ["ä½ å¥½ï¼ŒMini-LLM!", "Transformer æ¶æ„æ¼”ç¤º", "å°æ¨¡å‹ä¹Ÿèƒ½è®­ç»ƒ"]

   tokenizer = BPETokenizer(vocab_size=256)
   tokenizer.train(texts)

   dataset = LanguageModelingDataset(texts, tokenizer, max_length=64)
   dataloader = DataLoader(dataset, batch_size=2)

   config = get_tiny_config()
   model = MiniGPT(config)

   trainer = PreTrainer(model, tokenizer, device="cpu")
   loss = trainer.train_epoch(dataloader)
   print(f"epoch loss: {loss:.4f}")
   ```

3. **æ–‡æœ¬ç”Ÿæˆ**
   ```python
   import torch
   from src.inference.generator import TextGenerator, GenerationConfig

   generator = TextGenerator(model, tokenizer)
   prompt_ids = tokenizer.encode("Mini-LLM", add_special_tokens=True)
   output_ids = generator.sample_generate(
       input_ids=torch.tensor([prompt_ids]),
       config=GenerationConfig(max_length=40, top_p=0.9)
   )
   print(tokenizer.decode(output_ids[0].tolist()))
   ```

## ğŸ“š æ·±å…¥é˜…è¯»
- [docs/README.md](docs/README.md)ï¼šæ–‡æ¡£ç´¢å¼•ä¸é˜…è¯»æŒ‡å¼•
- [docs/getting_started.md](docs/getting_started.md)ï¼šç¯å¢ƒé…ç½®ä¸å®è·µç¤ºä¾‹
- [docs/model.md](docs/model.md)ï¼šæ¨¡å‹ä¸é…ç½®è¯´æ˜
- [docs/data.md](docs/data.md)ï¼šæ•°æ®ä¸åˆ†è¯æµç¨‹
- [docs/training.md](docs/training.md)ï¼šè®­ç»ƒå¾ªç¯ä¸å†…å­˜ä¼˜åŒ–
- [docs/inference.md](docs/inference.md)ï¼šæ¨ç†ç­–ç•¥ä¸é…ç½®
- [docs/rlhf.md](docs/rlhf.md)ï¼šRLHF æµç¨‹æ¦‚è§ˆä¸æ‰©å±•æ€è·¯

æ¬¢è¿åœ¨é˜…è¯»æºç çš„åŒæ—¶é…åˆæ–‡æ¡£ç†è§£æ¯ä¸ªç»„ä»¶çš„èŒè´£ï¼Œä¾¿äºæ ¹æ®è‡ªèº«éœ€æ±‚è¿›è¡Œè£å‰ªæˆ–æ‰©å±•ã€‚
