# Mini-LLM è®­ç»ƒæ¡†æ¶

Mini-LLM æ˜¯ä¸€ä¸ªé¢å‘æ•™å­¦ä¸åŸå‹éªŒè¯çš„å°å‹è¯­è¨€æ¨¡å‹è®­ç»ƒæ¡†æ¶ã€‚ä»£ç å…¨éƒ¨é‡‡ç”¨ PyTorch å®ç°ï¼Œèšç„¦äºè®©å¼€å‘è€…å¿«é€Ÿç†è§£å¹¶å®éªŒ Transformer æ¶æ„ã€æ•°æ®æµæ°´çº¿ã€è®­ç»ƒå¾ªç¯ä»¥åŠ RLHF ç›¸å…³ç»„ä»¶ã€‚

## âœ¨ é¡¹ç›®äº®ç‚¹
- **æ¨¡å—åŒ– Transformer å®ç°**ï¼š`MiniGPTConfig` ä¸ `MiniGPT` æä¾›å¯é…ç½®çš„éšè—å±‚æ•°ã€æ³¨æ„åŠ›å¤´æ•°ã€RoPEã€GQAã€SwiGLUã€MoE ç­‰ç°ä»£ç»„ä»¶ï¼Œä¾¿äºæŒ‰éœ€è£å‰ªæ¨¡å‹è§„æ¨¡ä¸ç‰¹æ€§ã€‚
- **è®­ç»ƒæµæ°´çº¿æŠ½è±¡**ï¼š`training.pipeline` ä¸­çš„ `TrainingEnvironment`ã€`DatasetPreparer`ã€`TrainingLoopRunner`ã€`CheckpointManager` å’Œ `TrainingMonitor` ä¸²è”èµ·è®¾å¤‡åˆå§‹åŒ–ã€æ•°æ®é‡‡æ ·ã€è°ƒåº¦å™¨/ä¼˜åŒ–å™¨ã€éªŒè¯ä¸æ—©åœç­‰æµç¨‹ï¼Œå¯ç›´æ¥é€šè¿‡ `scripts/train.py` å¤ç°å®Œæ•´è®­ç»ƒå›è·¯ã€‚ã€F:src/training/pipeline/app.pyâ€ L25-L162ã€‘ã€F:src/training/pipeline/training_loop.pyâ€ L18-L214ã€‘
- **æ•°æ®ä¸åˆ†è¯æ”¯æŒ**ï¼š`training.datasets` æä¾›è¯­è¨€å»ºæ¨¡ä¸å¯¹è¯ SFT æ•°æ®é›†å®ç°ï¼Œæ”¯æŒè§’è‰²æ ‡è®°ã€æ©ç ç­–ç•¥ä¸è½®æ¬¡æˆªæ–­å¢å¼ºï¼›`TokenizerManager` ç®¡ç†åˆ†è¯å™¨è®­ç»ƒä¸ç¼“å­˜å¤ç”¨ï¼Œé™ä½é‡å¤å¼€é”€ã€‚ã€F:src/training/datasets/conversation.pyâ€ L10-L145ã€‘ã€F:src/training/pipeline/tokenizer_manager.pyâ€ L1-L118ã€‘
- **ç›‘æ§ä¸å®éªŒè¿½è¸ª**ï¼šå¢å¼ºç‰ˆ `TrainingMonitor` è®°å½•è®­ç»ƒ/éªŒè¯æŸå¤±ã€PPLã€ç³»ç»Ÿèµ„æºä¸æ¢¯åº¦å¥åº·æŒ‡æ ‡ï¼Œå¹¶åœ¨è®­ç»ƒç»“æŸè‡ªåŠ¨ç”Ÿæˆ TensorBoard ä¸å¯è§†åŒ–æ‘˜è¦ã€‚ã€F:src/training/training_monitor.pyâ€ L120-L332ã€‘
- **æ¨ç†ä¸è¯„ä¼°**ï¼š`TextGenerator` æä¾›è´ªå¿ƒã€Top-kã€Top-pã€Beam Search ç­‰ç”Ÿæˆç­–ç•¥ï¼›`benchmarks/performance_benchmark.py` å¯ç”¨äºå¿«é€Ÿè¯„ä¼°ä¸åŒé…ç½®çš„æ€§èƒ½ã€‚
- **RLHF ç®¡é“é›å½¢**ï¼š`RLHFPipeline` ä¸²è”ç›‘ç£å¾®è°ƒã€å¥–åŠ±æ¨¡å‹è®­ç»ƒä¸ PPO ç­–ç•¥ä¼˜åŒ–ï¼Œå±•ç¤º RLHF ç«¯åˆ°ç«¯æµç¨‹çš„å…³é”®æ­¥éª¤ã€‚

## ğŸ“ ä»“åº“ç»“æ„
```
mini-llm/
â”œâ”€â”€ data/                    # ç¤ºä¾‹æ•°æ®åŠé…ç½®
â”œâ”€â”€ docs/                    # é¡¹ç›®æ–‡æ¡£ï¼ˆè§ä¸‹æ–‡ï¼‰
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ benchmarks/          # æ€§èƒ½åŸºå‡†è„šæœ¬
â”‚   â”œâ”€â”€ data/                # æ•°æ®åŠ è½½ä¸åˆ‡åˆ†
â”‚   â”œâ”€â”€ inference/           # æ–‡æœ¬ç”Ÿæˆå·¥å…·
â”‚   â”œâ”€â”€ model/               # æ¨¡å‹ä¸é…ç½®å®ç°
â”‚   â”œâ”€â”€ rl/                  # å¥–åŠ±æ¨¡å‹ä¸ PPO
â”‚   â”œâ”€â”€ tokenizer/           # BPE åˆ†è¯å™¨ä¸ç®¡ç†å™¨
â”‚   â””â”€â”€ training/
â”‚       â”œâ”€â”€ datasets/        # é¢„è®­ç»ƒ/SFT æ•°æ®é›†å®ç°
â”‚       â”œâ”€â”€ pipeline/        # è®­ç»ƒç¯å¢ƒã€æ•°æ®ã€è®­ç»ƒå¾ªç¯ä¸ CLI
â”‚       â”œâ”€â”€ memory_optimizer.py
â”‚       â””â”€â”€ trainer.py       # æ•™å­¦ç”¨çš„è½»é‡è®­ç»ƒå™¨
â”œâ”€â”€ tokenizers/              # å·²è®­ç»ƒåˆ†è¯å™¨ç¼“å­˜
â”œâ”€â”€ utils/                   # é¢„ç•™å·¥å…·æ¨¡å—
â””â”€â”€ test_lightweight_monitor.py
```

## ğŸš€ å¿«é€Ÿå¼€å§‹
1. **å®‰è£…ä¾èµ–**
   ```bash
   git clone https://github.com/your-org/mini-llm.git
   cd mini-llm
   pip install -e .
   ```

2. **è¿è¡Œè®­ç»ƒæµæ°´çº¿**ï¼ˆè‡ªåŠ¨åˆ›å»ºè¾“å‡ºç›®å½•ã€é‡‡æ ·æ•°æ®å¹¶ä¿å­˜æ£€æŸ¥ç‚¹ï¼‰
   ```bash
   uv run python scripts/train.py --mode sft --config medium --auto-resume
   ```
   > é€šè¿‡ `--mode pretrain` / `--mode dpo` / `--mode rlhf` åˆ‡æ¢ä¸åŒé˜¶æ®µï¼Œ`--retrain-tokenizer` å¯å¼ºåˆ¶é‡æ–°è®­ç»ƒåˆ†è¯å™¨ã€‚ã€F:scripts/train.pyâ€ L1-L21ã€‘ã€F:src/training/pipeline/cli.pyâ€ L8-L117ã€‘

3. **è®­ç»ƒæœ€å°ç¤ºä¾‹**ï¼ˆä¿ç•™æ•™å­¦ç”¨é€”ï¼Œä¾¿äºç†è§£åŸºç¡€è®­ç»ƒå¾ªç¯ï¼‰
   ```python
   from torch.utils.data import DataLoader

   from src.model.config import get_tiny_config
   from src.model.transformer import MiniGPT
   from src.tokenizer.bpe_tokenizer import BPETokenizer
   from src.training.datasets import LanguageModelingDataset
   from src.training.trainer import PreTrainer

   texts = ["ä½ å¥½ï¼ŒMini-LLM!", "Transformer æ¶æ„æ¼”ç¤º", "å°æ¨¡å‹ä¹Ÿèƒ½è®­ç»ƒ"]

   tokenizer = BPETokenizer(vocab_size=256)
   tokenizer.train(texts)

   dataset = LanguageModelingDataset(texts, tokenizer, max_length=64)
   dataloader = DataLoader(dataset, batch_size=2)

   config = get_tiny_config()
   model = MiniGPT(config)

   trainer = PreTrainer(model, tokenizer, device="cpu")
   loss = trainer.train_epoch(dataloader)
   print(f"epoch loss: {loss:.4f}")
   ```

4. **æ–‡æœ¬ç”Ÿæˆ**
   ```python
   import torch
   from src.inference.generator import TextGenerator, GenerationConfig

   generator = TextGenerator(model, tokenizer)
   prompt_ids = tokenizer.encode("Mini-LLM", add_special_tokens=True)
   output_ids = generator.sample_generate(
       input_ids=torch.tensor([prompt_ids]),
       config=GenerationConfig(max_length=40, top_p=0.9)
   )
   print(tokenizer.decode(output_ids[0].tolist()))
   ```

## ğŸ“š æ·±å…¥é˜…è¯»
- [docs/README.md](docs/README.md)ï¼šæ–‡æ¡£ç´¢å¼•ä¸é˜…è¯»æŒ‡å¼•
- [docs/getting_started.md](docs/getting_started.md)ï¼šç¯å¢ƒé…ç½®ä¸å®è·µç¤ºä¾‹
- [docs/model.md](docs/model.md)ï¼šæ¨¡å‹ä¸é…ç½®è¯´æ˜
- [docs/data.md](docs/data.md)ï¼šæ•°æ®ä¸åˆ†è¯æµç¨‹
- [docs/training.md](docs/training.md)ï¼šè®­ç»ƒå¾ªç¯ä¸å†…å­˜ä¼˜åŒ–
- [docs/inference.md](docs/inference.md)ï¼šæ¨ç†ç­–ç•¥ä¸é…ç½®
- [docs/rlhf.md](docs/rlhf.md)ï¼šRLHF æµç¨‹æ¦‚è§ˆä¸æ‰©å±•æ€è·¯

æ¬¢è¿åœ¨é˜…è¯»æºç çš„åŒæ—¶é…åˆæ–‡æ¡£ç†è§£æ¯ä¸ªç»„ä»¶çš„èŒè´£ï¼Œä¾¿äºæ ¹æ®è‡ªèº«éœ€æ±‚è¿›è¡Œè£å‰ªæˆ–æ‰©å±•ã€‚
