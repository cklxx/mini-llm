# æ¢¯åº¦æ¶ˆå¤±é—®é¢˜å®Œæ•´åˆ†æ

## ğŸš¨ æ£€æµ‹åˆ°çš„å¼‚å¸¸

```
âš ï¸ Anomaly detected at step 14: gradient_vanishing
è§¦å‘æ¡ä»¶: grad_norm < 1e-6
```

---

## âœ… å¥½æ¶ˆæ¯ï¼šä½ çš„æ¨¡å‹å·²æœ‰å®Œæ•´ä¿æŠ¤ï¼

æ£€æŸ¥ä»£ç å‘ç°ï¼Œä½ çš„Transformeræ¨¡å‹**å·²å®ç°äº†æ‰€æœ‰ä¸»æµçš„æ¢¯åº¦æ¶ˆå¤±é˜²æŠ¤æœºåˆ¶**ï¼š

### 1. âœ… æ®‹å·®è¿æ¥ (Residual Connections)
```python
# src/model/transformer.py:249-250, 256-257
x = x + self.dropout(attn_output)  # æ®‹å·®è¿æ¥1
x = x + self.dropout(ff_output)     # æ®‹å·®è¿æ¥2
```

**ä½œç”¨**: åˆ›å»ºæ¢¯åº¦é«˜é€Ÿå…¬è·¯ï¼Œå…è®¸æ¢¯åº¦ç›´æ¥ä¼ æ’­
```
æ¢¯åº¦æµ: âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚output Ã— (1 + âˆ‚attention/âˆ‚x)
                               â†‘ å…³é”®ï¼šå§‹ç»ˆæœ‰1ï¼Œä¿è¯æ¢¯åº¦ä¸ä¼šå®Œå…¨æ¶ˆå¤±
```

### 2. âœ… Pre-Normæ¶æ„ (Layer Normalization First)
```python
# src/model/transformer.py:236-237, 253
normalized_x = self.norm1(x)  # å…ˆå½’ä¸€åŒ–
attn_output = self.attention(normalized_x, ...)

normalized_x = self.norm2(x)  # å…ˆå½’ä¸€åŒ–
ff_output = self.feed_forward(normalized_x)
```

**ä½œç”¨**:
- ç¨³å®šæ¢¯åº¦ä¼ æ’­
- é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±
- Pre-Normæ¯”Post-Normæ›´ç¨³å®šï¼ˆGPT-3, LLaMAç­‰éƒ½ç”¨Pre-Normï¼‰

### 3. âœ… RMSNorm (ç°ä»£å½’ä¸€åŒ–å±‚)
```python
# src/model/transformer.py:15-36
class RMSNorm(nn.Module):
    """Root Mean Square Layer Normalization
    æ¯”LayerNormæ›´é«˜æ•ˆï¼Œæ¢¯åº¦æ›´ç¨³å®š
    """
```

**ä¼˜åŠ¿**:
- è®¡ç®—æ›´ç®€å•é«˜æ•ˆ
- æ¢¯åº¦æ›´ç¨³å®š
- Meta LLaMAã€Google PaLMç­‰å¤§æ¨¡å‹éƒ½ä½¿ç”¨RMSNorm

### 4. âœ… SwiGLUæ¿€æ´»å‡½æ•°
```python
# src/model/transformer.py:107-133
class SwiGLUFeedForward:
    output = F.silu(gate) * up  # Swishæ¿€æ´»
```

**ä¼˜åŠ¿**:
- Swish/SiLUæ— é¥±å’ŒåŒºï¼ˆä¸åƒSigmoid/Tanhä¼šé¥±å’Œï¼‰
- å¹³æ»‘å¯å¯¼ï¼Œæ¢¯åº¦æµç•…
- PaLMã€LLaMAç­‰æ¨¡å‹æ ‡é…

### 5. âœ… æ¢¯åº¦è£å‰ª (Gradient Clipping)
```python
# scripts/train.py:407, 411
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

**ä½œç”¨**: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼ŒåŒæ—¶ä¿æŠ¤æ¢¯åº¦æ¶ˆå¤±

---

## ğŸ” é‚£ä¸ºä»€ä¹ˆè¿˜ä¼šè§¦å‘gradient_vanishingè­¦å‘Šï¼Ÿ

### å¯èƒ½åŸå› åˆ†æ

#### 1. ğŸŸ¡ æ£€æµ‹é˜ˆå€¼è¿‡äºä¸¥æ ¼
```python
# src/training/training_monitor.py:176
if grad_norm < 1e-6:  # 0.000001
    status = 'gradient_vanishing'
```

**åˆ†æ**:
- é˜ˆå€¼ `1e-6` éå¸¸å°
- **Step 14** æ˜¯è®­ç»ƒåˆæœŸï¼Œæ¢¯åº¦å¯èƒ½è¿˜åœ¨ç¨³å®šä¸­
- è¿™å¯èƒ½åªæ˜¯**ç¬æ—¶æ³¢åŠ¨**ï¼Œä¸æ˜¯çœŸæ­£çš„æ¢¯åº¦æ¶ˆå¤±

#### 2. ğŸŸ¢ è®­ç»ƒåˆæœŸæ­£å¸¸ç°è±¡
```python
è®­ç»ƒæ­¥éª¤è¿›åº¦:
Step 1-10:   æ¨¡å‹å‚æ•°åˆå§‹åŒ–ï¼Œæ¢¯åº¦ä¸ç¨³å®š
Step 10-50:  æ¢¯åº¦é€æ¸ç¨³å®š  â† ä½ åœ¨è¿™é‡Œ
Step 50+:    æ¢¯åº¦æ­£å¸¸
```

#### 3. ğŸŸ¡ å­¦ä¹ ç‡warmupæœŸé—´
ä½ çš„é…ç½®ä½¿ç”¨äº†warmup:
```python
# config/training_config.py:159
warmup_steps = 4000  # 4000æ­¥warmup
```

åœ¨warmupæœŸé—´ï¼Œå­¦ä¹ ç‡ä»0é€æ¸å¢åŠ åˆ°ç›®æ ‡å€¼ï¼Œæ¢¯åº¦å¯èƒ½ä¼šè¾ƒå°ã€‚

---

## ğŸ“Š éªŒè¯æ˜¯å¦ä¸ºçœŸæ­£çš„æ¢¯åº¦æ¶ˆå¤±

### æ£€æŸ¥1: æŸ¥çœ‹å®Œæ•´è®­ç»ƒæ—¥å¿—
```bash
# æŸ¥çœ‹Step 14å‰åçš„æ¢¯åº¦å˜åŒ–
grep "grad_norm" logs/training.log | head -30

# æˆ–æŸ¥çœ‹TensorBoard
tensorboard --logdir=checkpoints/medium_*/monitor_logs
```

**æ­£å¸¸æ¨¡å¼**:
```
Step 10: grad_norm=0.0001
Step 11: grad_norm=0.000008  â† å¶å°”æ³¢åŠ¨
Step 12: grad_norm=0.0002
Step 13: grad_norm=0.0003
Step 14: grad_norm=0.0000005 â† è§¦å‘è­¦å‘Š
Step 15: grad_norm=0.0002    â† æ¢å¤æ­£å¸¸
```

**çœŸæ­£æ¢¯åº¦æ¶ˆå¤±**:
```
Step 10: grad_norm=0.01
Step 20: grad_norm=0.001
Step 30: grad_norm=0.0001
Step 40: grad_norm=0.00001
Step 50: grad_norm=0.000001  â† æŒç»­ä¸‹é™
```

### æ£€æŸ¥2: è§‚å¯ŸLosså˜åŒ–
```bash
# å¦‚æœLossæŒç»­ä¸‹é™ï¼Œè¯´æ˜è®­ç»ƒæ­£å¸¸
grep "Loss:" logs/training.log | tail -20
```

**æ­£å¸¸**: LossæŒç»­ä¸‹é™
```
Step 10: Loss: 8.2345
Step 20: Loss: 7.8901
Step 30: Loss: 7.4321  â† æŒç»­æ”¹å–„
```

**æ¢¯åº¦æ¶ˆå¤±**: Lossåœæ­¢ä¸‹é™
```
Step 10: Loss: 8.2345
Step 20: Loss: 8.2340
Step 30: Loss: 8.2342  â† å‡ ä¹ä¸å˜
```

### æ£€æŸ¥3: ç›‘æ§å‚æ•°æ›´æ–°
```python
# åœ¨è®­ç»ƒè„šæœ¬ä¸­æ·»åŠ 
if step % 10 == 0:
    total_norm = 0
    for p in model.parameters():
        if p.grad is not None:
            param_norm = p.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
    total_norm = total_norm ** 0.5
    print(f"Step {step}: Total Grad Norm = {total_norm:.6f}")
```

---

## ğŸ› ï¸ è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: è°ƒæ•´æ£€æµ‹é˜ˆå€¼ (æ¨è)
å½“å‰é˜ˆå€¼å¯èƒ½è¿‡äºæ•æ„Ÿï¼š

```python
# ä¿®æ”¹ src/training/training_monitor.py:176
# ä» 1e-6 æ”¹ä¸º 1e-8
if grad_norm < 1e-8:  # æ›´å®½æ¾çš„é˜ˆå€¼
    status = 'gradient_vanishing'
```

### æ–¹æ¡ˆ2: å¢åŠ warmupæ­¥æ•°
```python
# config/training_config.py:159
# ä» 4000 æ”¹ä¸º 8000
warmup_steps = 8000  # æ›´é•¿çš„warmupï¼Œæ¢¯åº¦æ›´å¹³ç¨³
```

### æ–¹æ¡ˆ3: è°ƒæ•´å­¦ä¹ ç‡
```python
# config/training_config.py:157
# ç¨å¾®æé«˜å­¦ä¹ ç‡
learning_rate = 5e-4  # ä»3e-4æé«˜åˆ°5e-4
```

### æ–¹æ¡ˆ4: å¯ç”¨æ¢¯åº¦ç¼©æ”¾
```python
# scripts/train.py (å·²å¯ç”¨æ··åˆç²¾åº¦)
# ç¡®ä¿ä½¿ç”¨GradScaler
if self.config.mixed_precision:
    scaler = torch.cuda.amp.GradScaler()
```

### æ–¹æ¡ˆ5: ç›‘æ§å¹¶å¿½ç•¥åˆæœŸå¼‚å¸¸
å¦‚æœä»…åœ¨è®­ç»ƒåˆæœŸå‡ºç°ï¼Œå¯ä»¥æ·»åŠ å¿½ç•¥é€»è¾‘ï¼š

```python
# src/training/training_monitor.py:176
# æ·»åŠ æ­¥æ•°æ£€æŸ¥
if grad_norm < 1e-6 and step > 100:  # ä»…åœ¨100æ­¥åæ£€æµ‹
    status = 'gradient_vanishing'
```

---

## ğŸ¯ ç«‹å³è¡ŒåŠ¨å»ºè®®

### 1. ä¸è¦æƒŠæ…Œ âœ…
- ä½ çš„æ¨¡å‹æ¶æ„éå¸¸å¥åº·
- å·²æœ‰å®Œæ•´çš„æ¢¯åº¦ä¿æŠ¤æœºåˆ¶
- Step 14æ˜¯è®­ç»ƒåˆæœŸï¼Œæ³¢åŠ¨æ­£å¸¸

### 2. ç»§ç»­è®­ç»ƒè§‚å¯Ÿ
```bash
# è®©è®­ç»ƒç»§ç»­è¿è¡Œåˆ°Step 100+
# è§‚å¯Ÿæ¢¯åº¦æ˜¯å¦ç¨³å®š
python scripts/train.py --mode pretrain --config medium
```

### 3. æ£€æŸ¥è®­ç»ƒæŒ‡æ ‡
```bash
# æŸ¥çœ‹TensorBoard
tensorboard --logdir=checkpoints/pretrain_medium/monitor_logs

# å…³æ³¨ä»¥ä¸‹æŒ‡æ ‡:
# - Lossæ˜¯å¦ä¸‹é™
# - Grad Normæ˜¯å¦ç¨³å®šåœ¨æ­£å¸¸èŒƒå›´(1e-4åˆ°1e-2)
# - Learning Rateæ˜¯å¦æ­£å¸¸å¢é•¿(warmupæœŸé—´)
```

### 4. å¦‚æœæŒç»­å‡ºç°
ä»…åœ¨ä»¥ä¸‹æƒ…å†µæ‰éœ€è¦å¹²é¢„ï¼š
- [ ] è­¦å‘Š**æŒç»­**å‡ºç°ï¼ˆæ¯æ­¥éƒ½è§¦å‘ï¼‰
- [ ] Loss**åœæ­¢ä¸‹é™**
- [ ] Grad Norm**æŒç»­<1e-6**ï¼ˆä¸æ˜¯å¶å°”ï¼‰
- [ ] è®­ç»ƒ**100æ­¥å**ä»ç„¶é¢‘ç¹è§¦å‘

---

## ğŸ“ˆ é¢„æœŸè¡Œä¸º

### æ­£å¸¸è®­ç»ƒæ›²çº¿
```
Step Range     | Grad Norm Range | çŠ¶æ€
---------------|-----------------|-------
1-50           | 1e-5 to 1e-3    | åˆæœŸæ³¢åŠ¨ï¼ˆæ­£å¸¸ï¼‰
50-500         | 1e-4 to 1e-2    | é€æ¸ç¨³å®š
500-5000       | 1e-3 to 1e-2    | ç¨³å®šè®­ç»ƒ
5000+          | 1e-3 to 5e-3    | æ”¶æ•›é˜¶æ®µ
```

### ä½ çš„é…ç½®é¢„æµ‹
```python
Model: Medium (16 layers, 512 hidden)
æ¶æ„: Pre-Norm + RMSNorm + SwiGLU + Residual
é¢„æœŸ: æ¢¯åº¦åº”ç¨³å®šåœ¨ 1e-3 åˆ° 1e-2 èŒƒå›´
```

---

## ğŸ”¬ é«˜çº§è¯Šæ–­å·¥å…·

### åˆ›å»ºæ¢¯åº¦è¯Šæ–­è„šæœ¬
```bash
# è¿è¡Œæ¢¯åº¦è¯Šæ–­
python scripts/optimize_memory.py --analyze

# æŸ¥çœ‹é€å±‚æ¢¯åº¦
python -c "
import torch
checkpoint = torch.load('checkpoints/pretrain_medium/checkpoint_step_100.pt')
for name, param in checkpoint['model_state_dict'].items():
    if 'weight' in name:
        print(f'{name}: {param.norm():.6f}')
"
```

### ç›‘æ§å»ºè®®
```bash
# å®æ—¶ç›‘æ§GPUå’Œæ¢¯åº¦
watch -n 1 'nvidia-smi && tail -5 logs/training.log'
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

### è®ºæ–‡
1. **Residual Networks** (He et al., 2015)
   - "Deep Residual Learning for Image Recognition"

2. **Pre-Norm Transformers** (Xiong et al., 2020)
   - "On Layer Normalization in the Transformer Architecture"

3. **RMSNorm** (Zhang & Sennrich, 2019)
   - "Root Mean Square Layer Normalization"

4. **SwiGLU** (Shazeer, 2020)
   - "GLU Variants Improve Transformer"

### æœ€ä½³å®è·µ
- **GPT-3**: Pre-Norm + Residual
- **LLaMA**: Pre-Norm + RMSNorm + SwiGLU
- **PaLM**: Pre-Norm + RMSNorm + SwiGLU

ä½ çš„æ¨¡å‹éµå¾ªäº†æ‰€æœ‰è¿™äº›æœ€ä½³å®è·µï¼âœ…

---

## âœ… æ€»ç»“

| é—®é¢˜ | çŠ¶æ€ | è¯´æ˜ |
|------|------|------|
| æ˜¯å¦çœŸæ­£çš„æ¢¯åº¦æ¶ˆå¤±ï¼Ÿ | â“ å¾…è§‚å¯Ÿ | Step 14å¤ªæ—©ï¼Œéœ€è¦è§‚å¯Ÿåç»­ |
| æ¨¡å‹æ¶æ„æ˜¯å¦å¥åº·ï¼Ÿ | âœ… ä¼˜ç§€ | Pre-Norm + RMSNorm + Residual + SwiGLU |
| æ˜¯å¦éœ€è¦ç«‹å³ä¿®æ”¹ï¼Ÿ | âŒ ä¸éœ€è¦ | ç»§ç»­è®­ç»ƒè§‚å¯Ÿ |
| é˜ˆå€¼æ˜¯å¦åˆç†ï¼Ÿ | ğŸŸ¡ åä¸¥æ ¼ | å»ºè®®ä»1e-6æ”¹ä¸º1e-8 |
| ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼Ÿ | âœ… ç»§ç»­è®­ç»ƒ | è§‚å¯Ÿåˆ°Step 100+ |

**ç»“è®º**: ä½ çš„æ¨¡å‹é…ç½®éå¸¸å¥åº·ï¼ŒStep 14çš„è­¦å‘Šå¾ˆå¯èƒ½æ˜¯è®­ç»ƒåˆæœŸçš„æ­£å¸¸æ³¢åŠ¨ã€‚**å»ºè®®ç»§ç»­è®­ç»ƒå¹¶è§‚å¯Ÿ**ã€‚

---

**åˆ›å»ºæ—¶é—´**: 2025-10-08
**é€‚ç”¨ç‰ˆæœ¬**: MiniGPT Training v0.1.0
**æ¨¡å‹é…ç½®**: Medium (16 layers, 512 hidden, Pre-Norm + RMSNorm)
