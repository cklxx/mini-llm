# 第二章：预训练篇 - 语言建模的统计学习

## 引言：从统计语言模型到神经语言模型的演进

预训练是大语言模型能力的根基，其核心思想是在大规模无标注文本上学习语言的统计规律。从 n-gram 模型到神经网络模型，语言建模经历了从离散统计到连续表示的根本性转变。

## 2.1 语言建模的概率论基础

### 2.1.1 自回归语言模型的数学定义

给定长度为 $T$ 的序列 $x_{1:T} = (x_1, x_2, ..., x_T)$，自回归语言模型通过链式法则分解联合概率：

```math
P(x_{1:T}) = \prod_{t=1}^{T} P(x_t | x_{<t})
```

其中 $x_{<t} = (x_1, x_2, ..., x_{t-1})$ 表示前 $t-1$ 个 token 的历史上下文。

**神经语言模型的参数化：**

Transformer 通过神经网络参数 $\theta$ 建模条件概率：

```math
P_\theta(x_t | x_{<t}) = \text{softmax}(f_\theta(x_{<t}))_{x_t}
```

其中 $f_\theta(x_{<t}) \in \mathbb{R}^{|V|}$ 是模型输出的 logits 向量，$|V|$ 是词汇表大小。

### 2.1.2 最大似然估计与交叉熵损失

**最大似然估计目标：**

给定训练语料 $\mathcal{D} = \{x^{(i)}_{1:T_i}\}_{i=1}^N$，我们希望最大化对数似然：

```math
\mathcal{L}_{MLE}(\theta) = \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^{T_i} \log P_\theta(x^{(i)}_t | x^{(i)}_{<t})
```

**交叉熵损失推导：**

最大似然等价于最小化交叉熵损失：

```math
\mathcal{L}_{CE}(\theta) = -\frac{1}{N} \sum_{i=1}^N \sum_{t=1}^{T_i} \log P_\theta(x^{(i)}_t | x^{(i)}_{<t})
```

**代码实现分析** (`src/training/trainer.py:115-125`)：

```python
def compute_loss(self, logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
    """计算语言模型损失"""
    # 展平张量
    logits = logits.reshape(-1, logits.size(-1))  # (batch_size * seq_len, vocab_size)
    labels = labels.reshape(-1)                   # (batch_size * seq_len,)
    
    # 忽略PAD token
    loss_fn = nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_id)
    loss = loss_fn(logits, labels)
    
    return loss
```

**数学分析：**
- `CrossEntropyLoss` 内部实现了 log-softmax 和负对数似然的计算
- `ignore_index` 确保 PAD token 不参与损失计算
- 批量计算提高训练效率

### 2.1.3 困惑度的信息论意义

**困惑度定义：**

困惑度（Perplexity）是交叉熵损失的指数：

```math
\text{PPL} = \exp\left(\mathcal{L}_{CE}\right) = \exp\left(-\frac{1}{T}\sum_{t=1}^{T} \log P_\theta(x_t | x_{<t})\right)
```

**信息论解释：**

1. **平均分支因子**：困惑度表示模型在每个位置平均"困惑"于多少个选择
2. **编码长度**：$\log_2(\text{PPL})$ 是每个 token 的平均编码长度（以 bit 为单位）
3. **不确定性度量**：困惑度越低，模型对下一个 token 的预测越确定

**理想困惑度分析：**
- 随机模型：$\text{PPL} = |V|$（词汇表大小）
- 完美模型：$\text{PPL} = 1$
- 实际模型：通常在 10-100 之间

## 2.2 因果掩码与自注意力约束

### 2.2.1 下三角掩码矩阵的数学表示

为了实现自回归约束，我们需要确保位置 $i$ 只能看到位置 $j \leq i$ 的信息。这通过因果掩码矩阵实现：

```math
M_{ij} = \begin{cases}
0 & \text{if } i < j \\
-\infty & \text{if } i \geq j
\end{cases}
```

**注意力计算中的应用：**

```math
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T + M}{\sqrt{d_k}}\right)V
```

其中掩码矩阵 $M$ 被加到注意力分数上。

**代码实现分析** (`src/model/transformer.py:243-249`)：

```python
def create_causal_mask(self, seq_len: int) -> torch.Tensor:
    """创建因果掩码（下三角矩阵）
    
    防止模型在预测时看到未来的token
    """
    mask = torch.tril(torch.ones(seq_len, seq_len))
    return mask  # 1表示可见，0表示掩码
```

**掩码应用** (`src/model/transformer.py:88-93`)：

```python
# 应用掩码（如果有）
if mask is not None:
    # 扩展mask维度以匹配scores
    if mask.dim() == 3:  # (batch_size, seq_len, seq_len)
        mask = mask.unsqueeze(1)  # (batch_size, 1, seq_len, seq_len)
    scores = scores.masked_fill(mask == 0, -1e9)  # 使用-1e9而非-inf避免数值问题
```

### 2.2.2 自回归约束对注意力模式的影响

**注意力模式分析：**

1. **局部性偏好**：相邻位置的注意力权重通常较高
2. **衰减效应**：随着距离增加，注意力权重一般衰减
3. **位置敏感性**：不同位置的注意力模式差异显著

**数学建模：**

设 $A_{ij}$ 为位置 $i$ 对位置 $j$ 的注意力权重，经验观察表明：

```math
A_{ij} \propto \exp\left(-\alpha |i-j|^\beta\right) \cdot \text{sim}(h_i, h_j)
```

其中 $\alpha, \beta$ 是位置衰减参数，$\text{sim}(h_i, h_j)$ 是语义相似度。

### 2.2.3 训练与推理时的一致性保证

**训练时的并行计算：**

训练时，整个序列同时输入模型，通过因果掩码确保每个位置只能看到历史信息：

```python
# 训练时：并行计算所有位置
logits = model(input_ids)  # (batch_size, seq_len, vocab_size)
```

**推理时的递归生成：**

推理时，逐个生成 token，保持与训练时的因果约束一致：

```python
# 推理时：逐步生成
for _ in range(max_length):
    logits = model(input_ids)
    next_token = sample(logits[:, -1, :])  # 只使用最后一个位置的输出
    input_ids = torch.cat([input_ids, next_token], dim=1)
```

**数学一致性证明：**

训练时位置 $t$ 的预测：
```math
P_{\text{train}}(x_t | x_{<t}) = \text{softmax}(f_\theta(x_{1:t-1}))_{x_t}
```

推理时位置 $t$ 的预测：
```math
P_{\text{infer}}(x_t | x_{<t}) = \text{softmax}(f_\theta(x_{1:t-1}))_{x_t}
```

因果掩码确保两者完全一致。

## 2.3 预训练数据与分词策略

### 2.3.1 BPE 算法的信息压缩原理

**Byte Pair Encoding (BPE) 算法：**

BPE 是一种基于频率的子词分割算法，其核心思想是迭代合并最频繁的字节对。

**算法步骤：**

1. **初始化**：将文本分解为字符序列
2. **统计频率**：计算所有相邻字符对的出现频率
3. **合并操作**：将最频繁的字符对合并为新的子词
4. **迭代更新**：重复步骤 2-3 直到达到目标词汇表大小

**数学建模：**

设 $C$ 为字符集合，$S = (c_1, c_2, ..., c_n)$ 为字符序列。BPE 的目标是找到最优分割：

```math
\arg\min_{segmentation} \sum_{i} -\log P(w_i)
```

其中 $P(w_i)$ 是子词 $w_i$ 的频率。

**代码实现分析** (`src/tokenizer/bpe_tokenizer.py`)：

```python
def train(self, texts: List[str]):
    """训练BPE分词器"""
    # 1. 初始化词汇表（字符级别）
    vocab = set()
    for text in texts:
        vocab.update(text)
    
    # 2. 迭代合并最频繁的字节对
    for _ in range(self.vocab_size - len(vocab)):
        pairs = self._get_pairs(texts)
        if not pairs:
            break
        
        best_pair = max(pairs, key=pairs.get)
        texts = self._merge_vocab(texts, best_pair)
        vocab.add(''.join(best_pair))
```

### 2.3.2 词汇表大小对模型性能的影响

**权衡分析：**

1. **词汇表过小**：
   - 优势：参数量少，训练快速
   - 劣势：序列长度增加，长程依赖难以建模

2. **词汇表过大**：
   - 优势：序列紧凑，信息密度高
   - 劣势：参数量大，稀有词训练不充分

**数学分析：**

设词汇表大小为 $V$，平均子词长度为 $\ell$，则：

- **参数开销**：嵌入层和输出层参数量为 $2VD$
- **序列长度**：平均序列长度为 $T/\ell$
- **计算复杂度**：自注意力复杂度为 $O((T/\ell)^2)$

**最优词汇表大小：**

经验上，词汇表大小通常选择为：
- 小模型：10K-30K
- 中等模型：30K-50K  
- 大模型：50K-100K

### 2.3.3 数据预处理与批处理优化

**数据处理流程：**

**代码实现** (`src/training/trainer.py:16-50`)：

```python
class LanguageModelingDataset(Dataset):
    def __init__(self, texts: List[str], tokenizer, max_length: int = 512):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        
        # 编码文本
        token_ids = self.tokenizer.encode(text, add_special_tokens=True)
        
        # 截断或填充到固定长度
        if len(token_ids) > self.max_length:
            token_ids = token_ids[:self.max_length]
        else:
            token_ids.extend([self.tokenizer.pad_id] * (self.max_length - len(token_ids)))
        
        return torch.tensor(token_ids, dtype=torch.long)
```

**批处理策略：**

1. **静态批处理**：固定最大长度，填充短序列
2. **动态批处理**：按长度分组，减少填充浪费
3. **梯度累积**：模拟大批次训练，节省内存

**内存优化技巧：**

```python
# 梯度累积实现大批次效果
accumulation_steps = 4
for i, batch in enumerate(dataloader):
    loss = model(batch) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

## 2.4 优化算法与学习率调度

### 2.4.1 AdamW 优化器的自适应机制

**Adam 算法回顾：**

Adam 结合了动量和自适应学习率：

```math
\begin{align}
m_t &= \beta_1 m_{t-1} + (1-\beta_1) g_t \\
v_t &= \beta_2 v_{t-1} + (1-\beta_2) g_t^2 \\
\hat{m}_t &= \frac{m_t}{1-\beta_1^t} \\
\hat{v}_t &= \frac{v_t}{1-\beta_2^t} \\
\theta_t &= \theta_{t-1} - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
\end{align}
```

**AdamW 的权重衰减修正：**

传统 Adam 中的 L2 正则化存在问题，AdamW 将权重衰减与梯度更新解耦：

```math
\theta_t = \theta_{t-1} - \alpha \left(\frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \theta_{t-1}\right)
```

其中 $\lambda$ 是权重衰减系数。

**代码实现** (`src/training/trainer.py:106`)：

```python
# 优化器配置
self.optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)
```

**超参数选择：**
- 学习率：$10^{-4}$ 到 $10^{-3}$
- $\beta_1 = 0.9$，$\beta_2 = 0.999$
- 权重衰减：$0.01$ 到 $0.1$

### 2.4.2 Cosine Annealing 调度的收敛分析

**余弦退火公式：**

```math
\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos\left(\frac{t}{T}\pi\right)\right)
```

其中 $\eta_{\max}$ 是初始学习率，$\eta_{\min}$ 是最小学习率，$T$ 是总训练步数。

**收敛性质：**

1. **平滑衰减**：避免学习率突变导致的震荡
2. **周期重启**：可以通过重启避免局部最优
3. **细粒度调整**：在训练后期进行精细优化

**代码实现** (`src/training/trainer.py:109`)：

```python
# 学习率调度器
self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=1000)
```

### 2.4.3 梯度裁剪防止梯度爆炸

**梯度裁剪原理：**

当梯度范数超过阈值时，将其缩放到阈值大小：

```math
g \leftarrow \begin{cases}
g & \text{if } ||g|| \leq \tau \\
\frac{\tau}{||g||} g & \text{if } ||g|| > \tau
\end{cases}
```

其中 $\tau$ 是裁剪阈值。

**代码实现** (`src/training/trainer.py:176`)：

```python
# 梯度裁剪
torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
```

**数学分析：**

梯度裁剪保证了更新步长的上界：
```math
||\Delta \theta|| = ||\alpha g|| \leq \alpha \tau
```

这防止了单次更新过大导致的训练不稳定。

## 2.5 预训练的训练循环与监控

### 2.5.1 完整的训练流程

**代码实现分析** (`src/training/trainer.py:127-206`)：

```python
def train_epoch(self, dataloader: DataLoader) -> float:
    """训练一个epoch"""
    self.model.train()
    total_loss = 0
    num_batches = 0
    
    progress_bar = tqdm(dataloader, desc="预训练")
    
    for batch_idx, batch in enumerate(progress_bar):
        # 数据准备
        if isinstance(batch, dict):
            input_ids = batch['input_ids'].to(self.device)
            labels = batch['labels'].to(self.device)
        else:
            input_ids = batch.to(self.device)
            # 对于预训练，标签就是输入向右移动一位
            labels = torch.cat([input_ids[:, 1:], 
                              torch.full((input_ids.size(0), 1), 
                                       self.tokenizer.pad_id, device=self.device)], dim=1)
        
        # 前向传播
        logits = self.model(input_ids)
        loss = self.compute_loss(logits, labels)
        
        # 反向传播
        self.optimizer.zero_grad()
        loss.backward()
        
        # 梯度裁剪
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        
        self.optimizer.step()
        self.scheduler.step()
        
        total_loss += loss.item()
        num_batches += 1
        
        # 更新进度条
        progress_bar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'lr': f'{self.scheduler.get_last_lr()[0]:.6f}'
        })
    
    return total_loss / num_batches
```

**关键技术细节：**

1. **标签构造**：预训练时标签是输入序列向右移动一位
2. **设备管理**：确保数据和模型在同一设备上
3. **梯度处理**：清零、反向传播、裁剪、更新的标准流程
4. **学习率更新**：每个 batch 后更新学习率

### 2.5.2 验证与早停策略

**验证流程** (`src/training/trainer.py:208-231`)：

```python
def validate(self, dataloader: DataLoader) -> float:
    """验证"""
    self.model.eval()
    total_loss = 0
    num_batches = 0
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="验证"):
            # 数据处理（与训练相同）
            if isinstance(batch, dict):
                input_ids = batch['input_ids'].to(self.device)
                labels = batch['labels'].to(self.device)
            else:
                input_ids = batch.to(self.device)
                labels = torch.cat([input_ids[:, 1:], 
                                  torch.full((input_ids.size(0), 1), 
                                           self.tokenizer.pad_id, device=self.device)], dim=1)
            
            logits = self.model(input_ids)
            loss = self.compute_loss(logits, labels)
            
            total_loss += loss.item()
            num_batches += 1
    
    return total_loss / num_batches
```

**早停策略：**

```python
# 保存最佳模型
if val_loss < best_val_loss:
    best_val_loss = val_loss
    self.save_checkpoint(os.path.join(save_dir, "best_model.pt"))
    patience_counter = 0
else:
    patience_counter += 1
    if patience_counter >= patience:
        print("Early stopping triggered")
        break
```

### 2.5.3 训练监控与可视化

**损失曲线绘制** (`src/training/trainer.py:286-303`)：

```python
def plot_training_curve(self, save_dir: str):
    """绘制训练曲线"""
    plt.figure(figsize=(10, 6))
    
    epochs = range(1, len(self.train_losses) + 1)
    plt.plot(epochs, self.train_losses, 'b-', label='训练损失')
    
    if self.val_losses:
        plt.plot(epochs, self.val_losses, 'r-', label='验证损失')
    
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('训练过程')
    plt.legend()
    plt.grid(True)
    
    plt.savefig(os.path.join(save_dir, 'training_curve.png'))
    plt.close()
```

**关键监控指标：**

1. **训练损失**：下降趋势和收敛速度
2. **验证损失**：过拟合检测
3. **学习率**：调度策略的有效性
4. **梯度范数**：训练稳定性指标
5. **困惑度**：模型性能的可解释指标

## 2.6 预训练的数据效率与扩展法则

### 2.6.1 数据规模与模型性能的关系

**Scaling Laws 经验定律：**

根据 OpenAI 的研究，模型性能遵循幂律关系：

```math
L(N, D) = \left(\frac{N_c}{N}\right)^{\alpha_N} + \left(\frac{D_c}{D}\right)^{\alpha_D}
```

其中：
- $L$ 是测试损失
- $N$ 是模型参数量
- $D$ 是数据集大小
- $\alpha_N, \alpha_D$ 是经验常数

**实用启示：**

1. **数据优先**：在固定计算预算下，增加数据比增加模型大小更有效
2. **平衡配置**：模型大小和数据大小应协调增长
3. **收益递减**：性能提升呈对数关系，边际收益递减

### 2.6.2 计算最优的训练配置

**Chinchilla 最优配置：**

对于给定的计算预算 $C$，最优的参数量 $N$ 和数据量 $D$ 应满足：

```math
N \propto C^a, \quad D \propto C^b
```

其中 $a + b = 1$，经验值为 $a \approx 0.5, b \approx 0.5$。

这意味着参数量和数据量应该同步增长。

### 2.6.3 预训练阶段的评估基准

**内在评估指标：**

1. **困惑度**：语言建模的直接指标
2. **位元率**：信息论角度的压缩效率
3. **下游任务性能**：零样本和少样本能力

**外在评估任务：**

1. **阅读理解**：RACE, SQuAD
2. **自然语言推理**：SNLI, MultiNLI
3. **常识推理**：CommonsenseQA, PIQA
4. **数学推理**：GSM8K, MATH

## 小结

本章深入分析了预训练阶段的理论基础和实践要点：

1. **语言建模的数学本质**：通过最大似然估计学习文本的统计规律
2. **因果掩码机制**：确保自回归约束，实现并行训练和递归推理的一致性
3. **BPE 分词策略**：平衡词汇表大小、序列长度和表示效率
4. **优化算法选择**：AdamW + Cosine Annealing + 梯度裁剪的最佳实践
5. **训练监控与调试**：通过损失曲线、困惑度等指标监控训练进展
6. **扩展法则指导**：数据和参数的协调增长策略

预训练阶段奠定了模型的语言理解基础，为后续的监督微调和强化学习提供了强大的初始化。下一章将探讨如何通过监督微调将预训练模型对齐到特定任务和指令格式。