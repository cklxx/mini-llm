# MiniGPT Checkpointå’ŒDatasetå®Œæ•´è®¾ç½®æŒ‡å—

> **alex-ckl.com AIç ”å‘å›¢é˜Ÿ**
> **å®ç”¨éƒ¨ç½²æŒ‡å—**

---

## ğŸ“ Checkpointä½“ç§¯æŸ¥çœ‹å’Œç®¡ç†

### ğŸ” 1. æŸ¥çœ‹Checkpointæ–‡ä»¶ä½“ç§¯

#### åŸºæœ¬æ–¹æ³•

```bash
# æ–¹æ³•1: ä½¿ç”¨lså‘½ä»¤æŸ¥çœ‹æ–‡ä»¶å¤§å°
ls -lh checkpoints/
# è¾“å‡ºç¤ºä¾‹:
# -rw-r--r-- 1 user staff 213M Jan 15 10:30 model_medium_100mb.pt
# -rw-r--r-- 1 user staff 427M Jan 15 10:30 model_medium_fp32.pt

# æ–¹æ³•2: ä½¿ç”¨duå‘½ä»¤æŸ¥çœ‹ç›®å½•æ€»å¤§å°
du -sh checkpoints/
# è¾“å‡º: 640M    checkpoints/

# æ–¹æ³•3: æŸ¥çœ‹å…·ä½“æ–‡ä»¶å¤§å°
stat checkpoints/model.pt
```

#### ğŸ› ï¸ ä½¿ç”¨ä¸“ä¸šåˆ†æå·¥å…·

```bash
# ä½¿ç”¨æˆ‘ä»¬çš„checkpointåˆ†æå·¥å…·
python3 scripts/tools/checkpoint_analyzer.py --checkpoint checkpoints/model.pt

# æ‰¹é‡åˆ†æç›®å½•ä¸­çš„æ‰€æœ‰checkpoint
python3 scripts/tools/checkpoint_analyzer.py --directory checkpoints/ --compare

# åˆ›å»ºæ¼”ç¤ºcheckpointè¿›è¡Œå¯¹æ¯”
python3 scripts/tools/checkpoint_analyzer.py --create-demo --config medium
```

### ğŸ“Š 2. Checkpointå†…å®¹è§£æ

#### PyTorch Checkpointç»“æ„

```python
# æ ‡å‡†checkpointç»“æ„
checkpoint = {
    'model_state_dict': model.state_dict(),      # æ¨¡å‹æƒé‡ (~100-200MB)
    'optimizer_state_dict': optimizer.state_dict(),  # ä¼˜åŒ–å™¨çŠ¶æ€ (~200-400MB)
    'scheduler_state_dict': scheduler.state_dict(),  # è°ƒåº¦å™¨çŠ¶æ€ (~1KB)
    'epoch': 10,                                 # è®­ç»ƒè½®æ•°
    'step': 1000,                               # è®­ç»ƒæ­¥æ•°
    'loss': 2.5,                                # å½“å‰æŸå¤±
    'config': config_dict,                      # æ¨¡å‹é…ç½®
    'best_loss': 2.3,                          # æœ€ä½³æŸå¤±
    'model_info': {                             # æ¨¡å‹ä¿¡æ¯
        'total_params': 111949440,
        'trainable_params': 111949440
    }
}
```

#### ä¸åŒç±»å‹checkpointçš„ä½“ç§¯å¯¹æ¯”

| ç±»å‹ | åŒ…å«å†…å®¹ | å…¸å‹å¤§å° | ç”¨é€” |
|------|----------|----------|------|
| **æ¨¡å‹æƒé‡** | ä»…model_state_dict | 100-400MB | æ¨ç†éƒ¨ç½² |
| **å®Œæ•´checkpoint** | æ¨¡å‹+ä¼˜åŒ–å™¨+å…ƒæ•°æ® | 300-1200MB | æ¢å¤è®­ç»ƒ |
| **æœ€ä½³æ¨¡å‹** | æ¨¡å‹+é…ç½®+æ€§èƒ½æŒ‡æ ‡ | 100-450MB | æ¨¡å‹é€‰æ‹© |
| **å‹ç¼©checkpoint** | å‹ç¼©çš„å®Œæ•´çŠ¶æ€ | 200-800MB | å­˜å‚¨ä¼˜åŒ– |

### ğŸ’¾ 3. ä¸åŒç²¾åº¦çš„ä½“ç§¯å½±å“

#### ç²¾åº¦å¯¹æ¯”è¡¨ (ä»¥100Må‚æ•°æ¨¡å‹ä¸ºä¾‹)

```python
# 100Må‚æ•°æ¨¡å‹åœ¨ä¸åŒç²¾åº¦ä¸‹çš„ç†è®ºå¤§å°
model_params = 111_949_440

# ä¸åŒç²¾åº¦çš„å­˜å‚¨éœ€æ±‚
precisions = {
    'FP32': model_params * 4 / (1024**2),  # â‰ˆ 427MB
    'FP16': model_params * 2 / (1024**2),  # â‰ˆ 214MB
    'BF16': model_params * 2 / (1024**2),  # â‰ˆ 214MB
    'INT8': model_params * 1 / (1024**2),  # â‰ˆ 107MB
    'INT4': model_params * 0.5 / (1024**2) # â‰ˆ 54MB
}
```

#### å®é™…ä¿å­˜ç¤ºä¾‹

```python
# ä¿å­˜ä¸åŒç²¾åº¦çš„æ¨¡å‹
import torch

# FP32 (é»˜è®¤)
torch.save(model.state_dict(), 'model_fp32.pt')

# FP16
model_fp16 = model.half()
torch.save(model_fp16.state_dict(), 'model_fp16.pt')

# ä»…ä¿å­˜æ¨¡å‹æƒé‡ (æœ€å°ä½“ç§¯)
torch.save(model.state_dict(), 'model_weights_only.pt')

# å‹ç¼©ä¿å­˜
torch.save(checkpoint, 'model_compressed.pt',
           _use_new_zipfile_serialization=True)
```

---

## ğŸ“š Datasetè®¾ç½®å®Œæ•´æŒ‡å—

### ğŸ¯ 1. DatasetåŸºæœ¬é…ç½®

#### MiniGPTæ”¯æŒçš„æ•°æ®æ ¼å¼

```python
# 1. å¯¹è¯æ ¼å¼ (SFTè®­ç»ƒ)
{
    "conversations": [
        {"role": "user", "content": "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"},
        {"role": "assistant", "content": "ä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œæ¸©åº¦é€‚å®œï¼Œæ˜¯ä¸ªä¸é”™çš„å¤©æ°”ã€‚"}
    ]
}

# 2. é¢„è®­ç»ƒæ ¼å¼
{
    "text": "è¿™æ˜¯ä¸€æ®µç”¨äºé¢„è®­ç»ƒçš„é•¿æ–‡æœ¬å†…å®¹..."
}

# 3. å·¥å…·è°ƒç”¨æ ¼å¼
{
    "conversations": [
        {"role": "user", "content": "å¸®æˆ‘æŸ¥è¯¢åŒ—äº¬çš„å¤©æ°”"},
        {"role": "assistant", "content": "æˆ‘æ¥å¸®æ‚¨æŸ¥è¯¢åŒ—äº¬çš„å¤©æ°”ã€‚",
         "tool_calls": [{"name": "weather_api", "arguments": {"city": "åŒ—äº¬"}}]},
        {"role": "tool", "content": "åŒ—äº¬ä»Šå¤©æ™´ï¼Œæ¸©åº¦20-25åº¦"},
        {"role": "assistant", "content": "æ ¹æ®æŸ¥è¯¢ç»“æœï¼ŒåŒ—äº¬ä»Šå¤©æ™´å¤©ï¼Œæ¸©åº¦åœ¨20-25åº¦ä¹‹é—´ã€‚"}
    ]
}

# 4. DPOæ ¼å¼ (åå¥½å­¦ä¹ )
{
    "chosen": {"role": "assistant", "content": "è¿™æ˜¯æ›´å¥½çš„å›ç­”"},
    "rejected": {"role": "assistant", "content": "è¿™æ˜¯è¾ƒå·®çš„å›ç­”"},
    "prompt": {"role": "user", "content": "ç”¨æˆ·çš„é—®é¢˜"}
}
```

### ğŸ“ 2. Datasetç›®å½•ç»“æ„

#### æ¨èçš„æ•°æ®ç»„ç»‡ç»“æ„

```
data/
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ minimind_dataset/           # ä¸»è¦è®­ç»ƒæ•°æ®
â”‚   â”‚   â”œâ”€â”€ pretrain_hq.jsonl     # é¢„è®­ç»ƒæ•°æ® (1.6GB)
â”‚   â”‚   â”œâ”€â”€ sft_mini_512.jsonl    # SFTç²¾ç®€ç‰ˆ (1.2GB) â­æ¨è
â”‚   â”‚   â”œâ”€â”€ sft_512.jsonl         # SFTå®Œæ•´ç‰ˆ (7.5GB)
â”‚   â”‚   â”œâ”€â”€ sft_1024.jsonl        # é•¿åºåˆ—SFT (5.6GB)
â”‚   â”‚   â”œâ”€â”€ sft_2048.jsonl        # è¶…é•¿åºåˆ—SFT (9GB)
â”‚   â”‚   â”œâ”€â”€ dpo.jsonl             # DPOåå¥½æ•°æ® (909MB)
â”‚   â”‚   â”œâ”€â”€ tool_calling_basic.jsonl    # åŸºç¡€å·¥å…·è°ƒç”¨
â”‚   â”‚   â”œâ”€â”€ tool_calling_advanced.jsonl # é«˜çº§å·¥å…·è°ƒç”¨
â”‚   â”‚   â”œâ”€â”€ agent_ultra_think.jsonl     # Ultra Thinkæ¨ç†
â”‚   â”‚   â””â”€â”€ r1_mix_1024.jsonl     # DeepSeek-R1æ¨ç† (340MB)
â”‚   â”œâ”€â”€ custom/                    # è‡ªå®šä¹‰æ•°æ®
â”‚   â””â”€â”€ preprocessed/              # é¢„å¤„ç†åçš„æ•°æ®
â””â”€â”€ tokenizer/                     # åˆ†è¯å™¨æ–‡ä»¶
    â”œâ”€â”€ tokenizer.pkl
    â””â”€â”€ vocab.txt
```

### âš™ï¸ 3. Dataseté…ç½®å‚æ•°

#### è®­ç»ƒé…ç½®æ–‡ä»¶ (config/training_config.py)

```python
def get_dataset_config(dataset_type: str = "sft"):
    """è·å–æ•°æ®é›†é…ç½®"""

    configs = {
        "pretrain": {
            "data_path": "data/dataset/minimind_dataset/pretrain_hq.jsonl",
            "max_seq_len": 512,
            "batch_size": 16,
            "format_type": "pretrain",
            "data_size": "1.6GB",
            "description": "é«˜è´¨é‡é¢„è®­ç»ƒæ•°æ®"
        },

        "sft_mini": {
            "data_path": "data/dataset/minimind_dataset/sft_mini_512.jsonl",
            "max_seq_len": 512,
            "batch_size": 8,
            "format_type": "conversation",
            "data_size": "1.2GB",
            "description": "å¿«é€ŸSFTè®­ç»ƒ (æ¨è)"
        },

        "sft_full": {
            "data_path": "data/dataset/minimind_dataset/sft_512.jsonl",
            "max_seq_len": 512,
            "batch_size": 4,
            "format_type": "conversation",
            "data_size": "7.5GB",
            "description": "å®Œæ•´SFTè®­ç»ƒ"
        },

        "sft_long": {
            "data_path": "data/dataset/minimind_dataset/sft_1024.jsonl",
            "max_seq_len": 1024,
            "batch_size": 2,
            "format_type": "conversation",
            "data_size": "5.6GB",
            "description": "é•¿åºåˆ—SFTè®­ç»ƒ"
        },

        "dpo": {
            "data_path": "data/dataset/minimind_dataset/dpo.jsonl",
            "max_seq_len": 1024,
            "batch_size": 4,
            "format_type": "dpo",
            "data_size": "909MB",
            "description": "DPOåå¥½å¯¹é½"
        },

        "tool_calling": {
            "data_path": [
                "data/dataset/minimind_dataset/tool_calling_basic.jsonl",
                "data/dataset/minimind_dataset/tool_calling_advanced.jsonl"
            ],
            "max_seq_len": 1024,
            "batch_size": 4,
            "format_type": "conversation",
            "data_size": "~50MB",
            "description": "å·¥å…·è°ƒç”¨è®­ç»ƒ"
        },

        "ultra_think": {
            "data_path": "data/dataset/minimind_dataset/agent_ultra_think.jsonl",
            "max_seq_len": 2048,
            "batch_size": 2,
            "format_type": "conversation",
            "data_size": "~10MB",
            "description": "Ultra Thinkæ·±åº¦æ¨ç†"
        },

        "reasoning": {
            "data_path": "data/dataset/minimind_dataset/r1_mix_1024.jsonl",
            "max_seq_len": 1024,
            "batch_size": 4,
            "format_type": "conversation",
            "data_size": "340MB",
            "description": "DeepSeek-R1æ¨ç†èƒ½åŠ›"
        }
    }

    return configs.get(dataset_type, configs["sft_mini"])
```

### ğŸš€ 4. ä¸åŒè®­ç»ƒåœºæ™¯çš„Datasetè®¾ç½®

#### A. å¿«é€ŸåŸå‹éªŒè¯

```python
# æœ€å°é…ç½® - é€‚åˆå¿«é€Ÿæµ‹è¯•
config = {
    "dataset": "sft_mini",           # ä½¿ç”¨ç²¾ç®€æ•°æ®é›†
    "max_seq_len": 512,             # è¾ƒçŸ­åºåˆ—
    "batch_size": 8,                # é€‚ä¸­æ‰¹æ¬¡
    "num_epochs": 1,                # å•è½®è®­ç»ƒ
    "data_subset": 0.1,             # ä»…ä½¿ç”¨10%æ•°æ®
    "validation_split": 0.1         # 10%ç”¨äºéªŒè¯
}

# é¢„æœŸè®­ç»ƒæ—¶é—´: ~30åˆ†é’Ÿ (GPU)
# æ•°æ®é‡: ~120MB
```

#### B. ç”Ÿäº§çº§è®­ç»ƒ

```python
# å®Œæ•´è®­ç»ƒé…ç½®
config = {
    "dataset": "sft_full",          # å®Œæ•´æ•°æ®é›†
    "max_seq_len": 1024,            # æ ‡å‡†åºåˆ—é•¿åº¦
    "batch_size": 4,                # æ ¹æ®GPUå†…å­˜è°ƒæ•´
    "num_epochs": 3,                # å¤šè½®è®­ç»ƒ
    "gradient_accumulation": 4,     # æ¢¯åº¦ç´¯ç§¯
    "validation_split": 0.05,       # 5%ç”¨äºéªŒè¯
    "early_stopping": True,         # æ—©åœæœºåˆ¶
    "save_best_only": True         # ä»…ä¿å­˜æœ€ä½³æ¨¡å‹
}

# é¢„æœŸè®­ç»ƒæ—¶é—´: ~6-12å°æ—¶ (GPU)
# æ•°æ®é‡: ~7.5GB
```

#### C. ç‰¹æ®Šèƒ½åŠ›è®­ç»ƒ

```python
# å·¥å…·è°ƒç”¨ + Ultra Think ç»„åˆè®­ç»ƒ
config = {
    "datasets": [                   # å¤šæ•°æ®é›†æ··åˆ
        ("sft_mini", 0.7),         # 70% åŸºç¡€å¯¹è¯
        ("tool_calling", 0.2),     # 20% å·¥å…·è°ƒç”¨
        ("ultra_think", 0.1)       # 10% æ·±åº¦æ¨ç†
    ],
    "max_seq_len": 1024,
    "batch_size": 4,
    "mixing_strategy": "weighted",  # åŠ æƒæ··åˆç­–ç•¥
    "curriculum_learning": True,    # è¯¾ç¨‹å­¦ä¹ 
    "special_tokens": [             # ç‰¹æ®Štoken
        "<tool_call>", "</tool_call>",
        "<ultra_think>", "</ultra_think>"
    ]
}
```

### ğŸ”§ 5. Dataseté¢„å¤„ç†å·¥å…·

#### æ•°æ®é¢„å¤„ç†è„šæœ¬

```python
# scripts/data_processing/prepare_datasets.py

def preprocess_dataset(
    input_path: str,
    output_path: str,
    max_seq_len: int = 512,
    tokenizer_path: str = None,
    format_type: str = "conversation"
):
    """
    æ•°æ®é¢„å¤„ç†ä¸»å‡½æ•°

    Args:
        input_path: åŸå§‹æ•°æ®è·¯å¾„
        output_path: å¤„ç†åæ•°æ®è·¯å¾„
        max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦
        tokenizer_path: åˆ†è¯å™¨è·¯å¾„
        format_type: æ•°æ®æ ¼å¼ç±»å‹
    """

    # 1. æ•°æ®æ ¼å¼éªŒè¯
    validate_data_format(input_path, format_type)

    # 2. æ•°æ®æ¸…æ´—
    cleaned_data = clean_conversations(input_path)

    # 3. é•¿åº¦è¿‡æ»¤
    filtered_data = filter_by_length(cleaned_data, max_seq_len)

    # 4. è´¨é‡è¯„ä¼°
    quality_score = assess_data_quality(filtered_data)

    # 5. ä¿å­˜å¤„ç†ç»“æœ
    save_processed_data(filtered_data, output_path)

    return {
        "original_samples": len(load_data(input_path)),
        "processed_samples": len(filtered_data),
        "quality_score": quality_score,
        "average_length": calculate_average_length(filtered_data)
    }
```

#### ä½¿ç”¨æ•°æ®é¢„å¤„ç†

```bash
# é¢„å¤„ç†SFTæ•°æ®
python3 scripts/data_processing/prepare_datasets.py \
    --input data/dataset/minimind_dataset/sft_mini_512.jsonl \
    --output data/preprocessed/sft_mini_processed.jsonl \
    --max-seq-len 512 \
    --format conversation \
    --quality-filter

# é¢„å¤„ç†å·¥å…·è°ƒç”¨æ•°æ®
python3 scripts/data_processing/prepare_datasets.py \
    --input data/dataset/minimind_dataset/tool_calling_basic.jsonl \
    --output data/preprocessed/tool_calling_processed.jsonl \
    --max-seq-len 1024 \
    --format conversation \
    --add-special-tokens
```

### ğŸ“Š 6. Datasetè´¨é‡ç›‘æ§

#### æ•°æ®è´¨é‡æŒ‡æ ‡

```python
def analyze_dataset_quality(dataset_path: str) -> Dict[str, Any]:
    """åˆ†ææ•°æ®é›†è´¨é‡"""

    with open(dataset_path, 'r', encoding='utf-8') as f:
        data = [json.loads(line) for line in f if line.strip()]

    analysis = {
        # åŸºæœ¬ç»Ÿè®¡
        "total_samples": len(data),
        "average_length": calculate_average_length(data),
        "length_distribution": get_length_distribution(data),

        # å†…å®¹è´¨é‡
        "duplicate_rate": calculate_duplicate_rate(data),
        "language_diversity": analyze_language_diversity(data),
        "topic_coverage": analyze_topic_coverage(data),

        # æ ¼å¼æ­£ç¡®æ€§
        "format_errors": validate_all_samples(data),
        "encoding_issues": check_encoding_issues(data),

        # ç‰¹æ®Šç»Ÿè®¡ (é’ˆå¯¹ä¸åŒç±»å‹)
        "conversation_turns": analyze_conversation_turns(data),
        "tool_call_frequency": count_tool_calls(data),
        "ultra_think_frequency": count_ultra_think(data)
    }

    return analysis
```

### ğŸ’¡ 7. Dataseté…ç½®æœ€ä½³å®è·µ

#### è®­ç»ƒé˜¶æ®µé…ç½®å»ºè®®

```python
# é˜¶æ®µ1: é¢„è®­ç»ƒ (å¦‚æœä»å¤´å¼€å§‹)
stage1_config = {
    "dataset": "pretrain",
    "max_seq_len": 512,
    "batch_size": 16,
    "learning_rate": 1e-4,
    "epochs": 1,
    "objective": "next_token_prediction"
}

# é˜¶æ®µ2: ç›‘ç£å¾®è°ƒ
stage2_config = {
    "dataset": "sft_mini",
    "max_seq_len": 512,
    "batch_size": 8,
    "learning_rate": 5e-5,
    "epochs": 3,
    "objective": "conversation_modeling"
}

# é˜¶æ®µ3: å·¥å…·è°ƒç”¨è®­ç»ƒ
stage3_config = {
    "dataset": "tool_calling",
    "max_seq_len": 1024,
    "batch_size": 4,
    "learning_rate": 2e-5,
    "epochs": 2,
    "objective": "tool_use_optimization"
}

# é˜¶æ®µ4: DPOå¯¹é½ (å¯é€‰)
stage4_config = {
    "dataset": "dpo",
    "max_seq_len": 1024,
    "batch_size": 4,
    "learning_rate": 1e-5,
    "epochs": 1,
    "objective": "preference_alignment"
}
```

#### ç¡¬ä»¶é…ç½®åŒ¹é…

```python
# æ ¹æ®ç¡¬ä»¶è°ƒæ•´dataseté…ç½®
def get_hardware_optimized_config(hardware_type: str):
    """æ ¹æ®ç¡¬ä»¶ç±»å‹ä¼˜åŒ–é…ç½®"""

    configs = {
        "apple_silicon_8gb": {
            "batch_size": 4,
            "max_seq_len": 512,
            "gradient_accumulation": 2,
            "dataset": "sft_mini",
            "precision": "fp16"
        },

        "apple_silicon_16gb": {
            "batch_size": 8,
            "max_seq_len": 1024,
            "gradient_accumulation": 2,
            "dataset": "sft_full",
            "precision": "fp16"
        },

        "rtx_4090_24gb": {
            "batch_size": 16,
            "max_seq_len": 2048,
            "gradient_accumulation": 1,
            "dataset": "sft_full",
            "precision": "fp16"
        },

        "cpu_only": {
            "batch_size": 1,
            "max_seq_len": 256,
            "gradient_accumulation": 8,
            "dataset": "sft_mini",
            "precision": "fp32"
        }
    }

    return configs.get(hardware_type, configs["apple_silicon_8gb"])
```

---

## ğŸ¯ å®ç”¨å‘½ä»¤é€ŸæŸ¥

### Checkpointåˆ†æå‘½ä»¤

```bash
# æŸ¥çœ‹å•ä¸ªcheckpointè¯¦æƒ…
python3 scripts/tools/checkpoint_analyzer.py -c checkpoints/model.pt

# å¯¹æ¯”ç›®å½•ä¸­æ‰€æœ‰checkpoint
python3 scripts/tools/checkpoint_analyzer.py -d checkpoints/ --compare

# åˆ›å»ºæ¼”ç¤ºcheckpointç”¨äºå­¦ä¹ 
python3 scripts/tools/checkpoint_analyzer.py --create-demo --config medium

# æŸ¥çœ‹æ–‡ä»¶ç³»ç»Ÿçº§åˆ«çš„å¤§å°
ls -lh checkpoints/               # äººæ€§åŒ–æ˜¾ç¤º
du -sh checkpoints/              # ç›®å½•æ€»å¤§å°
find checkpoints/ -name "*.pt" -exec ls -lh {} \;  # æŸ¥æ‰¾æ‰€æœ‰.ptæ–‡ä»¶
```

### Dataseté…ç½®å‘½ä»¤

```bash
# éªŒè¯æ•°æ®é›†æ ¼å¼
python3 scripts/data_processing/prepare_datasets.py --validate-only \
    --input data/dataset/minimind_dataset/sft_mini_512.jsonl

# æ•°æ®é›†ç»Ÿè®¡åˆ†æ
python3 scripts/data_processing/prepare_datasets.py --analyze \
    --input data/dataset/minimind_dataset/

# å¿«é€Ÿè®­ç»ƒé…ç½®
python3 scripts/training/train_optimized.py \
    --config small \
    --dataset sft_mini \
    --max-seq-len 512 \
    --batch-size 4 \
    --epochs 1

# ç”Ÿäº§è®­ç»ƒé…ç½®
python3 scripts/training/train_optimized.py \
    --config medium \
    --dataset sft_full \
    --max-seq-len 1024 \
    --batch-size 4 \
    --epochs 3 \
    --gradient-accumulation 4
```

---

## ğŸ“ˆ ç›‘æ§å’Œä¼˜åŒ–

### è®­ç»ƒè¿‡ç¨‹ç›‘æ§

```python
# åœ¨è®­ç»ƒè„šæœ¬ä¸­æ·»åŠ ç›‘æ§
def monitor_training_progress(
    epoch: int,
    step: int,
    loss: float,
    model: nn.Module,
    optimizer: torch.optim.Optimizer
):
    """ç›‘æ§è®­ç»ƒè¿›åº¦"""

    # 1. æ¨¡å‹å¤§å°ç›‘æ§
    model_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / 1024**2

    # 2. å†…å­˜ä½¿ç”¨ç›‘æ§
    if torch.cuda.is_available():
        memory_allocated = torch.cuda.memory_allocated() / 1024**2
        memory_reserved = torch.cuda.memory_reserved() / 1024**2
    else:
        memory_allocated = memory_reserved = 0

    # 3. ä¼˜åŒ–å™¨çŠ¶æ€å¤§å°
    optimizer_size = get_optimizer_memory_size(optimizer)

    # 4. æ¢¯åº¦ç»Ÿè®¡
    grad_norm = get_gradient_norm(model)

    print(f"Epoch {epoch}, Step {step}:")
    print(f"  Loss: {loss:.4f}")
    print(f"  Model Size: {model_size_mb:.1f}MB")
    print(f"  Memory: {memory_allocated:.1f}MB / {memory_reserved:.1f}MB")
    print(f"  Optimizer Size: {optimizer_size:.1f}MB")
    print(f"  Grad Norm: {grad_norm:.4f}")
```

### è‡ªåŠ¨åŒ–ä¼˜åŒ–å»ºè®®

```python
def get_optimization_suggestions(
    checkpoint_size_mb: float,
    training_time_hours: float,
    target_deployment: str = "mobile"
) -> List[str]:
    """æ ¹æ®å½“å‰çŠ¶æ€æä¾›ä¼˜åŒ–å»ºè®®"""

    suggestions = []

    # æ–‡ä»¶å¤§å°ä¼˜åŒ–
    if checkpoint_size_mb > 500:
        suggestions.append("è€ƒè™‘æ¨¡å‹å‰ªæå‡å°‘å‚æ•°é‡")
        suggestions.append("ä½¿ç”¨INT8é‡åŒ–å‡å°‘å­˜å‚¨éœ€æ±‚")

    if checkpoint_size_mb > 200 and target_deployment == "mobile":
        suggestions.append("ç§»åŠ¨ç«¯éƒ¨ç½²å»ºè®®ä½¿ç”¨æ›´å°çš„æ¨¡å‹é…ç½®")
        suggestions.append("å¯ç”¨åŠ¨æ€é‡åŒ–ä¼˜åŒ–æ¨ç†é€Ÿåº¦")

    # è®­ç»ƒæ•ˆç‡ä¼˜åŒ–
    if training_time_hours > 12:
        suggestions.append("è€ƒè™‘ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹èŠ‚çœå†…å­˜")
        suggestions.append("å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒåŠ é€Ÿè®­ç»ƒ")
        suggestions.append("ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡å¤§å°æå‡GPUåˆ©ç”¨ç‡")

    # éƒ¨ç½²ä¼˜åŒ–
    if target_deployment == "edge":
        suggestions.append("ä½¿ç”¨ONNXæ ¼å¼ä¼˜åŒ–è·¨å¹³å°éƒ¨ç½²")
        suggestions.append("è€ƒè™‘æ¨¡å‹è’¸é¦è¿›ä¸€æ­¥å‹ç¼©")

    return suggestions
```

---

**ğŸ’¡ æ€»ç»“ï¼šé€šè¿‡åˆç†çš„checkpointç®¡ç†å’Œdataseté…ç½®ï¼Œå¯ä»¥æ˜¾è‘—ä¼˜åŒ–æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œéƒ¨ç½²æ€§èƒ½ã€‚å»ºè®®æ ¹æ®å…·ä½“çš„ç¡¬ä»¶æ¡ä»¶å’Œåº”ç”¨åœºæ™¯é€‰æ‹©é€‚å½“çš„é…ç½®ç­–ç•¥ã€‚**

---

*alex-ckl.com AIç ”å‘å›¢é˜Ÿç‰ˆæƒæ‰€æœ‰*
*ğŸš€ Generated with MiniGPT Technology*