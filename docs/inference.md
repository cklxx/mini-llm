# ğŸ—£ï¸ æ¨ç†ä¸æ–‡æœ¬ç”Ÿæˆ

Mini-LLM çš„æ¨ç†ç»„ä»¶ä½äº `src/inference/generator.py`ã€‚`TextGenerator` åœ¨ä¿æŒ API ç®€æ´çš„åŒæ—¶ï¼Œæä¾›å¤šç§ä¸»æµçš„è§£ç ç­–ç•¥ã€‚

## GenerationConfig
æ ¸å¿ƒå­—æ®µï¼š
- `max_length`ï¼šæœ€å¤§ç”Ÿæˆæ­¥æ•°
- `temperature`ï¼šé‡‡æ ·æ¸©åº¦ï¼Œè¶Šå¤§è¶Šéšæœº
- `top_k` / `top_p`ï¼šTop-k ä¸ Nucleus (Top-p) é‡‡æ ·
- `repetition_penalty`ï¼šé‡å¤æƒ©ç½šç³»æ•°
- `num_beams` / `early_stopping`ï¼šæŸæœç´¢å‚æ•°
- `do_sample`ï¼šæ˜¯å¦å¯ç”¨éšæœºé‡‡æ ·

## TextGenerator åŠŸèƒ½
- **greedy_search**ï¼šé€æ­¥é€‰æ‹©æœ€å¤§æ¦‚ç‡ token
- **sample_generate**ï¼šæ”¯æŒæ¸©åº¦ã€Top-kã€Top-pã€é‡å¤æƒ©ç½šçš„éšæœºé‡‡æ ·
- **beam_search**ï¼šç»´æŠ¤å¤šä¸ªå€™é€‰åºåˆ—å¹¶æ ¹æ®å¹³å‡å¯¹æ•°æ¦‚ç‡é€‰æ‹©æœ€ä¼˜è¾“å‡º
- **è¾…åŠ©æ–¹æ³•**ï¼š`apply_repetition_penalty`ã€`top_k_filtering`ã€`top_p_filtering`

## ä½¿ç”¨ç¤ºä¾‹
```python
import torch
from src.inference.generator import TextGenerator, GenerationConfig

text_generator = TextGenerator(model, tokenizer, device="cpu")
prompt_ids = tokenizer.encode("Mini-LLM", add_special_tokens=True)

config = GenerationConfig(
    max_length=60,
    temperature=0.8,
    top_k=40,
    top_p=0.95,
    repetition_penalty=1.1,
)
output_ids = text_generator.sample_generate(
    input_ids=torch.tensor([prompt_ids]),
    config=config
)
print(tokenizer.decode(output_ids[0].tolist()))
```

## ä¸æ¨¡å‹é›†æˆçš„å»ºè®®
1. è°ƒç”¨æ¨ç†å‰è¯·ç¡®ä¿æ¨¡å‹å¤„äº `eval()` æ¨¡å¼ï¼Œé¿å… Dropout å½±å“ç»“æœ
2. å½“ä½¿ç”¨ `beam_search` æ—¶ï¼Œå¯æ ¹æ® `config.length_penalty` è°ƒæ•´çŸ­åºåˆ—åå¥½
3. å¯¹äºé•¿æ–‡æœ¬ç”Ÿæˆï¼Œå»ºè®®ç»“åˆ `MemoryOptimizer.optimize_model_for_inference()` é‡Šæ”¾å¤šä½™æ˜¾å­˜
4. å¦‚éœ€è‡ªå®šä¹‰ç­–ç•¥ï¼ˆä¾‹å¦‚å¤šæ ·æ€§æƒ©ç½šã€åŠ¨æ€æ¸©åº¦ï¼‰ï¼Œå¯åœ¨ `TextGenerator` åŸºç¡€ä¸Šæ‰©å±•

`TextGenerator` åªä¾èµ– PyTorch ä¸åˆ†è¯å™¨æ¥å£ï¼Œå¯è½»æ¾ç§»æ¤åˆ°å…¶ä»–é¡¹ç›®æˆ–è„šæœ¬ä¸­ã€‚
