# ğŸ¤ RLHF æµç¨‹æ¦‚è§ˆ

`src/rl` æ¨¡å—å±•ç¤ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„ RLHFï¼ˆReinforcement Learning from Human Feedbackï¼‰åŸå‹ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€å¥–åŠ±æ¨¡å‹è®­ç»ƒä¸ PPO ç­–ç•¥ä¼˜åŒ–ã€‚æœ¬æ–‡æ¡£æŒ‰é˜¶æ®µæ‹†è§£é…ç½®é¡¹ã€æ‰§è¡Œæµç¨‹ä»¥åŠéœ€è¦å…³æ³¨çš„é£é™©ç‚¹ã€‚

## é…ç½® (`RLHFConfig`)

`RLHFConfig` æ±‡æ€»äº†ä¸‰ä¸ªé˜¶æ®µçš„æ ¸å¿ƒè¶…å‚æ•°ï¼Œå¹¶å…è®¸é€šè¿‡ `create_rlhf_pipeline` ä»å­—å…¸æˆ– JSON æ–‡ä»¶æ„é€ å®ä¾‹ã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L37-L440ã€‘

| åˆ†ç»„ | å­—æ®µ | è¯´æ˜ |
| ---- | ---- | ---- |
| åŸºç¡€ | `model_name` | ç”¨äºè®°å½•å½“å‰å®éªŒä½¿ç”¨çš„åŸºåº§æ¨¡å‹åç§°ï¼Œé…åˆæ—¥å¿—è¾“å‡º | 
|      | `tokenizer_path` | åºåˆ—åŒ–åˆ†è¯å™¨è·¯å¾„ï¼Œ`load_tokenizer` ä¼šç›´æ¥ååºåˆ—åŒ–å¹¶åœ¨åç»­é˜¶æ®µå¤ç”¨ã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L137-L144ã€‘ |
|      | `device='auto'` | è‡ªåŠ¨æ£€æµ‹ CUDA/MPS/CPU å¹¶è®°å½•æ—¥å¿—ï¼Œç¡®ä¿ä¸‰é˜¶æ®µåœ¨åŒä¸€è®¾å¤‡ä¸Šè¿è¡Œã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L116-L129ã€‘ |
| SFT  | `sft_data_path`/`sft_epochs`/`sft_batch_size`/`sft_lr` | æ§åˆ¶ç›‘ç£å¾®è°ƒçš„æ•°æ®æºã€è½®æ¬¡ä¸å­¦ä¹ ç‡ï¼›`run_sft` ä¼šè¯»å–è¿™äº›å‚æ•°æ„é€  DataLoader ä¸è®­ç»ƒå™¨ã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L168-L203ã€‘ |
| å¥–åŠ± | `reward_data_path`/`reward_epochs`/`reward_lr`/`reward_batch_size`/`freeze_reward_backbone` | å®šä¹‰åå¥½æ•°æ®ä¸å¥–åŠ±å¤´è®­ç»ƒç­–ç•¥ï¼›`freeze_reward_backbone=True` æ—¶ä¼šå†»ç»“è¯­è¨€æ¨¡å‹ä¸»å¹²ï¼Œä»…æ›´æ–°å¥–åŠ±å¤´ã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L205-L253ã€‘ |
| PPO | `ppo_data_path`/`ppo_iterations`/`ppo_lr_policy`/`ppo_lr_value`/`ppo_batch_size`/`ppo_mini_batch_size`/`ppo_epochs` | æ§åˆ¶ç­–ç•¥ä¼˜åŒ–çš„è¿­ä»£æ¬¡æ•°ã€å­¦ä¹ ç‡ä¸ mini-batch åˆ’åˆ†ï¼Œç¡®ä¿é‡‡æ ·ä¸æ›´æ–°æ­¥æ•°å¹³è¡¡ã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L255-L313ã€‘ |
| é€šç”¨ | `max_length`/`save_dir`/`save_interval`/`log_level` | é™åˆ¶åºåˆ—æœ€å¤§é•¿åº¦ã€ä¿å­˜é˜¶æ®µäº§ç‰©å¹¶é…ç½®æ—¥å¿—ç­‰çº§ï¼›åˆå§‹åŒ–æ—¶ä¼šå°†é…ç½®å†™å…¥ `save_dir/config.json` ä¾¿äºå¤ç°ã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L87-L136ã€‘ |

## ç®¡é“æ‰§è¡Œé¡ºåº

1. **åˆå§‹åŒ–**ï¼š`RLHFPipeline.__init__` ä¼šé…ç½®æ—¥å¿—ã€è‡ªåŠ¨é€‰æ‹©è®¾å¤‡ã€åˆ›å»ºè¾“å‡ºç›®å½•å¹¶ä¿å­˜å®Œæ•´é…ç½®å¿«ç…§ï¼Œä¸ºåç»­é˜¶æ®µæä¾›ç»Ÿä¸€çš„è¿è¡Œä¸Šä¸‹æ–‡ã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L75-L136ã€‘
2. **åŠ è½½èµ„æº**ï¼š`run_full_pipeline` ä¼šä¾æ¬¡åŠ è½½åˆ†è¯å™¨ã€åŸºç¡€æ¨¡å‹ï¼Œç„¶åä¸²è¡Œæ‰§è¡Œä¸‰ä¸ªè®­ç»ƒé˜¶æ®µï¼›ä¹Ÿå¯ä»¥æŒ‰éœ€è°ƒç”¨ `run_sft`ã€`run_reward_training`ã€`run_ppo_training` ç»„åˆè‡ªå®šä¹‰æµç¨‹ã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L315-L341ã€‘
3. **ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰**ï¼š
   - ä½¿ç”¨ `create_trainer('sft', ...)` ç»„è£… `SFTTrainer`ï¼Œå¹¶ç”¨ `ConversationDataset` + `DataLoader` è¯»å–å¯¹è¯æ ·æœ¬ã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L168-L200ã€‘
   - `train()` ä¼šæ ¹æ®é…ç½®è½®æ¬¡ä¿å­˜æœ€ä½³æ¨¡å‹åˆ° `save_dir/sft/best_model.pt`ï¼Œä¾›å¥–åŠ±å’Œ PPO é˜¶æ®µå¤ç”¨ã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L193-L203ã€‘
4. **å¥–åŠ±æ¨¡å‹è®­ç»ƒ**ï¼š
   - `create_reward_model` åœ¨ SFT backbone ä¸ŠæŒ‚è½½å¥–åŠ±å¤´ï¼Œå¹¶å¯æ ¹æ® `freeze_reward_backbone` å†³å®šæ˜¯å¦æ›´æ–°ä¸»å¹²å‚æ•°ã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L217-L227ã€‘
   - åå¥½æ•°æ®é€šè¿‡ `create_preference_dataloader` è½¬æ¢ä¸ºæˆå¯¹æ ·æœ¬ï¼Œå†…éƒ¨ä¼šè°ƒç”¨æ’åºæŸå¤±ä¸æ­£åˆ™é¡¹ä»¥æå‡è®­ç»ƒç¨³å®šæ€§ã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L228-L253ã€‘ã€F:src/rl/reward_model/reward_trainer.pyâ€ L33-L200ã€‘
   - è®­ç»ƒå®Œæˆåä¿å­˜ `reward_model/best_model.pt`ï¼Œå¹¶åœ¨æ—¥å¿—ä¸­è¾“å‡ºè·¯å¾„ã€‚
5. **PPO ç­–ç•¥ä¼˜åŒ–**ï¼š
   - è½½å…¥å‰ä¸¤é˜¶æ®µæƒé‡ï¼Œæ„é€ ç­–ç•¥æ¨¡å‹ã€ä»·å€¼æ¨¡å‹ä¸ PPO è®­ç»ƒå™¨ï¼Œå…¶ä¸­ `create_value_model` å¤ç”¨ SFT æƒé‡ä»¥ä¿æŒåˆå§‹åŒ–ä¸€è‡´æ€§ã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L255-L295ã€‘
   - `_load_ppo_prompts` ä¼šè¯»å– JSONL ä¸­çš„ `prompt` å­—æ®µæˆ–è£¸å­—ç¬¦ä¸²ä½œä¸ºé‡‡æ ·èµ·ç‚¹ã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L298-L362ã€‘
   - è®­ç»ƒå¾ªç¯ä¼šæŒ‰ `ppo_iterations` è¿è¡Œç­–ç•¥æ›´æ–°ï¼Œå¹¶æŒ‰ç…§ `save_interval` è½ç›˜ä¸­é—´ç»“æœï¼Œæœ€åè¾“å‡º `ppo/final_model.pt`ã€‚

### è¾“å‡ºç›®å½•ç»“æ„

æ‰§è¡Œ `run_full_pipeline` åï¼Œ`save_dir` ä¼šåŒ…å«å¦‚ä¸‹å­ç›®å½•ï¼Œæ–¹ä¾¿é˜¶æ®µåŒ–è°ƒè¯•ï¼š

| å­ç›®å½• | å†…å®¹ | æ¥æº |
| ------ | ---- | ---- |
| `sft/` | æœ€ä½³ SFT æ¨¡å‹æƒé‡ä¸é…ç½®å¿«ç…§ | `run_sft` ä¿å­˜ `best_model.pt` ä¸æŒ‡æ ‡æ—¥å¿—ã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L168-L203ã€‘ |
| `reward_model/` | å¥–åŠ±æ¨¡å‹ checkpoint åŠåå¥½è®­ç»ƒæ—¥å¿— | `run_reward_training` ä¿å­˜ `best_model.pt` ä¸æŸå¤±æ›²çº¿ã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L205-L253ã€‘ |
| `ppo/` | PPO ç­–ç•¥/ä»·å€¼æ¨¡å‹åŠä¸­é—´æ£€æŸ¥ç‚¹ | `run_ppo_training` åœ¨æ¯æ¬¡ `save_interval` å†™å…¥ `iteration_xxx.pt`ï¼Œæœ€ç»ˆè¾“å‡º `final_model.pt`ã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L255-L341ã€‘ |
| `logs/` | ç»Ÿä¸€æ—¥å¿—æ–‡ä»¶å’Œé…ç½®å¿«ç…§ | åˆå§‹åŒ–é˜¶æ®µè°ƒç”¨ `_init_logging` ä¸ `_save_config` ç”Ÿæˆã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L75-L136ã€‘ |

å»ºè®®åœ¨æ¯ä¸ªé˜¶æ®µå®Œæˆåæ‰‹åŠ¨éªŒè¯è¾“å‡ºç›®å½•ä¸­çš„æ¨¡å‹æ˜¯å¦å¯åŠ è½½ï¼Œé¿å…ä¸‹ä¸€é˜¶æ®µå› è·¯å¾„é”™è¯¯å¯¼è‡´é‡å¤è®­ç»ƒã€‚

## æ•°æ®è¦æ±‚ä¸æ ¡éªŒ

- **SFT é˜¶æ®µ**ï¼šè¾“å…¥éœ€å…¼å®¹ `ConversationDataset`ï¼Œè‡³å°‘åŒ…å« userâ†’assistant ä¸€è½®ï¼›å¦‚æ•°æ®æœªé¢„å…ˆæ’å…¥è§’è‰²æ ‡è®°ï¼Œå¯äº¤ç”±æ•°æ®é›†ç±»è‡ªåŠ¨è¡¥é½ã€‚ã€F:src/training/datasets/conversation.pyâ€ L12-L178ã€‘
- **å¥–åŠ±æ¨¡å‹**ï¼šåå¥½æ ·æœ¬å¿…é¡»æä¾› `(prompt, chosen, rejected)` å­—æ®µï¼Œ`create_preference_dataloader` ä¼šåœ¨å†…éƒ¨æ„é€  positive/negative pair å¹¶ç”Ÿæˆæ³¨æ„åŠ›æ©ç ï¼Œç¼ºå¤±å­—æ®µä¼šè¢«è·³è¿‡ã€‚ã€F:src/rl/reward_model/preference_data.pyâ€ L9-L144ã€‘
- **PPO é˜¶æ®µ**ï¼šæç¤ºè¯­å¯ä»¥æ˜¯ JSONL ä¸­çš„å¯¹è±¡æˆ–çº¯æ–‡æœ¬ï¼›è‹¥ä¸ºå¯¹è±¡ï¼Œéœ€åŒ…å« `prompt` å­—æ®µï¼Œå¦åˆ™è§†ä¸ºæ™®é€šå­—ç¬¦ä¸²å¤„ç†ã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L352-L362ã€‘

åœ¨ä¸‰é˜¶æ®µä¹‹é—´å…±äº«åˆ†è¯å™¨ä¸æ¨¡å‹æƒé‡ï¼Œå› æ­¤ç¡®ä¿ `tokenizer_path` å¯¹åº”çš„ç‰¹æ®Š token ID ä¸è®­ç»ƒé…ç½®ä¸€è‡´ï¼Œé¿å…å¥–åŠ±æ¨¡å‹ä¸ PPO é˜¶æ®µè§£ç å‡ºé”™ã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L137-L165ã€‘

## å¸¸è§å¤±è´¥æ¨¡å¼ä¸åº”å¯¹

- **å¥–åŠ±æ¨¡å‹è¿‡æ‹Ÿåˆ**ï¼šè‹¥å‘ç°å¥–åŠ±æŸå¤±è¿…é€Ÿä¸‹é™ä½† PPO é˜¶æ®µæ— æ³•æ”¶æ•›ï¼Œå¯å¼€å¯ `freeze_reward_backbone` æˆ–é™ä½ `reward_lr`ï¼Œåªè®­ç»ƒå¤´éƒ¨å‚æ•°ã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L205-L253ã€‘
- **PPO å‘æ•£**ï¼šå½“ KL å¥–åŠ±é¡¹æŒç»­å¢å¤§æ—¶ï¼Œè°ƒå° `ppo_lr_policy` æˆ–å¢åŠ  `ppo_mini_batch_size`ï¼Œä½¿æ¯æ¬¡æ›´æ–°æ›´ç¨³å®šï¼›å¿…è¦æ—¶å¯å‡å°‘ `ppo_iterations` ç¼©çŸ­è®­ç»ƒå‘¨æœŸã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L255-L313ã€‘
- **æç¤ºæ— æ•ˆ**ï¼š`_load_ppo_prompts` è‹¥è¯»å–åˆ°ç©ºè¡Œä¼šç›´æ¥è·³è¿‡ï¼Œå¯¼è‡´æœ‰æ•ˆæ ·æœ¬è¿‡å°‘ï¼›è¯·å…ˆåœ¨æ•°æ®å‡†å¤‡é˜¶æ®µæ¸…ç†ç©ºè¡Œå¹¶ç¡®è®¤ JSONL ä¸­ `prompt` å­—æ®µå­˜åœ¨ã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L298-L362ã€‘
- **æ˜¾å­˜å ç”¨è¿‡é«˜**ï¼šå¥–åŠ±/ç­–ç•¥/ä»·å€¼æ¨¡å‹é»˜è®¤åŒæ—¶é©»ç•™åœ¨ GPUï¼Œå¯é€šè¿‡ `device='cpu'` å¼ºåˆ¶åœ¨ CPU ä¸Šè®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œå†å°†ç»“æœè¿å› GPUã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L116-L253ã€‘

## å¸¸è§æ‰©å±•æ–¹å‘

- **æ›¿æ¢æ¨¡å‹è§„æ¨¡**ï¼šå¯åœ¨ `load_base_model` ä¸­è°ƒç”¨è‡ªå®šä¹‰çš„ `create_model` æˆ–åŠ è½½å¤–éƒ¨ checkpointï¼Œåªè¦è¿”å›çš„æ¨¡å‹æ¥å£å…¼å®¹ `create_trainer`ã€`create_reward_model` å³å¯ã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L145-L167ã€‘
- **å®šåˆ¶å¥–åŠ±ç›®æ ‡**ï¼š`RewardTrainer` é»˜è®¤ç»„åˆæ’åºæŸå¤±ä¸æ­£åˆ™é¡¹ï¼Œå¯åœ¨ `create_preference_loss` ä¸­åŠ å…¥ KL æƒ©ç½šæˆ–å…¶ä»–åå¥½å­¦ä¹ ç›®æ ‡ï¼Œå†åœ¨ `reward_trainer` ä¸­æ³¨å…¥æ–°çš„æŸå¤±æƒé‡ã€‚ã€F:src/rl/reward_model/ranking_loss.pyâ€ L11-L196ã€‘ã€F:src/rl/reward_model/reward_trainer.pyâ€ L152-L284ã€‘
- **æ‰©å±• PPO ç­–ç•¥**ï¼š`ppo` å­æ¨¡å—å†…çš„ `PPOTrainer` ä½¿ç”¨ç­–ç•¥æ¢¯åº¦æ¨¡æ¿ï¼Œå¯å¤åˆ¶è¯¥ç»“æ„å®ç° DPOã€KTO ç­‰ç®—æ³•ï¼Œå¹¶åœ¨ `RLHFPipeline` ä¸­æ–°å¢å…¥å£é€‰æ‹©ä¸åŒä¼˜åŒ–å™¨ã€‚ã€F:src/rl/ppo/ppo_trainer.pyâ€ L11-L274ã€‘
- **ç›‘æ§ä¸è¯„ä¼°**ï¼š`evaluate_model` é¢„ç•™äº†å›°æƒ‘åº¦ã€BLEU/ROUGE æŒ‡æ ‡ä½ç½®ï¼Œå¯åœ¨è®­ç»ƒå®Œæˆåè°ƒç”¨è‡ªå®šä¹‰è¯„ä¼°è„šæœ¬å½¢æˆé—­ç¯ï¼›åŒæ—¶å»ºè®®åœ¨ `save_dir` ä¸‹ä¿å­˜è®­ç»ƒæ—¥å¿—ä¸æ ·ä¾‹ï¼Œä¾¿äºå®¡æŸ¥ã€‚ã€F:src/rl/rlhf_pipeline.pyâ€ L364-L418ã€‘

ç”±äºå½“å‰å®ç°ä¸»è¦èšç„¦æµç¨‹ä¸²è”ï¼Œè‹¥åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²ï¼Œè¿˜éœ€è¡¥å……åˆ†å¸ƒå¼è®­ç»ƒã€ç­–ç•¥è¯„ä¼°ä¸å®‰å…¨è¿‡æ»¤ç­‰æ¨¡å—ï¼Œç¡®ä¿è¾“å‡ºè´¨é‡ä¸åˆè§„æ€§ã€‚
