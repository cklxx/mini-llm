# ğŸ¤ RLHF æµç¨‹æ¦‚è§ˆ

`src/rl` æ¨¡å—å±•ç¤ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„ RLHFï¼ˆReinforcement Learning from Human Feedbackï¼‰åŸå‹ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒã€å¥–åŠ±æ¨¡å‹è®­ç»ƒä¸ PPO ç­–ç•¥ä¼˜åŒ–ã€‚

## é…ç½® (`RLHFConfig`)
å…³é”®å­—æ®µï¼š
- `tokenizer_path`ï¼šåºåˆ—åŒ–åˆ†è¯å™¨è·¯å¾„
- `sft_data_path` / `reward_data_path` / `ppo_data_path`ï¼šä¸‰ä¸ªé˜¶æ®µå¯¹åº”çš„æ•°æ®æ–‡ä»¶
- `sft_epochs`ã€`reward_epochs`ã€`ppo_iterations` ç­‰è®­ç»ƒè½®æ¬¡
- `ppo_lr_policy` / `ppo_lr_value` / `ppo_batch_size` ç­‰ PPO å‚æ•°
- `device='auto'`ï¼šæ ¹æ®ç¡¬ä»¶è‡ªåŠ¨é€‰æ‹© CUDA/MPS/CPU

## ç®¡é“ç»“æ„ (`RLHFPipeline`)
1. **åˆå§‹åŒ–**ï¼šé…ç½®æ—¥å¿—ã€åˆ›å»ºè®¾å¤‡ä¸è¾“å‡ºç›®å½•ï¼Œå¹¶ä¿å­˜é…ç½®æ–‡ä»¶
2. **SFT é˜¶æ®µ**ï¼š
   - é€šè¿‡ `create_trainer('sft', ...)` æ„å»º `SFTTrainer`
   - ä½¿ç”¨ `ConversationDataset` æ„é€  DataLoader
   - è®­ç»ƒå®Œæˆåä¿å­˜æ¨¡å‹æƒé‡
3. **å¥–åŠ±æ¨¡å‹é˜¶æ®µ**ï¼š
   - `create_reward_model` / `create_reward_trainer` æ„å»ºå¥–åŠ±æ¨¡å‹
   - `create_preference_dataloader` è¯»å–åå¥½æ•°æ®
   - è®­ç»ƒå®Œæˆåç”Ÿæˆå¥–åŠ±æ¨¡å‹ checkpoint
4. **PPO é˜¶æ®µ**ï¼š
   - `create_value_model` ä¸ `create_ppo_trainer` ç”Ÿæˆç­–ç•¥/ä»·å€¼ç½‘ç»œ
   - åœ¨é‡‡æ ·æ•°æ®ä¸Šæ‰§è¡Œå¤šè½®ç­–ç•¥ä¼˜åŒ–ï¼ŒæŒ‰ `save_interval` ä¿å­˜ä¸­é—´ç»“æœ

## æ•°æ®æ ¼å¼
- **SFT**ï¼šä¸ `ConversationDataset` è¦æ±‚ä¸€è‡´ï¼ˆåŒ…å« user/assistant å¯¹ï¼‰
- **å¥–åŠ±æ¨¡å‹**ï¼šåå¥½æ•°æ®éœ€è¦åŒ…å« `chosen` ä¸ `rejected`
- **PPO**ï¼šprompt åˆ—è¡¨æˆ–å…¶ä»–é‡‡æ ·æ•°æ®ï¼Œç”¨äºç”Ÿæˆæ¢ç´¢åºåˆ—

## è‡ªå®šä¹‰å»ºè®®
- **æ›¿æ¢æ¨¡å‹è§„æ¨¡**ï¼šåœ¨ `load_base_model` ä¸­è°ƒæ•´ `create_model` è°ƒç”¨æˆ–åŠ è½½å·²æœ‰ checkpoint
- **é›†æˆè‡ªæœ‰æ•°æ®**ï¼šé‡å†™ `_load_sft_data`ã€`_load_reward_data`ã€`_load_ppo_data` ä»¥é€‚é…ä¼ä¸šå†…éƒ¨æ ¼å¼
- **ç›‘æ§æ—¥å¿—**ï¼š`save_dir` ä¸‹ä¼šç”Ÿæˆé˜¶æ®µæ€§é…ç½®ä¸æ¨¡å‹ï¼Œå¯ç»“åˆ TensorBoard/Weights & Biases è®°å½•è®­ç»ƒæ›²çº¿
- **æ‰©å±•ç®—æ³•**ï¼šè‹¥éœ€æ”¯æŒæ›´å¤æ‚çš„ RL ç®—æ³•ï¼Œå¯åœ¨ `ppo` å­æ¨¡å—ä¸­æ–°å¢å®ç°ï¼Œå¹¶åœ¨ `RLHFPipeline` ä¸­è°ƒç”¨

å½“å‰å®ç°ä¾§é‡å±•ç¤ºæµç¨‹ç»“æ„ï¼Œå…·ä½“çš„è¯„ä¼°æŒ‡æ ‡ä¸é«˜æ€§èƒ½åˆ†å¸ƒå¼è®­ç»ƒå¯åœ¨æ­¤åŸºç¡€ä¸Šè¿›ä¸€æ­¥æ‰©å±•ã€‚
