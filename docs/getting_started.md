# ğŸš€ å¿«é€Ÿä¸Šæ‰‹ Mini-LLM

æœ¬æŒ‡å—å¸®åŠ©ä½ åœ¨æœ¬åœ°ç¯å¢ƒä¸­å®Œæˆå®‰è£…ã€å‡†å¤‡æœ€å°æ•°æ®é›†ï¼Œå¹¶è·‘é€šä¸€æ¬¡è®­ç»ƒä¸æ¨ç†æµç¨‹ã€‚

## ç¯å¢ƒè¦æ±‚
- Python 3.10+ï¼ˆæ¨è 3.11ï¼Œä¸ `pyproject.toml` ä¿æŒä¸€è‡´ï¼‰
- PyTorch 2.1 åŠä»¥ä¸Šç‰ˆæœ¬ï¼ˆåŒ…å« CUDA/MPS æ”¯æŒæ—¶å¯è‡ªåŠ¨æ£€æµ‹ï¼‰
- `pip` æˆ– `uv` ç”¨äºå®‰è£…ä¾èµ–

## å®‰è£…æ­¥éª¤
```bash
git clone https://github.com/your-org/mini-llm.git
cd mini-llm
pip install -e .
```
> å¦‚æœä½ ä½¿ç”¨ `uv`ï¼Œå¯ä»¥åœ¨ä»“åº“æ ¹ç›®å½•æ‰§è¡Œ `uv sync`ã€‚

## è¿è¡Œæ ‡å‡†è®­ç»ƒæµæ°´çº¿
æ¨èé€šè¿‡è„šæœ¬å…¥å£å¤ç°å®éªŒï¼Œå®ƒä¼šè‡ªåŠ¨è§£æé…ç½®ã€å‡†å¤‡åˆ†è¯å™¨ã€é‡‡æ ·æ•°æ®å¹¶ä¿å­˜æ£€æŸ¥ç‚¹ã€‚

```bash
uv run python scripts/train.py --mode sft --config medium --auto-resume
```

å¸¸ç”¨å‚æ•°ï¼š

- `--mode {pretrain,sft,dpo,rlhf}`ï¼šåˆ‡æ¢è®­ç»ƒé˜¶æ®µï¼›
- `--retrain-tokenizer`ï¼šå¼ºåˆ¶é‡æ–°è®­ç»ƒå¹¶è¦†ç›–ç°æœ‰åˆ†è¯å™¨ï¼›
- `--resume` / `--auto-resume`ï¼šä»æŒ‡å®šæˆ–æœ€æ–°æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒï¼›
- `--learning-rate`ã€`--batch-size`ã€`--warmup-steps`ï¼šå‘½ä»¤è¡Œè¦†ç›–é…ç½®æ–‡ä»¶æ•°å€¼ã€‚ã€F:src/training/pipeline/cli.pyâ€ L8-L117ã€‘

è„šæœ¬å†…éƒ¨ä¼šæ„å»º `MiniGPTTrainer`ï¼Œå®ƒå°† `TrainingEnvironment`ã€`DatasetPreparer`ã€`TrainingLoopRunner` ç­‰æ¨¡å—ä¸²è”èµ·æ¥ï¼Œå¹¶åœ¨è¾“å‡ºç›®å½•ä¸­æŒä¹…åŒ–é…ç½®å¿«ç…§ä¸æ•°æ®é›†ç»Ÿè®¡ï¼Œæ–¹ä¾¿è¿½è¸ªå®éªŒã€‚ã€F:src/training/pipeline/app.pyâ€ L25-L162ã€‘ã€F:src/training/pipeline/data_manager.pyâ€ L24-L214ã€‘

## æœ€å°åŒ–æ•™å­¦ç¤ºä¾‹
è‹¥ä½ æƒ³å¿«é€Ÿç†è§£åº•å±‚è®­ç»ƒå¾ªç¯ï¼Œå¯ä»¥ä½¿ç”¨ä¸‹åˆ—å°‘é‡ä»£ç å¤ç°ä¸€ä¸ªè¯­è¨€æ¨¡å‹ batch çš„è®­ç»ƒï¼š

```python
import torch
from torch.utils.data import DataLoader

from src.model.config import get_tiny_config
from src.model.transformer import MiniGPT
from src.tokenizer.bpe_tokenizer import BPETokenizer
from src.training.datasets import LanguageModelingDataset
from src.training.trainer import PreTrainer

texts = [
    "ä½ å¥½ï¼ŒMini-LLM!",
    "Transformer æ¶æ„æ¼”ç¤º",
    "å°æ¨¡å‹ä¹Ÿèƒ½è®­ç»ƒ",
]

tokenizer = BPETokenizer(vocab_size=256)
tokenizer.train(texts)

dataset = LanguageModelingDataset(texts, tokenizer, max_length=64)
dataloader = DataLoader(dataset, batch_size=2)

config = get_tiny_config()
model = MiniGPT(config)
trainer = PreTrainer(model, tokenizer, device="cpu")

loss = trainer.train_epoch(dataloader)
print(f"epoch loss: {loss:.4f}")
```

## æ¨ç†ç¤ºä¾‹
è®­ç»ƒç»“æŸåå¯ä»¥ç›´æ¥å¤ç”¨æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼š

```python
import torch
from src.inference.generator import TextGenerator, GenerationConfig

prompt = "Mini-LLM"
prompt_ids = tokenizer.encode(prompt, add_special_tokens=True)

text_generator = TextGenerator(model, tokenizer, device="cpu")
output_ids = text_generator.sample_generate(
    input_ids=torch.tensor([prompt_ids]),
    config=GenerationConfig(max_length=40, top_p=0.9, temperature=0.8)
)
print(tokenizer.decode(output_ids[0].tolist()))
```

## ä¸‹ä¸€æ­¥
- é˜…è¯» [model.md](model.md) ç†è§£ `MiniGPTConfig` å¯é…ç½®é¡¹
- é˜…è¯» [data.md](data.md) äº†è§£çœŸå®æ•°æ®é›†éœ€è¦éµå¾ªçš„å­—æ®µæ ¼å¼
- é˜…è¯» [training.md](training.md) æŒæ¡æ··åˆç²¾åº¦ã€æ¢¯åº¦ç´¯ç§¯ã€æ£€æŸ¥ç‚¹ç­‰é«˜çº§ç‰¹æ€§

è‡³æ­¤ï¼Œä½ å·²ç»å¯ä»¥åœ¨æœ¬åœ°å¤ç°å®éªŒå¹¶å¯¹æ¡†æ¶è¿›è¡ŒäºŒæ¬¡å¼€å‘ã€‚
