# ğŸš€ å¿«é€Ÿä¸Šæ‰‹ Mini-LLM

æœ¬æŒ‡å—å¸®åŠ©ä½ åœ¨æœ¬åœ°ç¯å¢ƒä¸­å®Œæˆå®‰è£…ã€å‡†å¤‡æœ€å°æ•°æ®é›†ï¼Œå¹¶è·‘é€šä¸€æ¬¡è®­ç»ƒä¸æ¨ç†æµç¨‹ã€‚

## ç¯å¢ƒè¦æ±‚
- Python 3.10+ï¼ˆæ¨è 3.11ï¼Œä¸ `pyproject.toml` ä¿æŒä¸€è‡´ï¼‰
- PyTorch 2.1 åŠä»¥ä¸Šç‰ˆæœ¬ï¼ˆåŒ…å« CUDA/MPS æ”¯æŒæ—¶å¯è‡ªåŠ¨æ£€æµ‹ï¼‰
- `pip` æˆ– `uv` ç”¨äºå®‰è£…ä¾èµ–

## å®‰è£…æ­¥éª¤
```bash
git clone https://github.com/your-org/mini-llm.git
cd mini-llm
pip install -e .
```
> å¦‚æœä½ ä½¿ç”¨ `uv`ï¼Œå¯ä»¥åœ¨ä»“åº“æ ¹ç›®å½•æ‰§è¡Œ `uv sync`ã€‚

## æœ€å°è®­ç»ƒç¤ºä¾‹
ä¸‹é¢çš„ç¤ºä¾‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨ä»“åº“å†…ç½®ç»„ä»¶è®­ç»ƒä¸€ä¸ªæœ€å°çš„è¯­è¨€æ¨¡å‹ batchï¼Œå¹¶è®°å½•æŸå¤±ã€‚

```python
import torch
from torch.utils.data import DataLoader

from src.model.config import get_tiny_config
from src.model.transformer import MiniGPT
from src.tokenizer.bpe_tokenizer import BPETokenizer
from src.training.trainer import LanguageModelingDataset, PreTrainer

# 1. å‡†å¤‡è®­ç»ƒè¯­æ–™
texts = [
    "ä½ å¥½ï¼ŒMini-LLM!",
    "Transformer æ¶æ„æ¼”ç¤º",
    "å°æ¨¡å‹ä¹Ÿèƒ½è®­ç»ƒ",
]

# 2. è®­ç»ƒä¸€ä¸ªæœ€å° BPE åˆ†è¯å™¨
#    ï¼ˆå®é™…é¡¹ç›®ä¸­è¯·ä½¿ç”¨æ›´å¤§çš„è¯­æ–™å¹¶ç¼“å­˜ tokenizerï¼‰
tokenizer = BPETokenizer(vocab_size=256)
tokenizer.train(texts)

# 3. æ„é€ æ•°æ®é›†ä¸ DataLoader
dataset = LanguageModelingDataset(texts, tokenizer, max_length=64)
dataloader = DataLoader(dataset, batch_size=2)

# 4. åˆå§‹åŒ–æ¨¡å‹ä¸è®­ç»ƒå™¨
config = get_tiny_config()
model = MiniGPT(config)
trainer = PreTrainer(model, tokenizer, device="cpu")

# 5. è¿è¡Œä¸€ä¸ª epoch å¹¶æŸ¥çœ‹æŸå¤±
loss = trainer.train_epoch(dataloader)
print(f"epoch loss: {loss:.4f}")
```

## æ¨ç†ç¤ºä¾‹
è®­ç»ƒç»“æŸåå¯ä»¥ç›´æ¥å¤ç”¨æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼š

```python
import torch
from src.inference.generator import TextGenerator, GenerationConfig

prompt = "Mini-LLM"
prompt_ids = tokenizer.encode(prompt, add_special_tokens=True)

text_generator = TextGenerator(model, tokenizer, device="cpu")
output_ids = text_generator.sample_generate(
    input_ids=torch.tensor([prompt_ids]),
    config=GenerationConfig(max_length=40, top_p=0.9, temperature=0.8)
)
print(tokenizer.decode(output_ids[0].tolist()))
```

## ä¸‹ä¸€æ­¥
- é˜…è¯» [model.md](model.md) ç†è§£ `MiniGPTConfig` å¯é…ç½®é¡¹
- é˜…è¯» [data.md](data.md) äº†è§£çœŸå®æ•°æ®é›†éœ€è¦éµå¾ªçš„å­—æ®µæ ¼å¼
- é˜…è¯» [training.md](training.md) æŒæ¡æ··åˆç²¾åº¦ã€æ¢¯åº¦ç´¯ç§¯ã€æ£€æŸ¥ç‚¹ç­‰é«˜çº§ç‰¹æ€§

è‡³æ­¤ï¼Œä½ å·²ç»å¯ä»¥åœ¨æœ¬åœ°å¤ç°å®éªŒå¹¶å¯¹æ¡†æ¶è¿›è¡ŒäºŒæ¬¡å¼€å‘ã€‚
