# 2024å¹´å¤§è¯­è¨€æ¨¡å‹ä¼˜åŒ–æŠ€æœ¯æ·±åº¦è§£æ

## ğŸ¯ æ‘˜è¦

æœ¬æ–‡æ¡£åŸºäºç¬¬ä¸€æ€§åŸç†ï¼Œæ·±å…¥è§£æ2024å¹´å¤§è¯­è¨€æ¨¡å‹é¢†åŸŸçš„æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯ã€‚é€šè¿‡å¯¹ä¸šç•Œæœ€ä½³å®è·µçš„ç³»ç»Ÿè°ƒç ”ï¼Œä»æ•°å­¦åŸç†åˆ°å·¥ç¨‹å®ç°ï¼Œå…¨é¢å‰–æç°ä»£LLMæ¶æ„çš„æŠ€æœ¯é©æ–°ã€‚

## ğŸ“‹ ç›®å½•

- [Transformeræ¶æ„æ¼”è¿›](#transformeræ¶æ„æ¼”è¿›)
- [ä½ç½®ç¼–ç æŠ€æœ¯ï¼šRoPE](#ä½ç½®ç¼–ç æŠ€æœ¯rope)
- [æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–ï¼šGQA](#æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–gqa)
- [æ¿€æ´»å‡½æ•°é©æ–°ï¼šSwiGLU](#æ¿€æ´»å‡½æ•°é©æ–°swiglu)
- [å½’ä¸€åŒ–æŠ€æœ¯ï¼šRMSNorm](#å½’ä¸€åŒ–æŠ€æœ¯rmsnorm)
- [å†…å­˜ä¼˜åŒ–ï¼šFlash Attention](#å†…å­˜ä¼˜åŒ–flash-attention)
- [æ¶æ„è®¾è®¡åŸåˆ™](#æ¶æ„è®¾è®¡åŸåˆ™)
- [æƒé‡å…±äº«æŠ€æœ¯](#æƒé‡å…±äº«æŠ€æœ¯)
- [å®è·µå»ºè®®](#å®è·µå»ºè®®)

---

## Transformeræ¶æ„æ¼”è¿›

### ä»2017åˆ°2024çš„æŠ€æœ¯é©å‘½

Transformeræ¶æ„è‡ª2017å¹´"Attention Is All You Need"è®ºæ–‡å‘è¡¨ä»¥æ¥ï¼Œç»å†äº†7å¹´çš„æŒç»­ä¼˜åŒ–ã€‚2024å¹´çš„ç°ä»£Transformerä¸åŸç‰ˆç›¸æ¯”ï¼Œåœ¨è®­ç»ƒç¨³å®šæ€§ã€è®¡ç®—æ•ˆç‡å’Œå®ç”¨æ€§æ–¹é¢æœ‰äº†è´¨çš„é£è·ƒã€‚

#### æ ¸å¿ƒå˜åŒ–å¯¹æ¯”

| ç»„ä»¶ | 2017åŸç‰ˆ | 2024ç°ä»£ç‰ˆ | æ”¹è¿›æ•ˆæœ |
|------|----------|------------|----------|
| **å½’ä¸€åŒ–ä½ç½®** | Post-Norm | Pre-Norm | æ¢¯åº¦æµåŠ¨æ”¹å–„ï¼Œè®­ç»ƒç¨³å®šæ€§æå‡ |
| **å½’ä¸€åŒ–æ–¹æ³•** | LayerNorm | RMSNorm | è®¡ç®—é‡å‡å°‘7-64%ï¼Œå†…å­˜æ•ˆç‡æå‡ |
| **ä½ç½®ç¼–ç ** | ç»å¯¹æ­£å¼¦ç¼–ç  | RoPEæ—‹è½¬ç¼–ç  | é•¿åºåˆ—å¤–æ¨èƒ½åŠ›æ˜¾è‘—æå‡ |
| **æ³¨æ„åŠ›æœºåˆ¶** | å¤šå¤´æ³¨æ„åŠ›(MHA) | åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›(GQA) | å†…å­˜ä½¿ç”¨å‡å°‘50-70% |
| **æ¿€æ´»å‡½æ•°** | ReLU/GELU | SwiGLU | æ€§èƒ½æå‡ï¼Œè¡¨è¾¾èƒ½åŠ›å¢å¼º |

### ç°ä»£æ¶æ„ä¼˜åŠ¿

**1. è®­ç»ƒç¨³å®šæ€§é©å‘½**
- Pre-normalizationæ”¹å–„æ·±åº¦ç½‘ç»œçš„æ¢¯åº¦æµåŠ¨
- RMSNormå‡å°‘æ•°å€¼ä¸ç¨³å®šæ€§
- æ®‹å·®è¿æ¥ä¼˜åŒ–ï¼Œæ”¯æŒæ›´æ·±çš„ç½‘ç»œ

**2. è®¡ç®—æ•ˆç‡çªç ´**
- GQAå°†æ¨ç†å†…å­˜éœ€æ±‚é™ä½è‡³åŸæ¥çš„25-50%
- Flash Attentionä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
- æ·±ç˜¦æ¶æ„æå‡å‚æ•°æ•ˆç‡

**3. é•¿åºåˆ—å¤„ç†èƒ½åŠ›**
- RoPEä½ç½®ç¼–ç æ”¯æŒåºåˆ—é•¿åº¦å¤–æ¨
- çº¿æ€§æ³¨æ„åŠ›æ›¿ä»£äºŒæ¬¡å¤æ‚åº¦
- åˆ†æ®µå¤„ç†æŠ€æœ¯çªç ´ä¸Šä¸‹æ–‡é™åˆ¶

---

## ä½ç½®ç¼–ç æŠ€æœ¯ï¼šRoPE

### ç¬¬ä¸€æ€§åŸç†åˆ†æ

æ—‹è½¬ä½ç½®åµŒå…¥(RoPE)æ˜¯2021å¹´æå‡ºçš„é©å‘½æ€§ä½ç½®ç¼–ç æŠ€æœ¯ï¼Œåœ¨2024å¹´å·²æˆä¸ºä¸»æµLLMçš„æ ‡å‡†é…ç½®ã€‚

#### æ•°å­¦åŸºç¡€

**æ ¸å¿ƒæ€æƒ³ï¼šé€šè¿‡å¤æ•°æ—‹è½¬ç¼–ç ç›¸å¯¹ä½ç½®ä¿¡æ¯**

```
RoPE(x, pos) = x * e^(i * pos * Î¸)
```

å…¶ä¸­ï¼š
- `x` æ˜¯è¾“å…¥å‘é‡
- `pos` æ˜¯ä½ç½®ç´¢å¼•
- `Î¸` æ˜¯é¢‘ç‡å‚æ•°ï¼Œé€šå¸¸ä¸º10000
- `i` æ˜¯è™šæ•°å•ä½

**å®é™…å®ç°ï¼ˆå®æ•°å½¢å¼ï¼‰ï¼š**

```python
def apply_rotary_pos_emb(q, k, cos, sin, position_ids):
    """åº”ç”¨æ—‹è½¬ä½ç½®åµŒå…¥

    Args:
        q, k: æŸ¥è¯¢å’Œé”®å¼ é‡ [batch_size, seq_len, num_heads, head_dim]
        cos, sin: é¢„è®¡ç®—çš„ä½™å¼¦å’Œæ­£å¼¦å€¼
        position_ids: ä½ç½®ç´¢å¼•
    """
    # å°†qå’Œkåˆ†ä¸ºå¥‡å¶éƒ¨åˆ†
    q_even, q_odd = q[..., ::2], q[..., 1::2]
    k_even, k_odd = k[..., ::2], k[..., 1::2]

    # åº”ç”¨æ—‹è½¬å˜æ¢
    q_rotated = torch.cat([
        q_even * cos - q_odd * sin,
        q_even * sin + q_odd * cos
    ], dim=-1)

    k_rotated = torch.cat([
        k_even * cos - k_odd * sin,
        k_even * sin + k_odd * cos
    ], dim=-1)

    return q_rotated, k_rotated
```

#### ä¸ºä»€ä¹ˆRoPEä¼˜äºä¼ ç»Ÿä½ç½®ç¼–ç ï¼Ÿ

**1. ç›¸å¯¹ä½ç½®å»ºæ¨¡**
- ä¼ ç»Ÿç»å¯¹ä½ç½®ç¼–ç ï¼šå›ºå®šä½ç½®ä¿¡æ¯ï¼Œæ— æ³•æ³›åŒ–
- RoPEï¼šç¼–ç ç›¸å¯¹ä½ç½®å…³ç³»ï¼Œå¤©ç„¶æ”¯æŒé•¿åº¦å¤–æ¨

**2. å‡ ä½•æ„ä¹‰**
- åœ¨å¤å¹³é¢ä¸Šï¼ŒRoPEå°†ä½ç½®ç¼–ç ä¸ºæ—‹è½¬æ“ä½œ
- ç›¸å¯¹ä½ç½®å·®å¼‚å¯¹åº”æ—‹è½¬è§’åº¦å·®å¼‚
- æ•°å­¦ä¸Šä¼˜é›…ï¼Œç‰©ç†æ„ä¹‰æ¸…æ™°

**3. é•¿åºåˆ—å¤–æ¨èƒ½åŠ›**
```python
# RoPEçš„å¤–æ¨å…¬å¼
def extrapolate_rope(base_theta, max_len, target_len):
    """è®¡ç®—å¤–æ¨æ‰€éœ€çš„thetaè°ƒæ•´"""
    scale = target_len / max_len
    return base_theta * (scale ** (head_dim / (head_dim - 2)))
```

#### ä¸šç•Œåº”ç”¨ç°çŠ¶

**ä¸»æµæ¨¡å‹é‡‡ç”¨æƒ…å†µï¼š**
- âœ… LLaMAç³»åˆ—ï¼šå®Œæ•´RoPEå®ç°
- âœ… ChatGLMï¼šRoPE + GLMæ¶æ„
- âœ… Qwenç³»åˆ—ï¼šRoPE + ä¼˜åŒ–å®ç°
- âœ… DeepSeekï¼šRoPE + MoEæ¶æ„

**æ€§èƒ½åŸºå‡†ï¼š**
- é•¿åºåˆ—ä»»åŠ¡æ€§èƒ½æå‡15-30%
- å¤–æ¨èƒ½åŠ›æå‡æ˜¾è‘—ï¼ˆ4kâ†’32kæ— æ€§èƒ½ä¸‹é™ï¼‰
- è®¡ç®—å¼€é”€å¢åŠ <5%

---

## æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–ï¼šGQA

### ç¬¬ä¸€æ€§åŸç†åˆ†æ

åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›(GQA)æ˜¯2023å¹´æå‡ºçš„æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–æŠ€æœ¯ï¼Œåœ¨å†…å­˜æ•ˆç‡å’Œæ¨ç†é€Ÿåº¦æ–¹é¢å®ç°äº†çªç ´æ€§æ”¹è¿›ã€‚

#### æ ¸å¿ƒåŸç†

**ä¼ ç»Ÿå¤šå¤´æ³¨æ„åŠ›(MHA)çš„ç“¶é¢ˆï¼š**
```
MHAå†…å­˜å¤æ‚åº¦ = O(seq_lenÂ² Ã— num_heads)
KVç¼“å­˜å¤§å° = 2 Ã— seq_len Ã— hidden_dim Ã— num_heads
```

**GQAçš„è§£å†³æ–¹æ¡ˆï¼š**
```
GQAå†…å­˜å¤æ‚åº¦ = O(seq_lenÂ² Ã— num_kv_heads)
KVç¼“å­˜å¤§å° = 2 Ã— seq_len Ã— hidden_dim Ã— num_kv_heads
```

å…¶ä¸­ `num_kv_heads << num_heads`ï¼ˆé€šå¸¸ä¸º1/4ï¼‰

#### æ•°å­¦å»ºæ¨¡

**åˆ†ç»„å…±äº«æœºåˆ¶ï¼š**

```python
def grouped_query_attention(q, k, v, num_groups):
    """åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›å®ç°

    Args:
        q: æŸ¥è¯¢ [batch, seq_len, num_heads, head_dim]
        k, v: é”®å€¼ [batch, seq_len, num_kv_heads, head_dim]
        num_groups: åˆ†ç»„æ•°é‡
    """
    batch_size, seq_len, num_heads, head_dim = q.shape
    num_kv_heads = k.shape[2]

    # è®¡ç®—æ¯ç»„çš„å¤´æ•°
    group_size = num_heads // num_kv_heads

    # é‡å¡‘kå’Œvä»¥åŒ¹é…æŸ¥è¯¢å¤´æ•°
    k_grouped = k.repeat_interleave(group_size, dim=2)
    v_grouped = v.repeat_interleave(group_size, dim=2)

    # æ ‡å‡†æ³¨æ„åŠ›è®¡ç®—
    scores = torch.matmul(q, k_grouped.transpose(-2, -1))
    scores = scores / math.sqrt(head_dim)
    attn_weights = F.softmax(scores, dim=-1)

    output = torch.matmul(attn_weights, v_grouped)
    return output
```

#### å†…å­˜ä¼˜åŒ–æ•ˆæœåˆ†æ

**ç†è®ºåˆ†æï¼š**

å‡è®¾æ¨¡å‹é…ç½®ï¼š
- åºåˆ—é•¿åº¦ï¼š2048
- éšè—ç»´åº¦ï¼š4096
- æ³¨æ„åŠ›å¤´æ•°ï¼š32
- GQAæ¯”ä¾‹ï¼š4:1ï¼ˆ8ä¸ªKVå¤´ï¼‰

**å†…å­˜å¯¹æ¯”ï¼š**
```
MHA KVç¼“å­˜ = 2 Ã— 2048 Ã— 4096 Ã— 32 = 536MB
GQA KVç¼“å­˜ = 2 Ã— 2048 Ã— 4096 Ã— 8 = 134MB
å†…å­˜èŠ‚çœ = 75%
```

**å®é™…æ€§èƒ½åŸºå‡†ï¼š**
- Batch Size=1æ¨ç†ï¼šå†…å­˜å‡å°‘50-70%
- Batch Size=8æ¨ç†ï¼šå†…å­˜å‡å°‘60-80%
- æ¨ç†é€Ÿåº¦æå‡ï¼š20-40%ï¼ˆå—å†…å­˜å¸¦å®½é™åˆ¶åœºæ™¯ï¼‰

#### è´¨é‡ä¿æŒæœºåˆ¶

**ä¸ºä»€ä¹ˆGQAä¸ä¼šæ˜¾è‘—é™ä½æ¨¡å‹è´¨é‡ï¼Ÿ**

1. **ä¿¡æ¯ç†è®ºè§’åº¦**ï¼šé”®å€¼ä¿¡æ¯çš„å†—ä½™åº¦è¾ƒé«˜
2. **æ³¨æ„åŠ›æ¨¡å¼åˆ†æ**ï¼šå¤šæ•°å¤´å­¦ä¹ ç›¸ä¼¼çš„æ³¨æ„åŠ›æ¨¡å¼
3. **å‚æ•°æ•ˆç‡**ï¼šèŠ‚çœçš„å‚æ•°å¯ç”¨äºå¢åŠ æ·±åº¦

**å®éªŒéªŒè¯ï¼š**
- LLaMA-2é‡‡ç”¨GQAï¼Œè´¨é‡ä¸MHAç›¸å½“
- è®­ç»ƒæ•ˆç‡æå‡ï¼Œæ”¶æ•›é€Ÿåº¦æ›´å¿«
- é•¿åºåˆ—ä»»åŠ¡è¡¨ç°æ›´ä¼˜

---

## æ¿€æ´»å‡½æ•°é©æ–°ï¼šSwiGLU

### ç¬¬ä¸€æ€§åŸç†åˆ†æ

SwiGLUæ˜¯GLU(Gated Linear Unit)å®¶æ—çš„æœ€æ–°æˆå‘˜ï¼Œç»“åˆäº†Swishæ¿€æ´»å‡½æ•°å’Œé—¨æ§æœºåˆ¶ï¼Œåœ¨ç°ä»£LLMä¸­è¡¨ç°ä¼˜å¼‚ã€‚

#### æ•°å­¦åŸç†

**SwiGLUå®šä¹‰ï¼š**
```
SwiGLU(x, W, V) = Swish(xW) âŠ™ (xV)
å…¶ä¸­ï¼šSwish(x) = x Ã— Ïƒ(x) = x Ã— (1/(1+e^(-x)))
```

**å®Œæ•´å‰é¦ˆç½‘ç»œï¼š**
```python
class SwiGLUFFN(nn.Module):
    def __init__(self, hidden_size, intermediate_size):
        super().__init__()
        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)

    def forward(self, x):
        # é—¨æ§è·¯å¾„å’Œä¸ŠæŠ•å½±è·¯å¾„
        gate = self.gate_proj(x)
        up = self.up_proj(x)

        # SwiGLUæ¿€æ´»
        hidden = F.silu(gate) * up  # F.silu = Swish

        # ä¸‹æŠ•å½±
        return self.down_proj(hidden)
```

#### ä¸ºä»€ä¹ˆSwiGLUä¼˜äºGELUï¼Ÿ

**1. é—¨æ§æœºåˆ¶çš„ä¼˜åŠ¿**
- **ä¿¡æ¯æµæ§åˆ¶**ï¼šé—¨æ§è·¯å¾„å¯ä»¥åŠ¨æ€å†³å®šä¿¡æ¯ä¼ é€’
- **è¡¨è¾¾èƒ½åŠ›**ï¼šåŒè·¯å¾„è®¾è®¡å¢å¼ºäº†ç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›
- **æ¢¯åº¦ç‰¹æ€§**ï¼šé¿å…äº†ReLUçš„æ­»ç¥ç»å…ƒé—®é¢˜

**2. Swishæ¿€æ´»çš„ä¼˜åŠ¿**
```python
# æ¢¯åº¦å¯¹æ¯”åˆ†æ
def activation_gradients():
    x = torch.linspace(-5, 5, 1000, requires_grad=True)

    # ReLUæ¢¯åº¦ï¼š0æˆ–1ï¼Œä¸è¿ç»­
    relu_out = F.relu(x)
    relu_grad = torch.autograd.grad(relu_out.sum(), x)[0]

    # GELUæ¢¯åº¦ï¼šå¹³æ»‘ä½†åœ¨è´Ÿå€¼åŒºåŸŸæ¥è¿‘0
    gelu_out = F.gelu(x)
    gelu_grad = torch.autograd.grad(gelu_out.sum(), x)[0]

    # Swishæ¢¯åº¦ï¼šå¹³æ»‘ä¸”åœ¨è´Ÿå€¼åŒºåŸŸéé›¶
    swish_out = F.silu(x)
    swish_grad = torch.autograd.grad(swish_out.sum(), x)[0]

    return relu_grad, gelu_grad, swish_grad
```

**3. è®­ç»ƒç¨³å®šæ€§**
- å¹³æ»‘çš„æ¿€æ´»å‡½æ•°å‡å°‘äº†æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±
- é—¨æ§æœºåˆ¶æä¾›äº†é¢å¤–çš„æ­£åˆ™åŒ–æ•ˆæœ
- æ·±åº¦ç½‘ç»œè®­ç»ƒæ›´åŠ ç¨³å®š

#### æ€§èƒ½åŸºå‡†åˆ†æ

**è®¡ç®—å¤æ‚åº¦å¯¹æ¯”ï¼š**
```
ReLU:    O(n)          # æœ€ç®€å•
GELU:    O(n)          # éœ€è¦erfå‡½æ•°è®¡ç®—
SwiGLU:  O(2n)         # åŒè·¯å¾„ï¼Œä½†å¹¶è¡Œåº¦é«˜
```

**å®é™…æ€§èƒ½è¡¨ç°ï¼š**
- ç›¸åŒå‚æ•°é‡ä¸‹ï¼Œå›°æƒ‘åº¦é™ä½5-10%
- è®­ç»ƒæ”¶æ•›é€Ÿåº¦æå‡15-25%
- ç”Ÿæˆè´¨é‡ä¸»è§‚è¯„ä¼°æå‡æ˜¾è‘—

**ä¸»æµæ¨¡å‹é‡‡ç”¨ï¼š**
- âœ… PaLMï¼šGoogleé¦–ä¸ªå¤§è§„æ¨¡é‡‡ç”¨
- âœ… LLaMAï¼šMetaå…¨ç³»åˆ—é‡‡ç”¨
- âœ… Qwenï¼šé˜¿é‡Œå·´å·´é‡‡ç”¨
- âœ… ChatGLMï¼šæ¸…åé‡‡ç”¨

---

## å½’ä¸€åŒ–æŠ€æœ¯ï¼šRMSNorm

### ç¬¬ä¸€æ€§åŸç†åˆ†æ

å‡æ–¹æ ¹å±‚å½’ä¸€åŒ–(RMSNorm)æ˜¯å¯¹LayerNormçš„ç®€åŒ–ï¼Œé€šè¿‡å»é™¤å‡å€¼ä¸­å¿ƒåŒ–æ“ä½œï¼Œå®ç°äº†è®¡ç®—æ•ˆç‡çš„æ˜¾è‘—æå‡ã€‚

#### æ•°å­¦å¯¹æ¯”

**LayerNormå…¬å¼ï¼š**
```
LayerNorm(x) = Î³ Ã— (x - Î¼) / âˆš(ÏƒÂ² + Îµ) + Î²

å…¶ä¸­ï¼š
Î¼ = (1/d) Î£ xáµ¢           # å‡å€¼
ÏƒÂ² = (1/d) Î£ (xáµ¢ - Î¼)Â²   # æ–¹å·®
```

**RMSNormå…¬å¼ï¼š**
```
RMSNorm(x) = Î³ Ã— x / âˆš((1/d) Î£ xáµ¢Â² + Îµ)

å…¶ä¸­ï¼š
RMS = âˆš((1/d) Î£ xáµ¢Â²)     # å‡æ–¹æ ¹å€¼
```

#### è®¡ç®—å¤æ‚åº¦åˆ†æ

**æ“ä½œæ•°å¯¹æ¯”ï¼ˆå‡è®¾å‘é‡ç»´åº¦ä¸ºdï¼‰ï¼š**

| æ“ä½œ | LayerNorm | RMSNorm | èŠ‚çœæ¯”ä¾‹ |
|------|-----------|---------|----------|
| **åŠ æ³•è¿ç®—** | 2d | 0 | 100% |
| **ä¹˜æ³•è¿ç®—** | 2d | d | 50% |
| **é™¤æ³•è¿ç®—** | d | d | 0% |
| **å¼€æ–¹è¿ç®—** | 1 | 1 | 0% |
| **æ€»è®¡ç®—é‡** | ~5d | ~2d | **60%** |

#### å®ç°ä¼˜åŒ–

**é«˜æ•ˆå®ç°ï¼š**
```python
class RMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)

        # è®¡ç®—å‡æ–¹æ ¹
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)

        return self.weight * hidden_states.to(input_dtype)
```

**CUDAä¼˜åŒ–å®ç°ï¼š**
```cpp
// ä¼ªä»£ç ï¼šCUDAå†…æ ¸ä¼˜åŒ–
__global__ void rms_norm_kernel(float* input, float* output, float* weight,
                               int batch_size, int hidden_size, float eps) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;

    if (tid < batch_size) {
        float sum_sq = 0.0f;

        // è®¡ç®—å¹³æ–¹å’Œ
        for (int i = 0; i < hidden_size; i++) {
            float val = input[tid * hidden_size + i];
            sum_sq += val * val;
        }

        // è®¡ç®—RMS
        float rms = rsqrtf(sum_sq / hidden_size + eps);

        // åº”ç”¨å½’ä¸€åŒ–
        for (int i = 0; i < hidden_size; i++) {
            int idx = tid * hidden_size + i;
            output[idx] = input[idx] * rms * weight[i];
        }
    }
}
```

#### ç†è®ºåŸºç¡€

**ä¸ºä»€ä¹ˆå»é™¤å‡å€¼ä¸­å¿ƒåŒ–ä»ç„¶æœ‰æ•ˆï¼Ÿ**

1. **å¤§æ•°å®šå¾‹æ•ˆåº”**ï¼šåœ¨å¤§æ¨¡å‹ä¸­ï¼Œæ¿€æ´»å€¼çš„å‡å€¼è¶‹äºç¨³å®š
2. **ç›¸å¯¹é‡è¦æ€§**ï¼šæ–¹å·®å½’ä¸€åŒ–çš„æ•ˆæœå ä¸»å¯¼åœ°ä½
3. **ä¿¡æ¯ä¿æŒ**ï¼šå»ä¸­å¿ƒåŒ–ä¸»è¦å½±å“DCåˆ†é‡ï¼Œå¯¹æ¢¯åº¦æµå½±å“è¾ƒå°

**ä½•æ—¶RMSNormç­‰ä»·äºLayerNormï¼Ÿ**
```
å½“ E[x] â‰ˆ 0 æ—¶ï¼ŒRMSNorm â‰ˆ LayerNorm
```

è¿™åœ¨æ·±åº¦ç½‘ç»œçš„ä¸­é—´å±‚ç»å¸¸æˆç«‹ã€‚

#### æ€§èƒ½åŸºå‡†

**å®é™…æµ‹è¯•ç»“æœï¼š**
- **é€Ÿåº¦æå‡**ï¼š7%-64%ï¼ˆå–å†³äºç¡¬ä»¶å’Œæ‰¹å¤§å°ï¼‰
- **å†…å­˜ä½¿ç”¨**ï¼šå‡å°‘çº¦30%ï¼ˆæ— éœ€å­˜å‚¨å‡å€¼ï¼‰
- **æ•°å€¼ç¨³å®šæ€§**ï¼šä¸LayerNormç›¸å½“
- **æ¨¡å‹è´¨é‡**ï¼šåœ¨å¤§æ¨¡å‹ä¸Šæ€§èƒ½ç›¸å½“

**é€‚ç”¨åœºæ™¯å»ºè®®ï¼š**
- âœ… å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå‚æ•°>1Bï¼‰
- âœ… æ¨ç†é€Ÿåº¦æ•æ„Ÿçš„åº”ç”¨
- âœ… å†…å­˜å—é™çš„éƒ¨ç½²ç¯å¢ƒ
- âš ï¸ å°æ¨¡å‹å¯èƒ½éœ€è¦LayerNormçš„ç¨³å®šæ€§

---

## å†…å­˜ä¼˜åŒ–ï¼šFlash Attention

### ç¬¬ä¸€æ€§åŸç†åˆ†æ

Flash Attentionæ˜¯æ–¯å¦ç¦å¤§å­¦2022å¹´æå‡ºçš„IOæ„ŸçŸ¥æ³¨æ„åŠ›ç®—æ³•ï¼Œé€šè¿‡ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼å®ç°äº†é©å‘½æ€§çš„æ•ˆç‡æå‡ã€‚

#### å†…å­˜å±‚æ¬¡ç»“æ„åˆ†æ

**GPUå†…å­˜å±‚æ¬¡ï¼š**
```
SRAM (on-chip):  ~20MB,   å¸¦å®½ 19TB/s    (è¶…å¿«)
HBM (off-chip): ~40GB,   å¸¦å®½ 1.5TB/s   (12xæ…¢)
```

**ä¼ ç»Ÿæ³¨æ„åŠ›çš„å†…å­˜ç“¶é¢ˆï¼š**
```python
# æ ‡å‡†æ³¨æ„åŠ›å®ç°ï¼ˆå†…å­˜å¯†é›†å‹ï¼‰
def standard_attention(Q, K, V):
    # æ­¥éª¤1ï¼šè®¡ç®—æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µ
    S = Q @ K.T  # å¤§å°ï¼š[seq_len, seq_len] - å­˜å‚¨åœ¨HBM

    # æ­¥éª¤2ï¼šåº”ç”¨softmax
    P = softmax(S)  # éœ€è¦è¯»å–Sï¼Œå†™å…¥P - HBMè¯»å†™

    # æ­¥éª¤3ï¼šè®¡ç®—è¾“å‡º
    O = P @ V  # éœ€è¦è¯»å–På’ŒV - å†æ¬¡HBMè¯»å†™

    return O
```

**å†…å­˜è®¿é—®åˆ†æï¼š**
- æ€»HBMè¯»å†™æ¬¡æ•°ï¼šO(seq_lenÂ²)
- ä¸´æ—¶çŸ©é˜µå¤§å°ï¼šseq_len Ã— seq_len
- å†…å­˜å¸¦å®½æˆä¸ºç“¶é¢ˆ

#### Flash Attentionç®—æ³•

**æ ¸å¿ƒæ€æƒ³ï¼šåˆ†å—è®¡ç®— + é‡è®¡ç®—**

```python
def flash_attention(Q, K, V, block_size=64):
    """Flash Attentionä¼ªä»£ç å®ç°"""
    seq_len, head_dim = Q.shape
    num_blocks = (seq_len + block_size - 1) // block_size

    # åˆå§‹åŒ–è¾“å‡ºå’Œç»Ÿè®¡é‡
    O = torch.zeros_like(Q)
    max_vals = torch.full((seq_len,), -torch.inf)
    sum_exp = torch.zeros(seq_len)

    # å¤–å±‚å¾ªç¯ï¼šéå†é”®å€¼å—
    for j in range(num_blocks):
        # åŠ è½½å½“å‰KVå—åˆ°SRAM
        K_j = K[j*block_size:(j+1)*block_size]
        V_j = V[j*block_size:(j+1)*block_size]

        # å†…å±‚å¾ªç¯ï¼šéå†æŸ¥è¯¢å—
        for i in range(num_blocks):
            # åŠ è½½å½“å‰Qå—åˆ°SRAM
            Q_i = Q[i*block_size:(i+1)*block_size]

            # åœ¨SRAMä¸­è®¡ç®—æ³¨æ„åŠ›
            S_ij = Q_i @ K_j.T

            # åœ¨çº¿softmaxæ›´æ–°
            max_new = torch.max(S_ij, dim=-1)[0]
            max_old = max_vals[i*block_size:(i+1)*block_size]
            max_vals[i*block_size:(i+1)*block_size] = torch.max(max_new, max_old)

            # æ›´æ–°è¾“å‡ºï¼ˆç»†èŠ‚çœç•¥ï¼‰
            # ...

    return O
```

#### å…³é”®æŠ€æœ¯åˆ›æ–°

**1. åˆ†å—çŸ©é˜µè®¡ç®—**
```
å°†å¤§çŸ©é˜µ [seq_len, seq_len] åˆ†è§£ä¸ºå°å— [block_size, block_size]
æ¯å—å¯æ”¾å…¥SRAMï¼Œé¿å…HBMè®¿é—®
```

**2. åœ¨çº¿Softmaxç®—æ³•**
```python
def online_softmax(prev_max, prev_sum, new_values):
    """æ•°å€¼ç¨³å®šçš„åœ¨çº¿softmaxæ›´æ–°"""
    new_max = torch.max(new_values)
    global_max = torch.max(prev_max, new_max)

    # é‡æ–°ç¼©æ”¾ä¹‹å‰çš„å’Œ
    prev_sum_rescaled = prev_sum * torch.exp(prev_max - global_max)

    # è®¡ç®—æ–°çš„å’Œ
    new_sum = torch.sum(torch.exp(new_values - global_max))

    return global_max, prev_sum_rescaled + new_sum
```

**3. é‡è®¡ç®—ç­–ç•¥**
- å‰å‘ä¼ æ’­ï¼šä¸å­˜å‚¨å®Œæ•´æ³¨æ„åŠ›çŸ©é˜µ
- åå‘ä¼ æ’­ï¼šæ ¹æ®éœ€è¦é‡æ–°è®¡ç®—
- å†…å­˜æ¢æ—¶é—´ï¼šSRAMè®¡ç®—æ¯”HBMè®¿é—®å¿«12å€

#### å¤æ‚åº¦åˆ†æ

**å†…å­˜å¤æ‚åº¦ï¼š**
```
æ ‡å‡†æ³¨æ„åŠ›ï¼šO(seq_lenÂ²)
Flash Attentionï¼šO(seq_len)  # çº¿æ€§å¤æ‚åº¦ï¼
```

**æ—¶é—´å¤æ‚åº¦ï¼š**
```
æ ‡å‡†æ³¨æ„åŠ›ï¼šO(seq_lenÂ²)
Flash Attentionï¼šO(seq_lenÂ²)  # è®¡ç®—é‡ç›¸åŒï¼Œä½†IOæ›´é«˜æ•ˆ
```

**IOå¤æ‚åº¦ï¼ˆå…³é”®æŒ‡æ ‡ï¼‰ï¼š**
```
æ ‡å‡†æ³¨æ„åŠ›ï¼šO(seq_lenÂ² Ã— head_dim)  # HBMè¯»å†™
Flash Attentionï¼šO(seq_len Ã— head_dim)  # å‡å°‘ä¸€ä¸ªæ•°é‡çº§
```

#### å®é™…æ€§èƒ½æå‡

**åŸºå‡†æµ‹è¯•ç»“æœï¼š**

| åºåˆ—é•¿åº¦ | æ ‡å‡†æ³¨æ„åŠ› | Flash Attention | é€Ÿåº¦æå‡ | å†…å­˜èŠ‚çœ |
|----------|------------|----------------|----------|----------|
| 512 | 1.0x | 1.2x | 20% | 2x |
| 2048 | 1.0x | 2.1x | 110% | 8x |
| 8192 | 1.0x | 3.8x | 280% | 32x |
| 16384 | OOM | 5.2x | âˆ | âˆ |

**ç«¯åˆ°ç«¯æ€§èƒ½ï¼š**
- BERT-largeï¼š15%åŠ é€Ÿ
- GPT-2ï¼š3xåŠ é€Ÿ
- é•¿åºåˆ—ä»»åŠ¡ï¼š5-10xåŠ é€Ÿ

---

## æ¶æ„è®¾è®¡åŸåˆ™

### æ·±ç˜¦æ¶æ„(Deep-Thin Architecture)

#### ç†è®ºåŸºç¡€

**å‚æ•°æ•ˆç‡ç†è®ºï¼š**
åŸºäºMobileLLMç­‰ç ”ç©¶ï¼Œæ·±ç˜¦æ¶æ„åœ¨å›ºå®šå‚æ•°é¢„ç®—ä¸‹èƒ½å®ç°æ›´å¥½çš„æ€§èƒ½ã€‚

**æ•°å­¦å»ºæ¨¡ï¼š**
```
æ€»å‚æ•°é‡ = è¯æ±‡è¡¨åµŒå…¥ + L Ã— (æ³¨æ„åŠ›å‚æ•° + FFNå‚æ•°)

å…¶ä¸­ï¼š
- æ³¨æ„åŠ›å‚æ•° â‰ˆ 4 Ã— dÂ²
- FFNå‚æ•° â‰ˆ 8 Ã— dÂ² (SwiGLUéœ€è¦2/3å€å‚æ•°)
- æœ€ä¼˜æ¯”ä¾‹ï¼šL âˆ âˆš(P/dÂ²)ï¼Œå…¶ä¸­Pä¸ºæ€»å‚æ•°é¢„ç®—
```

**æ·±ç˜¦æ¶æ„çš„ä¼˜åŠ¿ï¼š**

1. **è¡¨è¾¾èƒ½åŠ›**ï¼šæ›´å¤šå±‚æ•°æä¾›æ›´ä¸°å¯Œçš„è¡¨å¾å­¦ä¹ 
2. **å‚æ•°æ•ˆç‡**ï¼šæ·±åº¦ä¼˜äºå®½åº¦çš„æ”¶ç›Šé€’å‡
3. **è®­ç»ƒç¨³å®šæ€§**ï¼šç°ä»£å½’ä¸€åŒ–æŠ€æœ¯æ”¯æŒæ·±åº¦ç½‘ç»œ

#### MiniGPTçš„æ·±ç˜¦è®¾è®¡

**é…ç½®å¯¹æ¯”ï¼š**

| æ¨¡å‹ | å±‚æ•° | éšè—ç»´åº¦ | å‚æ•°é‡ | æ·±å®½æ¯” |
|------|------|----------|--------|--------|
| **Tiny** | 8 | 128 | ~1M | 16:1 |
| **Small** | 12 | 384 | ~25M | 8:1 |
| **Medium** | 20 | 640 | ~112M | 6.25:1 |

**ä¸ä¼ ç»Ÿè®¾è®¡å¯¹æ¯”ï¼š**
```
ä¼ ç»Ÿè®¾è®¡: 6å±‚ Ã— 768ç»´ = ~25Må‚æ•°
æ·±ç˜¦è®¾è®¡: 12å±‚ Ã— 384ç»´ = ~25Må‚æ•°
æ€§èƒ½æå‡: +2.7% ~ +4.3% (åœ¨ç›¸åŒå‚æ•°é‡ä¸‹)
```

### å‚æ•°åˆ†é…ç­–ç•¥

#### æœ€ä¼˜åˆ†é…åŸç†

**Kaplan Scaling Lawsé€‚é…ï¼š**
```python
def optimal_config(param_budget):
    """æ ¹æ®å‚æ•°é¢„ç®—è®¡ç®—æœ€ä¼˜é…ç½®"""
    # åŸºäºscaling lawçš„ç»éªŒå…¬å¼
    vocab_ratio = 0.15  # è¯æ±‡è¡¨å æ€»å‚æ•°15%
    attention_ratio = 0.25  # æ³¨æ„åŠ›å 25%
    ffn_ratio = 0.60  # FFNå 60%

    vocab_params = param_budget * vocab_ratio
    model_params = param_budget * (1 - vocab_ratio)

    # è®¡ç®—æœ€ä¼˜éšè—ç»´åº¦
    hidden_size = int(math.sqrt(model_params / (12 * 1.5)))  # 12å±‚åŸºå‡†
    num_layers = int(model_params / (hidden_size ** 2 * 12))

    return hidden_size, num_layers
```

#### å®é™…é…ç½®éªŒè¯

**100MBç›®æ ‡æ¨¡å‹åˆ†æï¼š**
```python
# MiniGPT Mediumé…ç½®
config = {
    'vocab_size': 20000,        # è¯æ±‡è¡¨: 20K
    'hidden_size': 640,         # éšè—ç»´åº¦: 640
    'num_layers': 20,           # å±‚æ•°: 20
    'num_heads': 16,            # æ³¨æ„åŠ›å¤´: 16
    'num_kv_heads': 4,          # KVå¤´: 4 (GQAä¼˜åŒ–)
    'intermediate_size': 2048,   # FFNä¸­é—´ç»´åº¦
}

# å‚æ•°é‡åˆ†æ
embedding_params = 20000 * 640 = 12.8M
transformer_params = 20 * (640^2 * 12) = 98.3M  # æ³¨æ„åŠ›+FFN
total_params = 111.1M â‰ˆ 100MBç›®æ ‡
```

---

## æƒé‡å…±äº«æŠ€æœ¯

### ç¬¬ä¸€æ€§åŸç†åˆ†æ

æƒé‡å…±äº«(Weight Tying)æ˜¯ä¸€ç§å‡å°‘å‚æ•°é‡å¹¶æå‡æ¨¡å‹æ€§èƒ½çš„æŠ€æœ¯ï¼Œé€šè¿‡å…±äº«è¾“å…¥åµŒå…¥å±‚å’Œè¾“å‡ºæŠ•å½±å±‚çš„æƒé‡å®ç°ã€‚

#### ç†è®ºåŸºç¡€

**è¯­è¨€å»ºæ¨¡çš„å¯¹å¶æ€§ï¼š**
```
è¾“å…¥åµŒå…¥: word_id â†’ vector
è¾“å‡ºæŠ•å½±: vector â†’ word_id
```

ä¸¤ä¸ªè¿‡ç¨‹åœ¨è¯­ä¹‰ä¸Šå­˜åœ¨å¯¹å¶å…³ç³»ï¼Œå¯ä»¥å…±äº«å‚æ•°ã€‚

#### æ•°å­¦å»ºæ¨¡

**ä¼ ç»Ÿå®ç°ï¼š**
```python
class TraditionalLM(nn.Module):
    def __init__(self, vocab_size, hidden_size):
        self.embeddings = nn.Embedding(vocab_size, hidden_size)  # VÃ—H
        self.lm_head = nn.Linear(hidden_size, vocab_size)        # HÃ—V
        # æ€»å‚æ•°: 2 Ã— V Ã— H
```

**æƒé‡å…±äº«å®ç°ï¼š**
```python
class TiedLM(nn.Module):
    def __init__(self, vocab_size, hidden_size):
        self.embeddings = nn.Embedding(vocab_size, hidden_size)
        # å…±äº«æƒé‡ï¼Œæ— éœ€é¢å¤–å‚æ•°
        # æ€»å‚æ•°: V Ã— H (èŠ‚çœ50%)

    def forward(self, input_ids):
        hidden = self.transformer(self.embeddings(input_ids))
        # ä½¿ç”¨åµŒå…¥æƒé‡çš„è½¬ç½®ä½œä¸ºè¾“å‡ºæŠ•å½±
        logits = F.linear(hidden, self.embeddings.weight)
        return logits
```

#### æ¢¯åº¦è®¡ç®—

**å…±äº«æƒé‡çš„æ¢¯åº¦èšåˆï¼š**
```python
def tied_backward():
    """æƒé‡å…±äº«æ—¶çš„æ¢¯åº¦è®¡ç®—"""
    # åµŒå…¥å±‚æ¢¯åº¦
    grad_embedding = compute_embedding_grad()

    # è¾“å‡ºå±‚æ¢¯åº¦
    grad_output = compute_output_grad()

    # å…±äº«æƒé‡æ¥æ”¶ä¸¤éƒ¨åˆ†æ¢¯åº¦çš„å’Œ
    shared_weight.grad = grad_embedding + grad_output.T
```

#### æ€§èƒ½å½±å“åˆ†æ

**å‚æ•°èŠ‚çœæ•ˆæœï¼š**
```python
def weight_tying_savings(vocab_size, hidden_size):
    """è®¡ç®—æƒé‡å…±äº«çš„å‚æ•°èŠ‚çœ"""
    original_params = 2 * vocab_size * hidden_size
    tied_params = vocab_size * hidden_size
    savings = (original_params - tied_params) / original_params
    return savings  # é€šå¸¸ä¸º50%
```

**åœ¨MiniGPTä¸­çš„åº”ç”¨ï¼š**
```
Mediumé…ç½®: vocab_size=20000, hidden_size=640
èŠ‚çœå‚æ•°: 20000 Ã— 640 = 12.8M
èŠ‚çœæ¯”ä¾‹: 12.8M / 111.1M = 11.5%
```

#### è´¨é‡å½±å“

**æ­£å‘æ•ˆæœï¼š**
1. **æ­£åˆ™åŒ–æ•ˆåº”**ï¼šå¼ºåˆ¶è¾“å…¥è¾“å‡ºè¡¨å¾ä¸€è‡´æ€§
2. **å‚æ•°æ•ˆç‡**ï¼šç›¸åŒå‚æ•°ä¸‹æ¨¡å‹å®¹é‡æ›´å¤§
3. **è®­ç»ƒç¨³å®šæ€§**ï¼šå‡å°‘è¿‡æ‹Ÿåˆé£é™©

**å®éªŒè¯æ®ï¼š**
- åœ¨ä¸­å°æ¨¡å‹ä¸Šæ™®éæå‡æ€§èƒ½
- å¤§æ¨¡å‹(>10B)æ•ˆæœä¸æ˜æ˜¾
- ç‰¹å®šä»»åŠ¡(ç¿»è¯‘)æ”¶ç›Šæ˜¾è‘—

---

## å®è·µå»ºè®®

### æŠ€æœ¯é€‰å‹æŒ‡å—

#### æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯ä¼˜å…ˆçº§

**Tier 1 (å¿…é€‰æŠ€æœ¯):**
1. **RMSNorm**: ç®€å•é«˜æ•ˆï¼Œæ— å‰¯ä½œç”¨
2. **RoPE**: ä½ç½®ç¼–ç çš„æ˜ç¡®å‡çº§
3. **Pre-Norm**: è®­ç»ƒç¨³å®šæ€§æ˜¾è‘—æå‡

**Tier 2 (å¼ºçƒˆæ¨è):**
1. **GQA**: å†…å­˜æ•ˆç‡å¤§å¹…æå‡
2. **æƒé‡å…±äº«**: å‚æ•°æ•ˆç‡æå‡
3. **SwiGLU**: è¡¨è¾¾èƒ½åŠ›å¢å¼º

**Tier 3 (æ¡ä»¶é‡‡ç”¨):**
1. **Flash Attention**: é•¿åºåˆ—å¿…éœ€
2. **Deep-Thinè®¾è®¡**: å‚æ•°å—é™æ—¶é‡‡ç”¨

#### é…ç½®å‚æ•°å»ºè®®

**å°æ¨¡å‹(1-10Må‚æ•°):**
```python
tiny_config = {
    'hidden_size': 128,
    'num_layers': 8,           # æ·±ç˜¦è®¾è®¡
    'num_heads': 4,
    'num_kv_heads': 1,         # æ¿€è¿›çš„GQA
    'use_rope': True,
    'hidden_act': 'swiglu',
    'tie_word_embeddings': True,
}
```

**ä¸­ç­‰æ¨¡å‹(10-100Må‚æ•°):**
```python
small_config = {
    'hidden_size': 384,
    'num_layers': 12,          # å¹³è¡¡æ·±åº¦
    'num_heads': 12,
    'num_kv_heads': 3,         # 4:1 GQAæ¯”ä¾‹
    'use_rope': True,
    'hidden_act': 'swiglu',
    'tie_word_embeddings': True,
}
```

**å¤§æ¨¡å‹(100M+å‚æ•°):**
```python
medium_config = {
    'hidden_size': 640,
    'num_layers': 20,          # æ·±ç˜¦æ¶æ„
    'num_heads': 16,
    'num_kv_heads': 4,         # ä¿å®ˆçš„GQA
    'use_rope': True,
    'hidden_act': 'swiglu',
    'tie_word_embeddings': True,
    'flash_attn': True,        # é•¿åºåˆ—ä¼˜åŒ–
}
```

### è®­ç»ƒä¼˜åŒ–ç­–ç•¥

#### å­¦ä¹ ç‡è°ƒåº¦

**ç°ä»£LLMçš„æœ€ä½³å®è·µï¼š**
```python
def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):
    """ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦"""
    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            # çº¿æ€§warmup
            return float(current_step) / float(max(1, num_warmup_steps))

        # ä½™å¼¦é€€ç«
        progress = float(current_step - num_warmup_steps)
        progress /= float(max(1, num_training_steps - num_warmup_steps))
        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))

    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
```

**å‚æ•°å»ºè®®ï¼š**
- Warmupæ­¥æ•°ï¼šæ€»æ­¥æ•°çš„3-5%
- å³°å€¼å­¦ä¹ ç‡ï¼š1e-4 (å°æ¨¡å‹) åˆ° 5e-5 (å¤§æ¨¡å‹)
- æœ€å°å­¦ä¹ ç‡ï¼šå³°å€¼çš„10%

#### ä¼˜åŒ–å™¨é€‰æ‹©

**AdamWé…ç½®ï¼š**
```python
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=1e-4,
    betas=(0.9, 0.95),      # Î²2=0.95 for LLM
    eps=1e-8,
    weight_decay=0.1,       # æƒé‡è¡°å‡
)
```

**å…³é”®å‚æ•°è¯´æ˜ï¼š**
- `beta2=0.95`: é€‚åˆå¤§æ¨¡å‹çš„äºŒé˜¶çŸ©ä¼°è®¡
- `weight_decay=0.1`: é˜²æ­¢è¿‡æ‹Ÿåˆ
- `eps=1e-8`: æ•°å€¼ç¨³å®šæ€§

### éƒ¨ç½²ä¼˜åŒ–

#### æ¨ç†ä¼˜åŒ–æŠ€æœ¯

**1. KV Cacheç®¡ç†ï¼š**
```python
class KVCache:
    def __init__(self, max_batch_size, max_seq_len, num_heads, head_dim):
        self.k_cache = torch.zeros(max_batch_size, num_heads, max_seq_len, head_dim)
        self.v_cache = torch.zeros(max_batch_size, num_heads, max_seq_len, head_dim)
        self.seq_len = 0

    def update(self, new_k, new_v):
        batch_size, num_heads, seq_len, head_dim = new_k.shape

        # æ›´æ–°ç¼“å­˜
        self.k_cache[:batch_size, :, self.seq_len:self.seq_len+seq_len] = new_k
        self.v_cache[:batch_size, :, self.seq_len:self.seq_len+seq_len] = new_v
        self.seq_len += seq_len

        return (
            self.k_cache[:batch_size, :, :self.seq_len],
            self.v_cache[:batch_size, :, :self.seq_len]
        )
```

**2. æ‰¹å¤„ç†ä¼˜åŒ–ï¼š**
```python
def dynamic_batching(requests, max_batch_size, max_tokens):
    """åŠ¨æ€æ‰¹å¤„ç†ï¼Œå¹³è¡¡å»¶è¿Ÿå’Œååé‡"""
    batches = []
    current_batch = []
    current_tokens = 0

    for req in requests:
        req_tokens = len(req.input_ids)

        if (len(current_batch) >= max_batch_size or
            current_tokens + req_tokens > max_tokens):
            if current_batch:
                batches.append(current_batch)
                current_batch = []
                current_tokens = 0

        current_batch.append(req)
        current_tokens += req_tokens

    if current_batch:
        batches.append(current_batch)

    return batches
```

### ç›‘æ§å’Œè°ƒè¯•

#### å…³é”®æŒ‡æ ‡ç›‘æ§

**è®­ç»ƒæŒ‡æ ‡ï¼š**
```python
def log_training_metrics(loss, grad_norm, lr, step):
    """è®°å½•è®­ç»ƒå…³é”®æŒ‡æ ‡"""
    metrics = {
        'loss': loss.item(),
        'perplexity': math.exp(loss.item()),
        'grad_norm': grad_norm,
        'learning_rate': lr,
        'step': step,
    }

    # å†…å­˜ä½¿ç”¨ç›‘æ§
    if torch.cuda.is_available():
        metrics['gpu_memory_allocated'] = torch.cuda.memory_allocated() / 1e9
        metrics['gpu_memory_reserved'] = torch.cuda.memory_reserved() / 1e9

    return metrics
```

**æ¨ç†æ€§èƒ½ç›‘æ§ï¼š**
```python
def benchmark_inference(model, tokenizer, test_prompts):
    """æ¨ç†æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    import time

    total_tokens = 0
    total_time = 0

    for prompt in test_prompts:
        input_ids = tokenizer.encode(prompt)

        start_time = time.time()
        with torch.no_grad():
            output = model.generate(
                input_ids=torch.tensor([input_ids]),
                max_length=100,
                do_sample=True,
                temperature=0.7
            )
        end_time = time.time()

        generated_tokens = len(output[0]) - len(input_ids)
        total_tokens += generated_tokens
        total_time += (end_time - start_time)

    throughput = total_tokens / total_time
    print(f"æ¨ç†ååé‡: {throughput:.1f} tokens/sec")

    return throughput
```

---

## æ€»ç»“

### 2024å¹´LLMæŠ€æœ¯æ ˆ

ç°ä»£å¤§è¯­è¨€æ¨¡å‹å·²ç»å½¢æˆäº†ç›¸å¯¹æ ‡å‡†åŒ–çš„æŠ€æœ¯æ ˆï¼š

**æ ¸å¿ƒæ¶æ„ç»„ä»¶ï¼š**
- âœ… **Transformer + Pre-Norm**: ç¨³å®šè®­ç»ƒçš„åŸºç¡€
- âœ… **RoPEä½ç½®ç¼–ç **: é•¿åºåˆ—å¤„ç†çš„æ ‡å‡†
- âœ… **GQAæ³¨æ„åŠ›**: å†…å­˜æ•ˆç‡çš„å…³é”®
- âœ… **SwiGLUæ¿€æ´»**: è¡¨è¾¾èƒ½åŠ›çš„æå‡
- âœ… **RMSNormå½’ä¸€åŒ–**: è®¡ç®—æ•ˆç‡çš„ä¼˜åŒ–

**è®­ç»ƒä¼˜åŒ–æŠ€æœ¯ï¼š**
- âœ… **æƒé‡å…±äº«**: å‚æ•°æ•ˆç‡æå‡
- âœ… **æ·±ç˜¦æ¶æ„**: åœ¨å—é™é¢„ç®—ä¸‹çš„æœ€ä¼˜è®¾è®¡
- âœ… **Flash Attention**: é•¿åºåˆ—çš„å¿…éœ€æŠ€æœ¯

### æŠ€æœ¯æ¼”è¿›è¶‹åŠ¿

**2025å¹´å±•æœ›ï¼š**
1. **ç¨€ç–æ³¨æ„åŠ›**ï¼šè¿›ä¸€æ­¥å‡å°‘è®¡ç®—å¤æ‚åº¦
2. **MoEæ¶æ„**ï¼šå‚æ•°ä¸è®¡ç®—çš„è§£è€¦
3. **é‡åŒ–æŠ€æœ¯**ï¼šæ¨ç†æ•ˆç‡çš„æè‡´ä¼˜åŒ–
4. **å¤šæ¨¡æ€èåˆ**ï¼šè¶…è¶Šæ–‡æœ¬çš„è¡¨å¾å­¦ä¹ 

**å®è·µå»ºè®®ï¼š**
1. ä¼˜å…ˆé‡‡ç”¨å·²éªŒè¯çš„Tier 1æŠ€æœ¯
2. æ ¹æ®å…·ä½“åœºæ™¯é€‰æ‹©Tier 2æŠ€æœ¯
3. æŒç»­å…³æ³¨æ–°å…´æŠ€æœ¯çš„å‘å±•
4. é‡è§†å·¥ç¨‹å®ç°çš„ç»†èŠ‚ä¼˜åŒ–

è¿™äº›æŠ€æœ¯çš„ç»„åˆä½¿ç”¨ï¼Œä½¿å¾—ç°ä»£LLMåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œå…·å¤‡äº†æ›´å¥½çš„å¯éƒ¨ç½²æ€§å’Œå®ç”¨æ€§ã€‚MiniGPTé¡¹ç›®æ­£æ˜¯è¿™äº›æŠ€æœ¯çš„å®Œæ•´å®ç°ï¼Œä¸ºç†è§£å’Œåº”ç”¨ç°ä»£LLMæŠ€æœ¯æä¾›äº†å®è´µçš„å‚è€ƒã€‚

---

*æœ¬æ–‡æ¡£åŸºäº2024å¹´æœ€æ–°ç ”ç©¶å’Œå·¥ä¸šç•Œæœ€ä½³å®è·µç¼–å†™ï¼Œæ—¨åœ¨ä¸ºå¤§è¯­è¨€æ¨¡å‹çš„ç ”ç©¶è€…å’Œå·¥ç¨‹å¸ˆæä¾›æ·±å…¥çš„æŠ€æœ¯å‚è€ƒã€‚*