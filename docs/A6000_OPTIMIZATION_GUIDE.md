# A6000 GPU è®­ç»ƒä¼˜åŒ–æŒ‡å—

## ç¡¬ä»¶é…ç½®
- **GPU**: NVIDIA A6000 (48GB VRAM)
- **CPU**: 16æ ¸
- **å†…å­˜**: 60GB
- **å­˜å‚¨**: 200GB å·¥ä½œç©ºé—´

## ä¼˜åŒ–å‰çš„é—®é¢˜

### ğŸ”´ æ€§èƒ½ç“¶é¢ˆ
1. **GPUåˆ©ç”¨ç‡ä½**: ä»…30%
2. **æ˜¾å­˜å ç”¨å¤§**: æœªå……åˆ†åˆ©ç”¨48GBæ˜¾å­˜
3. **æ•°æ®åŠ è½½æ…¢**: å•è¿›ç¨‹åŠ è½½(num_workers=0)
4. **Batch Sizeè¿‡å°**: batch_size=12å¯¹äº48GBæ˜¾å­˜å¤ªä¿å®ˆ

### ğŸ”´ æ ¹æœ¬åŸå› 
- æ•°æ®åŠ è½½æˆä¸ºç“¶é¢ˆï¼ŒGPUç­‰å¾…æ•°æ®
- æœªå¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
- æ¢¯åº¦ç´¯ç§¯é…ç½®ä¸åˆç†
- æœªå……åˆ†åˆ©ç”¨å¤šæ ¸CPU

## ä¼˜åŒ–æ–¹æ¡ˆ

### âœ… 1. Batch Sizeä¼˜åŒ– (training_config.py:137-138)
```python
# ä¼˜åŒ–å‰
batch_size = 12
gradient_accumulation_steps = max(1, 128 // 12) = 10

# ä¼˜åŒ–å
batch_size = 32  # æå‡2.7å€
gradient_accumulation_steps = 4
# æœ‰æ•ˆbatch = 32 Ã— 4 = 128 (ä¿æŒä¸å˜)
```

**é¢„æœŸæ•ˆæœ**:
- æ›´é«˜çš„GPUåˆ©ç”¨ç‡
- æ›´å¥½çš„å†…å­˜å¸¦å®½åˆ©ç”¨
- æ›´ç¨³å®šçš„æ¢¯åº¦æ›´æ–°

### âœ… 2. æ•°æ®åŠ è½½ä¼˜åŒ– (training_config.py:107-114)
```python
# ä¼˜åŒ–å‰
num_workers = 0  # å•è¿›ç¨‹
prefetch_factor = None

# ä¼˜åŒ–å
num_workers = 8  # ä½¿ç”¨8ä¸ªworkerè¿›ç¨‹ (16æ ¸CPUçš„ä¸€åŠ)
prefetch_factor = 4  # æ¯ä¸ªworkeré¢„å–4ä¸ªbatch
pin_memory = True
persistent_workers = True
```

**é¢„æœŸæ•ˆæœ**:
- æ•°æ®åŠ è½½å¹¶è¡ŒåŒ–ï¼Œæ¶ˆé™¤CPU-GPUæ•°æ®ä¼ è¾“ç“¶é¢ˆ
- é¢„å–æœºåˆ¶ç¡®ä¿GPUå§‹ç»ˆæœ‰æ•°æ®å¯å¤„ç†
- GPUåˆ©ç”¨ç‡é¢„è®¡æå‡è‡³70-90%

### âœ… 3. æ··åˆç²¾åº¦è®­ç»ƒ (scripts/train.py:316-320)
```python
# å¯ç”¨FP16è‡ªåŠ¨æ··åˆç²¾åº¦
scaler = torch.cuda.amp.GradScaler()
with torch.cuda.amp.autocast():
    outputs = model(input_ids)
    loss = criterion(...)
```

**é¢„æœŸæ•ˆæœ**:
- æ˜¾å­˜å ç”¨å‡å°‘çº¦40-50%
- è®­ç»ƒé€Ÿåº¦æå‡30-40%
- æ”¯æŒæ›´å¤§çš„batch size
- Tensor CoreåŠ é€Ÿ

### âœ… 4. æ¢¯åº¦ç´¯ç§¯ä¼˜åŒ– (scripts/train.py:360-446)
```python
# ä¼˜åŒ–åçš„æ¢¯åº¦ç´¯ç§¯é€»è¾‘
for batch_idx, batch in enumerate(data_loader):
    # å‰å‘+åå‘ä¼ æ’­
    loss = loss / accumulation_steps
    loss.backward()

    # åªåœ¨ç´¯ç§¯æ­¥æ•°è¾¾åˆ°æ—¶æ›´æ–°å‚æ•°
    if (batch_idx + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

**é¢„æœŸæ•ˆæœ**:
- å‡å°‘optimizer.step()è°ƒç”¨é¢‘ç‡
- æ›´ç¨³å®šçš„å¤§batchè®­ç»ƒ
- ä¿æŒæœ‰æ•ˆbatch size = 128

### âœ… 5. æ•°æ®ä¼ è¾“ä¼˜åŒ– (scripts/train.py:375)
```python
# ä¼˜åŒ–å‰
batch = batch.to(self.device)

# ä¼˜åŒ–å
batch = batch.to(self.device, non_blocking=True)
```

**é¢„æœŸæ•ˆæœ**:
- CPUåˆ°GPUå¼‚æ­¥æ•°æ®ä¼ è¾“
- ä¸è®¡ç®—å¹¶è¡Œæ‰§è¡Œ

## æ€§èƒ½é¢„æœŸ

### è®­ç»ƒé€Ÿåº¦æå‡
| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡ |
|------|--------|--------|------|
| GPUåˆ©ç”¨ç‡ | ~30% | 70-90% | **2-3å€** |
| Batchå¤„ç†é€Ÿåº¦ | åŸºçº¿ | 2-3å€ | **2-3å€** |
| æ¯æ­¥è®­ç»ƒæ—¶é—´ | åŸºçº¿ | 0.4-0.5å€ | **2-2.5å€** |
| æ˜¾å­˜å ç”¨ | ~30GB | ~20-25GB | **èŠ‚çœ20-30%** |

### èµ„æºåˆ©ç”¨
- **GPUæ˜¾å­˜**: ä»30GBé™è‡³20-25GB (æ··åˆç²¾åº¦)
- **GPUåˆ©ç”¨ç‡**: ä»30%æå‡è‡³70-90%
- **CPUåˆ©ç”¨ç‡**: 8ä¸ªworkerè¿›ç¨‹å……åˆ†åˆ©ç”¨å¤šæ ¸
- **æ•°æ®åå**: 8Ã—é¢„å–ä¿è¯æŒç»­æ•°æ®ä¾›åº”

## ä½¿ç”¨æ–¹æ³•

### 1. ç›´æ¥è®­ç»ƒï¼ˆè‡ªåŠ¨åº”ç”¨ä¼˜åŒ–ï¼‰
```bash
# Mediumæ¨¡å‹ (é’ˆå¯¹A6000ä¼˜åŒ–)
python3 scripts/train.py --mode pretrain --config medium

# æŸ¥çœ‹é…ç½®ä¿¡æ¯
python3 -c "from config.training_config import get_medium_config; get_medium_config()"
```

### 2. ç›‘æ§è®­ç»ƒæ€§èƒ½
```bash
# ä½¿ç”¨nvidia-smiç›‘æ§GPU
watch -n 1 nvidia-smi

# è®­ç»ƒæ—¥å¿—ä¼šæ˜¾ç¤º:
# - Batch size: 32
# - æ¢¯åº¦ç´¯ç§¯: 4
# - æœ‰æ•ˆbatch: 128
# - æ··åˆç²¾åº¦: True
```

### 3. æ€§èƒ½åŸºå‡†æµ‹è¯•
```bash
# æµ‹è¯•æ•°æ®åŠ è½½é€Ÿåº¦
python3 -c "
from config.training_config import get_medium_config
config = get_medium_config()
print(f'Workers: {config.num_workers}')
print(f'Prefetch: {config.prefetch_factor}')
print(f'Batch: {config.batch_size}')
print(f'Accumulation: {config.gradient_accumulation_steps}')
"
```

## æ•…éšœæ’æŸ¥

### å¦‚æœæ˜¾å­˜ä¸è¶³
```python
# åœ¨training_config.pyä¸­è°ƒæ•´
batch_size = 24  # é™ä½batch size
gradient_accumulation_steps = 5  # å¢åŠ ç´¯ç§¯æ­¥æ•°
```

### å¦‚æœæ•°æ®åŠ è½½æ…¢
```python
# å¢åŠ workeræ•°é‡
num_workers = 12  # å¯ä»¥å°è¯•æ›´å¤šworker
prefetch_factor = 6  # å¢åŠ é¢„å–
```

### å¦‚æœè®­ç»ƒä¸ç¨³å®š
```python
# ç¦ç”¨æ··åˆç²¾åº¦
mixed_precision = False

# æˆ–ä½¿ç”¨æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

## è¿›ä¸€æ­¥ä¼˜åŒ–å»ºè®®

### 1. å¯ç”¨PyTorchç¼–è¯‘ (ä»…PyTorch 2.0+)
```python
model = torch.compile(model, mode="reduce-overhead")
```

### 2. ä½¿ç”¨Flash Attention 2
```bash
pip install flash-attn --no-build-isolation
```

### 3. åˆ†å¸ƒå¼è®­ç»ƒ (å¤šGPU)
```bash
torchrun --nproc_per_node=2 scripts/train.py --mode pretrain
```

### 4. ä¼˜åŒ–æ•°æ®é›†åŠ è½½
- ä½¿ç”¨å†…å­˜æ˜ å°„(mmap)åŠ è½½å¤§æ–‡ä»¶
- é¢„å¤„ç†æ•°æ®å¹¶ç¼“å­˜tokenåŒ–ç»“æœ
- ä½¿ç”¨WebDatasetæ ¼å¼

## éªŒè¯æ¸…å•

- [x] Batch sizeä»12æå‡è‡³32
- [x] æ¢¯åº¦ç´¯ç§¯ä»10é™è‡³4
- [x] DataLoader workerä»0æå‡è‡³8
- [x] å¯ç”¨prefetch_factor=4
- [x] å¯ç”¨pin_memory=True
- [x] å¯ç”¨persistent_workers=True
- [x] å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ(FP16)
- [x] ä¼˜åŒ–æ¢¯åº¦ç´¯ç§¯é€»è¾‘
- [x] å¯ç”¨non_blockingæ•°æ®ä¼ è¾“

## å‚è€ƒèµ„æ–™
- [PyTorch Performance Tuning Guide](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)
- [NVIDIA A6000 Specifications](https://www.nvidia.com/en-us/design-visualization/rtx-a6000/)
- [Mixed Precision Training](https://pytorch.org/docs/stable/amp.html)
- [Efficient DataLoader](https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading)
