# Foundation 配置架构分析

## 目标概述
- **目标参数量**：≈2 亿参数，具备基础推理/对话能力，适配 32GB GPU（单卡或两卡）
- **任务类型**：通用中文 + 英文混合语料的自回归预训练，可扩展至指令对齐
- **关键约束**：显存紧张、需要较高吞吐、希望未来易于扩展更大模型

## 架构选型评估
| 设计项 | 原始设定（32L×704） | 调整后设定（24L×768） | 评估 |
| --- | --- | --- | --- |
| 深度 vs. 宽度 | 32 层、隐藏维 704，注意力头 22（GQA 11:1） | 24 层、隐藏维 768，注意力头 16（GQA 4:1） | 32 层深度较高但 head_dim=32，注意力表示能力受限；GQA 11:1 稀疏程度过高，梯度稳定性较差。新的 48 维 head_dim 与 4:1 GQA 更贴近社区经验（LLaMA/SOLAR 系列），可减轻训练难度。 |
| FFN 宽度 | 2176 (≈3.1×) | 2688 (3.5×) | 原设定 FFN 略窄，限制表达力；3.5× 在吞吐与性能间取得折衷，并兼容 SwiGLU 的最佳点位。 |
| 参数量 | ≈2.04 亿 | ≈2.09 亿 | 维持在目标区间内，差异 <3%。 |
| 稳定性 | Q/KV 比例 11:1、head_dim=32 对 AdamW/FlashAttention 的数值范围要求高 | Q/KV 比例 4:1、head_dim=48 更稳定 | 新配置可减少 NaN/梯度爆炸风险，并提升注意力质量。 |

## 最终推荐配置摘要
- **结构**：24 层 Decoder-only Transformer，hidden=768，heads=16，KV heads=4
- **注意力**：RoPE + Flash Attention + GQA（4:1）
- **激活**：SwiGLU，FFN hidden=2688
- **归一化**：RMSNorm，eps=1e-6
- **优化选项**：梯度检查点、权重共享、dropout=0.1、attention dropout=0.1
- **上下文长度**：4K token，可通过压缩位置编码扩展
- **参数估算**：`estimate_params(get_config('foundation')) ≈ 208,638,720`

## 训练建议
### 数据与批次
- 语料规模建议 ≥500B token，以覆盖多语场景；若资源有限可使用 200B token + 重复 2~3 epoch。
- 建议 **global batch size 2M token** 左右（例如 16 张 80G GPU × 每卡 4K token × 梯度累积 8 次）。单卡 32G 环境下，可采用：
  - 序列长度 2K token、每卡 batch size 4；
  - 梯度累积 16 次，得到 128 token/step → global batch ≈ 8K token；
  - 混合精度 `bfloat16` 或 `fp16` + `grad_clip=1.0`。

### 优化器与调度
- 使用 AdamW：`lr=3e-4`、`betas=(0.9, 0.95)`、`weight_decay=0.1`。
- 线性预热 2k~5k steps → 余弦退火或 constant-with-decay。
- 为进一步稳定，可启用 `adafactor` 变种或梯度噪声注入（后期阶段）。

### 正则化与对齐
- 训练早期保持 `dropout=0.1`；当语料质量较高且训练稳定，可在后期降至 0.05。
- 预训练完成后可接入 LoRA + SFT/强化学习，以补足指令跟随能力。

## 可扩展性与演进路线
1. **上下文扩展**：利用 RoPE scaling (NTK 或 LLaMA3 风格) 将上下文提升至 8K/16K。
2. **多卡并行**：目前配置适配 ZeRO-2/ZeRO-3；若继续缩短训练时间，可结合张量并行 (TP=2) 与流水线并行。
3. **大模型迁移**：保持 768 hidden 使得参数矩阵与 1B/3B 级别模型的宽度对齐，便于未来蒸馏或继续增宽。

## MoE 选型参考
若后续希望以 MoE（Mixture-of-Experts）提升模型容量但保持训练/推理算力在可控范围，可参考以下建议：

- **专家布局**：保持与基础模型相同的 24 层骨干结构，将部分 FFN 层替换为 MoE。典型组合为 `top-2 gating` + `8~16 个路由专家` + `1 个共享专家`，使每个 token 仅激活两个专家。
- **专家宽度**：专家内部 FFN hidden 建议维持在 `hidden_size * 3.5 ≈ 2688`，与基础 FFN 一致，可避免需要额外的缩放修正。若显存允许，可放宽至 3072~3328 提升表达力。
- **路由正则**：启用负载均衡辅助损失（`aux_loss_alpha ≈ 0.1`）与 `seq_aux=True`，并保持 `norm_topk_prob=True`，缓解专家利用率失衡与训练早期的梯度震荡。
- **容量对比**：在 8 专家 + top-2 设置下，每层有效参数量约等价于 4 倍密集 FFN，但推理 FLOPs 仅增长 ~1.7×；16 专家约等价 8 倍容量，需额外关注通信开销。
- **推理部署**：若计划在线服务，建议在导出权重时保留密集 FFN 的回退路径（shared expert），并评估推理引擎是否支持动态专家路由；否则可在部署前进行专家蒸馏或稀疏到密集的转换。

结合上述 MoE 配置，可在保持基础算力预算的同时扩展模型表示能力，适用于在高质量语料上继续挖掘长尾知识或细分任务表现。

### 小模型适合 MoE 吗？
在 1 亿参数以下（如 `small`/`medium` 预设）的模型上直接引入 MoE 通常得不偿失，原因包括：

- **参数基线过低**：稠密 FFN 仅占用数千万参数，引入 4~8 专家后虽能放大表示空间，但 gating、路由和通信的额外常数开销与显存碎片化会抵消优势。
- **训练数据瓶颈**：小模型常见的 50B 以下 token 语料不足以支撑专家之间的差异化学习，易出现“所有专家趋同”的退化现象。
- **实现复杂度**：MoE 需要稳定的负载均衡、容错与推理引擎支持。对部署预算有限的团队而言，这些工程成本往往高于将模型升级到更宽的稠密架构（例如直接使用 foundation 配置）。

若确有稀疏化需求，可考虑以下折衷：

1. **局部 MoE**：仅在最深的 2~4 层替换为 MoE，以控制额外专家数与通信开销。
2. **共享专家增强**：提升 `n_shared_experts`（例如设为 2），保证即使路由退化也能维持稠密性能。
3. **蒸馏回稠密模型**：在 MoE 训练完成后，将专家输出蒸馏回等参数量的稠密 FFN，以便部署。

整体而言，小模型更适合通过增加稠密宽度/深度、改进正则化或数据质量来获得收益；MoE 更适合 1B 级别以上、追求极致参数效率但仍有充足算力与工程资源的大模型场景。

## 风险与监控要点
- **Flash Attention 版本**：请使用 v2.3+，以避免在 head_dim=48 下的编译错误。
- **GQA 实现**：确保推理侧同时支持 4 个 KV 头；若部署库不支持，可回退至 KV=8（参数增加 ~+8M）。
- **梯度检查点**：节省显存但带来 ~7% 额外算力成本，需要在训练脚本中开启 recompute。

---
如需进一步微调（例如目标语种占比、推理速度优化等），可以基于此分析继续迭代。
