# 02 å¤šå¤´æ³¨æ„åŠ›å­ç©ºé—´åˆ†è§£

> **ä»å•ä¸€ç©ºé—´åˆ°å¤šç»´è¯­ä¹‰çš„å¹¶è¡Œå»ºæ¨¡**

## æ ¸å¿ƒæ€æƒ³

å¤šå¤´æ³¨æ„åŠ›æ˜¯ Transformer æ¶æ„çš„å…³é”®åˆ›æ–°ï¼Œå®ƒå°†å•ä¸ªæ³¨æ„åŠ›æœºåˆ¶æ‰©å±•ä¸ºå¤šä¸ªå¹¶è¡Œçš„"æ³¨æ„åŠ›å¤´"ã€‚æ¯ä¸ªå¤´åœ¨ä¸åŒçš„è¡¨å¾å­ç©ºé—´ä¸­ç‹¬ç«‹æ“ä½œï¼Œæœ€åå°†ç»“æœæ‹¼æ¥èåˆã€‚è¿™ç§è®¾è®¡è®©æ¨¡å‹èƒ½å¤ŸåŒæ—¶å…³æ³¨ä¸åŒç±»å‹çš„è¯­ä¹‰å…³ç³»ã€‚

**æ ¸å¿ƒæ´å¯Ÿ**ï¼š
- æ¯ä¸ªæ³¨æ„åŠ›å¤´å…³æ³¨ä¸åŒçš„**è¯­ä¹‰ç»´åº¦**ï¼ˆè¯­æ³•ã€è¯­ä¹‰ã€ä½ç½®ç­‰ï¼‰
- å­ç©ºé—´åˆ†è§£æä¾›äº†**è¡¨å¾å¤šæ ·æ€§**ï¼Œé¿å…å•ä¸€è§†è§’çš„å±€é™
- å¹¶è¡Œè®¡ç®—å®ç°äº†**è®¡ç®—æ•ˆç‡**çš„æå‡
- å‚æ•°å…±äº«ä¸ç‹¬ç«‹æ€§çš„å¹³è¡¡å®ç°äº†**æ¨¡å‹å®¹é‡**çš„ä¼˜åŒ–

**æ•°å­¦è¡¨è¾¾**ï¼š
$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$$

å…¶ä¸­ï¼š
$$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

## 2.1 å­ç©ºé—´åˆ†è§£çš„æ•°å­¦æ¡†æ¶

### çº¿æ€§å­ç©ºé—´çš„å®šä¹‰

ç»™å®šå‘é‡ç©ºé—´ $\mathbb{R}^d$ï¼Œ**å­ç©ºé—´** $S \subseteq \mathbb{R}^d$ æ˜¯ä¸€ä¸ªæ»¡è¶³ä»¥ä¸‹æ¡ä»¶çš„é›†åˆï¼š
1. **é›¶å‘é‡**ï¼š$\mathbf{0} \in S$
2. **åŠ æ³•å°é—­**ï¼š$\mathbf{u}, \mathbf{v} \in S \Rightarrow \mathbf{u} + \mathbf{v} \in S$
3. **æ ‡é‡ä¹˜æ³•å°é—­**ï¼š$\mathbf{u} \in S, c \in \mathbb{R} \Rightarrow c\mathbf{u} \in S$

åœ¨å¤šå¤´æ³¨æ„åŠ›ä¸­ï¼Œæ¯ä¸ªæŠ•å½±çŸ©é˜µ $W_i^Q, W_i^K, W_i^V \in \mathbb{R}^{d \times d_k}$ å®šä¹‰äº†ä¸€ä¸ª $d_k$ ç»´å­ç©ºé—´ã€‚

**å­ç©ºé—´æŠ•å½±çš„æ•°å­¦æ€§è´¨**ï¼š
```python
def analyze_subspace_properties(W_q, W_k, W_v):
    """åˆ†æå­ç©ºé—´æŠ•å½±çŸ©é˜µçš„æ•°å­¦æ€§è´¨"""
    
    # W_q, W_k, W_v: (d_model, d_k)
    d_model, d_k = W_q.shape
    
    print(f"åŸå§‹ç©ºé—´ç»´åº¦: {d_model}")
    print(f"å­ç©ºé—´ç»´åº¦: {d_k}")
    print(f"ç»´åº¦å‹ç¼©æ¯”: {d_k/d_model:.2%}")
    
    # 1. åˆ†ææŠ•å½±çŸ©é˜µçš„ç§©
    rank_q = torch.linalg.matrix_rank(W_q)
    rank_k = torch.linalg.matrix_rank(W_k)
    rank_v = torch.linalg.matrix_rank(W_v)
    
    print(f"\\næŠ•å½±çŸ©é˜µçš„ç§©:")
    print(f"  W^Q: {rank_q}/{d_k} (ç†è®ºæœ€å¤§: {min(d_model, d_k)})")
    print(f"  W^K: {rank_k}/{d_k}")
    print(f"  W^V: {rank_v}/{d_k}")
    
    # 2. è®¡ç®—æŠ•å½±çŸ©é˜µçš„å¥‡å¼‚å€¼
    U_q, S_q, V_q = torch.svd(W_q)
    U_k, S_k, V_k = torch.svd(W_k)
    U_v, S_v, V_v = torch.svd(W_v)
    
    print(f"\\nå¥‡å¼‚å€¼åˆ†æ:")
    print(f"  W^Qæœ€å¤§å¥‡å¼‚å€¼: {S_q.max():.4f}, æœ€å°å¥‡å¼‚å€¼: {S_q.min():.4f}")
    print(f"  W^Kæœ€å¤§å¥‡å¼‚å€¼: {S_k.max():.4f}, æœ€å°å¥‡å¼‚å€¼: {S_k.min():.4f}")
    print(f"  W^Væœ€å¤§å¥‡å¼‚å€¼: {S_v.max():.4f}, æœ€å°å¥‡å¼‚å€¼: {S_v.min():.4f}")
    
    # 3. æ¡ä»¶æ•°åˆ†æï¼ˆæ•°å€¼ç¨³å®šæ€§ï¼‰
    cond_q = S_q.max() / S_q.min()
    cond_k = S_k.max() / S_k.min()
    cond_v = S_v.max() / S_v.min()
    
    print(f"\\næ¡ä»¶æ•° (æ•°å€¼ç¨³å®šæ€§æŒ‡æ ‡):")
    print(f"  W^Q: {cond_q:.2f}")
    print(f"  W^K: {cond_k:.2f}")
    print(f"  W^V: {cond_v:.2f}")
    
    return {
        'ranks': (rank_q, rank_k, rank_v),
        'singular_values': (S_q, S_k, S_v),
        'condition_numbers': (cond_q, cond_k, cond_v)
    }
```

### ç›´å’Œåˆ†è§£çš„ç†è®ºåŸºç¡€

ç†æƒ³æƒ…å†µä¸‹ï¼Œå¤šå¤´æ³¨æ„åŠ›çš„å­ç©ºé—´åº”è¯¥å½¢æˆ**ç›´å’Œåˆ†è§£**ï¼š

$$\mathbb{R}^d = S_1 \oplus S_2 \oplus ... \oplus S_h$$

å…¶ä¸­ $S_i \cap S_j = \{\mathbf{0}\}, \forall i \neq j$ã€‚

è¿™æ„å‘³ç€ä¸åŒçš„å¤´å…³æ³¨å®Œå…¨ä¸åŒçš„ç‰¹å¾ç»´åº¦ï¼Œæ²¡æœ‰ä¿¡æ¯å†—ä½™ã€‚

**å®é™…åˆ†æ**ï¼š
```python
def analyze_subspace_orthogonality(multi_head_attention):
    """åˆ†æå¤šå¤´æ³¨æ„åŠ›å­ç©ºé—´çš„æ­£äº¤æ€§"""
    
    # è·å–æ‰€æœ‰å¤´çš„æŠ•å½±çŸ©é˜µ
    W_q = multi_head_attention.w_q.weight.data  # (d_model, d_model)
    W_k = multi_head_attention.w_k.weight.data
    W_v = multi_head_attention.w_v.weight.data
    
    n_heads = multi_head_attention.n_heads
    d_k = multi_head_attention.d_k
    
    # é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼: (n_heads, d_k, d_model)
    W_q_heads = W_q.t().view(n_heads, d_k, -1)
    W_k_heads = W_k.t().view(n_heads, d_k, -1)
    W_v_heads = W_v.t().view(n_heads, d_k, -1)
    
    # è®¡ç®—å¤´ä¹‹é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ
    similarity_matrix = torch.zeros(n_heads, n_heads)
    
    for i in range(n_heads):
        for j in range(n_heads):
            if i != j:
                # è®¡ç®—ä¸¤ä¸ªå­ç©ºé—´çš„Grassmannè·ç¦»
                # è¿™é‡Œç®€åŒ–ä¸ºæŠ•å½±çŸ©é˜µçš„Frobeniuså†…ç§¯
                sim_q = F.cosine_similarity(
                    W_q_heads[i].flatten(), 
                    W_q_heads[j].flatten(), 
                    dim=0
                )
                similarity_matrix[i, j] = sim_q.abs()
    
    # åˆ†æç»“æœ
    avg_similarity = similarity_matrix.sum() / (n_heads * (n_heads - 1))
    max_similarity = similarity_matrix.max()
    
    print(f"å­ç©ºé—´ç›¸ä¼¼åº¦åˆ†æ:")
    print(f"  å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f} (è¶Šå°è¶Šæ­£äº¤)")
    print(f"  æœ€å¤§ç›¸ä¼¼åº¦: {max_similarity:.4f}")
    
    # ç†æƒ³æƒ…å†µä¸‹åº”è¯¥æ¥è¿‘0ï¼ˆå®Œå…¨æ­£äº¤ï¼‰
    if avg_similarity < 0.1:
        print("  âœ“ å­ç©ºé—´åŸºæœ¬æ­£äº¤ï¼Œä¿¡æ¯å†—ä½™è¾ƒå°‘")
    elif avg_similarity < 0.3:
        print("  âš  å­ç©ºé—´éƒ¨åˆ†é‡å ï¼Œå­˜åœ¨ä¸€å®šå†—ä½™")  
    else:
        print("  âŒ å­ç©ºé—´é«˜åº¦é‡å ï¼Œå†—ä½™ä¸¥é‡")
    
    return similarity_matrix
```

## 2.2 å¤šå¤´å¹¶è¡Œçš„å‡ ä½•æ„ä¹‰

### ä¸åŒè¯­ä¹‰ç»´åº¦çš„ç‹¬ç«‹å»ºæ¨¡

å¤šå¤´æ³¨æ„åŠ›çš„æ ¸å¿ƒæ€æƒ³æ˜¯è®©ä¸åŒçš„å¤´å­¦ä¹ ä¸åŒç±»å‹çš„ä¾èµ–å…³ç³»ï¼š

- **å¥æ³•å…³ç³»**ï¼šä¸»è°“å®¾ã€å®šçŠ¶è¡¥ç­‰è¯­æ³•ç»“æ„
- **è¯­ä¹‰å…³ç³»**ï¼šåŒä¹‰è¯ã€åä¹‰è¯ã€ä¸Šä¸‹ä½å…³ç³»
- **ä½ç½®å…³ç³»**ï¼šç›¸å¯¹ä½ç½®ã€è·ç¦»ä¿¡æ¯
- **è¯é¢˜å…³ç³»**ï¼šä¸»é¢˜ä¸€è‡´æ€§ã€è¯é¢˜è½¬æ¢

**ä»£ç å®ç°è§£æ**ï¼š
```python
# MiniGPTä¸­çš„å¤šå¤´æ³¨æ„åŠ›å®ç° (src/model/transformer.py:62-79)
def forward(self, query, key, value, mask=None):
    batch_size, seq_len, d_model = query.size()
    
    # 1. çº¿æ€§æŠ•å½±åˆ°å„ä¸ªå­ç©ºé—´
    Q = self.w_q(query)  # (batch_size, seq_len, d_model)
    K = self.w_k(key)    # (batch_size, seq_len, d_model)  
    V = self.w_v(value)  # (batch_size, seq_len, d_model)
    
    # 2. é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼ï¼šåˆ†å‰²åˆ°ä¸åŒå­ç©ºé—´
    Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
    K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
    V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
    # ç°åœ¨å½¢çŠ¶: (batch_size, n_heads, seq_len, d_k)
    
    # 3. åœ¨æ¯ä¸ªå­ç©ºé—´ä¸­ç‹¬ç«‹è®¡ç®—æ³¨æ„åŠ›
    attention_output = self.scaled_dot_product_attention(Q, K, V, mask)
    
    # 4. æ‹¼æ¥æ‰€æœ‰å­ç©ºé—´çš„ç»“æœ
    attention_output = attention_output.transpose(1, 2).contiguous()
    attention_output = attention_output.view(batch_size, seq_len, d_model)
    
    # 5. æœ€ç»ˆçš„çº¿æ€§å˜æ¢ï¼ˆä¿¡æ¯èåˆï¼‰
    output = self.w_o(attention_output)
    
    return output
```

### ç»´åº¦å˜æ¢çš„å‡ ä½•è§£é‡Š

å¤šå¤´æ³¨æ„åŠ›æ¶‰åŠä¸€ç³»åˆ—ç»´åº¦å˜æ¢ï¼Œæ¯ä¸€æ­¥éƒ½æœ‰æ˜ç¡®çš„å‡ ä½•æ„ä¹‰ï¼š

```python
def trace_dimension_transformations(batch_size=2, seq_len=10, d_model=512, n_heads=8):
    """è¿½è¸ªå¤šå¤´æ³¨æ„åŠ›çš„ç»´åº¦å˜æ¢è¿‡ç¨‹"""
    
    d_k = d_model // n_heads
    print(f"å¤šå¤´æ³¨æ„åŠ›ç»´åº¦å˜æ¢è¿½è¸ª:")
    print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"  åºåˆ—é•¿åº¦: {seq_len}")  
    print(f"  æ¨¡å‹ç»´åº¦: {d_model}")
    print(f"  æ³¨æ„åŠ›å¤´æ•°: {n_heads}")
    print(f"  æ¯å¤´ç»´åº¦: {d_k}")
    print()
    
    # æ¨¡æ‹Ÿè¾“å…¥
    x = torch.randn(batch_size, seq_len, d_model)
    print(f"1. è¾“å…¥: {x.shape}")
    print(f"   å‡ ä½•æ„ä¹‰: {batch_size}ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªåŒ…å«{seq_len}ä¸ª{d_model}ç»´å‘é‡")
    
    # çº¿æ€§æŠ•å½±
    W_q = torch.randn(d_model, d_model)
    Q = torch.matmul(x, W_q)
    print(f"\\n2. æŸ¥è¯¢æŠ•å½±: {Q.shape}")
    print(f"   å‡ ä½•æ„ä¹‰: å°†è¾“å…¥å‘é‡æŠ•å½±åˆ°æŸ¥è¯¢ç©ºé—´")
    
    # é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
    Q_reshaped = Q.view(batch_size, seq_len, n_heads, d_k)
    print(f"\\n3. å¤šå¤´é‡å¡‘: {Q_reshaped.shape}")
    print(f"   å‡ ä½•æ„ä¹‰: å°†{d_model}ç»´å‘é‡åˆ†å‰²ä¸º{n_heads}ä¸ª{d_k}ç»´å­å‘é‡")
    
    # è½¬ç½®ä»¥ä¾¿å¹¶è¡Œè®¡ç®—
    Q_transposed = Q_reshaped.transpose(1, 2)
    print(f"\\n4. è½¬ç½®: {Q_transposed.shape}")
    print(f"   å‡ ä½•æ„ä¹‰: é‡æ’ç»´åº¦ä»¥æ”¯æŒå¹¶è¡Œçš„å¤´è®¡ç®—")
    
    # æ³¨æ„åŠ›è®¡ç®—ï¼ˆç®€åŒ–ï¼‰
    K_transposed = Q_transposed  # ç®€åŒ–ï¼Œå®é™…ä¸­Kæ¥è‡ªä¸åŒæŠ•å½±
    attention_scores = torch.matmul(Q_transposed, K_transposed.transpose(-2, -1))
    print(f"\\n5. æ³¨æ„åŠ›åˆ†æ•°: {attention_scores.shape}")
    print(f"   å‡ ä½•æ„ä¹‰: æ¯ä¸ªå¤´è®¡ç®—{seq_len}Ã—{seq_len}çš„ç›¸ä¼¼åº¦çŸ©é˜µ")
    
    # æ³¨æ„åŠ›æƒé‡å’Œè¾“å‡º
    attention_weights = F.softmax(attention_scores / math.sqrt(d_k), dim=-1)
    V_transposed = Q_transposed  # ç®€åŒ–
    attention_output = torch.matmul(attention_weights, V_transposed)
    print(f"\\n6. æ³¨æ„åŠ›è¾“å‡º: {attention_output.shape}")
    print(f"   å‡ ä½•æ„ä¹‰: æ¯ä¸ªå¤´äº§ç”Ÿ{seq_len}ä¸ª{d_k}ç»´åŠ æƒå‘é‡")
    
    # æ‹¼æ¥
    attention_concat = attention_output.transpose(1, 2).contiguous()
    attention_concat = attention_concat.view(batch_size, seq_len, d_model)
    print(f"\\n7. æ‹¼æ¥ç»“æœ: {attention_concat.shape}")
    print(f"   å‡ ä½•æ„ä¹‰: å°†{n_heads}ä¸ª{d_k}ç»´å‘é‡æ‹¼æ¥å›{d_model}ç»´")
    
    # è¾“å‡ºæŠ•å½±
    W_o = torch.randn(d_model, d_model)
    final_output = torch.matmul(attention_concat, W_o)
    print(f"\\n8. æœ€ç»ˆè¾“å‡º: {final_output.shape}")
    print(f"   å‡ ä½•æ„ä¹‰: èåˆå¤šå¤´ä¿¡æ¯ï¼Œå›åˆ°åŸå§‹ç©ºé—´ç»´åº¦")
```

## 2.3 å‚æ•°å…±äº«ä¸è¡¨å¾å¤šæ ·æ€§

### æŠ•å½±çŸ©é˜µçš„åˆå§‹åŒ–ç­–ç•¥

ä¸åŒå¤´çš„æŠ•å½±çŸ©é˜µéœ€è¦åˆç†åˆå§‹åŒ–ä»¥ç¡®ä¿è¡¨å¾å¤šæ ·æ€§ï¼š

```python
def analyze_initialization_strategies():
    """åˆ†æä¸åŒåˆå§‹åŒ–ç­–ç•¥å¯¹å¤šå¤´è¡¨å¾å¤šæ ·æ€§çš„å½±å“"""
    
    d_model, n_heads = 512, 8
    d_k = d_model // n_heads
    
    strategies = {
        'Xavier Uniform': lambda: nn.init.xavier_uniform_(torch.empty(d_model, d_model)),
        'Xavier Normal': lambda: nn.init.xavier_normal_(torch.empty(d_model, d_model)),
        'Kaiming Uniform': lambda: nn.init.kaiming_uniform_(torch.empty(d_model, d_model)),
        'Orthogonal': lambda: nn.init.orthogonal_(torch.empty(d_model, d_model)),
        'Random Normal': lambda: torch.randn(d_model, d_model) * 0.02
    }
    
    for name, init_fn in strategies.items():
        print(f"\\n=== {name} åˆå§‹åŒ– ===")
        
        # åˆå§‹åŒ–æŠ•å½±çŸ©é˜µ
        W_q = init_fn()
        
        # åˆ†æå¤šå¤´ç›¸ä¼¼åº¦
        W_q_heads = W_q.view(d_model, n_heads, d_k)
        
        similarities = []
        for i in range(n_heads):
            for j in range(i+1, n_heads):
                # è®¡ç®—ä¸¤ä¸ªå¤´çš„ä½™å¼¦ç›¸ä¼¼åº¦
                head_i = W_q_heads[:, i, :].flatten()
                head_j = W_q_heads[:, j, :].flatten()
                sim = F.cosine_similarity(head_i, head_j, dim=0)
                similarities.append(sim.abs().item())
        
        avg_sim = sum(similarities) / len(similarities)
        max_sim = max(similarities)
        
        print(f"  å¹³å‡å¤´é—´ç›¸ä¼¼åº¦: {avg_sim:.4f}")
        print(f"  æœ€å¤§å¤´é—´ç›¸ä¼¼åº¦: {max_sim:.4f}")
        
        # åˆ†ææƒé‡åˆ†å¸ƒ
        print(f"  æƒé‡å‡å€¼: {W_q.mean():.6f}")
        print(f"  æƒé‡æ ‡å‡†å·®: {W_q.std():.6f}")
        
        # è°±åˆ†æ
        U, S, V = torch.svd(W_q)
        print(f"  æ¡ä»¶æ•°: {S.max()/S.min():.2f}")
```

### è¡¨å¾å¤šæ ·æ€§çš„é‡åŒ–æŒ‡æ ‡

```python
def measure_representation_diversity(multi_head_attention, input_data):
    """é‡åŒ–å¤šå¤´æ³¨æ„åŠ›çš„è¡¨å¾å¤šæ ·æ€§"""
    
    model = multi_head_attention
    model.eval()
    
    with torch.no_grad():
        # è·å–æ‰€æœ‰å¤´çš„è¾“å‡º
        batch_size, seq_len, d_model = input_data.shape
        
        # å‰å‘ä¼ æ’­åˆ°æ³¨æ„åŠ›è®¡ç®—
        Q = model.w_q(input_data)
        K = model.w_k(input_data)
        V = model.w_v(input_data)
        
        # é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
        Q = Q.view(batch_size, seq_len, model.n_heads, model.d_k).transpose(1, 2)
        K = K.view(batch_size, seq_len, model.n_heads, model.d_k).transpose(1, 2)
        V = V.view(batch_size, seq_len, model.n_heads, model.d_k).transpose(1, 2)
        
        # è®¡ç®—æ¯ä¸ªå¤´çš„æ³¨æ„åŠ›
        head_outputs = []
        head_attentions = []
        
        for head in range(model.n_heads):
            q_h = Q[:, head:head+1]  # ä¿æŒç»´åº¦
            k_h = K[:, head:head+1]
            v_h = V[:, head:head+1]
            
            # è®¡ç®—æ³¨æ„åŠ›
            scores = torch.matmul(q_h, k_h.transpose(-2, -1)) / math.sqrt(model.d_k)
            attention = F.softmax(scores, dim=-1)
            output = torch.matmul(attention, v_h)
            
            head_outputs.append(output.squeeze(1))  # (batch_size, seq_len, d_k)
            head_attentions.append(attention.squeeze(1))  # (batch_size, seq_len, seq_len)
    
    # åˆ†æè¡¨å¾å¤šæ ·æ€§
    print("=== è¡¨å¾å¤šæ ·æ€§åˆ†æ ===")
    
    # 1. è¾“å‡ºç›¸ä¼¼åº¦åˆ†æ
    output_similarities = []
    for i in range(model.n_heads):
        for j in range(i+1, model.n_heads):
            # è®¡ç®—ä¸¤ä¸ªå¤´è¾“å‡ºçš„ç›¸ä¼¼åº¦
            out_i = head_outputs[i].flatten()
            out_j = head_outputs[j].flatten()
            sim = F.cosine_similarity(out_i, out_j, dim=0)
            output_similarities.append(sim.item())
    
    avg_output_sim = sum(output_similarities) / len(output_similarities)
    print(f"å¹³å‡è¾“å‡ºç›¸ä¼¼åº¦: {avg_output_sim:.4f}")
    
    # 2. æ³¨æ„åŠ›æ¨¡å¼ç›¸ä¼¼åº¦
    attention_similarities = []
    for i in range(model.n_heads):
        for j in range(i+1, model.n_heads):
            att_i = head_attentions[i].flatten()
            att_j = head_attentions[j].flatten()
            sim = F.cosine_similarity(att_i, att_j, dim=0)
            attention_similarities.append(sim.item())
    
    avg_attention_sim = sum(attention_similarities) / len(attention_similarities)
    print(f"å¹³å‡æ³¨æ„åŠ›æ¨¡å¼ç›¸ä¼¼åº¦: {avg_attention_sim:.4f}")
    
    # 3. ä¿¡æ¯ç†µåˆ†æ
    entropies = []
    for head in range(model.n_heads):
        attention = head_attentions[head]
        entropy = -(attention * torch.log(attention + 1e-8)).sum(dim=-1).mean()
        entropies.append(entropy.item())
    
    entropy_diversity = torch.tensor(entropies).std().item()
    print(f"æ³¨æ„åŠ›ç†µå¤šæ ·æ€§: {entropy_diversity:.4f}")
    
    return {
        'output_similarity': avg_output_sim,
        'attention_similarity': avg_attention_sim,
        'entropy_diversity': entropy_diversity,
        'head_entropies': entropies
    }
```

## 2.4 å¤´é—´ä¿¡æ¯èåˆçš„ç†è®º

### è¾“å‡ºæŠ•å½±çš„ä½œç”¨æœºåˆ¶

è¾“å‡ºæŠ•å½±çŸ©é˜µ $W^O$ è´Ÿè´£èåˆæ¥è‡ªä¸åŒå¤´çš„ä¿¡æ¯ï¼š

$$\text{Output} = \text{Concat}(\text{head}_1, ..., \text{head}_h) W^O$$

**èåˆç­–ç•¥åˆ†æ**ï¼š
```python
def analyze_output_projection_fusion(W_o, n_heads, d_k):
    """åˆ†æè¾“å‡ºæŠ•å½±çŸ©é˜µçš„ä¿¡æ¯èåˆæœºåˆ¶"""
    
    d_model = n_heads * d_k
    
    # å°†è¾“å‡ºæŠ•å½±çŸ©é˜µæŒ‰å¤´åˆ†å—
    W_o_blocks = W_o.view(d_model, n_heads, d_k)
    
    print(f"è¾“å‡ºæŠ•å½±çŸ©é˜µåˆ†æ:")
    print(f"  å½¢çŠ¶: {W_o.shape}")
    print(f"  åˆ†å—å: {W_o_blocks.shape}")
    
    # åˆ†ææ¯ä¸ªå¤´çš„è´¡çŒ®æƒé‡
    head_contributions = []
    for head in range(n_heads):
        # è®¡ç®—è¯¥å¤´å¯¹åº”çš„æŠ•å½±å—çš„FrobeniusèŒƒæ•°
        head_weight = torch.norm(W_o_blocks[:, head, :], 'fro')
        head_contributions.append(head_weight.item())
    
    head_contributions = torch.tensor(head_contributions)
    
    print(f"\\nå„å¤´è´¡çŒ®æƒé‡:")
    for head in range(n_heads):
        print(f"  å¤´ {head+1}: {head_contributions[head]:.4f}")
    
    # åˆ†æè´¡çŒ®çš„å‡è¡¡æ€§
    contribution_std = head_contributions.std()
    contribution_cv = contribution_std / head_contributions.mean()  # å˜å¼‚ç³»æ•°
    
    print(f"\\nè´¡çŒ®å‡è¡¡æ€§:")
    print(f"  æ ‡å‡†å·®: {contribution_std:.4f}")
    print(f"  å˜å¼‚ç³»æ•°: {contribution_cv:.4f}")
    
    if contribution_cv < 0.2:
        print("  âœ“ å„å¤´è´¡çŒ®å‡è¡¡")
    elif contribution_cv < 0.5:
        print("  âš  å„å¤´è´¡çŒ®ç•¥æœ‰å·®å¼‚")
    else:
        print("  âŒ å„å¤´è´¡çŒ®å·®å¼‚è¾ƒå¤§ï¼Œå¯èƒ½å­˜åœ¨å¤´å†—ä½™")
    
    # åˆ†æå¤´é—´äº¤äº’
    interaction_matrix = torch.zeros(n_heads, n_heads)
    for i in range(n_heads):
        for j in range(n_heads):
            if i != j:
                # è®¡ç®—ä¸¤ä¸ªå¤´å¯¹åº”æŠ•å½±å—çš„å†…ç§¯
                block_i = W_o_blocks[:, i, :].flatten()
                block_j = W_o_blocks[:, j, :].flatten() 
                interaction = torch.dot(block_i, block_j).abs()
                interaction_matrix[i, j] = interaction
    
    avg_interaction = interaction_matrix.sum() / (n_heads * (n_heads - 1))
    print(f"\\nå¤´é—´äº¤äº’å¼ºåº¦: {avg_interaction:.4f}")
    
    return {
        'head_contributions': head_contributions,
        'contribution_cv': contribution_cv,
        'interaction_matrix': interaction_matrix,
        'avg_interaction': avg_interaction
    }
```

### æ®‹å·®è¿æ¥ä¸å¤šå¤´èåˆ

å¤šå¤´æ³¨æ„åŠ›çš„è¾“å‡ºè¿˜éœ€è¦ä¸è¾“å…¥è¿›è¡Œæ®‹å·®è¿æ¥ï¼š

$$\text{Output} = \text{LayerNorm}(\text{Input} + \text{MultiHead}(\text{Input}))$$

**æ®‹å·®è¿æ¥çš„ä½œç”¨**ï¼š
```python
def analyze_residual_contribution(input_tensor, multihead_output):
    """åˆ†ææ®‹å·®è¿æ¥ä¸­åŸè¾“å…¥å’Œå¤šå¤´è¾“å‡ºçš„ç›¸å¯¹è´¡çŒ®"""
    
    # è®¡ç®—å„éƒ¨åˆ†çš„èŒƒæ•°
    input_norm = torch.norm(input_tensor, dim=-1).mean()
    output_norm = torch.norm(multihead_output, dim=-1).mean()
    
    # æ®‹å·®è¿æ¥åçš„ç»“æœ
    residual_sum = input_tensor + multihead_output
    residual_norm = torch.norm(residual_sum, dim=-1).mean()
    
    print(f"æ®‹å·®è¿æ¥åˆ†æ:")
    print(f"  è¾“å…¥èŒƒæ•°: {input_norm:.4f}")
    print(f"  å¤šå¤´è¾“å‡ºèŒƒæ•°: {output_norm:.4f}")
    print(f"  æ®‹å·®å’ŒèŒƒæ•°: {residual_norm:.4f}")
    
    # è®¡ç®—ç›¸å¯¹è´¡çŒ®
    input_contribution = input_norm / residual_norm
    output_contribution = output_norm / residual_norm
    
    print(f"\\nç›¸å¯¹è´¡çŒ®:")
    print(f"  è¾“å…¥è´¡çŒ®: {input_contribution:.4f} ({input_contribution*100:.1f}%)")
    print(f"  å¤šå¤´è´¡çŒ®: {output_contribution:.4f} ({output_contribution*100:.1f}%)")
    
    # åˆ†ææ–¹å‘ç›¸ä¼¼æ€§
    input_flat = input_tensor.flatten()
    output_flat = multihead_output.flatten()
    
    direction_similarity = F.cosine_similarity(input_flat, output_flat, dim=0)
    print(f"\\næ–¹å‘ç›¸ä¼¼æ€§: {direction_similarity:.4f}")
    
    if direction_similarity > 0.8:
        print("  å¤šå¤´è¾“å‡ºä¸è¾“å…¥æ–¹å‘é«˜åº¦ç›¸ä¼¼ï¼Œå¯èƒ½å­˜åœ¨é€€åŒ–")
    elif direction_similarity > 0.3:
        print("  å¤šå¤´è¾“å‡ºå¯¹è¾“å…¥è¿›è¡Œäº†é€‚åº¦ä¿®æ”¹")
    else:
        print("  å¤šå¤´è¾“å‡ºæ˜¾è‘—æ”¹å˜äº†è¾“å…¥çš„æ–¹å‘")
    
    return {
        'input_norm': input_norm.item(),
        'output_norm': output_norm.item(),
        'input_contribution': input_contribution.item(),
        'output_contribution': output_contribution.item(),
        'direction_similarity': direction_similarity.item()
    }
```

## 2.5 å®è·µï¼šMiniGPTä¸­çš„å¤šå¤´æ³¨æ„åŠ›ä¼˜åŒ–

### é«˜æ•ˆçš„å¤šå¤´è®¡ç®—å®ç°

```python
class OptimizedMultiHeadAttention(nn.Module):
    """ä¼˜åŒ–çš„å¤šå¤´æ³¨æ„åŠ›å®ç°ï¼ŒåŒ…å«è¯¦ç»†åˆ†æ"""
    
    def __init__(self, d_model, n_heads, dropout=0.1):
        super().__init__()
        assert d_model % n_heads == 0
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        # ä½¿ç”¨å•ä¸ªçŸ©é˜µè¿›è¡ŒæŠ•å½±ï¼Œç„¶ååˆ†å‰²ï¼ˆæ›´é«˜æ•ˆï¼‰
        self.w_qkv = nn.Linear(d_model, 3 * d_model, bias=False)
        self.w_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        self.scale = math.sqrt(self.d_k)
        
        # ç”¨äºåˆ†æçš„ç¼“å­˜
        self.attention_weights = None
        self.head_outputs = None
    
    def forward(self, x, mask=None, return_attention=False):
        batch_size, seq_len, d_model = x.size()
        
        # 1. ä¸€æ¬¡æ€§è®¡ç®—Q, K, VæŠ•å½±
        qkv = self.w_qkv(x)  # (batch_size, seq_len, 3*d_model)
        qkv = qkv.view(batch_size, seq_len, 3, self.n_heads, self.d_k)
        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch_size, n_heads, seq_len, d_k)
        
        Q, K, V = qkv[0], qkv[1], qkv[2]
        
        # 2. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # 3. è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # ç¼“å­˜ç”¨äºåˆ†æ
        self.attention_weights = attention_weights.detach()
        
        # 4. åº”ç”¨æ³¨æ„åŠ›æƒé‡
        attention_output = torch.matmul(attention_weights, V)
        
        # ç¼“å­˜å„å¤´è¾“å‡ºç”¨äºåˆ†æ
        self.head_outputs = attention_output.detach()
        
        # 5. æ‹¼æ¥å¹¶æŠ•å½±
        attention_output = attention_output.transpose(1, 2).contiguous()
        attention_output = attention_output.view(batch_size, seq_len, d_model)
        
        output = self.w_o(attention_output)
        
        if return_attention:
            return output, attention_weights
        return output
    
    def analyze_heads(self, head_names=None):
        """åˆ†æå„ä¸ªæ³¨æ„åŠ›å¤´çš„ç‰¹æ€§"""
        if self.attention_weights is None:
            print("è¯·å…ˆè¿›è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­")
            return
        
        attention = self.attention_weights  # (batch_size, n_heads, seq_len, seq_len)
        head_outputs = self.head_outputs    # (batch_size, n_heads, seq_len, d_k)
        
        batch_size, n_heads, seq_len, _ = attention.shape
        
        # å¹³å‡æ‰€æœ‰batchçš„ç»“æœ
        avg_attention = attention.mean(dim=0)  # (n_heads, seq_len, seq_len)
        avg_outputs = head_outputs.mean(dim=0)  # (n_heads, seq_len, d_k)
        
        print(f"=== å¤šå¤´æ³¨æ„åŠ›åˆ†æ ===")
        print(f"æ‰¹æ¬¡å¤§å°: {batch_size}, å¤´æ•°: {n_heads}, åºåˆ—é•¿åº¦: {seq_len}")
        
        for head in range(n_heads):
            head_name = head_names[head] if head_names else f"Head-{head+1}"
            print(f"\\n{head_name}:")
            
            # æ³¨æ„åŠ›ç‰¹æ€§
            attn = avg_attention[head]
            
            # 1. å¯¹è§’çº¿å¼ºåº¦ï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰
            diag_strength = torch.diag(attn).mean()
            
            # 2. å±€éƒ¨æ€§ï¼ˆç›¸é‚»ä½ç½®å…³æ³¨åº¦ï¼‰
            locality = 0
            if seq_len > 1:
                for i in range(seq_len - 1):
                    locality += attn[i, i+1] + attn[i+1, i]
                locality /= (2 * (seq_len - 1))
            
            # 3. æ³¨æ„åŠ›ç†µ
            entropy = -(attn * torch.log(attn + 1e-8)).sum(dim=-1).mean()
            
            # 4. æœ€å¤§æ³¨æ„åŠ›æƒé‡
            max_attention = attn.max()
            
            # 5. è¾“å‡ºæ¿€æ´»ç»Ÿè®¡
            output = avg_outputs[head]
            output_mean = output.mean()
            output_std = output.std()
            
            print(f"  è‡ªæ³¨æ„åŠ›å¼ºåº¦: {diag_strength:.4f}")
            print(f"  å±€éƒ¨å…³æ³¨åº¦: {locality:.4f}")
            print(f"  æ³¨æ„åŠ›ç†µ: {entropy:.4f}")
            print(f"  æœ€å¤§æƒé‡: {max_attention:.4f}")
            print(f"  è¾“å‡ºç»Ÿè®¡: å‡å€¼={output_mean:.4f}, æ ‡å‡†å·®={output_std:.4f}")
```

### å¤šå¤´æ³¨æ„åŠ›çš„å¯è§†åŒ–åˆ†æ

```python
def comprehensive_multihead_visualization(model, input_data, tokens, save_dir="./attention_analysis"):
    """å¤šå¤´æ³¨æ„åŠ›çš„ç»¼åˆå¯è§†åŒ–åˆ†æ"""
    
    import matplotlib.pyplot as plt
    import seaborn as sns
    import os
    
    os.makedirs(save_dir, exist_ok=True)
    
    model.eval()
    with torch.no_grad():
        # è·å–æ³¨æ„åŠ›æƒé‡
        output, attention_weights = model(input_data, return_attention=True)
    
    # attention_weights: (batch_size, n_heads, seq_len, seq_len)
    batch_size, n_heads, seq_len, _ = attention_weights.shape
    
    # å¹³å‡æ‰€æœ‰batch
    avg_attention = attention_weights.mean(dim=0)  # (n_heads, seq_len, seq_len)
    
    # 1. ç»˜åˆ¶æ‰€æœ‰å¤´çš„æ³¨æ„åŠ›æ¨¡å¼
    fig, axes = plt.subplots(2, n_heads//2, figsize=(20, 8))
    axes = axes.flatten()
    
    for head in range(n_heads):
        attn_matrix = avg_attention[head].cpu().numpy()
        
        sns.heatmap(
            attn_matrix,
            ax=axes[head],
            cmap='Blues',
            square=True,
            cbar=True,
            xticklabels=tokens if len(tokens) <= 10 else False,
            yticklabels=tokens if len(tokens) <= 10 else False
        )
        axes[head].set_title(f'Head {head+1}')
    
    plt.tight_layout()
    plt.savefig(f"{save_dir}/all_heads_attention.png", dpi=300, bbox_inches='tight')
    plt.show()
    
    # 2. æ³¨æ„åŠ›æ¨¡å¼èšç±»åˆ†æ
    attention_flat = avg_attention.view(n_heads, -1)  # (n_heads, seq_len*seq_len)
    
    # è®¡ç®—å¤´é—´ç›¸ä¼¼åº¦çŸ©é˜µ
    similarity_matrix = F.cosine_similarity(
        attention_flat.unsqueeze(1), 
        attention_flat.unsqueeze(0), 
        dim=2
    )
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(
        similarity_matrix.cpu().numpy(),
        annot=True,
        fmt='.3f',
        cmap='coolwarm',
        center=0,
        square=True,
        xticklabels=[f'H{i+1}' for i in range(n_heads)],
        yticklabels=[f'H{i+1}' for i in range(n_heads)]
    )
    plt.title('å¤´é—´æ³¨æ„åŠ›æ¨¡å¼ç›¸ä¼¼åº¦')
    plt.tight_layout()
    plt.savefig(f"{save_dir}/head_similarity.png", dpi=300, bbox_inches='tight')
    plt.show()
    
    # 3. æ³¨æ„åŠ›ç»Ÿè®¡åˆ†æ
    stats = []
    for head in range(n_heads):
        attn = avg_attention[head]
        
        # è®¡ç®—å„ç§ç»Ÿè®¡é‡
        diag_strength = torch.diag(attn).mean().item()
        entropy = -(attn * torch.log(attn + 1e-8)).sum(dim=-1).mean().item()
        max_weight = attn.max().item()
        
        # å±€éƒ¨æ€§åº¦é‡
        locality = 0
        if seq_len > 1:
            for i in range(seq_len - 1):
                locality += attn[i, i+1] + attn[i+1, i]
            locality = locality.item() / (2 * (seq_len - 1))
        
        stats.append({
            'head': head + 1,
            'diag_strength': diag_strength,
            'entropy': entropy,
            'max_weight': max_weight,
            'locality': locality
        })
    
    # ç»˜åˆ¶ç»Ÿè®¡å›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    
    heads = [s['head'] for s in stats]
    
    # å¯¹è§’çº¿å¼ºåº¦
    axes[0,0].bar(heads, [s['diag_strength'] for s in stats])
    axes[0,0].set_title('è‡ªæ³¨æ„åŠ›å¼ºåº¦')
    axes[0,0].set_xlabel('å¤´ç¼–å·')
    axes[0,0].set_ylabel('å¼ºåº¦')
    
    # æ³¨æ„åŠ›ç†µ
    axes[0,1].bar(heads, [s['entropy'] for s in stats])
    axes[0,1].set_title('æ³¨æ„åŠ›ç†µ')
    axes[0,1].set_xlabel('å¤´ç¼–å·')
    axes[0,1].set_ylabel('ç†µå€¼')
    
    # æœ€å¤§æƒé‡
    axes[1,0].bar(heads, [s['max_weight'] for s in stats])
    axes[1,0].set_title('æœ€å¤§æ³¨æ„åŠ›æƒé‡')
    axes[1,0].set_xlabel('å¤´ç¼–å·')
    axes[1,0].set_ylabel('æƒé‡')
    
    # å±€éƒ¨æ€§
    axes[1,1].bar(heads, [s['locality'] for s in stats])
    axes[1,1].set_title('å±€éƒ¨å…³æ³¨åº¦')
    axes[1,1].set_xlabel('å¤´ç¼–å·')
    axes[1,1].set_ylabel('å±€éƒ¨æ€§')
    
    plt.tight_layout()
    plt.savefig(f"{save_dir}/attention_statistics.png", dpi=300, bbox_inches='tight')
    plt.show()
    
    # æ‰“å°è¯¦ç»†ç»Ÿè®¡
    print("\\n=== å¤šå¤´æ³¨æ„åŠ›è¯¦ç»†ç»Ÿè®¡ ===")
    for stat in stats:
        print(f"å¤´ {stat['head']}:")
        print(f"  è‡ªæ³¨æ„åŠ›: {stat['diag_strength']:.4f}")
        print(f"  æ³¨æ„åŠ›ç†µ: {stat['entropy']:.4f}")
        print(f"  æœ€å¤§æƒé‡: {stat['max_weight']:.4f}")
        print(f"  å±€éƒ¨æ€§: {stat['locality']:.4f}")
        print()
    
    return stats, similarity_matrix
```

## å°ç»“ä¸æ€è€ƒ

æœ¬èŠ‚æ·±å…¥åˆ†æäº†å¤šå¤´æ³¨æ„åŠ›çš„å­ç©ºé—´åˆ†è§£ç†è®ºï¼š

1. **å­ç©ºé—´åˆ†è§£**ï¼šå°†é«˜ç»´ç©ºé—´åˆ†è§£ä¸ºå¤šä¸ªä½ç»´å­ç©ºé—´ï¼Œå®ç°å¹¶è¡Œè¯­ä¹‰å»ºæ¨¡
2. **è¡¨å¾å¤šæ ·æ€§**ï¼šä¸åŒå¤´å…³æ³¨ä¸åŒçš„è¯­ä¹‰ç»´åº¦ï¼Œå¢å¼ºæ¨¡å‹çš„è¡¨å¾èƒ½åŠ›
3. **ä¿¡æ¯èåˆ**ï¼šè¾“å‡ºæŠ•å½±çŸ©é˜µè´Ÿè´£èåˆå¤šå¤´ä¿¡æ¯ï¼Œå¹³è¡¡å„å¤´è´¡çŒ®
4. **å‡ ä½•æ„ä¹‰**ï¼šå¤šå¤´æœºåˆ¶åœ¨å‡ ä½•ä¸Šå®ç°äº†å¤šè§†è§’çš„ç›¸ä¼¼åº¦è®¡ç®—
5. **ä¼˜åŒ–ç­–ç•¥**ï¼šé€šè¿‡åˆç†åˆå§‹åŒ–å’Œèåˆè®¾è®¡æå‡å¤šå¤´æ•ˆæœ

**æ€è€ƒé¢˜**ï¼š
1. å¤šå¤´æ³¨æ„åŠ›ä¸­ï¼Œå¤´æ•°å¢åŠ æ˜¯å¦æ€»èƒ½æå‡æ€§èƒ½ï¼Ÿå­˜åœ¨ä»€ä¹ˆæƒè¡¡ï¼Ÿ
2. å¦‚ä½•è®¾è®¡æ›´å¥½çš„å¤´é—´ä¿¡æ¯èåˆæœºåˆ¶ï¼Ÿ
3. å­ç©ºé—´æ­£äº¤æ€§å¯¹æ¨¡å‹æ€§èƒ½æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ
4. èƒ½å¦è®¾è®¡è‡ªé€‚åº”çš„å¤´æ•°é€‰æ‹©æœºåˆ¶ï¼Ÿ

**ä¸‹ä¸€èŠ‚é¢„å‘Š**ï¼šæˆ‘ä»¬å°†å­¦ä¹ ä½ç½®ç¼–ç çš„å‡ ä½•å­¦åŸç†ï¼Œç†è§£å¦‚ä½•åœ¨Transformerä¸­å¼•å…¥ä½ç½®ä¿¡æ¯ã€‚

---

*å¤šå¤´æ³¨æ„åŠ›çš„ç²¾å¦™ä¹‹å¤„åœ¨äºç”¨å¹¶è¡Œçš„å­ç©ºé—´åˆ†è§£å®ç°äº†è®¤çŸ¥çš„å¤šç»´åº¦å»ºæ¨¡ï¼Œè¿™æ­£æ˜¯äººç±»ç†è§£å¤æ‚æ¦‚å¿µæ—¶å¤šè§’åº¦æ€è€ƒçš„æ•°å­¦ä½“ç°ã€‚* ğŸ§ 