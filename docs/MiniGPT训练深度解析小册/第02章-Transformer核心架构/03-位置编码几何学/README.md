# 03 ä½ç½®ç¼–ç å‡ ä½•å­¦

> **ä»å‚…é‡Œå¶å˜æ¢åˆ°ç›¸å¯¹ä½ç½®çš„æ•°å­¦ä¹‹ç¾**

## æ ¸å¿ƒæ€æƒ³

Transformeræ¶æ„çš„ä¸€ä¸ªæ ¹æœ¬æŒ‘æˆ˜æ˜¯ï¼š**æ³¨æ„åŠ›æœºåˆ¶æœ¬èº«æ˜¯ä½ç½®æ— å…³çš„**ã€‚å¦‚æœæˆ‘ä»¬æ‰“ä¹±åºåˆ—ä¸­tokençš„é¡ºåºï¼Œæ³¨æ„åŠ›æƒé‡çŸ©é˜µä¿æŒä¸å˜ã€‚è¿™å¯¹äºè¯­è¨€å»ºæ¨¡æ˜¯è‡´å‘½çš„ï¼Œå› ä¸ºè¯­è¨€å…·æœ‰å¼ºçƒˆçš„é¡ºåºä¾èµ–æ€§ã€‚

ä½ç½®ç¼–ç (Positional Encoding)å·§å¦™åœ°è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚å®ƒä¸æ˜¯ç®€å•åœ°ä¸ºæ¯ä¸ªä½ç½®åˆ†é…ä¸€ä¸ªIDï¼Œè€Œæ˜¯åŸºäº**å‚…é‡Œå¶åˆ†æ**çš„æ€æƒ³ï¼Œä½¿ç”¨ä¸åŒé¢‘ç‡çš„æ­£å¼¦å’Œä½™å¼¦å‡½æ•°æ¥ç¼–ç ä½ç½®ä¿¡æ¯ã€‚è¿™ç§è®¾è®¡å…·æœ‰æ·±åˆ»çš„æ•°å­¦ç¾æ„Ÿå’Œå‡ ä½•ç›´è§‰ã€‚

**å…³é”®æ´å¯Ÿ**ï¼š
- ä½ç½®ç¼–ç æ˜¯ä¸€ç§**å¯†é›†è¡¨ç¤º**ï¼Œå°†ç¦»æ•£ä½ç½®æ˜ å°„åˆ°è¿ç»­å‘é‡ç©ºé—´
- **ä¸‰è§’å‡½æ•°åŸºç¡€**ä½¿å¾—ç›¸å¯¹ä½ç½®å…³ç³»å¯ä»¥é€šè¿‡çº¿æ€§å˜æ¢è¡¨è¾¾
- **é¢‘ç‡é€’å‡**çš„è®¾è®¡è®©æ¨¡å‹èƒ½å¤Ÿæ•æ‰ä¸åŒå°ºåº¦çš„ä½ç½®æ¨¡å¼
- **ç¡®å®šæ€§ç¼–ç **ä¿è¯äº†ä½ç½®è¡¨ç¤ºçš„ä¸€è‡´æ€§å’Œå¯è§£é‡Šæ€§

## 3.1 å‚…é‡Œå¶åŸºå‡½æ•°çš„ä½ç½®è¡¨ç¤º

### ä»ç¦»æ•£ä½ç½®åˆ°è¿ç»­åµŒå…¥

**ç»å…¸ä½ç½®ç¼–ç å…¬å¼**ï¼š
$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$
$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$

å…¶ä¸­ï¼š
- $pos$ï¼šä½ç½®ç´¢å¼• ($0, 1, 2, ..., max\_len-1$)
- $i$ï¼šç»´åº¦ç´¢å¼• ($0, 1, 2, ..., d_{model}/2-1$)
- $d_{model}$ï¼šæ¨¡å‹ç»´åº¦

**å‚…é‡Œå¶çº§æ•°çš„è§†è§’**ï¼š

ä»»ä½•å‘¨æœŸå‡½æ•°éƒ½å¯ä»¥è¡¨ç¤ºä¸ºæ­£å¼¦å’Œä½™å¼¦å‡½æ•°çš„çº¿æ€§ç»„åˆï¼š
$$f(x) = a_0 + \sum_{n=1}^{\infty} [a_n \cos(n\omega x) + b_n \sin(n\omega x)]$$

ä½ç½®ç¼–ç æœ¬è´¨ä¸Šæ˜¯åœ¨æ„é€ ä¸€ä¸ª**ä½ç½®çš„å‚…é‡Œå¶è¡¨ç¤º**ï¼Œå…¶ä¸­ä¸åŒçš„ç»´åº¦å¯¹åº”ä¸åŒçš„é¢‘ç‡åˆ†é‡ã€‚

```python
# MiniGPTä¸­çš„ä½ç½®ç¼–ç å®ç° (src/model/transformer.py:134-151)
def __init__(self, d_model: int, max_len: int = 5000):
    super().__init__()
    
    # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ
    pe = torch.zeros(max_len, d_model)
    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
    
    # è®¡ç®—é™¤æ•°é¡¹ï¼š10000^(2i/d_model)
    div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                       (-math.log(10000.0) / d_model))
    
    # è®¡ç®—æ­£å¼¦å’Œä½™å¼¦
    pe[:, 0::2] = torch.sin(position * div_term)  # å¶æ•°ä½ç½®
    pe[:, 1::2] = torch.cos(position * div_term)  # å¥‡æ•°ä½ç½®
    
    # æ·»åŠ æ‰¹æ¬¡ç»´åº¦å¹¶æ³¨å†Œä¸ºbuffer
    pe = pe.unsqueeze(0).transpose(0, 1)
    self.register_buffer('pe', pe)
```

### é¢‘ç‡åˆ†é‡çš„å‡ ä½•åˆ†æ

**é™¤æ•°é¡¹çš„æ•°å­¦æ¨å¯¼**ï¼š

è®¾ $\omega_i = \frac{1}{10000^{2i/d_{model}}}$ï¼Œåˆ™ï¼š
$$\omega_i = \frac{1}{10000^{2i/d_{model}}} = \exp\left(-\frac{2i \ln(10000)}{d_{model}}\right)$$

è¿™åˆ›é€ äº†ä¸€ä¸ª**å‡ ä½•çº§æ•°**çš„é¢‘ç‡åºåˆ—ï¼š
- ä½ç»´åº¦($i=0$)ï¼šé«˜é¢‘ç‡ï¼ŒçŸ­å‘¨æœŸ ($2\pi$)
- é«˜ç»´åº¦($i$ å¤§)ï¼šä½é¢‘ç‡ï¼Œé•¿å‘¨æœŸ ($2\pi \times 10000$)

```python
def analyze_frequency_spectrum(d_model=512, max_len=1000):
    """åˆ†æä½ç½®ç¼–ç çš„é¢‘ç‡è°±ç‰¹æ€§"""
    
    # è®¡ç®—æ‰€æœ‰é¢‘ç‡
    frequencies = []
    periods = []
    
    for i in range(d_model // 2):
        omega = 1.0 / (10000 ** (2 * i / d_model))
        period = 2 * math.pi / omega
        
        frequencies.append(omega)
        periods.append(period)
    
    frequencies = torch.tensor(frequencies)
    periods = torch.tensor(periods)
    
    print(f"ä½ç½®ç¼–ç é¢‘ç‡åˆ†æ (d_model={d_model}):")
    print(f"  æœ€é«˜é¢‘ç‡: {frequencies.max():.6f} (å‘¨æœŸ: {periods.min():.2f})")
    print(f"  æœ€ä½é¢‘ç‡: {frequencies.min():.6f} (å‘¨æœŸ: {periods.max():.2f})")
    print(f"  é¢‘ç‡æ¯”å€¼: {frequencies.max()/frequencies.min():.2f}")
    
    # ç»˜åˆ¶é¢‘ç‡è°±
    import matplotlib.pyplot as plt
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    
    # é¢‘ç‡åˆ†å¸ƒ
    ax1.semilogy(range(len(frequencies)), frequencies)
    ax1.set_xlabel('ç»´åº¦ç´¢å¼• i')
    ax1.set_ylabel('é¢‘ç‡ Ï‰_i')
    ax1.set_title('ä½ç½®ç¼–ç é¢‘ç‡åˆ†å¸ƒ')
    ax1.grid(True)
    
    # å‘¨æœŸåˆ†å¸ƒ
    ax2.semilogy(range(len(periods)), periods)
    ax2.set_xlabel('ç»´åº¦ç´¢å¼• i')
    ax2.set_ylabel('å‘¨æœŸ')
    ax2.set_title('ä½ç½®ç¼–ç å‘¨æœŸåˆ†å¸ƒ')
    ax2.grid(True)
    
    plt.tight_layout()
    plt.show()
    
    return frequencies, periods
```

### ä½ç½®ç¼–ç çš„å‡ ä½•ç»“æ„

åœ¨ $d_{model}$ ç»´ç©ºé—´ä¸­ï¼Œä½ç½®ç¼–ç å½¢æˆäº†ä¸€ä¸ª**èºæ—‹ç»“æ„**ï¼š

```python
def visualize_positional_encoding_geometry(d_model=64, max_len=100):
    """å¯è§†åŒ–ä½ç½®ç¼–ç çš„å‡ ä½•ç»“æ„"""
    
    # ç”Ÿæˆä½ç½®ç¼–ç 
    pe = torch.zeros(max_len, d_model)
    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
    
    div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                       (-math.log(10000.0) / d_model))
    
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    
    import matplotlib.pyplot as plt
    from mpl_toolkits.mplot3d import Axes3D
    
    # 1. åœ¨å‰ä¸‰ä¸ªç»´åº¦ä¸­å¯è§†åŒ–èºæ—‹ç»“æ„
    fig = plt.figure(figsize=(15, 5))
    
    # 3Dèºæ—‹
    ax1 = fig.add_subplot(131, projection='3d')
    ax1.plot(pe[:50, 0], pe[:50, 1], pe[:50, 2], 'b-', alpha=0.7)
    ax1.scatter(pe[::5, 0], pe[::5, 1], pe[::5, 2], 
               c=range(0, 50, 5), cmap='viridis', s=50)
    ax1.set_xlabel('PE[0] (sin)')
    ax1.set_ylabel('PE[1] (cos)')
    ax1.set_zlabel('PE[2] (sin)')
    ax1.set_title('3Dä½ç½®ç¼–ç èºæ—‹')
    
    # ä¸åŒé¢‘ç‡åˆ†é‡çš„å¯è§†åŒ–
    ax2 = fig.add_subplot(132)
    positions = torch.arange(50)
    ax2.plot(positions, pe[:50, 0], 'r-', label='ç»´åº¦0 (é«˜é¢‘)', alpha=0.8)
    ax2.plot(positions, pe[:50, 10], 'g-', label='ç»´åº¦10 (ä¸­é¢‘)', alpha=0.8)
    ax2.plot(positions, pe[:50, 30], 'b-', label='ç»´åº¦30 (ä½é¢‘)', alpha=0.8)
    ax2.set_xlabel('ä½ç½®')
    ax2.set_ylabel('ç¼–ç å€¼')
    ax2.set_title('ä¸åŒé¢‘ç‡åˆ†é‡')
    ax2.legend()
    ax2.grid(True)
    
    # ä½ç½®ç¼–ç çƒ­åŠ›å›¾
    ax3 = fig.add_subplot(133)
    im = ax3.imshow(pe[:50, :20].T, cmap='coolwarm', aspect='auto')
    ax3.set_xlabel('ä½ç½®')
    ax3.set_ylabel('ç¼–ç ç»´åº¦')
    ax3.set_title('ä½ç½®ç¼–ç çƒ­åŠ›å›¾')
    plt.colorbar(im, ax=ax3)
    
    plt.tight_layout()
    plt.show()
    
    # åˆ†æå‡ ä½•æ€§è´¨
    print("\\n=== å‡ ä½•æ€§è´¨åˆ†æ ===")
    
    # 1. ç›¸é‚»ä½ç½®çš„æ¬§æ°è·ç¦»
    distances = []
    for i in range(min(49, max_len-1)):
        dist = torch.norm(pe[i+1] - pe[i])
        distances.append(dist.item())
    
    avg_distance = sum(distances) / len(distances)
    print(f"ç›¸é‚»ä½ç½®å¹³å‡è·ç¦»: {avg_distance:.4f}")
    
    # 2. ä½ç½®ç¼–ç çš„èŒƒæ•°
    norms = torch.norm(pe, dim=1)
    print(f"ä½ç½®ç¼–ç èŒƒæ•°: å‡å€¼={norms.mean():.4f}, æ ‡å‡†å·®={norms.std():.4f}")
    
    # 3. åŸç‚¹åˆ°å„ä½ç½®çš„è·ç¦»å˜åŒ–
    origin_distances = norms[:min(20, max_len)]
    print(f"å‰20ä¸ªä½ç½®åˆ°åŸç‚¹è·ç¦»å˜åŒ–: {origin_distances.std():.6f}")
    
    return pe
```

## 3.2 ç›¸å¯¹ä½ç½®çš„çº¿æ€§è¡¨ç¤º

### ä¸‰è§’æ’ç­‰å¼çš„å·§å¦™åº”ç”¨

ä½ç½®ç¼–ç æœ€ç²¾å¦™çš„è®¾è®¡åœ¨äºï¼š**ä»»ä½•ä½ç½®çš„ç¼–ç éƒ½å¯ä»¥è¡¨ç¤ºä¸ºå…¶ä»–ä½ç½®ç¼–ç çš„çº¿æ€§ç»„åˆ**ã€‚

**æ•°å­¦æ¨å¯¼**ï¼š

å¯¹äºä½ç½® $pos + k$ï¼Œå…¶ç¼–ç å¯ä»¥å†™æˆï¼š
$$PE_{pos+k} = \mathbf{T}_k \cdot PE_{pos}$$

å…¶ä¸­ $\mathbf{T}_k$ æ˜¯ä»…ä¾èµ–äºç›¸å¯¹åç§» $k$ çš„å˜æ¢çŸ©é˜µã€‚

**è¯æ˜**ï¼š
åˆ©ç”¨ä¸‰è§’æ’ç­‰å¼ï¼š
$$\sin(A + B) = \sin A \cos B + \cos A \sin B$$
$$\cos(A + B) = \cos A \cos B - \sin A \sin B$$

è®¾ $\omega_i = \frac{1}{10000^{2i/d_{model}}}$ï¼Œåˆ™ï¼š
$$PE_{(pos+k, 2i)} = \sin((pos+k)\omega_i) = \sin(pos\omega_i)\cos(k\omega_i) + \cos(pos\omega_i)\sin(k\omega_i)$$

è¿™è¡¨æ˜ä½ç½® $pos+k$ çš„ç¼–ç å¯ä»¥é€šè¿‡ä½ç½® $pos$ çš„ç¼–ç çº¿æ€§è¡¨ç¤ºï¼

```python
def verify_linear_transformation_property(d_model=64, max_len=100):
    """éªŒè¯ä½ç½®ç¼–ç çš„çº¿æ€§å˜æ¢æ€§è´¨"""
    
    # ç”Ÿæˆä½ç½®ç¼–ç 
    pe = torch.zeros(max_len, d_model)
    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
    
    div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                       (-math.log(10000.0) / d_model))
    
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    
    # æ„é€ ç›¸å¯¹ä½ç½®å˜æ¢çŸ©é˜µ
    def create_relative_transform_matrix(k, d_model):
        """ä¸ºç›¸å¯¹åç§»kåˆ›å»ºå˜æ¢çŸ©é˜µ"""
        T = torch.zeros(d_model, d_model)
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        for i, omega in enumerate(div_term):
            # å¯¹äºæ¯ä¸€å¯¹(sin, cos)ç»´åº¦
            sin_idx = 2 * i
            cos_idx = 2 * i + 1
            
            if cos_idx < d_model:
                # æ„é€ 2x2æ—‹è½¬çŸ©é˜µ
                cos_k_omega = math.cos(k * omega)
                sin_k_omega = math.sin(k * omega)
                
                T[sin_idx, sin_idx] = cos_k_omega
                T[sin_idx, cos_idx] = sin_k_omega
                T[cos_idx, sin_idx] = -sin_k_omega
                T[cos_idx, cos_idx] = cos_k_omega
        
        return T
    
    # æµ‹è¯•ç›¸å¯¹ä½ç½®å˜æ¢
    test_positions = [5, 10, 15, 20]
    relative_offsets = [1, 3, 5, 10]
    
    print("=== ç›¸å¯¹ä½ç½®çº¿æ€§å˜æ¢éªŒè¯ ===")
    
    for pos in test_positions:
        for k in relative_offsets:
            if pos + k < max_len:
                # ç›´æ¥è®¡ç®—çš„ä½ç½®ç¼–ç 
                direct_encoding = pe[pos + k]
                
                # é€šè¿‡çº¿æ€§å˜æ¢è®¡ç®—çš„ä½ç½®ç¼–ç 
                T_k = create_relative_transform_matrix(k, d_model)
                transformed_encoding = torch.matmul(T_k, pe[pos])
                
                # è®¡ç®—è¯¯å·®
                error = torch.norm(direct_encoding - transformed_encoding)
                
                print(f"ä½ç½®{pos} -> {pos+k} (åç§»{k}): è¯¯å·®={error:.8f}")
                
                if error < 1e-6:
                    print("  âœ“ çº¿æ€§å˜æ¢æ€§è´¨æˆç«‹")
                else:
                    print("  âŒ å­˜åœ¨æ•°å€¼è¯¯å·®")
    
    # å¯è§†åŒ–å˜æ¢çŸ©é˜µ
    T_1 = create_relative_transform_matrix(1, d_model)
    T_5 = create_relative_transform_matrix(5, d_model)
    
    import matplotlib.pyplot as plt
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    im1 = ax1.imshow(T_1[:32, :32], cmap='coolwarm')
    ax1.set_title('ç›¸å¯¹ä½ç½®å˜æ¢çŸ©é˜µ T_1')
    ax1.set_xlabel('è¾“å…¥ç»´åº¦')
    ax1.set_ylabel('è¾“å‡ºç»´åº¦')
    plt.colorbar(im1, ax=ax1)
    
    im2 = ax2.imshow(T_5[:32, :32], cmap='coolwarm')
    ax2.set_title('ç›¸å¯¹ä½ç½®å˜æ¢çŸ©é˜µ T_5')
    ax2.set_xlabel('è¾“å…¥ç»´åº¦')
    ax2.set_ylabel('è¾“å‡ºç»´åº¦')
    plt.colorbar(im2, ax=ax2)
    
    plt.tight_layout()
    plt.show()
    
    return T_1, T_5
```

### ç›¸å¯¹ä½ç½®æ³¨æ„åŠ›çš„æ•°å­¦æ¨¡å‹

åŸºäºçº¿æ€§å˜æ¢æ€§è´¨ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ³¨æ„åŠ›åˆ†æ•°å†™æˆç›¸å¯¹ä½ç½®çš„å‡½æ•°ï¼š

$$\text{Attention}_{ij} = \text{softmax}\left(\frac{(x_i + PE_i)W^Q \cdot (x_j + PE_j)W^K}{\sqrt{d_k}}\right)$$

å±•å¼€åå¯ä»¥å¾—åˆ°ï¼š
$$= \text{softmax}\left(\frac{x_iW^Q \cdot x_jW^K + x_iW^Q \cdot PE_jW^K + PE_iW^Q \cdot x_jW^K + PE_iW^Q \cdot PE_jW^K}{\sqrt{d_k}}\right)$$

æœ€åä¸€é¡¹ $PE_iW^Q \cdot PE_jW^K$ çº¯ç²¹ä¾èµ–äºä½ç½® $i$ å’Œ $j$ï¼Œå¯ä»¥é¢„è®¡ç®—ä¸ºç›¸å¯¹ä½ç½®åç½®ã€‚

```python
def analyze_relative_position_bias(d_model=512, n_heads=8, max_len=50):
    """åˆ†æç›¸å¯¹ä½ç½®åç½®çš„ä½œç”¨"""
    
    d_k = d_model // n_heads
    
    # ç”Ÿæˆä½ç½®ç¼–ç 
    pe = torch.zeros(max_len, d_model)
    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
    
    div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                       (-math.log(10000.0) / d_model))
    
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    
    # æ¨¡æ‹ŸæŸ¥è¯¢å’Œé”®çš„æŠ•å½±çŸ©é˜µ
    W_q = torch.randn(d_model, d_model) * 0.02
    W_k = torch.randn(d_model, d_model) * 0.02
    
    # è®¡ç®—çº¯ä½ç½®ç›¸å…³çš„æ³¨æ„åŠ›åˆ†æ•°
    PE_q = torch.matmul(pe, W_q)  # (max_len, d_model)
    PE_k = torch.matmul(pe, W_k)  # (max_len, d_model)
    
    # é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
    PE_q = PE_q.view(max_len, n_heads, d_k)
    PE_k = PE_k.view(max_len, n_heads, d_k)
    
    # è®¡ç®—ä½ç½®åç½®çŸ©é˜µ
    position_bias = torch.zeros(n_heads, max_len, max_len)
    
    for head in range(n_heads):
        for i in range(max_len):
            for j in range(max_len):
                bias = torch.dot(PE_q[i, head], PE_k[j, head]) / math.sqrt(d_k)
                position_bias[head, i, j] = bias
    
    # åˆ†æåç½®æ¨¡å¼
    import matplotlib.pyplot as plt
    
    fig, axes = plt.subplots(2, 4, figsize=(16, 8))
    axes = axes.flatten()
    
    for head in range(min(8, n_heads)):
        im = axes[head].imshow(position_bias[head].numpy(), cmap='coolwarm')
        axes[head].set_title(f'å¤´ {head+1} ä½ç½®åç½®')
        axes[head].set_xlabel('é”®ä½ç½®')
        axes[head].set_ylabel('æŸ¥è¯¢ä½ç½®')
        plt.colorbar(im, ax=axes[head])
    
    plt.tight_layout()
    plt.show()
    
    # åˆ†æç›¸å¯¹ä½ç½®æ¨¡å¼
    print("=== ç›¸å¯¹ä½ç½®åç½®åˆ†æ ===")
    
    for head in range(min(4, n_heads)):
        bias_matrix = position_bias[head]
        
        # åˆ†æå¯¹è§’çº¿æ¨¡å¼
        diag_values = torch.diag(bias_matrix)
        print(f"\\nå¤´ {head+1}:")
        print(f"  å¯¹è§’çº¿åç½®å‡å€¼: {diag_values.mean():.4f}")
        print(f"  å¯¹è§’çº¿åç½®æ ‡å‡†å·®: {diag_values.std():.4f}")
        
        # åˆ†æç›¸å¯¹è·ç¦»æ¨¡å¼
        relative_biases = {}
        for distance in range(1, min(10, max_len)):
            if distance < max_len:
                # æå–ç›¸å¯¹è·ç¦»ä¸ºdistanceçš„æ‰€æœ‰åç½®å€¼
                values = []
                for i in range(max_len - distance):
                    values.append(bias_matrix[i, i + distance].item())
                
                relative_biases[distance] = {
                    'mean': sum(values) / len(values),
                    'std': torch.tensor(values).std().item()
                }
        
        # æ‰“å°ç›¸å¯¹è·ç¦»åç½®
        for dist, stats in relative_biases.items():
            print(f"  ç›¸å¯¹è·ç¦»{dist}: å‡å€¼={stats['mean']:.4f}, æ ‡å‡†å·®={stats['std']:.4f}")
    
    return position_bias
```

## 3.3 ä½ç½®ç¼–ç çš„é¢‘è°±åˆ†æ

### ä¸åŒé¢‘ç‡åˆ†é‡çš„ä½œç”¨

ä½ç½®ç¼–ç çš„é¢‘ç‡è®¾è®¡éµå¾ªå‡ ä½•çº§æ•°ï¼Œè¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•æ‰ä¸åŒå°ºåº¦çš„ä½ç½®æ¨¡å¼ï¼š

- **é«˜é¢‘åˆ†é‡**ï¼ˆä½ç»´åº¦ï¼‰ï¼šæ•æ‰å±€éƒ¨ä½ç½®å…³ç³»ï¼Œå¦‚ç›¸é‚»è¯çš„é¡ºåº
- **ä½é¢‘åˆ†é‡**ï¼ˆé«˜ç»´åº¦ï¼‰ï¼šæ•æ‰å…¨å±€ä½ç½®å…³ç³»ï¼Œå¦‚å¥å­ç»“æ„ã€æ®µè½ç»„ç»‡

```python
def analyze_frequency_contributions(d_model=512, max_len=200):
    """åˆ†æä¸åŒé¢‘ç‡åˆ†é‡å¯¹ä½ç½®å»ºæ¨¡çš„è´¡çŒ®"""
    
    # ç”Ÿæˆå®Œæ•´ä½ç½®ç¼–ç 
    pe_full = torch.zeros(max_len, d_model)
    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
    
    div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                       (-math.log(10000.0) / d_model))
    
    pe_full[:, 0::2] = torch.sin(position * div_term)
    pe_full[:, 1::2] = torch.cos(position * div_term)
    
    # åˆ†æä¸åŒé¢‘ç‡æ®µçš„è´¡çŒ®
    frequency_bands = {
        'é«˜é¢‘ (dim 0-31)': (0, 32),
        'ä¸­é«˜é¢‘ (dim 32-127)': (32, 128),
        'ä¸­ä½é¢‘ (dim 128-255)': (128, 256),
        'ä½é¢‘ (dim 256-511)': (256, 512)
    }
    
    import matplotlib.pyplot as plt
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    axes = axes.flatten()
    
    for idx, (band_name, (start, end)) in enumerate(frequency_bands.items()):
        if end <= d_model:
            # æå–è¯¥é¢‘ç‡æ®µçš„ç¼–ç 
            pe_band = torch.zeros_like(pe_full)
            pe_band[:, start:end] = pe_full[:, start:end]
            
            # è®¡ç®—ç›¸é‚»ä½ç½®çš„ç›¸ä¼¼åº¦
            similarities = []
            for i in range(max_len - 1):
                sim = F.cosine_similarity(pe_band[i], pe_band[i+1], dim=0)
                similarities.append(sim.item())
            
            # ç»˜åˆ¶ç›¸ä¼¼åº¦å˜åŒ–
            axes[idx].plot(similarities, alpha=0.7)
            axes[idx].set_title(f'{band_name}\\nç›¸é‚»ä½ç½®ç›¸ä¼¼åº¦')
            axes[idx].set_xlabel('ä½ç½®')
            axes[idx].set_ylabel('ä½™å¼¦ç›¸ä¼¼åº¦')
            axes[idx].grid(True)
            
            # ç»Ÿè®¡ä¿¡æ¯
            avg_sim = sum(similarities) / len(similarities)
            print(f"{band_name}: å¹³å‡ç›¸é‚»ç›¸ä¼¼åº¦={avg_sim:.4f}")
    
    plt.tight_layout()
    plt.show()
    
    # åˆ†æä½ç½®åŒºåˆ†èƒ½åŠ›
    print("\\n=== ä½ç½®åŒºåˆ†èƒ½åŠ›åˆ†æ ===")
    
    # è®¡ç®—æ‰€æœ‰ä½ç½®å¯¹çš„ç›¸ä¼¼åº¦åˆ†å¸ƒ
    all_similarities = []
    distances = []
    
    for i in range(0, max_len, 5):  # é‡‡æ ·ä»¥å‡å°‘è®¡ç®—é‡
        for j in range(i+1, max_len, 5):
            sim = F.cosine_similarity(pe_full[i], pe_full[j], dim=0)
            distance = abs(j - i)
            
            all_similarities.append(sim.item())
            distances.append(distance)
    
    # æŒ‰è·ç¦»åˆ†ç»„åˆ†æ
    distance_bins = [1, 5, 10, 20, 50, 100]
    
    for i, max_dist in enumerate(distance_bins):
        if i == 0:
            min_dist = 1
        else:
            min_dist = distance_bins[i-1] + 1
        
        # ç­›é€‰è¯¥è·ç¦»èŒƒå›´çš„ç›¸ä¼¼åº¦
        filtered_sims = [sim for sim, dist in zip(all_similarities, distances) 
                        if min_dist <= dist <= max_dist]
        
        if filtered_sims:
            avg_sim = sum(filtered_sims) / len(filtered_sims)
            print(f"è·ç¦» {min_dist}-{max_dist}: å¹³å‡ç›¸ä¼¼åº¦={avg_sim:.4f}")
    
    return pe_full
```

### ä½ç½®ç¼–ç çš„å‚…é‡Œå¶å˜æ¢åˆ†æ

```python
def fourier_analysis_of_positional_encoding(d_model=512, max_len=1000):
    """å¯¹ä½ç½®ç¼–ç è¿›è¡Œå‚…é‡Œå¶å˜æ¢åˆ†æ"""
    
    # ç”Ÿæˆä½ç½®ç¼–ç 
    pe = torch.zeros(max_len, d_model)
    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
    
    div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                       (-math.log(10000.0) / d_model))
    
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    
    import numpy as np
    import matplotlib.pyplot as plt
    
    # å¯¹å‡ ä¸ªå…¸å‹ç»´åº¦è¿›è¡ŒFFTåˆ†æ
    selected_dims = [0, 10, 50, 100, 200]
    
    fig, axes = plt.subplots(len(selected_dims), 2, figsize=(15, 12))
    
    for idx, dim in enumerate(selected_dims):
        if dim < d_model:
            signal = pe[:, dim].numpy()
            
            # æ—¶åŸŸä¿¡å·
            axes[idx, 0].plot(signal)
            axes[idx, 0].set_title(f'ç»´åº¦ {dim} - æ—¶åŸŸä¿¡å·')
            axes[idx, 0].set_xlabel('ä½ç½®')
            axes[idx, 0].set_ylabel('ç¼–ç å€¼')
            axes[idx, 0].grid(True)
            
            # é¢‘åŸŸåˆ†æ
            fft_result = np.fft.fft(signal)
            freqs = np.fft.fftfreq(len(signal))
            
            # åªæ˜¾ç¤ºæ­£é¢‘ç‡éƒ¨åˆ†
            pos_freqs = freqs[:len(freqs)//2]
            pos_fft = np.abs(fft_result[:len(fft_result)//2])
            
            axes[idx, 1].semilogy(pos_freqs, pos_fft)
            axes[idx, 1].set_title(f'ç»´åº¦ {dim} - é¢‘åŸŸè°±')
            axes[idx, 1].set_xlabel('é¢‘ç‡')
            axes[idx, 1].set_ylabel('å¹…åº¦ (å¯¹æ•°)')
            axes[idx, 1].grid(True)
            
            # æ‰¾åˆ°ä¸»é¢‘ç‡
            main_freq_idx = np.argmax(pos_fft[1:]) + 1  # æ’é™¤DCåˆ†é‡
            main_freq = pos_freqs[main_freq_idx]
            
            print(f"ç»´åº¦ {dim}: ä¸»é¢‘ç‡={main_freq:.6f}, å¯¹åº”å‘¨æœŸ={1/main_freq:.2f}")
    
    plt.tight_layout()
    plt.show()
    
    # åˆ†ææ•´ä½“é¢‘è°±ç‰¹æ€§
    print("\\n=== æ•´ä½“é¢‘è°±ç‰¹æ€§ ===")
    
    # è®¡ç®—æ¯ä¸ªç»´åº¦çš„ä¸»é¢‘ç‡
    main_frequencies = []
    theoretical_frequencies = []
    
    for dim in range(0, d_model, 2):  # åªåˆ†æsinç»´åº¦
        signal = pe[:, dim].numpy()
        fft_result = np.fft.fft(signal)
        freqs = np.fft.fftfreq(len(signal))
        
        pos_freqs = freqs[:len(freqs)//2]
        pos_fft = np.abs(fft_result[:len(fft_result)//2])
        
        # å®é™…ä¸»é¢‘ç‡
        main_freq_idx = np.argmax(pos_fft[1:]) + 1
        actual_freq = pos_freqs[main_freq_idx]
        main_frequencies.append(actual_freq)
        
        # ç†è®ºé¢‘ç‡
        i = dim // 2
        theoretical_freq = 1 / (2 * math.pi * (10000 ** (2 * i / d_model)))
        theoretical_frequencies.append(theoretical_freq)
    
    # æ¯”è¾ƒå®é™…é¢‘ç‡å’Œç†è®ºé¢‘ç‡
    main_frequencies = np.array(main_frequencies)
    theoretical_frequencies = np.array(theoretical_frequencies)
    
    plt.figure(figsize=(10, 6))
    plt.semilogy(main_frequencies, 'b-', label='å®é™…é¢‘ç‡', alpha=0.7)
    plt.semilogy(theoretical_frequencies, 'r--', label='ç†è®ºé¢‘ç‡', alpha=0.7)
    plt.xlabel('ç»´åº¦ç´¢å¼•')
    plt.ylabel('é¢‘ç‡ (å¯¹æ•°)')
    plt.title('ä½ç½®ç¼–ç é¢‘ç‡ï¼šç†è®º vs å®é™…')
    plt.legend()
    plt.grid(True)
    plt.show()
    
    # è®¡ç®—è¯¯å·®
    freq_error = np.abs(main_frequencies - theoretical_frequencies) / theoretical_frequencies
    print(f"é¢‘ç‡è¯¯å·®: å¹³å‡={freq_error.mean():.6f}, æœ€å¤§={freq_error.max():.6f}")
    
    return main_frequencies, theoretical_frequencies
```

## 3.4 é•¿åºåˆ—å¤–æ¨çš„æ•°å­¦åŸºç¡€

### å¤–æ¨æ€§èƒ½çš„ç†è®ºåˆ†æ

ä½ç½®ç¼–ç çš„ä¸€ä¸ªå…³é”®ä¼˜åŠ¿æ˜¯å…¶å¯¹é•¿åºåˆ—çš„**å¤–æ¨èƒ½åŠ›**ã€‚ç†è®ºä¸Šï¼Œç”±äºä½¿ç”¨çš„æ˜¯è¿ç»­çš„ä¸‰è§’å‡½æ•°ï¼Œæ¨¡å‹å¯ä»¥å¤„ç†è®­ç»ƒæ—¶æœªè§è¿‡çš„æ›´é•¿åºåˆ—ã€‚

**å¤–æ¨æ€§è´¨çš„æ•°å­¦åŸºç¡€**ï¼š

1. **å‡½æ•°è¿ç»­æ€§**ï¼šä¸‰è§’å‡½æ•°åœ¨æ•´ä¸ªå®æ•°åŸŸä¸Šè¿ç»­
2. **å‘¨æœŸæ€§ä¿æŒ**ï¼šé¢‘ç‡ç»“æ„ä¿æŒä¸å˜
3. **ç›¸å¯¹å…³ç³»ç¨³å®š**ï¼šç›¸å¯¹ä½ç½®çš„çº¿æ€§å˜æ¢æ€§è´¨ä¿æŒ

```python
def test_extrapolation_capability(trained_max_len=100, test_max_len=200, d_model=256):
    """æµ‹è¯•ä½ç½®ç¼–ç çš„å¤–æ¨èƒ½åŠ›"""
    
    print(f"=== å¤–æ¨èƒ½åŠ›æµ‹è¯• ===")
    print(f"è®­ç»ƒæœ€å¤§é•¿åº¦: {trained_max_len}")
    print(f"æµ‹è¯•æœ€å¤§é•¿åº¦: {test_max_len}")
    
    # ç”Ÿæˆè®­ç»ƒé•¿åº¦çš„ä½ç½®ç¼–ç 
    pe_train = torch.zeros(trained_max_len, d_model)
    position_train = torch.arange(0, trained_max_len, dtype=torch.float).unsqueeze(1)
    
    div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                       (-math.log(10000.0) / d_model))
    
    pe_train[:, 0::2] = torch.sin(position_train * div_term)
    pe_train[:, 1::2] = torch.cos(position_train * div_term)
    
    # ç”Ÿæˆæ‰©å±•é•¿åº¦çš„ä½ç½®ç¼–ç 
    pe_extended = torch.zeros(test_max_len, d_model)
    position_extended = torch.arange(0, test_max_len, dtype=torch.float).unsqueeze(1)
    
    pe_extended[:, 0::2] = torch.sin(position_extended * div_term)
    pe_extended[:, 1::2] = torch.cos(position_extended * div_term)
    
    # åˆ†æå¤–æ¨åŒºåŸŸçš„æ€§è´¨
    extrapolation_region = pe_extended[trained_max_len:]
    train_region = pe_extended[:trained_max_len]
    
    print(f"\\nè®­ç»ƒåŒºåŸŸç»Ÿè®¡:")
    print(f"  ç¼–ç èŒƒæ•°å‡å€¼: {torch.norm(train_region, dim=1).mean():.4f}")
    print(f"  ç¼–ç èŒƒæ•°æ ‡å‡†å·®: {torch.norm(train_region, dim=1).std():.4f}")
    
    print(f"\\nå¤–æ¨åŒºåŸŸç»Ÿè®¡:")
    print(f"  ç¼–ç èŒƒæ•°å‡å€¼: {torch.norm(extrapolation_region, dim=1).mean():.4f}")
    print(f"  ç¼–ç èŒƒæ•°æ ‡å‡†å·®: {torch.norm(extrapolation_region, dim=1).std():.4f}")
    
    # åˆ†æç›¸é‚»ä½ç½®ç›¸ä¼¼åº¦çš„è¿ç»­æ€§
    similarities_train = []
    for i in range(trained_max_len - 1):
        sim = F.cosine_similarity(pe_extended[i], pe_extended[i+1], dim=0)
        similarities_train.append(sim.item())
    
    similarities_extrap = []
    for i in range(trained_max_len, test_max_len - 1):
        sim = F.cosine_similarity(pe_extended[i], pe_extended[i+1], dim=0)
        similarities_extrap.append(sim.item())
    
    # åœ¨è¾¹ç•Œå¤„çš„è¿ç»­æ€§
    boundary_sim = F.cosine_similarity(
        pe_extended[trained_max_len-1], 
        pe_extended[trained_max_len], 
        dim=0
    )
    
    print(f"\\nè¿ç»­æ€§åˆ†æ:")
    print(f"  è®­ç»ƒåŒºåŸŸå¹³å‡ç›¸é‚»ç›¸ä¼¼åº¦: {sum(similarities_train)/len(similarities_train):.4f}")
    print(f"  å¤–æ¨åŒºåŸŸå¹³å‡ç›¸é‚»ç›¸ä¼¼åº¦: {sum(similarities_extrap)/len(similarities_extrap):.4f}")
    print(f"  è¾¹ç•Œå¤„ç›¸é‚»ç›¸ä¼¼åº¦: {boundary_sim:.4f}")
    
    # å¯è§†åŒ–å¤–æ¨æ•ˆæœ
    import matplotlib.pyplot as plt
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # ç¼–ç å€¼éšä½ç½®çš„å˜åŒ–ï¼ˆé€‰æ‹©å‡ ä¸ªç»´åº¦ï¼‰
    selected_dims = [0, 10, 50, 100]
    colors = ['red', 'blue', 'green', 'orange']
    
    for i, dim in enumerate(selected_dims):
        if dim < d_model:
            axes[0, 0].plot(pe_extended[:, dim], color=colors[i], 
                          label=f'ç»´åº¦ {dim}', alpha=0.7)
    
    axes[0, 0].axvline(x=trained_max_len, color='black', linestyle='--', 
                      label='è®­ç»ƒè¾¹ç•Œ')
    axes[0, 0].set_xlabel('ä½ç½®')
    axes[0, 0].set_ylabel('ç¼–ç å€¼')
    axes[0, 0].set_title('ä½ç½®ç¼–ç å¤–æ¨')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # ç›¸é‚»ç›¸ä¼¼åº¦
    all_similarities = similarities_train + similarities_extrap
    axes[0, 1].plot(all_similarities, 'b-', alpha=0.7)
    axes[0, 1].axvline(x=len(similarities_train), color='red', 
                      linestyle='--', label='è®­ç»ƒè¾¹ç•Œ')
    axes[0, 1].set_xlabel('ä½ç½®')
    axes[0, 1].set_ylabel('ç›¸é‚»ä½ç½®ç›¸ä¼¼åº¦')
    axes[0, 1].set_title('ç›¸é‚»ä½ç½®ç›¸ä¼¼åº¦è¿ç»­æ€§')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    # ä½ç½®ç¼–ç èŒƒæ•°
    norms = torch.norm(pe_extended, dim=1)
    axes[1, 0].plot(norms, 'g-', alpha=0.7)
    axes[1, 0].axvline(x=trained_max_len, color='red', linestyle='--', 
                      label='è®­ç»ƒè¾¹ç•Œ')
    axes[1, 0].set_xlabel('ä½ç½®')
    axes[1, 0].set_ylabel('ç¼–ç èŒƒæ•°')
    axes[1, 0].set_title('ç¼–ç èŒƒæ•°ç¨³å®šæ€§')
    axes[1, 0].legend()
    axes[1, 0].grid(True)
    
    # ä¸è®­ç»ƒåŒºåŸŸçš„æœ€å°è·ç¦»
    min_distances = []
    for i in range(trained_max_len, test_max_len):
        distances = [torch.norm(pe_extended[i] - pe_extended[j]) 
                    for j in range(trained_max_len)]
        min_distances.append(min(distances).item())
    
    axes[1, 1].plot(range(trained_max_len, test_max_len), min_distances, 'purple', alpha=0.7)
    axes[1, 1].set_xlabel('å¤–æ¨ä½ç½®')
    axes[1, 1].set_ylabel('ä¸è®­ç»ƒåŒºåŸŸæœ€å°è·ç¦»')
    axes[1, 1].set_title('å¤–æ¨ä½ç½®çš„æ–°é¢–æ€§')
    axes[1, 1].grid(True)
    
    plt.tight_layout()
    plt.show()
    
    return pe_extended, similarities_train, similarities_extrap
```

### ä½ç½®ç¼–ç çš„å±€é™æ€§åˆ†æ

```python
def analyze_positional_encoding_limitations(d_model=512):
    """åˆ†æä½ç½®ç¼–ç çš„å±€é™æ€§å’Œæ”¹è¿›æ–¹å‘"""
    
    print("=== ä½ç½®ç¼–ç å±€é™æ€§åˆ†æ ===")
    
    # 1. ç»´åº¦åˆ©ç”¨æ•ˆç‡
    print("\\n1. ç»´åº¦åˆ©ç”¨æ•ˆç‡:")
    
    # è®¡ç®—æœ‰æ•ˆä¿¡æ¯ç»´åº¦
    max_len = 10000
    pe = torch.zeros(max_len, d_model)
    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
    
    div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                       (-math.log(10000.0) / d_model))
    
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    
    # è®¡ç®—ä½ç½®ç¼–ç çŸ©é˜µçš„æœ‰æ•ˆç§©
    U, S, V = torch.svd(pe)
    
    # åˆ†æå¥‡å¼‚å€¼åˆ†å¸ƒ
    cumulative_energy = torch.cumsum(S**2, dim=0) / torch.sum(S**2)
    
    # 99%èƒ½é‡å¯¹åº”çš„ç»´åº¦
    energy_99_idx = torch.where(cumulative_energy >= 0.99)[0][0]
    energy_95_idx = torch.where(cumulative_energy >= 0.95)[0][0]
    
    print(f"  95%èƒ½é‡ç»´åº¦: {energy_95_idx}/{d_model} ({energy_95_idx/d_model*100:.1f}%)")
    print(f"  99%èƒ½é‡ç»´åº¦: {energy_99_idx}/{d_model} ({energy_99_idx/d_model*100:.1f}%)")
    print(f"  æœ‰æ•ˆç§©: {torch.sum(S > S.max() * 1e-10)}/{d_model}")
    
    # 2. é¢‘ç‡åˆ†è¾¨ç‡åˆ†æ
    print("\\n2. é¢‘ç‡åˆ†è¾¨ç‡åˆ†æ:")
    
    frequencies = [1.0 / (10000 ** (2 * i / d_model)) for i in range(d_model // 2)]
    
    # è®¡ç®—é¢‘ç‡é—´éš”
    freq_ratios = [frequencies[i+1] / frequencies[i] for i in range(len(frequencies)-1)]
    avg_ratio = sum(freq_ratios) / len(freq_ratios)
    
    print(f"  é¢‘ç‡èŒƒå›´: {frequencies[0]:.6f} - {frequencies[-1]:.8f}")
    print(f"  å¹³å‡é¢‘ç‡æ¯”å€¼: {avg_ratio:.4f}")
    print(f"  é¢‘ç‡åˆ†å¸ƒ: å‡ ä½•çº§æ•° (æŒ‡æ•°è¡°å‡)")
    
    # 3. é•¿è·ç¦»å»ºæ¨¡èƒ½åŠ›
    print("\\n3. é•¿è·ç¦»å»ºæ¨¡èƒ½åŠ›:")
    
    # è®¡ç®—ä¸åŒè·ç¦»ä¸‹çš„ä½ç½®åŒºåˆ†åº¦
    test_positions = list(range(0, max_len, max_len//20))
    
    for distance in [1, 10, 100, 1000, 5000]:
        similarities = []
        
        for pos in test_positions:
            if pos + distance < max_len:
                sim = F.cosine_similarity(pe[pos], pe[pos + distance], dim=0)
                similarities.append(sim.item())
        
        if similarities:
            avg_sim = sum(similarities) / len(similarities)
            print(f"  è·ç¦» {distance}: å¹³å‡ç›¸ä¼¼åº¦ {avg_sim:.4f}")
    
    # 4. æ”¹è¿›æ–¹å‘å»ºè®®
    print("\\n4. æ”¹è¿›æ–¹å‘:")
    print("  - ç›¸å¯¹ä½ç½®ç¼–ç  (Relative Positional Encoding)")
    print("  - å¯å­¦ä¹ ä½ç½®ç¼–ç  (Learned Positional Embedding)")
    print("  - RoPE (Rotary Position Embedding)")
    print("  - ALiBi (Attention with Linear Biases)")
    
    return S, frequencies
```

## 3.5 å®è·µï¼šMiniGPTä¸­çš„ä½ç½®ç¼–ç ä¼˜åŒ–

### ä½ç½®ç¼–ç çš„é«˜æ•ˆå®ç°

```python
class OptimizedPositionalEncoding(nn.Module):
    """ä¼˜åŒ–çš„ä½ç½®ç¼–ç å®ç°ï¼ŒåŒ…å«åˆ†æåŠŸèƒ½"""
    
    def __init__(self, d_model, max_len=5000, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.max_len = max_len
        self.dropout = nn.Dropout(dropout)
        
        # é¢„è®¡ç®—ä½ç½®ç¼–ç 
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        
        # ä½¿ç”¨å¯¹æ•°ç©ºé—´è®¡ç®—ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        # æ³¨å†Œä¸ºbufferï¼Œä¸å‚ä¸æ¢¯åº¦è®¡ç®—
        pe = pe.unsqueeze(0).transpose(0, 1)  # (max_len, 1, d_model)
        self.register_buffer('pe', pe)
        
        # ç¼“å­˜åˆ†æç”¨çš„æ•°æ®
        self.last_input_length = None
        self.position_usage_stats = torch.zeros(max_len)
    
    def forward(self, x):
        """
        æ·»åŠ ä½ç½®ç¼–ç åˆ°è¾“å…¥åµŒå…¥
        
        Args:
            x: (batch_size, seq_len, d_model)
        Returns:
            x + PE: (batch_size, seq_len, d_model)
        """
        seq_len = x.size(1)
        self.last_input_length = seq_len
        
        # æ›´æ–°ä½ç½®ä½¿ç”¨ç»Ÿè®¡
        self.position_usage_stats[:seq_len] += 1
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pe[:seq_len, :].transpose(0, 1)
        return self.dropout(x)
    
    def get_position_encoding(self, seq_len):
        """è·å–æŒ‡å®šé•¿åº¦çš„ä½ç½®ç¼–ç """
        return self.pe[:seq_len, 0, :]  # (seq_len, d_model)
    
    def analyze_encoding_properties(self, seq_len=None):
        """åˆ†æä½ç½®ç¼–ç çš„å±æ€§"""
        if seq_len is None:
            seq_len = self.last_input_length or min(100, self.max_len)
        
        pe_matrix = self.get_position_encoding(seq_len)
        
        print(f"=== ä½ç½®ç¼–ç åˆ†æ (é•¿åº¦: {seq_len}) ===")
        
        # 1. åŸºæœ¬ç»Ÿè®¡
        print(f"ç¼–ç ç»´åº¦: {self.d_model}")
        print(f"ç¼–ç èŒƒå›´: [{pe_matrix.min():.4f}, {pe_matrix.max():.4f}]")
        print(f"ç¼–ç å‡å€¼: {pe_matrix.mean():.6f}")
        print(f"ç¼–ç æ ‡å‡†å·®: {pe_matrix.std():.4f}")
        
        # 2. ç›¸é‚»ä½ç½®ç›¸ä¼¼åº¦
        if seq_len > 1:
            similarities = []
            for i in range(seq_len - 1):
                sim = F.cosine_similarity(pe_matrix[i], pe_matrix[i+1], dim=0)
                similarities.append(sim.item())
            
            avg_similarity = sum(similarities) / len(similarities)
            print(f"ç›¸é‚»ä½ç½®å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f}")
        
        # 3. ä½ç½®åŒºåˆ†èƒ½åŠ›
        if seq_len > 10:
            # éšæœºé‡‡æ ·ä½ç½®å¯¹è®¡ç®—ç›¸ä¼¼åº¦åˆ†å¸ƒ
            import random
            sample_pairs = random.sample([(i, j) for i in range(seq_len) 
                                        for j in range(i+1, seq_len)], 
                                       min(100, seq_len*(seq_len-1)//2))
            
            similarities_dist = []
            distances = []
            
            for i, j in sample_pairs:
                sim = F.cosine_similarity(pe_matrix[i], pe_matrix[j], dim=0)
                similarities_dist.append(sim.item())
                distances.append(j - i)
            
            # æŒ‰è·ç¦»åˆ†ç»„
            distance_groups = {}
            for sim, dist in zip(similarities_dist, distances):
                if dist not in distance_groups:
                    distance_groups[dist] = []
                distance_groups[dist].append(sim)
            
            print("\\nè·ç¦»-ç›¸ä¼¼åº¦å…³ç³»:")
            for dist in sorted(distance_groups.keys())[:10]:  # å‰10ä¸ªè·ç¦»
                avg_sim = sum(distance_groups[dist]) / len(distance_groups[dist])
                print(f"  è·ç¦» {dist}: å¹³å‡ç›¸ä¼¼åº¦ {avg_sim:.4f}")
        
        return pe_matrix
    
    def visualize_encoding_heatmap(self, seq_len=50, save_path=None):
        """å¯è§†åŒ–ä½ç½®ç¼–ç çƒ­åŠ›å›¾"""
        pe_matrix = self.get_position_encoding(seq_len)
        
        import matplotlib.pyplot as plt
        
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        
        # 1. å®Œæ•´ç¼–ç çƒ­åŠ›å›¾
        im1 = axes[0].imshow(pe_matrix.T, cmap='RdBu', aspect='auto')
        axes[0].set_title('ä½ç½®ç¼–ç çƒ­åŠ›å›¾')
        axes[0].set_xlabel('ä½ç½®')
        axes[0].set_ylabel('ç¼–ç ç»´åº¦')
        plt.colorbar(im1, ax=axes[0])
        
        # 2. å‰32ç»´çš„è¯¦ç»†è§†å›¾
        im2 = axes[1].imshow(pe_matrix[:, :32].T, cmap='RdBu', aspect='auto')
        axes[1].set_title('å‰32ç»´ç¼–ç ï¼ˆé«˜é¢‘åˆ†é‡ï¼‰')
        axes[1].set_xlabel('ä½ç½®')
        axes[1].set_ylabel('ç¼–ç ç»´åº¦')
        plt.colorbar(im2, ax=axes[1])
        
        # 3. ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.zeros(seq_len, seq_len)
        for i in range(seq_len):
            for j in range(seq_len):
                similarity_matrix[i, j] = F.cosine_similarity(
                    pe_matrix[i], pe_matrix[j], dim=0
                )
        
        im3 = axes[2].imshow(similarity_matrix, cmap='viridis', aspect='auto')
        axes[2].set_title('ä½ç½®é—´ç›¸ä¼¼åº¦çŸ©é˜µ')
        axes[2].set_xlabel('ä½ç½® j')
        axes[2].set_ylabel('ä½ç½® i')
        plt.colorbar(im3, ax=axes[2])
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"å¯è§†åŒ–ç»“æœä¿å­˜åˆ°: {save_path}")
        
        plt.show()
        
        return similarity_matrix
    
    def compare_with_learned_embedding(self, vocab_size=1000, seq_len=100):
        """ä¸å¯å­¦ä¹ ä½ç½®åµŒå…¥è¿›è¡Œæ¯”è¾ƒ"""
        
        # åˆ›å»ºå¯å­¦ä¹ ä½ç½®åµŒå…¥
        learned_pe = nn.Embedding(seq_len, self.d_model)
        nn.init.normal_(learned_pe.weight, mean=0, std=0.02)
        
        # è·å–ç¼–ç 
        positions = torch.arange(seq_len)
        sinusoidal_encoding = self.get_position_encoding(seq_len)
        learned_encoding = learned_pe(positions)
        
        print("=== æ­£å¼¦ç¼–ç  vs å¯å­¦ä¹ ç¼–ç  ===")
        
        # 1. ç»Ÿè®¡å¯¹æ¯”
        print(f"æ­£å¼¦ç¼–ç  - å‡å€¼: {sinusoidal_encoding.mean():.6f}, "
              f"æ ‡å‡†å·®: {sinusoidal_encoding.std():.4f}")
        print(f"å¯å­¦ä¹ ç¼–ç  - å‡å€¼: {learned_encoding.mean():.6f}, "
              f"æ ‡å‡†å·®: {learned_encoding.std():.4f}")
        
        # 2. ç›¸é‚»ä½ç½®ç›¸ä¼¼åº¦å¯¹æ¯”
        sin_similarities = []
        learned_similarities = []
        
        for i in range(seq_len - 1):
            sin_sim = F.cosine_similarity(sinusoidal_encoding[i], 
                                        sinusoidal_encoding[i+1], dim=0)
            learned_sim = F.cosine_similarity(learned_encoding[i], 
                                            learned_encoding[i+1], dim=0)
            
            sin_similarities.append(sin_sim.item())
            learned_similarities.append(learned_sim.item())
        
        sin_avg = sum(sin_similarities) / len(sin_similarities)
        learned_avg = sum(learned_similarities) / len(learned_similarities)
        
        print(f"\\nç›¸é‚»ä½ç½®å¹³å‡ç›¸ä¼¼åº¦:")
        print(f"  æ­£å¼¦ç¼–ç : {sin_avg:.4f}")
        print(f"  å¯å­¦ä¹ ç¼–ç : {learned_avg:.4f}")
        
        # 3. å¯è§†åŒ–å¯¹æ¯”
        import matplotlib.pyplot as plt
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # ç¼–ç å€¼å¯¹æ¯”
        axes[0, 0].plot(sinusoidal_encoding[:, 0], label='æ­£å¼¦ - ç»´åº¦0', alpha=0.7)
        axes[0, 0].plot(learned_encoding[:, 0].detach(), label='å¯å­¦ä¹  - ç»´åº¦0', alpha=0.7)
        axes[0, 0].set_title('ç»´åº¦0ç¼–ç å€¼å¯¹æ¯”')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        axes[0, 1].plot(sinusoidal_encoding[:, 10], label='æ­£å¼¦ - ç»´åº¦10', alpha=0.7)
        axes[0, 1].plot(learned_encoding[:, 10].detach(), label='å¯å­¦ä¹  - ç»´åº¦10', alpha=0.7)
        axes[0, 1].set_title('ç»´åº¦10ç¼–ç å€¼å¯¹æ¯”')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        # ç›¸ä¼¼åº¦å¯¹æ¯”
        axes[1, 0].plot(sin_similarities, label='æ­£å¼¦ç¼–ç ', alpha=0.7)
        axes[1, 0].plot(learned_similarities, label='å¯å­¦ä¹ ç¼–ç ', alpha=0.7)
        axes[1, 0].set_title('ç›¸é‚»ä½ç½®ç›¸ä¼¼åº¦å¯¹æ¯”')
        axes[1, 0].set_xlabel('ä½ç½®')
        axes[1, 0].set_ylabel('ç›¸ä¼¼åº¦')
        axes[1, 0].legend()
        axes[1, 0].grid(True)
        
        # ç¼–ç èŒƒæ•°å¯¹æ¯”
        sin_norms = torch.norm(sinusoidal_encoding, dim=1)
        learned_norms = torch.norm(learned_encoding, dim=1)
        
        axes[1, 1].plot(sin_norms, label='æ­£å¼¦ç¼–ç ', alpha=0.7)
        axes[1, 1].plot(learned_norms.detach(), label='å¯å­¦ä¹ ç¼–ç ', alpha=0.7)
        axes[1, 1].set_title('ç¼–ç èŒƒæ•°å¯¹æ¯”')
        axes[1, 1].set_xlabel('ä½ç½®')
        axes[1, 1].set_ylabel('L2èŒƒæ•°')
        axes[1, 1].legend()
        axes[1, 1].grid(True)
        
        plt.tight_layout()
        plt.show()
        
        return sinusoidal_encoding, learned_encoding
```

## å°ç»“ä¸æ€è€ƒ

æœ¬èŠ‚æ·±å…¥åˆ†æäº†ä½ç½®ç¼–ç çš„æ•°å­¦æœ¬è´¨å’Œå‡ ä½•åŸç†ï¼š

1. **å‚…é‡Œå¶åŸºç¡€**ï¼šä½ç½®ç¼–ç æœ¬è´¨ä¸Šæ˜¯ä½ç½®çš„å‚…é‡Œå¶çº§æ•°è¡¨ç¤º
2. **é¢‘ç‡è®¾è®¡**ï¼šå‡ ä½•çº§æ•°çš„é¢‘ç‡åˆ†å¸ƒæ•æ‰å¤šå°ºåº¦ä½ç½®å…³ç³»  
3. **çº¿æ€§æ€§è´¨**ï¼šç›¸å¯¹ä½ç½®å…³ç³»å¯é€šè¿‡çº¿æ€§å˜æ¢è¡¨è¾¾
4. **å¤–æ¨èƒ½åŠ›**ï¼šè¿ç»­å‡½æ•°çš„æ€§è´¨æ”¯æŒé•¿åºåˆ—å¤–æ¨
5. **å‡ ä½•ç›´è§‰**ï¼šåœ¨é«˜ç»´ç©ºé—´ä¸­å½¢æˆèºæ—‹ç»“æ„çš„ä½ç½®è¡¨ç¤º

**å…³é”®æ´å¯Ÿ**ï¼š
- ä½ç½®ç¼–ç å·§å¦™åœ°å°†**ç¦»æ•£çš„ä½ç½®ä¿¡æ¯**è½¬åŒ–ä¸º**è¿ç»­çš„å‘é‡è¡¨ç¤º**
- **ä¸‰è§’å‡½æ•°çš„å‘¨æœŸæ€§**å’Œ**çº¿æ€§å˜æ¢æ€§è´¨**æ˜¯å…¶æ ¸å¿ƒæ•°å­¦åŸºç¡€
- **å¤šé¢‘ç‡åˆ†é‡**ä½¿æ¨¡å‹èƒ½å¤ŸåŒæ—¶å¤„ç†**å±€éƒ¨å’Œå…¨å±€**çš„ä½ç½®å…³ç³»
- **ç¡®å®šæ€§è®¾è®¡**ä¿è¯äº†ä½ç½®è¡¨ç¤ºçš„**ä¸€è‡´æ€§å’Œå¯è§£é‡Šæ€§**

**æ€è€ƒé¢˜**ï¼š
1. ä¸ºä»€ä¹ˆä½¿ç”¨ä¸‰è§’å‡½æ•°è€Œä¸æ˜¯å…¶ä»–å‘¨æœŸå‡½æ•°ï¼Ÿ
2. ä½ç½®ç¼–ç çš„é¢‘ç‡è®¾è®¡æ˜¯å¦æ˜¯æœ€ä¼˜çš„ï¼Ÿ
3. ç›¸å¯¹ä½ç½®ç¼–ç ç›¸æ¯”ç»å¯¹ä½ç½®ç¼–ç æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ
4. å¦‚ä½•è®¾è®¡æ›´å¥½çš„ä½ç½®ç¼–ç æ¥å¤„ç†äºŒç»´æˆ–æ›´é«˜ç»´çš„ä½ç½®ä¿¡æ¯ï¼Ÿ

**ä¸‹ä¸€èŠ‚é¢„å‘Š**ï¼šæˆ‘ä»¬å°†å­¦ä¹ æ®‹å·®è¿æ¥ä¸å±‚å½’ä¸€åŒ–ï¼Œç†è§£æ·±å±‚ç½‘ç»œè®­ç»ƒç¨³å®šæ€§çš„æ•°å­¦æœºåˆ¶ã€‚

---

*ä½ç½®ç¼–ç çš„æ•°å­¦ä¹‹ç¾åœ¨äºç”¨ç®€æ´çš„ä¸‰è§’å‡½æ•°å…¬å¼è§£å†³äº†åºåˆ—å»ºæ¨¡çš„æ ¹æœ¬æŒ‘æˆ˜ï¼Œè¿™æ­£ä½“ç°äº†æ•°å­¦åœ¨äººå·¥æ™ºèƒ½ä¸­çš„ä¼˜é›…åŠ›é‡ã€‚* ğŸ“