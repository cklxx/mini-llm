# 01 æ³¨æ„åŠ›æœºåˆ¶æ•°å­¦åŸç†

> **æ¨¡å‹çš„"çœ¼ç›"ï¼šæ•™æœºå™¨å­¦ä¼š"çœ‹é‡ç‚¹"**

## æ ¸å¿ƒæ€æƒ³

æ³¨æ„åŠ›æœºåˆ¶å¬èµ·æ¥å¾ˆç„ä¹ï¼Œå…¶å®å°±æ˜¯åœ¨æ¨¡æ‹Ÿäººç±»çœ‹æ–‡ç« æ—¶çš„è¡Œä¸ºâ€”â€”è¯»åˆ°æŸä¸ªè¯çš„æ—¶å€™ï¼Œä¼šè‡ªåŠ¨å›å¤´çœ‹çœ‹å‰é¢å“ªäº›è¯è·Ÿå®ƒæœ‰å…³ç³»ã€‚æ¯”å¦‚çœ‹åˆ°"ä»–"çš„æ—¶å€™ï¼Œä¼šè”æƒ³åˆ°å‰é¢æåˆ°çš„"å°æ˜"ã€‚

ä¼ ç»Ÿçš„RNNå°±åƒæ˜¯åªæœ‰ä¸€ä¸ªå°çº¸æ¡ä¼ è¯ï¼Œä¿¡æ¯ä¼ ç€ä¼ ç€å°±ä¸¢äº†ã€‚æ³¨æ„åŠ›æœºåˆ¶ä¸ä¸€æ ·ï¼Œå®ƒè®©æ¯ä¸ªè¯éƒ½èƒ½"å›å¤´çœ‹"æ•´ä¸ªå¥å­ï¼Œç›´æ¥æ‰¾åˆ°è·Ÿè‡ªå·±æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆTransformerèƒ½å¤Ÿå¤„ç†æ›´é•¿çš„åºåˆ—ï¼Œç†è§£æ›´å¤æ‚çš„è¯­è¨€å…³ç³»ã€‚

**å…³é”®æ´å¯Ÿ**ï¼š
- æ³¨æ„åŠ›æ˜¯ä¸€ç§**åŠ æƒå¹³å‡**æœºåˆ¶ï¼Œæƒé‡ç”±æŸ¥è¯¢å’Œé”®çš„ç›¸ä¼¼åº¦å†³å®š
- ç¼©æ”¾å› å­$\sqrt{d_k}$ç¡®ä¿äº†æ¢¯åº¦çš„ç¨³å®šæ€§
- Softmaxå½’ä¸€åŒ–ä¿è¯äº†æƒé‡çš„æ¦‚ç‡æ€§è´¨
- æ©ç æœºåˆ¶å®ç°äº†ä¸åŒçš„æ³¨æ„åŠ›æ¨¡å¼ï¼ˆå› æœã€åŒå‘ç­‰ï¼‰

## 1.1 æ³¨æ„åŠ›çš„ä¿¡æ¯è®ºåŸºç¡€

### ä¿¡æ¯é€‰æ‹©çš„æ•°å­¦å»ºæ¨¡

ä»ä¿¡æ¯è®ºçš„è§’åº¦çœ‹ï¼Œæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥ç†è§£ä¸ºä¸€ä¸ª**ä¿¡æ¯é€‰æ‹©å’Œèšåˆè¿‡ç¨‹**ã€‚ç»™å®šæŸ¥è¯¢$q$ï¼Œæˆ‘ä»¬éœ€è¦ä»ä¸€ç»„é”®å€¼å¯¹$\{(k_i, v_i)\}_{i=1}^n$ä¸­é€‰æ‹©æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚

è®¾æŸ¥è¯¢$q$ä¸é”®$k_i$çš„ç›¸å…³æ€§ä¸º$s_i = f(q, k_i)$ï¼Œåˆ™æ³¨æ„åŠ›æƒé‡å¯ä»¥è¡¨ç¤ºä¸ºï¼š

$$\alpha_i = \frac{\exp(s_i)}{\sum_{j=1}^{n} \exp(s_j)}$$

è¿™æ˜¯ä¸€ä¸ª**Boltzmannåˆ†å¸ƒ**ï¼Œå…¶ä¸­$s_i$å¯ä»¥ç†è§£ä¸º"èƒ½é‡"ï¼Œ$\alpha_i$æ˜¯å¯¹åº”çš„æ¦‚ç‡ã€‚

**ä»£ç å®ç°åˆ†æ**ï¼š
```python
# MiniGPTä¸­çš„æ ¸å¿ƒæ³¨æ„åŠ›è®¡ç®— (src/model/transformer.py:81-102)
def scaled_dot_product_attention(self, Q, K, V, mask=None):
    """ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›çš„æ•°å­¦å®ç°"""
    # 1. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°: Q @ K^T / âˆšd_k
    scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale
    # å½¢çŠ¶: (batch_size, n_heads, seq_len, seq_len)
    
    # 2. åº”ç”¨æ©ç ï¼ˆå¦‚æœæœ‰ï¼‰
    if mask is not None:
        # æ‰©å±•maskç»´åº¦ä»¥åŒ¹é…scores
        if mask.dim() == 3:  # (batch_size, seq_len, seq_len)
            mask = mask.unsqueeze(1)  # (batch_size, 1, seq_len, seq_len)
        scores = scores.masked_fill(mask == 0, -1e9)
    
    # 3. è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼šBoltzmannåˆ†å¸ƒ
    attention_weights = F.softmax(scores, dim=-1)
    attention_weights = self.dropout(attention_weights)
    
    # 4. åº”ç”¨æ³¨æ„åŠ›æƒé‡åˆ°å€¼ï¼šåŠ æƒå¹³å‡
    output = torch.matmul(attention_weights, V)
    
    return output
```

### äº’ä¿¡æ¯ä¸æ³¨æ„åŠ›æƒé‡

ä»ä¿¡æ¯è®ºè§’åº¦ï¼Œæ³¨æ„åŠ›æƒé‡$\alpha_{ij}$å¯ä»¥ç†è§£ä¸ºä½ç½®$i$ä¸ä½ç½®$j$ä¹‹é—´çš„**äº’ä¿¡æ¯**çš„æŸç§è¿‘ä¼¼ï¼š

$$\alpha_{ij} \propto \exp(I(x_i; x_j))$$

å…¶ä¸­$I(x_i; x_j)$æ˜¯äº’ä¿¡æ¯ã€‚è¿™æ„å‘³ç€æ³¨æ„åŠ›æœºåˆ¶å€¾å‘äºå…³æ³¨é‚£äº›ä¸å½“å‰æŸ¥è¯¢ä¿¡æ¯ç›¸å…³æ€§æœ€é«˜çš„ä½ç½®ã€‚

**ä¿¡æ¯ç†µçš„è§†è§’**ï¼š
```python
def analyze_attention_information(attention_weights):
    """åˆ†ææ³¨æ„åŠ›æƒé‡çš„ä¿¡æ¯è®ºç‰¹æ€§"""
    # attention_weights: (batch_size, n_heads, seq_len, seq_len)
    
    # 1. è®¡ç®—æ¯ä¸ªæŸ¥è¯¢ä½ç½®çš„æ³¨æ„åŠ›ç†µ
    attention_entropy = -(attention_weights * torch.log(attention_weights + 1e-8)).sum(dim=-1)
    
    # 2. ç†µçš„è§£é‡Šï¼š
    # - é«˜ç†µï¼šæ³¨æ„åŠ›åˆ†æ•£ï¼Œå…³æ³¨å¤šä¸ªä½ç½®
    # - ä½ç†µï¼šæ³¨æ„åŠ›é›†ä¸­ï¼Œå…³æ³¨å°‘æ•°ä½ç½®
    
    print(f"å¹³å‡æ³¨æ„åŠ›ç†µ: {attention_entropy.mean():.4f}")
    print(f"æ³¨æ„åŠ›ç†µæ ‡å‡†å·®: {attention_entropy.std():.4f}")
    
    # 3. æœ‰æ•ˆæ³¨æ„åŠ›èŒƒå›´ï¼ˆç†µçš„æŒ‡æ•°ï¼‰
    effective_range = torch.exp(attention_entropy)
    print(f"å¹³å‡æœ‰æ•ˆæ³¨æ„åŠ›èŒƒå›´: {effective_range.mean():.2f} ä¸ªä½ç½®")
    
    return attention_entropy
```

## 1.2 ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›çš„å‡ ä½•è§£é‡Š

### å‘é‡ç©ºé—´ä¸­çš„ç›¸ä¼¼åº¦è®¡ç®—

ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›çš„æ ¸å¿ƒå…¬å¼ä¸ºï¼š

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

**å‡ ä½•åˆ†æ**ï¼š

1. **å†…ç§¯è®¡ç®—**ï¼š$QK^T$è®¡ç®—æŸ¥è¯¢å‘é‡ä¸é”®å‘é‡çš„å†…ç§¯ï¼Œè¡¡é‡å®ƒä»¬çš„ç›¸ä¼¼åº¦
2. **å‡ ä½•æ„ä¹‰**ï¼š$q_i^T k_j = \|q_i\| \|k_j\| \cos\theta_{ij}$ï¼Œå…¶ä¸­$\theta_{ij}$æ˜¯ä¸¤å‘é‡çš„å¤¹è§’
3. **ç›¸ä¼¼åº¦åº¦é‡**ï¼šå¤¹è§’è¶Šå°ï¼Œå†…ç§¯è¶Šå¤§ï¼Œæ³¨æ„åŠ›æƒé‡è¶Šå¤§

```python
def geometric_analysis_attention(Q, K):
    """æ³¨æ„åŠ›æœºåˆ¶çš„å‡ ä½•åˆ†æ"""
    batch_size, seq_len, d_k = Q.shape
    
    # 1. è®¡ç®—å‘é‡æ¨¡é•¿
    Q_norm = torch.norm(Q, dim=-1)  # (batch_size, seq_len)
    K_norm = torch.norm(K, dim=-1)  # (batch_size, seq_len)
    
    # 2. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    Q_normalized = F.normalize(Q, dim=-1)
    K_normalized = F.normalize(K, dim=-1)
    cosine_sim = torch.matmul(Q_normalized, K_normalized.transpose(-2, -1))
    
    # 3. åˆ†æè§’åº¦åˆ†å¸ƒ
    angles = torch.acos(torch.clamp(cosine_sim, -1+1e-7, 1-1e-7))
    angles_deg = angles * 180 / math.pi
    
    print(f"æŸ¥è¯¢å‘é‡å¹³å‡æ¨¡é•¿: {Q_norm.mean():.4f}")
    print(f"é”®å‘é‡å¹³å‡æ¨¡é•¿: {K_norm.mean():.4f}")
    print(f"å¹³å‡å¤¹è§’: {angles_deg.mean():.2f}Â°")
    print(f"å¤¹è§’æ ‡å‡†å·®: {angles_deg.std():.2f}Â°")
    
    # 4. æ³¨æ„åŠ›åˆ†æ•°çš„åˆ†è§£
    raw_scores = torch.matmul(Q, K.transpose(-2, -1))
    magnitude_effect = Q_norm.unsqueeze(-1) * K_norm.unsqueeze(-2)
    
    return {
        'cosine_similarity': cosine_sim,
        'angles': angles_deg,
        'magnitude_effect': magnitude_effect,
        'raw_scores': raw_scores
    }
```

### ç¼©æ”¾å› å­çš„æ•°å­¦æ¨å¯¼

**ä¸ºä»€ä¹ˆéœ€è¦ç¼©æ”¾å› å­$\sqrt{d_k}$ï¼Ÿ**

å½“$d_k$è¾ƒå¤§æ—¶ï¼Œå†…ç§¯$q^T k$çš„æ–¹å·®ä¼šå¢å¤§ã€‚å‡è®¾$q$å’Œ$k$çš„åˆ†é‡æ˜¯ç‹¬ç«‹çš„éšæœºå˜é‡ï¼Œå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ï¼Œåˆ™ï¼š

$$\text{Var}(q^T k) = \text{Var}\left(\sum_{i=1}^{d_k} q_i k_i\right) = \sum_{i=1}^{d_k} \text{Var}(q_i k_i) = d_k$$

å› æ­¤ï¼Œå†…ç§¯çš„æ ‡å‡†å·®ä¸º$\sqrt{d_k}$ã€‚ä¸ºäº†ä¿æŒsoftmaxè¾“å…¥çš„æ–¹å·®ç¨³å®šï¼Œæˆ‘ä»¬éœ€è¦é™¤ä»¥$\sqrt{d_k}$ã€‚

**æ•°å€¼ç¨³å®šæ€§åˆ†æ**ï¼š
```python
def analyze_scaling_effect(d_k_values, num_samples=1000):
    """åˆ†æç¼©æ”¾å› å­å¯¹æ•°å€¼ç¨³å®šæ€§çš„å½±å“"""
    results = {}
    
    for d_k in d_k_values:
        # ç”ŸæˆéšæœºæŸ¥è¯¢å’Œé”®å‘é‡
        Q = torch.randn(num_samples, d_k)
        K = torch.randn(num_samples, d_k)
        
        # è®¡ç®—æœªç¼©æ”¾çš„å†…ç§¯
        raw_scores = torch.matmul(Q, K.t())
        
        # è®¡ç®—ç¼©æ”¾åçš„å†…ç§¯
        scaled_scores = raw_scores / math.sqrt(d_k)
        
        # åˆ†æç»Ÿè®¡ç‰¹æ€§
        results[d_k] = {
            'raw_mean': raw_scores.mean().item(),
            'raw_std': raw_scores.std().item(),
            'scaled_mean': scaled_scores.mean().item(),
            'scaled_std': scaled_scores.std().item(),
            'raw_max': raw_scores.max().item(),
            'scaled_max': scaled_scores.max().item()
        }
        
        print(f"d_k={d_k}:")
        print(f"  åŸå§‹åˆ†æ•° - å‡å€¼: {results[d_k]['raw_mean']:.4f}, "
              f"æ ‡å‡†å·®: {results[d_k]['raw_std']:.4f}, æœ€å¤§å€¼: {results[d_k]['raw_max']:.4f}")
        print(f"  ç¼©æ”¾åˆ†æ•° - å‡å€¼: {results[d_k]['scaled_mean']:.4f}, "
              f"æ ‡å‡†å·®: {results[d_k]['scaled_std']:.4f}, æœ€å¤§å€¼: {results[d_k]['scaled_max']:.4f}")
        print()
    
    return results
```

### Softmaxçš„æ¦‚ç‡è§£é‡Š

Softmaxå‡½æ•°å°†å®æ•°å‘é‡æ˜ å°„ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼š

$$\text{softmax}(x_i) = \frac{\exp(x_i)}{\sum_{j=1}^{n} \exp(x_j)}$$

**æ€§è´¨åˆ†æ**ï¼š
1. **éè´Ÿæ€§**ï¼š$\text{softmax}(x_i) \geq 0$
2. **å½’ä¸€åŒ–**ï¼š$\sum_{i=1}^{n} \text{softmax}(x_i) = 1$
3. **å•è°ƒæ€§**ï¼š$x_i > x_j \Rightarrow \text{softmax}(x_i) > \text{softmax}(x_j)$
4. **æ¸©åº¦å‚æ•°**ï¼š$\text{softmax}(x_i/T)$ä¸­ï¼Œ$T$æ§åˆ¶åˆ†å¸ƒçš„"å°–é”ç¨‹åº¦"

```python
def softmax_temperature_analysis(scores, temperatures=[0.1, 0.5, 1.0, 2.0, 5.0]):
    """åˆ†ææ¸©åº¦å‚æ•°å¯¹softmaxçš„å½±å“"""
    
    for T in temperatures:
        # åº”ç”¨æ¸©åº¦ç¼©æ”¾
        scaled_scores = scores / T
        probs = F.softmax(scaled_scores, dim=-1)
        
        # è®¡ç®—ç†µï¼ˆåˆ†å¸ƒçš„é›†ä¸­ç¨‹åº¦ï¼‰
        entropy = -(probs * torch.log(probs + 1e-8)).sum(dim=-1).mean()
        
        # è®¡ç®—æœ€å¤§æ¦‚ç‡ï¼ˆåˆ†å¸ƒçš„å³°å€¼ï¼‰
        max_prob = probs.max(dim=-1)[0].mean()
        
        print(f"æ¸©åº¦ T={T}:")
        print(f"  ç†µ: {entropy:.4f} (è¶Šé«˜è¶Šåˆ†æ•£)")
        print(f"  æœ€å¤§æ¦‚ç‡: {max_prob:.4f} (è¶Šé«˜è¶Šé›†ä¸­)")
        print()
```

## 1.3 æ³¨æ„åŠ›çŸ©é˜µçš„æ¦‚ç‡æ€§è´¨

### è¡ŒéšæœºçŸ©é˜µçš„è°±ç†è®º

æ³¨æ„åŠ›æƒé‡çŸ©é˜µ$\mathbf{A} \in \mathbb{R}^{n \times n}$å…·æœ‰ä»¥ä¸‹é‡è¦æ€§è´¨ï¼š

1. **è¡Œéšæœºæ€§**ï¼š$\sum_{j=1}^{n} A_{ij} = 1, \forall i$
2. **éè´Ÿæ€§**ï¼š$A_{ij} \geq 0, \forall i,j$
3. **æœ€å¤§ç‰¹å¾å€¼**ï¼š$\lambda_{\max} = 1$
4. **Perron-Frobeniusæ€§è´¨**ï¼šå¯¹åº”äºç‰¹å¾å€¼1çš„ç‰¹å¾å‘é‡ä¸ºæ­£å‘é‡

**æ•°å­¦åˆ†æ**ï¼š
```python
def analyze_attention_matrix_properties(attention_weights):
    """åˆ†ææ³¨æ„åŠ›çŸ©é˜µçš„æ•°å­¦æ€§è´¨"""
    # attention_weights: (batch_size, n_heads, seq_len, seq_len)
    
    batch_size, n_heads, seq_len, _ = attention_weights.shape
    
    for head in range(min(n_heads, 3)):  # åˆ†æå‰3ä¸ªå¤´
        A = attention_weights[0, head]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç¬¬headä¸ªå¤´
        
        print(f"\\n=== æ³¨æ„åŠ›å¤´ {head+1} ===")
        
        # 1. éªŒè¯è¡Œéšæœºæ€§
        row_sums = A.sum(dim=-1)
        print(f"è¡Œå’Œæ£€æŸ¥ (åº”è¯¥å…¨ä¸º1): æœ€å°å€¼={row_sums.min():.6f}, æœ€å¤§å€¼={row_sums.max():.6f}")
        
        # 2. è®¡ç®—ç‰¹å¾å€¼
        eigenvals, eigenvecs = torch.linalg.eig(A)
        eigenvals_real = eigenvals.real
        
        # 3. æœ€å¤§ç‰¹å¾å€¼åˆ†æ
        max_eigenval = eigenvals_real.max()
        print(f"æœ€å¤§ç‰¹å¾å€¼: {max_eigenval:.6f} (ç†è®ºå€¼: 1.0)")
        
        # 4. æ¡ä»¶æ•°åˆ†æ
        min_eigenval = eigenvals_real[eigenvals_real > 1e-6].min()
        condition_number = max_eigenval / min_eigenval
        print(f"æ¡ä»¶æ•°: {condition_number:.2f}")
        
        # 5. è°±åŠå¾„
        spectral_radius = eigenvals_real.abs().max()
        print(f"è°±åŠå¾„: {spectral_radius:.6f}")
        
        # 6. å¯¹è§’çº¿ä¼˜åŠ¿åº¦ï¼ˆè‡ªæ³¨æ„åŠ›å¼ºåº¦ï¼‰
        diag_strength = torch.diag(A).mean()
        print(f"å¹³å‡å¯¹è§’çº¿å¼ºåº¦: {diag_strength:.4f}")
```

### æ³¨æ„åŠ›æ¨¡å¼çš„åˆ†ç±»

æ ¹æ®æ³¨æ„åŠ›æƒé‡çš„åˆ†å¸ƒç‰¹å¾ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ³¨æ„åŠ›æ¨¡å¼åˆ†ä¸ºå‡ ç±»ï¼š

1. **å±€éƒ¨æ³¨æ„åŠ›**ï¼šæƒé‡é›†ä¸­åœ¨é‚»è¿‘ä½ç½®
2. **å…¨å±€æ³¨æ„åŠ›**ï¼šæƒé‡ç›¸å¯¹å‡åŒ€åˆ†å¸ƒ
3. **ç¨€ç–æ³¨æ„åŠ›**ï¼šæƒé‡é›†ä¸­åœ¨å°‘æ•°å‡ ä¸ªä½ç½®
4. **å¯¹è§’æ³¨æ„åŠ›**ï¼šä¸»è¦å…³æ³¨è‡ªèº«ä½ç½®

```python
def classify_attention_patterns(attention_weights, threshold=0.1):
    """åˆ†ç±»æ³¨æ„åŠ›æ¨¡å¼"""
    batch_size, n_heads, seq_len, _ = attention_weights.shape
    
    patterns = {}
    
    for head in range(n_heads):
        A = attention_weights[:, head].mean(dim=0)  # å¹³å‡æ‰€æœ‰batch
        
        # 1. å±€éƒ¨æ€§åº¦é‡ï¼šç›¸é‚»ä½ç½®çš„æ³¨æ„åŠ›å¼ºåº¦
        locality = 0
        for i in range(seq_len - 1):
            locality += A[i, i+1] + A[i+1, i]
        locality /= (2 * (seq_len - 1))
        
        # 2. å¯¹è§’çº¿å¼ºåº¦
        diag_strength = torch.diag(A).mean()
        
        # 3. ç¨€ç–æ€§ï¼šé«˜äºé˜ˆå€¼çš„æƒé‡æ¯”ä¾‹
        sparsity = (A > threshold).float().mean()
        
        # 4. ç†µï¼ˆåˆ†å¸ƒçš„å‡åŒ€ç¨‹åº¦ï¼‰
        entropy = -(A * torch.log(A + 1e-8)).sum(dim=-1).mean()
        
        # 5. æœ€å¤§æƒé‡
        max_weight = A.max()
        
        patterns[f'head_{head}'] = {
            'locality': locality.item(),
            'diag_strength': diag_strength.item(),
            'sparsity': sparsity.item(),
            'entropy': entropy.item(),
            'max_weight': max_weight.item()
        }
        
        # æ¨¡å¼åˆ†ç±»
        if diag_strength > 0.3:
            pattern_type = "å¯¹è§’ä¸»å¯¼"
        elif locality > 0.2:
            pattern_type = "å±€éƒ¨å…³æ³¨"
        elif entropy > math.log(seq_len) * 0.8:
            pattern_type = "å…¨å±€å…³æ³¨"
        else:
            pattern_type = "ç¨€ç–å…³æ³¨"
        
        print(f"å¤´ {head}: {pattern_type}")
        print(f"  å±€éƒ¨æ€§: {locality:.3f}, å¯¹è§’å¼ºåº¦: {diag_strength:.3f}")
        print(f"  ç¨€ç–æ€§: {sparsity:.3f}, ç†µ: {entropy:.3f}")
    
    return patterns
```

## 1.4 æ©ç æœºåˆ¶çš„æ•°å­¦å»ºæ¨¡

### å› æœæ©ç çš„çŸ©é˜µè¡¨ç¤º

å¯¹äºè‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿ä½ç½®$i$åªèƒ½çœ‹åˆ°ä½ç½®$j \leq i$çš„ä¿¡æ¯ã€‚è¿™é€šè¿‡å› æœæ©ç å®ç°ï¼š

$$M_{ij} = \begin{cases}
0 & \text{if } i < j \\
1 & \text{if } i \geq j
\end{cases}$$

åœ¨è®¡ç®—æ³¨æ„åŠ›æ—¶ï¼Œè¢«æ©ç çš„ä½ç½®ä¼šè¢«è®¾ç½®ä¸º$-\infty$ï¼š

$$\text{scores}_{ij} = \begin{cases}
\frac{q_i^T k_j}{\sqrt{d_k}} & \text{if } M_{ij} = 1 \\
-\infty & \text{if } M_{ij} = 0
\end{cases}$$

**ä»£ç å®ç°**ï¼š
```python
# MiniGPTä¸­çš„å› æœæ©ç å®ç° (src/model/transformer.py:243-249)
def create_causal_mask(self, seq_len: int) -> torch.Tensor:
    """åˆ›å»ºå› æœæ©ç ï¼ˆä¸‹ä¸‰è§’çŸ©é˜µï¼‰
    
    é˜²æ­¢æ¨¡å‹åœ¨é¢„æµ‹æ—¶çœ‹åˆ°æœªæ¥çš„token
    """
    mask = torch.tril(torch.ones(seq_len, seq_len))
    return mask  # 1è¡¨ç¤ºå¯è§ï¼Œ0è¡¨ç¤ºæ©ç 
```

### ä¸åŒæ©ç ç±»å‹çš„æ•°å­¦åˆ†æ

```python
def analyze_mask_effects(seq_len=10):
    """åˆ†æä¸åŒæ©ç ç±»å‹çš„æ•ˆæœ"""
    
    # 1. å› æœæ©ç ï¼ˆä¸‹ä¸‰è§’ï¼‰
    causal_mask = torch.tril(torch.ones(seq_len, seq_len))
    
    # 2. åŒå‘æ©ç ï¼ˆå…¨1çŸ©é˜µï¼‰
    bidirectional_mask = torch.ones(seq_len, seq_len)
    
    # 3. å±€éƒ¨æ©ç ï¼ˆå¸¦çŠ¶çŸ©é˜µï¼‰
    local_mask = torch.zeros(seq_len, seq_len)
    bandwidth = 3
    for i in range(seq_len):
        start = max(0, i - bandwidth)
        end = min(seq_len, i + bandwidth + 1)
        local_mask[i, start:end] = 1
    
    # 4. ç¨€ç–æ©ç ï¼ˆéšæœºé€‰æ‹©ï¼‰
    sparse_mask = torch.bernoulli(torch.ones(seq_len, seq_len) * 0.3)
    
    masks = {
        'Causal': causal_mask,
        'Bidirectional': bidirectional_mask, 
        'Local': local_mask,
        'Sparse': sparse_mask
    }
    
    for name, mask in masks.items():
        # è®¡ç®—æ©ç ç»Ÿè®¡
        total_positions = seq_len * seq_len
        visible_positions = mask.sum().item()
        density = visible_positions / total_positions
        
        # è®¡ç®—æ¯ä¸ªä½ç½®çš„å¯è§èŒƒå›´
        avg_visible = mask.sum(dim=-1).float().mean()
        
        print(f"{name} æ©ç :")
        print(f"  å¯†åº¦: {density:.2%}")
        print(f"  å¹³å‡å¯è§ä½ç½®æ•°: {avg_visible:.1f}")
        print(f"  æœ€å¤§å¯è§ä½ç½®æ•°: {mask.sum(dim=-1).max()}")
        print()
    
    return masks
```

### æ©ç å¯¹ä¿¡æ¯æµçš„å½±å“

```python
def analyze_information_flow_with_mask(attention_weights, mask):
    """åˆ†ææ©ç å¯¹ä¿¡æ¯æµçš„å½±å“"""
    
    # åº”ç”¨æ©ç 
    masked_attention = attention_weights * mask.unsqueeze(0).unsqueeze(1)
    
    # é‡æ–°å½’ä¸€åŒ–ï¼ˆå› ä¸ºæ©ç æ”¹å˜äº†è¡Œå’Œï¼‰
    row_sums = masked_attention.sum(dim=-1, keepdim=True)
    normalized_attention = masked_attention / (row_sums + 1e-8)
    
    # åˆ†æä¿¡æ¯æµç‰¹æ€§
    seq_len = attention_weights.size(-1)
    
    # 1. ä¿¡æ¯ä¼ æ’­è·ç¦»
    distances = torch.arange(seq_len).unsqueeze(0) - torch.arange(seq_len).unsqueeze(1)
    distances = distances.abs().float()
    
    # åŠ æƒå¹³å‡ä¼ æ’­è·ç¦»
    avg_distance = (normalized_attention.mean(dim=(0,1)) * distances).sum() / normalized_attention.mean(dim=(0,1)).sum()
    
    # 2. ä¿¡æ¯é›†ä¸­åº¦
    entropy = -(normalized_attention * torch.log(normalized_attention + 1e-8)).sum(dim=-1)
    avg_entropy = entropy.mean()
    
    # 3. æœ€è¿œä¿¡æ¯ä¼ æ’­
    max_distance = distances[normalized_attention.mean(dim=(0,1)) > 0.01].max()
    
    print(f"ä¿¡æ¯æµåˆ†æ:")
    print(f"  å¹³å‡ä¼ æ’­è·ç¦»: {avg_distance:.2f} ä¸ªä½ç½®")
    print(f"  å¹³å‡æ³¨æ„åŠ›ç†µ: {avg_entropy:.4f}")
    print(f"  æœ€è¿œæœ‰æ•ˆä¼ æ’­: {max_distance:.0f} ä¸ªä½ç½®")
    
    return {
        'avg_distance': avg_distance.item(),
        'avg_entropy': avg_entropy.item(),
        'max_distance': max_distance.item()
    }
```

## 1.5 å®è·µï¼šMiniGPTä¸­çš„æ³¨æ„åŠ›å®ç°

### å®Œæ•´çš„æ³¨æ„åŠ›æ¨¡å—è§£æ

```python
# MiniGPTä¸­çš„å®Œæ•´æ³¨æ„åŠ›å®ç°è§£æ
class ScaledDotProductAttention:
    """ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›çš„è¯¦ç»†å®ç°ä¸åˆ†æ"""
    
    def __init__(self, d_k, dropout=0.1):
        self.d_k = d_k
        self.scale = math.sqrt(d_k)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, Q, K, V, mask=None, return_attention=False):
        """
        å‰å‘ä¼ æ’­withè¯¦ç»†æ³¨é‡Š
        
        Args:
            Q: æŸ¥è¯¢çŸ©é˜µ (batch_size, n_heads, seq_len, d_k)
            K: é”®çŸ©é˜µ (batch_size, n_heads, seq_len, d_k)  
            V: å€¼çŸ©é˜µ (batch_size, n_heads, seq_len, d_v)
            mask: æ³¨æ„åŠ›æ©ç  (seq_len, seq_len)
            
        Returns:
            output: æ³¨æ„åŠ›è¾“å‡º (batch_size, n_heads, seq_len, d_v)
            attention_weights: æ³¨æ„åŠ›æƒé‡ (å¯é€‰)
        """
        
        # 1. è®¡ç®—åŸå§‹æ³¨æ„åŠ›åˆ†æ•°
        # æ•°å­¦å…¬å¼: S = Q @ K^T
        scores = torch.matmul(Q, K.transpose(-2, -1))
        print(f"åŸå§‹åˆ†æ•°èŒƒå›´: [{scores.min():.4f}, {scores.max():.4f}]")
        
        # 2. ç¼©æ”¾å¤„ç†
        # æ•°å­¦å…¬å¼: S_scaled = S / âˆšd_k
        scaled_scores = scores / self.scale
        print(f"ç¼©æ”¾ååˆ†æ•°èŒƒå›´: [{scaled_scores.min():.4f}, {scaled_scores.max():.4f}]")
        
        # 3. åº”ç”¨æ©ç 
        if mask is not None:
            # å°†æ©ç ä½ç½®è®¾ä¸ºè´Ÿæ— ç©·ï¼Œsoftmaxåæ¥è¿‘0
            scaled_scores = scaled_scores.masked_fill(mask == 0, -1e9)
            print(f"æ©ç ååˆ†æ•°èŒƒå›´: [{scaled_scores.min():.4f}, {scaled_scores.max():.4f}]")
        
        # 4. è®¡ç®—æ³¨æ„åŠ›æƒé‡
        # æ•°å­¦å…¬å¼: A = softmax(S_scaled)
        attention_weights = F.softmax(scaled_scores, dim=-1)
        
        # éªŒè¯æ¦‚ç‡æ€§è´¨
        row_sums = attention_weights.sum(dim=-1)
        print(f"è¡Œå’Œæ£€æŸ¥: æœ€å°å€¼={row_sums.min():.6f}, æœ€å¤§å€¼={row_sums.max():.6f}")
        
        # 5. åº”ç”¨dropoutï¼ˆè®­ç»ƒæ—¶ï¼‰
        attention_weights = self.dropout(attention_weights)
        
        # 6. è®¡ç®—åŠ æƒè¾“å‡º
        # æ•°å­¦å…¬å¼: O = A @ V
        output = torch.matmul(attention_weights, V)
        
        if return_attention:
            return output, attention_weights
        return output
```

### æ³¨æ„åŠ›å¯è§†åŒ–å·¥å…·

```python
def visualize_attention_patterns(attention_weights, tokens, save_path=None):
    """å¯è§†åŒ–æ³¨æ„åŠ›æ¨¡å¼"""
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    # attention_weights: (n_heads, seq_len, seq_len)
    n_heads, seq_len, _ = attention_weights.shape
    
    # åˆ›å»ºå­å›¾
    fig, axes = plt.subplots(2, (n_heads + 1) // 2, figsize=(15, 8))
    if n_heads == 1:
        axes = [axes]
    else:
        axes = axes.flatten()
    
    for head in range(n_heads):
        attn_matrix = attention_weights[head].detach().cpu().numpy()
        
        # ç»˜åˆ¶çƒ­åŠ›å›¾
        sns.heatmap(
            attn_matrix,
            xticklabels=tokens if len(tokens) <= 20 else False,
            yticklabels=tokens if len(tokens) <= 20 else False,
            ax=axes[head],
            cmap='Blues',
            cbar=True,
            square=True
        )
        
        axes[head].set_title(f'Head {head + 1}')
        axes[head].set_xlabel('Key Position')
        axes[head].set_ylabel('Query Position')
    
    # éšè—å¤šä½™çš„å­å›¾
    for i in range(n_heads, len(axes)):
        axes[i].set_visible(False)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"æ³¨æ„åŠ›å¯è§†åŒ–å·²ä¿å­˜åˆ°: {save_path}")
    
    plt.show()
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    for head in range(n_heads):
        attn_matrix = attention_weights[head]
        
        # è®¡ç®—å„ç§ç»Ÿè®¡é‡
        diag_strength = torch.diag(attn_matrix).mean()
        entropy = -(attn_matrix * torch.log(attn_matrix + 1e-8)).sum(dim=-1).mean()
        max_weight = attn_matrix.max()
        sparsity = (attn_matrix > 0.1).float().mean()
        
        print(f"\\nHead {head + 1} ç»Ÿè®¡:")
        print(f"  å¯¹è§’çº¿å¼ºåº¦: {diag_strength:.4f}")
        print(f"  å¹³å‡ç†µ: {entropy:.4f}")
        print(f"  æœ€å¤§æƒé‡: {max_weight:.4f}")
        print(f"  ç¨€ç–æ€§ (>0.1): {sparsity:.4f}")
```

## å°ç»“ä¸æ€è€ƒ

æœ¬èŠ‚æ·±å…¥åˆ†æäº†æ³¨æ„åŠ›æœºåˆ¶çš„æ•°å­¦åŸç†ï¼š

1. **ä¿¡æ¯è®ºåŸºç¡€**ï¼šæ³¨æ„åŠ›æ˜¯åŸºäºäº’ä¿¡æ¯çš„ä¿¡æ¯é€‰æ‹©æœºåˆ¶
2. **å‡ ä½•è§£é‡Š**ï¼šç¼©æ”¾ç‚¹ç§¯è®¡ç®—å‘é‡é—´çš„å‡ ä½•ç›¸ä¼¼åº¦
3. **æ¦‚ç‡æ€§è´¨**ï¼šæ³¨æ„åŠ›æƒé‡å½¢æˆè¡ŒéšæœºçŸ©é˜µçš„Boltzmannåˆ†å¸ƒ
4. **æ©ç æœºåˆ¶**ï¼šé€šè¿‡çŸ©é˜µæ“ä½œå®ç°ä¸åŒçš„ä¿¡æ¯æµçº¦æŸ
5. **æ•°å€¼ç¨³å®šæ€§**ï¼šç¼©æ”¾å› å­ç¡®ä¿æ¢¯åº¦ä¼ æ’­çš„ç¨³å®šæ€§

**æ€è€ƒé¢˜**ï¼š
1. ä¸ºä»€ä¹ˆæ³¨æ„åŠ›æœºåˆ¶æ¯”RNNæ›´é€‚åˆå¹¶è¡Œè®¡ç®—ï¼Ÿ
2. å¦‚ä½•ä»ä¿¡æ¯è®ºè§’åº¦ç†è§£ä¸åŒæ³¨æ„åŠ›æ¨¡å¼çš„ä½œç”¨ï¼Ÿ
3. ç¼©æ”¾å› å­çš„é€‰æ‹©æ˜¯å¦è¿˜æœ‰å…¶ä»–çš„æ•°å­¦ä¾æ®ï¼Ÿ
4. æ³¨æ„åŠ›æƒé‡çš„ç¨€ç–æ€§å¯¹æ¨¡å‹æ€§èƒ½æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ

**ä¸‹ä¸€èŠ‚é¢„å‘Š**ï¼šæˆ‘ä»¬å°†å­¦ä¹ å¤šå¤´æ³¨æ„åŠ›çš„å­ç©ºé—´åˆ†è§£ç†è®ºï¼Œç†è§£å¹¶è¡Œå¤„ç†å¦‚ä½•æå‡æ¨¡å‹èƒ½åŠ›ã€‚

---

*æ³¨æ„åŠ›æœºåˆ¶çš„æ•°å­¦ä¹‹ç¾åœ¨äºå®ƒç”¨ç®€å•çš„çº¿æ€§ä»£æ•°æ“ä½œå®ç°äº†å¤æ‚çš„ä¿¡æ¯é€‰æ‹©ï¼Œè¿™æ­£æ˜¯æ·±åº¦å­¦ä¹ ä¸­"ç®€å•è€Œå¼ºå¤§"çš„å®Œç¾ä½“ç°ã€‚* ğŸ¯