# 02 çº¿æ€§ä»£æ•°ä¸çŸ©é˜µè¿ç®—

> **ç†è§£Transformerè®¡ç®—çš„å‡ ä½•æœ¬è´¨**

## æ ¸å¿ƒæ€æƒ³

Transformer çš„æ‰€æœ‰è®¡ç®—éƒ½å¯ä»¥å½’ç»“ä¸ºçº¿æ€§ä»£æ•°è¿ç®—ã€‚ç†è§£è¿™äº›è¿ç®—çš„å‡ ä½•æ„ä¹‰ï¼Œæœ‰åŠ©äºæˆ‘ä»¬æ·±å…¥ç†è§£æ³¨æ„åŠ›æœºåˆ¶ã€å‰é¦ˆç½‘ç»œç­‰æ ¸å¿ƒç»„ä»¶çš„å·¥ä½œåŸç†ã€‚

**å…³é”®æ´å¯Ÿ**ï¼š
- å‘é‡è¡¨ç¤ºè¯­ä¹‰ç©ºé—´ä¸­çš„ç‚¹
- çŸ©é˜µå®ç°ç©ºé—´çš„çº¿æ€§å˜æ¢
- æ³¨æ„åŠ›æ˜¯å‘é‡é—´ç›¸ä¼¼åº¦çš„å‡ ä½•è®¡ç®—
- å¤šå¤´æ³¨æ„åŠ›æ˜¯å­ç©ºé—´çš„å¹¶è¡Œå¤„ç†

## 2.1 å‘é‡ç©ºé—´ä¸è¯­ä¹‰è¡¨ç¤º

### å‘é‡ç©ºé—´çš„åŸºæœ¬æ¦‚å¿µ

ä¸€ä¸ªå‘é‡ç©ºé—´ $V$ æ˜¯ä¸€ä¸ªæ»¡è¶³ç‰¹å®šå…¬ç†çš„é›†åˆï¼Œé…å¤‡ä¸¤ç§è¿ç®—ï¼š
- **å‘é‡åŠ æ³•**ï¼š$\mathbf{u} + \mathbf{v} \in V$
- **æ ‡é‡ä¹˜æ³•**ï¼š$c\mathbf{u} \in V$ï¼Œå…¶ä¸­ $c \in \mathbb{R}$

**åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„æ„ä¹‰**ï¼š
- è¯å‘é‡å­˜åœ¨äº $\mathbb{R}^d$ ç©ºé—´ä¸­
- ç›¸ä¼¼è¯æ±‡åœ¨ç©ºé—´ä¸­è·ç¦»è¾ƒè¿‘
- è¯­ä¹‰å…³ç³»å¯ä»¥ç”¨å‘é‡è¿ç®—è¡¨ç¤º

### åŸºä¸ç»´åº¦

ç»™å®šå‘é‡ç©ºé—´ $V$ï¼Œå¦‚æœå­˜åœ¨çº¿æ€§æ— å…³çš„å‘é‡é›†åˆ $\{\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_n\}$ ä½¿å¾— $V$ ä¸­ä»»æ„å‘é‡éƒ½å¯ä»¥è¡¨ç¤ºä¸ºå®ƒä»¬çš„çº¿æ€§ç»„åˆï¼Œåˆ™ç§°è¿™ä¸ªé›†åˆä¸º $V$ çš„ä¸€ç»„**åŸº**ã€‚

**æ•°å­¦è¡¨è¾¾**ï¼š
$$\mathbf{v} = c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + ... + c_n\mathbf{v}_n$$

**åœ¨Transformerä¸­çš„åº”ç”¨**ï¼š
```python
# è¯åµŒå…¥çŸ©é˜µå®šä¹‰äº†è¯æ±‡ç©ºé—´çš„åŸº
class TokenEmbedding(nn.Module):
    def __init__(self, vocab_size, d_model):
        super().__init__()
        # åµŒå…¥çŸ©é˜µï¼šæ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªåŸºå‘é‡
        self.embedding = nn.Embedding(vocab_size, d_model)
        
    def forward(self, input_ids):
        # å°†ç¦»æ•£tokenæ˜ å°„åˆ°è¿ç»­å‘é‡ç©ºé—´
        return self.embedding(input_ids)
```

### å†…ç§¯ä¸ç›¸ä¼¼åº¦

ä¸¤ä¸ªå‘é‡çš„å†…ç§¯å®šä¹‰ä¸ºï¼š
$$\langle \mathbf{u}, \mathbf{v} \rangle = \mathbf{u}^T \mathbf{v} = \sum_{i=1}^{d} u_i v_i$$

**å‡ ä½•æ„ä¹‰**ï¼š
$$\langle \mathbf{u}, \mathbf{v} \rangle = \|\mathbf{u}\| \|\mathbf{v}\| \cos \theta$$

å…¶ä¸­ $\theta$ æ˜¯ä¸¤å‘é‡é—´çš„å¤¹è§’ã€‚

**æ³¨æ„åŠ›æœºåˆ¶çš„å‡ ä½•è§£é‡Š**ï¼š
```python
def scaled_dot_product_attention(Q, K, V):
    """æ³¨æ„åŠ› = æŸ¥è¯¢ä¸é”®çš„ç›¸ä¼¼åº¦åŠ æƒå€¼"""
    # Q, K, V: (batch_size, seq_len, d_k)
    
    # 1. è®¡ç®—ç›¸ä¼¼åº¦ï¼šå†…ç§¯è¡¡é‡å‘é‡å¤¹è§’
    scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch_size, seq_len, seq_len)
    
    # 2. ç¼©æ”¾ï¼šé˜²æ­¢å†…ç§¯è¿‡å¤§
    scores = scores / math.sqrt(Q.size(-1))
    
    # 3. å½’ä¸€åŒ–ï¼šè½¬ä¸ºæ¦‚ç‡åˆ†å¸ƒ
    attention_weights = F.softmax(scores, dim=-1)
    
    # 4. åŠ æƒæ±‚å’Œï¼šæŒ‰ç›¸ä¼¼åº¦èšåˆä¿¡æ¯
    output = torch.matmul(attention_weights, V)
    
    return output, attention_weights
```

**ä»£ç è§£æ**ï¼šæ³¨æ„åŠ›æƒé‡ $A_{ij}$ çš„å‡ ä½•æ„ä¹‰
```python
def analyze_attention_geometry(Q, K):
    """åˆ†ææ³¨æ„åŠ›çš„å‡ ä½•ç‰¹æ€§"""
    # å½’ä¸€åŒ–æŸ¥è¯¢å’Œé”®å‘é‡
    Q_norm = F.normalize(Q, dim=-1)
    K_norm = F.normalize(K, dim=-1)
    
    # ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
    cosine_sim = torch.matmul(Q_norm, K_norm.transpose(-2, -1))
    
    # æ³¨æ„åŠ›åˆ†æ•°ï¼ˆæœªå½’ä¸€åŒ–ï¼‰
    attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(Q.size(-1))
    
    return cosine_sim, attention_scores
```

## 2.2 çŸ©é˜µåˆ†è§£ä¸ç‰¹å¾å€¼

### ç‰¹å¾å€¼åˆ†è§£

å¯¹äºæ–¹é˜µ $\mathbf{A} \in \mathbb{R}^{n \times n}$ï¼Œå¦‚æœå­˜åœ¨éé›¶å‘é‡ $\mathbf{v}$ å’Œæ ‡é‡ $\lambda$ ä½¿å¾—ï¼š
$$\mathbf{A}\mathbf{v} = \lambda\mathbf{v}$$

åˆ™ç§° $\lambda$ ä¸ºç‰¹å¾å€¼ï¼Œ$\mathbf{v}$ ä¸ºå¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚

**å‡ ä½•æ„ä¹‰**ï¼šç‰¹å¾å‘é‡æ˜¯çŸ©é˜µå˜æ¢ä¸‹æ–¹å‘ä¸å˜çš„å‘é‡ï¼Œç‰¹å¾å€¼æ˜¯ç¼©æ”¾å› å­ã€‚

**è°±åˆ†è§£**ï¼š
å¦‚æœ $\mathbf{A}$ æœ‰ $n$ ä¸ªçº¿æ€§æ— å…³çš„ç‰¹å¾å‘é‡ï¼Œåˆ™å¯ä»¥åˆ†è§£ä¸ºï¼š
$$\mathbf{A} = \mathbf{P}\mathbf{\Lambda}\mathbf{P}^{-1}$$

å…¶ä¸­ $\mathbf{P}$ æ˜¯ç‰¹å¾å‘é‡çŸ©é˜µï¼Œ$\mathbf{\Lambda}$ æ˜¯ç‰¹å¾å€¼å¯¹è§’çŸ©é˜µã€‚

**å®é™…åº”ç”¨**ï¼š
```python
def analyze_weight_matrix(weight_matrix):
    """åˆ†ææƒé‡çŸ©é˜µçš„è°±ç‰¹æ€§"""
    # è®¡ç®—ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
    eigenvals, eigenvecs = torch.linalg.eig(weight_matrix)
    
    # ç‰¹å¾å€¼çš„åˆ†å¸ƒåæ˜ äº†å˜æ¢çš„ä¸»è¦æ–¹å‘
    eigenvals_real = eigenvals.real
    
    # æ¡ä»¶æ•°ï¼šæœ€å¤§ç‰¹å¾å€¼/æœ€å°ç‰¹å¾å€¼
    condition_number = eigenvals_real.max() / eigenvals_real.min()
    
    print(f"ç‰¹å¾å€¼èŒƒå›´: [{eigenvals_real.min():.4f}, {eigenvals_real.max():.4f}]")
    print(f"æ¡ä»¶æ•°: {condition_number:.4f}")
    
    return eigenvals, eigenvecs
```

### å¥‡å¼‚å€¼åˆ†è§£(SVD)

ä»»æ„çŸ©é˜µ $\mathbf{A} \in \mathbb{R}^{m \times n}$ éƒ½å¯ä»¥åˆ†è§£ä¸ºï¼š
$$\mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^T$$

å…¶ä¸­ï¼š
- $\mathbf{U} \in \mathbb{R}^{m \times m}$ï¼šå·¦å¥‡å¼‚å‘é‡çŸ©é˜µ
- $\mathbf{\Sigma} \in \mathbb{R}^{m \times n}$ï¼šå¥‡å¼‚å€¼å¯¹è§’çŸ©é˜µ
- $\mathbf{V} \in \mathbb{R}^{n \times n}$ï¼šå³å¥‡å¼‚å‘é‡çŸ©é˜µ

**å‡ ä½•è§£é‡Š**ï¼šä»»æ„çº¿æ€§å˜æ¢éƒ½å¯ä»¥åˆ†è§£ä¸ºæ—‹è½¬-ç¼©æ”¾-æ—‹è½¬çš„å¤åˆã€‚

**åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨**ï¼š

1. **æƒé‡åˆå§‹åŒ–**ï¼š
```python
def xavier_uniform_init(weight):
    """åŸºäºSVDçš„æƒé‡åˆå§‹åŒ–"""
    fan_in = weight.size(1)
    fan_out = weight.size(0)
    
    # Xavieråˆå§‹åŒ–çš„æ–¹å·®
    std = math.sqrt(2.0 / (fan_in + fan_out))
    
    # ç”ŸæˆéšæœºçŸ©é˜µå¹¶SVDåˆ†è§£
    random_matrix = torch.randn_like(weight)
    U, _, V = torch.svd(random_matrix)
    
    # æ„é€ æ­£äº¤åˆå§‹åŒ–
    if U.shape == weight.shape:
        weight.data = U * std
    else:
        weight.data = V.t() * std
```

2. **ä½ç§©è¿‘ä¼¼**ï¼š
```python
def low_rank_approximation(matrix, rank):
    """ä½¿ç”¨SVDè¿›è¡Œä½ç§©è¿‘ä¼¼"""
    U, S, V = torch.svd(matrix)
    
    # ä¿ç•™å‰kä¸ªå¥‡å¼‚å€¼
    U_k = U[:, :rank]
    S_k = S[:rank]
    V_k = V[:, :rank]
    
    # é‡æ„çŸ©é˜µ
    approximation = U_k @ torch.diag(S_k) @ V_k.t()
    
    return approximation
```

## 2.3 å¤šå¤´æ³¨æ„åŠ›çš„å­ç©ºé—´åˆ†è§£

### å­ç©ºé—´çš„æ¦‚å¿µ

ç»™å®šå‘é‡ç©ºé—´ $V$ï¼Œå…¶**å­ç©ºé—´** $W$ æ˜¯ $V$ çš„ä¸€ä¸ªå­é›†ï¼Œä¸” $W$ æœ¬èº«ä¹Ÿæ„æˆå‘é‡ç©ºé—´ã€‚

**å¤šå¤´æ³¨æ„åŠ›çš„æ•°å­¦è¡¨ç¤º**ï¼š
$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$$

å…¶ä¸­æ¯ä¸ªå¤´ï¼š
$$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

**å­ç©ºé—´åˆ†è§£çš„å‡ ä½•æ„ä¹‰**ï¼š

æ¯ä¸ªæ³¨æ„åŠ›å¤´åœ¨ä¸åŒçš„å­ç©ºé—´ä¸­æ“ä½œï¼š
- $W_i^Q, W_i^K, W_i^V \in \mathbb{R}^{d_{model} \times d_k}$
- $d_k = d_{model} / h$ï¼ˆé€šå¸¸æƒ…å†µï¼‰

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        assert d_model % n_heads == 0
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        # å­ç©ºé—´æŠ•å½±çŸ©é˜µ
        self.w_q = nn.Linear(d_model, d_model)  # æŠ•å½±åˆ°æŸ¥è¯¢å­ç©ºé—´
        self.w_k = nn.Linear(d_model, d_model)  # æŠ•å½±åˆ°é”®å­ç©ºé—´
        self.w_v = nn.Linear(d_model, d_model)  # æŠ•å½±åˆ°å€¼å­ç©ºé—´
        self.w_o = nn.Linear(d_model, d_model)  # è¾“å‡ºæŠ•å½±
    
    def forward(self, query, key, value):
        batch_size = query.size(0)
        
        # 1. çº¿æ€§æŠ•å½±åˆ°å„ä¸ªå­ç©ºé—´
        Q = self.w_q(query)  # (batch_size, seq_len, d_model)
        K = self.w_k(key)
        V = self.w_v(value)
        
        # 2. é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼ï¼šåˆ†å‰²åˆ°ä¸åŒå­ç©ºé—´
        Q = Q.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        # ç°åœ¨ï¼š(batch_size, n_heads, seq_len, d_k)
        
        # 3. åœ¨æ¯ä¸ªå­ç©ºé—´ä¸­è®¡ç®—æ³¨æ„åŠ›
        attention_output = self.scaled_dot_product_attention(Q, K, V)
        
        # 4. æ‹¼æ¥æ‰€æœ‰å­ç©ºé—´çš„ç»“æœ
        attention_output = attention_output.transpose(1, 2).contiguous()
        attention_output = attention_output.view(batch_size, -1, self.d_model)
        
        # 5. æœ€ç»ˆçš„çº¿æ€§å˜æ¢
        output = self.w_o(attention_output)
        
        return output
```

**å­ç©ºé—´ç‹¬ç«‹æ€§åˆ†æ**ï¼š
```python
def analyze_subspace_independence(multi_head_attn):
    """åˆ†æå¤šå¤´æ³¨æ„åŠ›çš„å­ç©ºé—´ç‹¬ç«‹æ€§"""
    # è·å–æŠ•å½±çŸ©é˜µ
    W_q = multi_head_attn.w_q.weight  # (d_model, d_model)
    W_k = multi_head_attn.w_k.weight
    W_v = multi_head_attn.w_v.weight
    
    # é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
    n_heads = multi_head_attn.n_heads
    d_k = multi_head_attn.d_k
    
    W_q_heads = W_q.view(n_heads, d_k, -1)  # (n_heads, d_k, d_model)
    W_k_heads = W_k.view(n_heads, d_k, -1)
    
    # è®¡ç®—ä¸åŒå¤´ä¹‹é—´çš„ç›¸å…³æ€§
    correlations = []
    for i in range(n_heads):
        for j in range(i+1, n_heads):
            # è®¡ç®—æŠ•å½±çŸ©é˜µçš„ä½™å¼¦ç›¸ä¼¼åº¦
            qi_flat = W_q_heads[i].flatten()
            qj_flat = W_q_heads[j].flatten()
            
            correlation = F.cosine_similarity(qi_flat, qj_flat, dim=0)
            correlations.append(correlation.item())
    
    avg_correlation = sum(correlations) / len(correlations)
    print(f"å¹³å‡å¤´é—´ç›¸å…³æ€§: {avg_correlation:.4f}")
    
    return correlations
```

## 2.4 çŸ©é˜µå¾®åˆ†ä¸æ¢¯åº¦è®¡ç®—

### æ ‡é‡å¯¹å‘é‡çš„å¯¼æ•°

è®¾ $f: \mathbb{R}^n \rightarrow \mathbb{R}$ï¼Œåˆ™æ¢¯åº¦å®šä¹‰ä¸ºï¼š
$$\nabla_\mathbf{x} f = \frac{\partial f}{\partial \mathbf{x}} = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix}$$

### æ ‡é‡å¯¹çŸ©é˜µçš„å¯¼æ•°

è®¾ $f: \mathbb{R}^{m \times n} \rightarrow \mathbb{R}$ï¼Œåˆ™ï¼š
$$\frac{\partial f}{\partial \mathbf{A}} = \begin{bmatrix} \frac{\partial f}{\partial A_{11}} & \cdots & \frac{\partial f}{\partial A_{1n}} \\ \vdots & \ddots & \vdots \\ \frac{\partial f}{\partial A_{m1}} & \cdots & \frac{\partial f}{\partial A_{mn}} \end{bmatrix}$$

### é‡è¦çš„çŸ©é˜µå¾®åˆ†å…¬å¼

1. **çº¿æ€§å‡½æ•°**ï¼š$\frac{\partial (\mathbf{a}^T\mathbf{x})}{\partial \mathbf{x}} = \mathbf{a}$

2. **äºŒæ¬¡å‹**ï¼š$\frac{\partial (\mathbf{x}^T\mathbf{A}\mathbf{x})}{\partial \mathbf{x}} = \mathbf{A}\mathbf{x} + \mathbf{A}^T\mathbf{x}$

3. **çŸ©é˜µä¹˜æ³•**ï¼š$\frac{\partial \text{tr}(\mathbf{A}\mathbf{B})}{\partial \mathbf{A}} = \mathbf{B}^T$

**æ³¨æ„åŠ›æœºåˆ¶çš„æ¢¯åº¦æ¨å¯¼**ï¼š

è®¾æ³¨æ„åŠ›åˆ†æ•° $S = QK^T$ï¼Œæ³¨æ„åŠ›æƒé‡ $A = \text{softmax}(S)$ï¼Œè¾“å‡º $O = AV$ã€‚

å¯¹äºæŸå¤±å‡½æ•° $L$ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—ï¼š
$$\frac{\partial L}{\partial Q}, \frac{\partial L}{\partial K}, \frac{\partial L}{\partial V}$$

**åå‘ä¼ æ’­çš„é“¾å¼æ³•åˆ™**ï¼š
```python
def attention_backward(grad_output, Q, K, V, attention_weights):
    """æ³¨æ„åŠ›æœºåˆ¶çš„åå‘ä¼ æ’­"""
    # grad_output: (batch_size, seq_len, d_v)
    # attention_weights: (batch_size, seq_len, seq_len)
    
    # 1. å¯¹Vçš„æ¢¯åº¦
    grad_V = torch.matmul(attention_weights.transpose(-2, -1), grad_output)
    
    # 2. å¯¹attention_weightsçš„æ¢¯åº¦
    grad_attn = torch.matmul(grad_output, V.transpose(-2, -1))
    
    # 3. softmaxçš„åå‘ä¼ æ’­
    grad_scores = attention_weights * (grad_attn - 
        (grad_attn * attention_weights).sum(dim=-1, keepdim=True))
    
    # 4. å¯¹Qå’ŒKçš„æ¢¯åº¦
    d_k = Q.size(-1)
    grad_Q = torch.matmul(grad_scores, K) / math.sqrt(d_k)
    grad_K = torch.matmul(grad_scores.transpose(-2, -1), Q) / math.sqrt(d_k)
    
    return grad_Q, grad_K, grad_V
```

## 2.5 å®è·µï¼šMiniGPTä¸­çš„çº¿æ€§ä»£æ•°è¿ç®—

### æƒé‡çŸ©é˜µçš„å‡ ä½•åˆ†æ

```python
def analyze_transformer_weights(model):
    """åˆ†æTransformeræƒé‡çŸ©é˜µçš„å‡ ä½•ç‰¹æ€§"""
    
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            weight = module.weight.data  # (out_features, in_features)
            
            # 1. å¥‡å¼‚å€¼åˆ†æ
            U, S, V = torch.svd(weight)
            
            # 2. æ¡ä»¶æ•°
            condition_number = S.max() / S.min()
            
            # 3. æœ‰æ•ˆç§©ï¼ˆå¤§äºé˜ˆå€¼çš„å¥‡å¼‚å€¼ä¸ªæ•°ï¼‰
            threshold = 0.01 * S.max()
            effective_rank = (S > threshold).sum().item()
            
            # 4. è°±èŒƒæ•°ï¼ˆæœ€å¤§å¥‡å¼‚å€¼ï¼‰
            spectral_norm = S.max()
            
            print(f"Layer: {name}")
            print(f"  Shape: {weight.shape}")
            print(f"  Condition Number: {condition_number:.2f}")
            print(f"  Effective Rank: {effective_rank}/{min(weight.shape)}")
            print(f"  Spectral Norm: {spectral_norm:.4f}")
            print()
```

### æ³¨æ„åŠ›æƒé‡çš„å‡ ä½•å¯è§†åŒ–

```python
def visualize_attention_geometry(attention_weights, tokens):
    """å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡çš„å‡ ä½•ç»“æ„"""
    # attention_weights: (n_heads, seq_len, seq_len)
    
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    n_heads = attention_weights.size(0)
    
    fig, axes = plt.subplots(2, n_heads//2, figsize=(15, 6))
    axes = axes.flatten()
    
    for head in range(n_heads):
        attn_matrix = attention_weights[head].cpu().numpy()
        
        # ç»˜åˆ¶æ³¨æ„åŠ›çƒ­åŠ›å›¾
        sns.heatmap(attn_matrix, 
                   xticklabels=tokens, 
                   yticklabels=tokens,
                   ax=axes[head],
                   cmap='Blues')
        axes[head].set_title(f'Head {head+1}')
    
    plt.tight_layout()
    plt.show()
    
    # åˆ†ææ³¨æ„åŠ›çš„å‡ ä½•ç‰¹æ€§
    for head in range(n_heads):
        attn_matrix = attention_weights[head]
        
        # 1. å¯¹è§’çº¿å¼ºåº¦ï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰
        diag_strength = torch.diag(attn_matrix).mean()
        
        # 2. å±€éƒ¨æ€§ï¼ˆç›¸é‚»ä½ç½®çš„æ³¨æ„åŠ›å¼ºåº¦ï¼‰
        locality = 0
        for i in range(attn_matrix.size(0)-1):
            locality += attn_matrix[i, i+1] + attn_matrix[i+1, i]
        locality /= (2 * (attn_matrix.size(0) - 1))
        
        # 3. ç†µï¼ˆæ³¨æ„åŠ›åˆ†å¸ƒçš„é›†ä¸­ç¨‹åº¦ï¼‰
        attn_entropy = -(attn_matrix * torch.log(attn_matrix + 1e-8)).sum(dim=-1).mean()
        
        print(f"Head {head+1}:")
        print(f"  Self-attention strength: {diag_strength:.4f}")
        print(f"  Locality: {locality:.4f}")
        print(f"  Attention entropy: {attn_entropy:.4f}")
```

## å°ç»“ä¸æ€è€ƒ

æœ¬èŠ‚ä»‹ç»äº†æ”¯æ’‘Transformerè®¡ç®—çš„çº¿æ€§ä»£æ•°åŸºç¡€ï¼š

1. **å‘é‡ç©ºé—´**ä¸ºè¯­ä¹‰è¡¨ç¤ºæä¾›äº†å‡ ä½•æ¡†æ¶
2. **å†…ç§¯**å®ç°äº†ç›¸ä¼¼åº¦çš„å‡ ä½•è®¡ç®—
3. **çŸ©é˜µåˆ†è§£**æ­ç¤ºäº†æƒé‡çŸ©é˜µçš„å†…åœ¨ç»“æ„
4. **å­ç©ºé—´åˆ†è§£**æ˜¯å¤šå¤´æ³¨æ„åŠ›çš„å‡ ä½•æœ¬è´¨
5. **çŸ©é˜µå¾®åˆ†**æ˜¯åå‘ä¼ æ’­ç®—æ³•çš„æ•°å­¦åŸºç¡€

**æ€è€ƒé¢˜**ï¼š
1. ä¸ºä»€ä¹ˆå¤šå¤´æ³¨æ„åŠ›æ¯”å•å¤´æ³¨æ„åŠ›æ›´æœ‰æ•ˆï¼Ÿä»å­ç©ºé—´çš„è§’åº¦åˆ†æã€‚
2. å¦‚ä½•ä»å¥‡å¼‚å€¼åˆ†è§£çš„è§’åº¦ç†è§£æƒé‡çŸ©é˜µçš„è¡¨è¾¾èƒ½åŠ›ï¼Ÿ
3. æ³¨æ„åŠ›æƒé‡çŸ©é˜µçš„å‡ ä½•æ€§è´¨åæ˜ äº†ä»€ä¹ˆè¯­è¨€å­¦ç°è±¡ï¼Ÿ

**ä¸‹ä¸€èŠ‚é¢„å‘Š**ï¼šæˆ‘ä»¬å°†å­¦ä¹ ä¼˜åŒ–ç†è®ºï¼Œç†è§£è®­ç»ƒç®—æ³•çš„æ•°å­¦åŸç†ã€‚

---

*çº¿æ€§ä»£æ•°ä¸ºæ·±åº¦å­¦ä¹ æä¾›äº†è®¡ç®—çš„è¯­è¨€ï¼Œè€Œå‡ ä½•ç›´è§‰è®©æˆ‘ä»¬ç†è§£äº†è®¡ç®—çš„æ„ä¹‰ã€‚* ğŸ“