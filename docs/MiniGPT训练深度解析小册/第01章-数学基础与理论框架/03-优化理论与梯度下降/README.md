# 03 ä¼˜åŒ–ç†è®ºä¸æ¢¯åº¦ä¸‹é™

> **ç†è§£æ·±åº¦å­¦ä¹ è®­ç»ƒçš„æ•°å­¦æœ¬è´¨**

## æ ¸å¿ƒæ€æƒ³

æ·±åº¦å­¦ä¹ çš„è®­ç»ƒæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªä¼˜åŒ–é—®é¢˜ï¼šåœ¨å‚æ•°ç©ºé—´ä¸­å¯»æ‰¾ä½¿æŸå¤±å‡½æ•°æœ€å°çš„ç‚¹ã€‚ç†è§£ä¼˜åŒ–ç†è®ºï¼Œæœ‰åŠ©äºæˆ‘ä»¬è®¾è®¡æ›´å¥½çš„è®­ç»ƒç®—æ³•ï¼Œè¯Šæ–­è®­ç»ƒé—®é¢˜ï¼Œå¹¶æé«˜æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®æ´å¯Ÿ**ï¼š
- æ¢¯åº¦æŒ‡å‘å‡½æ•°å€¼å¢é•¿æœ€å¿«çš„æ–¹å‘
- å­¦ä¹ ç‡æ§åˆ¶ç€å‚æ•°æ›´æ–°çš„æ­¥é•¿
- è‡ªé€‚åº”ç®—æ³•èƒ½å¤Ÿé€‚åº”ä¸åŒå‚æ•°çš„æ›´æ–°é¢‘ç‡
- ä¼˜åŒ–æ™¯è§‚çš„å‡ ä½•æ€§è´¨å†³å®šäº†è®­ç»ƒçš„éš¾æ˜“ç¨‹åº¦

## 3.1 å‡¸ä¼˜åŒ–ä¸éå‡¸ä¼˜åŒ–

### å‡¸å‡½æ•°çš„å®šä¹‰

å‡½æ•° $f: \mathbb{R}^n \rightarrow \mathbb{R}$ æ˜¯å‡¸å‡½æ•°ï¼Œå½“ä¸”ä»…å½“å¯¹äºä»»æ„ $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$ å’Œ $\lambda \in [0,1]$ï¼š

$$f(\lambda \mathbf{x} + (1-\lambda)\mathbf{y}) \leq \lambda f(\mathbf{x}) + (1-\lambda)f(\mathbf{y})$$

**å‡ ä½•ç›´è§‰**ï¼šå‡½æ•°å›¾åƒä¸‹æ–¹çš„åŒºåŸŸæ˜¯å‡¸é›†ã€‚

**äºŒé˜¶æ¡ä»¶**ï¼šå¦‚æœ $f$ äºŒæ¬¡å¯å¾®ï¼Œåˆ™ $f$ ä¸ºå‡¸å‡½æ•°å½“ä¸”ä»…å½“å…¶HessiançŸ©é˜µåŠæ­£å®šï¼š
$$\mathbf{H} = \nabla^2 f(\mathbf{x}) \succeq 0$$

### æ·±åº¦å­¦ä¹ ä¸­çš„éå‡¸ä¼˜åŒ–

**é—®é¢˜ç‰¹å¾**ï¼š
- æŸå¤±å‡½æ•°æ˜¯éå‡¸çš„ï¼ˆå­˜åœ¨å¤šä¸ªå±€éƒ¨æœ€ä¼˜ï¼‰
- å‚æ•°ç©ºé—´ç»´åº¦æé«˜ï¼ˆæ•°ç™¾ä¸‡åˆ°æ•°åƒäº¿å‚æ•°ï¼‰
- å­˜åœ¨éç‚¹ï¼ˆHessiançŸ©é˜µæ—¢æœ‰æ­£ç‰¹å¾å€¼åˆæœ‰è´Ÿç‰¹å¾å€¼ï¼‰

**éå‡¸ä¼˜åŒ–çš„æŒ‘æˆ˜**ï¼š
```python
def analyze_loss_landscape(model, dataloader, param_name='transformer_blocks.0.attention.w_q.weight'):
    """åˆ†ææŸå¤±å‡½æ•°çš„æ™¯è§‚ç‰¹æ€§"""
    
    # è·å–ç›®æ ‡å‚æ•°
    target_param = None
    for name, param in model.named_parameters():
        if name == param_name:
            target_param = param
            break
    
    if target_param is None:
        return
    
    original_data = target_param.data.clone()
    
    # åœ¨å‚æ•°ç©ºé—´ä¸­é‡‡æ ·
    perturbations = torch.randn(10, *target_param.shape) * 0.01
    losses = []
    
    model.eval()
    with torch.no_grad():
        for perturbation in perturbations:
            # æ‰°åŠ¨å‚æ•°
            target_param.data = original_data + perturbation
            
            # è®¡ç®—æŸå¤±
            total_loss = 0
            for batch in dataloader:
                if isinstance(batch, dict):
                    input_ids, labels = batch['input_ids'], batch['labels']
                else:
                    input_ids = batch
                    labels = torch.cat([input_ids[:, 1:], 
                                      torch.zeros(input_ids.size(0), 1, dtype=torch.long)], dim=1)
                
                logits = model(input_ids)
                loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), 
                                     labels.reshape(-1), ignore_index=0)
                total_loss += loss.item()
                break  # åªç”¨ä¸€ä¸ªbatchè¿‘ä¼¼
            
            losses.append(total_loss)
    
    # æ¢å¤åŸå‚æ•°
    target_param.data = original_data
    
    # åˆ†æç»Ÿè®¡ç‰¹æ€§
    losses = torch.tensor(losses)
    print(f"æŸå¤±å‡½æ•°ç»Ÿè®¡ (å‚æ•°: {param_name}):")
    print(f"  å‡å€¼: {losses.mean():.4f}")
    print(f"  æ ‡å‡†å·®: {losses.std():.4f}")
    print(f"  æœ€å°å€¼: {losses.min():.4f}")
    print(f"  æœ€å¤§å€¼: {losses.max():.4f}")
    
    return losses
```

## 3.2 æ¢¯åº¦ä¸‹é™ç®—æ³•æ—

### æ™®é€šæ¢¯åº¦ä¸‹é™(Gradient Descent)

**ç®—æ³•**ï¼š
$$\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t)$$

å…¶ä¸­ $\eta$ æ˜¯å­¦ä¹ ç‡ï¼Œ$\nabla_\theta L$ æ˜¯æŸå¤±å‡½æ•°çš„æ¢¯åº¦ã€‚

**æ”¶æ•›æ€§åˆ†æ**ï¼š
å¯¹äº $L$-å…‰æ»‘çš„å‡¸å‡½æ•°ï¼Œæ¢¯åº¦ä¸‹é™çš„æ”¶æ•›ç‡ä¸º $O(1/T)$ï¼Œå…¶ä¸­ $T$ æ˜¯è¿­ä»£æ¬¡æ•°ã€‚

**ä»£ç å®ç°**ï¼š
```python
class SGD:
    def __init__(self, params, lr=0.01):
        self.params = list(params)
        self.lr = lr
    
    def step(self):
        """æ‰§è¡Œä¸€æ­¥å‚æ•°æ›´æ–°"""
        for param in self.params:
            if param.grad is not None:
                # Î¸ = Î¸ - Î·âˆ‡L
                param.data.add_(param.grad.data, alpha=-self.lr)
    
    def zero_grad(self):
        """æ¸…é›¶æ¢¯åº¦"""
        for param in self.params:
            if param.grad is not None:
                param.grad.zero_()
```

### åŠ¨é‡æ³•(Momentum)

**åŠ¨æœº**ï¼šåœ¨ç›¸å…³æ–¹å‘ä¸ŠåŠ é€Ÿï¼Œåœ¨éœ‡è¡æ–¹å‘ä¸Šå‡é€Ÿã€‚

**ç®—æ³•**ï¼š
$$\mathbf{v}_t = \beta \mathbf{v}_{t-1} + (1-\beta)\nabla_\theta L(\theta_t)$$
$$\theta_{t+1} = \theta_t - \eta \mathbf{v}_t$$

å…¶ä¸­ $\beta$ é€šå¸¸å–å€¼ 0.9ã€‚

**ç‰©ç†ç±»æ¯”**ï¼šæƒ³è±¡ä¸€ä¸ªçƒåœ¨æŸå¤±å‡½æ•°è¡¨é¢æ»šåŠ¨ï¼ŒåŠ¨é‡å¸®åŠ©çƒç©¿è¶Šå°çš„å±€éƒ¨æœ€ä¼˜ã€‚

```python
class Momentum:
    def __init__(self, params, lr=0.01, momentum=0.9):
        self.params = list(params)
        self.lr = lr
        self.momentum = momentum
        
        # åˆå§‹åŒ–åŠ¨é‡ç¼“å­˜
        self.velocity = {}
        for param in self.params:
            self.velocity[id(param)] = torch.zeros_like(param.data)
    
    def step(self):
        for param in self.params:
            if param.grad is not None:
                param_id = id(param)
                
                # v = Î²v + (1-Î²)âˆ‡L
                self.velocity[param_id] = (self.momentum * self.velocity[param_id] + 
                                         (1 - self.momentum) * param.grad.data)
                
                # Î¸ = Î¸ - Î·v
                param.data.add_(self.velocity[param_id], alpha=-self.lr)
```

### AdaGradç®—æ³•

**åŠ¨æœº**ï¼šä¸ºä¸åŒå‚æ•°ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡ï¼Œé¢‘ç¹æ›´æ–°çš„å‚æ•°å­¦ä¹ ç‡è¡°å‡æ›´å¿«ã€‚

**ç®—æ³•**ï¼š
$$G_t = G_{t-1} + \nabla_\theta L(\theta_t) \odot \nabla_\theta L(\theta_t)$$
$$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \odot \nabla_\theta L(\theta_t)$$

å…¶ä¸­ $\odot$ è¡¨ç¤ºé€å…ƒç´ ä¹˜æ³•ï¼Œ$\epsilon$ æ˜¯é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°ã€‚

**é—®é¢˜**ï¼šå­¦ä¹ ç‡å•è°ƒé€’å‡ï¼Œå¯èƒ½è¿‡æ—©åœæ­¢å­¦ä¹ ã€‚

```python
class AdaGrad:
    def __init__(self, params, lr=0.01, eps=1e-8):
        self.params = list(params)
        self.lr = lr
        self.eps = eps
        
        # åˆå§‹åŒ–æ¢¯åº¦å¹³æ–¹ç´¯ç§¯
        self.sum_squared_grads = {}
        for param in self.params:
            self.sum_squared_grads[id(param)] = torch.zeros_like(param.data)
    
    def step(self):
        for param in self.params:
            if param.grad is not None:
                param_id = id(param)
                
                # G = G + gÂ²
                self.sum_squared_grads[param_id] += param.grad.data ** 2
                
                # Î¸ = Î¸ - Î·/(âˆšG + Îµ) * g
                adaptive_lr = self.lr / (torch.sqrt(self.sum_squared_grads[param_id]) + self.eps)
                param.data.add_(param.grad.data * adaptive_lr, alpha=-1)
```

### Adamç®—æ³•

**åŠ¨æœº**ï¼šç»“åˆåŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡çš„ä¼˜ç‚¹ã€‚

**ç®—æ³•**ï¼š
$$\mathbf{m}_t = \beta_1 \mathbf{m}_{t-1} + (1-\beta_1)\nabla_\theta L(\theta_t)$$
$$\mathbf{v}_t = \beta_2 \mathbf{v}_{t-1} + (1-\beta_2)[\nabla_\theta L(\theta_t)]^2$$
$$\hat{\mathbf{m}}_t = \frac{\mathbf{m}_t}{1-\beta_1^t}$$
$$\hat{\mathbf{v}}_t = \frac{\mathbf{v}_t}{1-\beta_2^t}$$
$$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{\mathbf{v}}_t} + \epsilon}\hat{\mathbf{m}}_t$$

**åå·®ä¿®æ­£**ï¼š$\hat{\mathbf{m}}_t$ å’Œ $\hat{\mathbf{v}}_t$ ä¿®æ­£äº†åˆå§‹åŒ–åå·®ã€‚

```python
class Adam:
    def __init__(self, params, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):
        self.params = list(params)
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.eps = eps
        self.t = 0  # æ—¶é—´æ­¥
        
        # åˆå§‹åŒ–ä¸€é˜¶å’ŒäºŒé˜¶åŠ¨é‡
        self.m = {}
        self.v = {}
        for param in self.params:
            self.m[id(param)] = torch.zeros_like(param.data)
            self.v[id(param)] = torch.zeros_like(param.data)
    
    def step(self):
        self.t += 1
        
        for param in self.params:
            if param.grad is not None:
                param_id = id(param)
                grad = param.grad.data
                
                # æ›´æ–°ä¸€é˜¶åŠ¨é‡ï¼ˆæ¢¯åº¦çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰
                self.m[param_id] = self.beta1 * self.m[param_id] + (1 - self.beta1) * grad
                
                # æ›´æ–°äºŒé˜¶åŠ¨é‡ï¼ˆæ¢¯åº¦å¹³æ–¹çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰
                self.v[param_id] = self.beta2 * self.v[param_id] + (1 - self.beta2) * grad.pow(2)
                
                # åå·®ä¿®æ­£
                m_hat = self.m[param_id] / (1 - self.beta1 ** self.t)
                v_hat = self.v[param_id] / (1 - self.beta2 ** self.t)
                
                # å‚æ•°æ›´æ–°
                param.data.add_(m_hat / (torch.sqrt(v_hat) + self.eps), alpha=-self.lr)
```

### AdamWï¼šæƒé‡è¡°å‡çš„ä¿®æ­£

**é—®é¢˜**ï¼šä¼ ç»ŸAdamä¸­çš„L2æ­£åˆ™åŒ–ä¸è‡ªé€‚åº”å­¦ä¹ ç‡ç›¸äº’ä½œç”¨ï¼Œå¯¼è‡´æ¬¡ä¼˜æ•ˆæœã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šå°†æƒé‡è¡°å‡ä¸æ¢¯åº¦æ›´æ–°è§£è€¦ã€‚

**ç®—æ³•**ï¼š
$$\theta_{t+1} = \theta_t - \eta \left(\frac{\hat{\mathbf{m}}_t}{\sqrt{\hat{\mathbf{v}}_t} + \epsilon} + \lambda \theta_t\right)$$

å…¶ä¸­ $\lambda$ æ˜¯æƒé‡è¡°å‡ç³»æ•°ã€‚

```python
class AdamW:
    def __init__(self, params, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.01):
        self.params = list(params)
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.eps = eps
        self.weight_decay = weight_decay
        self.t = 0
        
        self.m = {}
        self.v = {}
        for param in self.params:
            self.m[id(param)] = torch.zeros_like(param.data)
            self.v[id(param)] = torch.zeros_like(param.data)
    
    def step(self):
        self.t += 1
        
        for param in self.params:
            if param.grad is not None:
                param_id = id(param)
                grad = param.grad.data
                
                # Adamæ›´æ–°
                self.m[param_id] = self.beta1 * self.m[param_id] + (1 - self.beta1) * grad
                self.v[param_id] = self.beta2 * self.v[param_id] + (1 - self.beta2) * grad.pow(2)
                
                m_hat = self.m[param_id] / (1 - self.beta1 ** self.t)
                v_hat = self.v[param_id] / (1 - self.beta2 ** self.t)
                
                # è§£è€¦çš„æƒé‡è¡°å‡
                param.data.mul_(1 - self.lr * self.weight_decay)
                
                # æ¢¯åº¦æ›´æ–°
                param.data.add_(m_hat / (torch.sqrt(v_hat) + self.eps), alpha=-self.lr)
```

## 3.3 å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥

### å›ºå®šå­¦ä¹ ç‡çš„é—®é¢˜

- **è¿‡å¤§**ï¼šè®­ç»ƒä¸ç¨³å®šï¼Œå¯èƒ½å‘æ•£
- **è¿‡å°**ï¼šæ”¶æ•›è¿‡æ…¢ï¼Œé™·å…¥å±€éƒ¨æœ€ä¼˜

### å­¦ä¹ ç‡è¡°å‡ç­–ç•¥

#### 1. é˜¶æ¢¯è¡°å‡(Step Decay)

$$\eta_t = \eta_0 \cdot \gamma^{\lfloor t/s \rfloor}$$

å…¶ä¸­ $s$ æ˜¯è¡°å‡é—´éš”ï¼Œ$\gamma$ æ˜¯è¡°å‡å› å­ã€‚

```python
class StepLR:
    def __init__(self, optimizer, step_size, gamma=0.1):
        self.optimizer = optimizer
        self.step_size = step_size
        self.gamma = gamma
        self.last_epoch = 0
    
    def step(self):
        if self.last_epoch % self.step_size == 0 and self.last_epoch > 0:
            for param_group in self.optimizer.param_groups:
                param_group['lr'] *= self.gamma
        self.last_epoch += 1
```

#### 2. æŒ‡æ•°è¡°å‡(Exponential Decay)

$$\eta_t = \eta_0 \cdot e^{-\lambda t}$$

```python
class ExponentialLR:
    def __init__(self, optimizer, gamma):
        self.optimizer = optimizer
        self.gamma = gamma
    
    def step(self):
        for param_group in self.optimizer.param_groups:
            param_group['lr'] *= self.gamma
```

#### 3. ä½™å¼¦é€€ç«(Cosine Annealing)

$$\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})\left(1 + \cos\left(\frac{t}{T}\pi\right)\right)$$

**ä¼˜åŠ¿**ï¼šå¹³æ»‘è¡°å‡ï¼Œæœ‰åˆ©äºæ”¶æ•›åˆ°æ›´å¥½çš„å±€éƒ¨æœ€ä¼˜ã€‚

```python
class CosineAnnealingLR:
    def __init__(self, optimizer, T_max, eta_min=0):
        self.optimizer = optimizer
        self.T_max = T_max
        self.eta_min = eta_min
        self.base_lr = optimizer.param_groups[0]['lr']
        self.last_epoch = 0
    
    def step(self):
        lr = self.eta_min + (self.base_lr - self.eta_min) * \
             (1 + math.cos(math.pi * self.last_epoch / self.T_max)) / 2
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
        
        self.last_epoch += 1
```

#### 4. æš–å¯åŠ¨(Warm-up)

åœ¨è®­ç»ƒåˆæœŸä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼Œç„¶åé€æ¸å¢åŠ åˆ°ç›®æ ‡å€¼ã€‚

**åŠ¨æœº**ï¼šé˜²æ­¢è®­ç»ƒåˆæœŸçš„æ¢¯åº¦çˆ†ç‚¸ã€‚

```python
class WarmupLR:
    def __init__(self, optimizer, warmup_steps, target_lr):
        self.optimizer = optimizer
        self.warmup_steps = warmup_steps
        self.target_lr = target_lr
        self.current_step = 0
    
    def step(self):
        if self.current_step < self.warmup_steps:
            lr = self.target_lr * (self.current_step + 1) / self.warmup_steps
        else:
            lr = self.target_lr
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
        
        self.current_step += 1
```

## 3.4 æ¢¯åº¦è£å‰ªä¸æ•°å€¼ç¨³å®šæ€§

### æ¢¯åº¦çˆ†ç‚¸é—®é¢˜

åœ¨æ·±å±‚ç½‘ç»œä¸­ï¼Œæ¢¯åº¦å¯èƒ½å‘ˆæŒ‡æ•°å¢é•¿ï¼Œå¯¼è‡´å‚æ•°æ›´æ–°è¿‡å¤§ã€‚

**æ£€æµ‹æ–¹æ³•**ï¼š
```python
def check_gradient_explosion(model, threshold=10.0):
    """æ£€æµ‹æ¢¯åº¦çˆ†ç‚¸"""
    total_norm = 0
    param_count = 0
    
    for param in model.parameters():
        if param.grad is not None:
            param_norm = param.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
            param_count += 1
    
    total_norm = total_norm ** (1. / 2)
    
    print(f"æ¢¯åº¦èŒƒæ•°: {total_norm:.4f}")
    if total_norm > threshold:
        print("âš ï¸  æ£€æµ‹åˆ°æ¢¯åº¦çˆ†ç‚¸ï¼")
    
    return total_norm
```

### æ¢¯åº¦è£å‰ªç­–ç•¥

#### 1. èŒƒæ•°è£å‰ª(Norm Clipping)

$$\mathbf{g} \leftarrow \begin{cases}
\mathbf{g} & \text{if } \|\mathbf{g}\| \leq \tau \\
\frac{\tau}{\|\mathbf{g}\|} \mathbf{g} & \text{if } \|\mathbf{g}\| > \tau
\end{cases}$$

```python
def clip_grad_norm(parameters, max_norm, norm_type=2):
    """æ¢¯åº¦èŒƒæ•°è£å‰ª"""
    if isinstance(parameters, torch.Tensor):
        parameters = [parameters]
    
    parameters = list(filter(lambda p: p.grad is not None, parameters))
    
    if len(parameters) == 0:
        return 0.0
    
    device = parameters[0].grad.device
    
    # è®¡ç®—æ€»æ¢¯åº¦èŒƒæ•°
    if norm_type == float('inf'):
        total_norm = max(p.grad.data.abs().max() for p in parameters)
        total_norm = total_norm.to(device)
    else:
        total_norm = torch.norm(torch.stack([torch.norm(p.grad.data, norm_type) 
                                           for p in parameters]), norm_type)
    
    # è£å‰ª
    clip_coef = max_norm / (total_norm + 1e-6)
    if clip_coef < 1:
        for p in parameters:
            p.grad.data.mul_(clip_coef)
    
    return total_norm.item()
```

#### 2. å€¼è£å‰ª(Value Clipping)

$$g_i \leftarrow \text{clip}(g_i, -\tau, \tau)$$

```python
def clip_grad_value(parameters, clip_value):
    """æ¢¯åº¦å€¼è£å‰ª"""
    if isinstance(parameters, torch.Tensor):
        parameters = [parameters]
    
    for p in filter(lambda p: p.grad is not None, parameters):
        p.grad.data.clamp_(-clip_value, clip_value)
```

## 3.5 å®è·µï¼šMiniGPTä¸­çš„ä¼˜åŒ–é…ç½®

### ä¼˜åŒ–å™¨é€‰æ‹©ä¸é…ç½®

```python
# MiniGPTä¸­çš„ä¼˜åŒ–å™¨é…ç½® (src/training/trainer.py)
class PreTrainer:
    def __init__(self, model, tokenizer, device='cpu'):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        
        # AdamWä¼˜åŒ–å™¨ï¼šæƒé‡è¡°å‡è§£è€¦
        self.optimizer = optim.AdamW(
            model.parameters(), 
            lr=1e-4,           # åŸºç¡€å­¦ä¹ ç‡
            weight_decay=0.01, # æƒé‡è¡°å‡
            betas=(0.9, 0.999), # åŠ¨é‡å‚æ•°
            eps=1e-8           # æ•°å€¼ç¨³å®šæ€§
        )
        
        # ä½™å¼¦é€€ç«è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, 
            T_max=1000  # æ€»è®­ç»ƒæ­¥æ•°
        )
```

### è®­ç»ƒå¾ªç¯ä¸­çš„ä¼˜åŒ–å®è·µ

```python
def train_epoch(self, dataloader):
    """ä¼˜åŒ–å®è·µçš„è®­ç»ƒå¾ªç¯"""
    self.model.train()
    total_loss = 0
    
    for batch_idx, batch in enumerate(dataloader):
        # æ•°æ®å‡†å¤‡
        input_ids, labels = self.prepare_batch(batch)
        
        # å‰å‘ä¼ æ’­
        logits = self.model(input_ids)
        loss = self.compute_loss(logits, labels)
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        grad_norm = torch.nn.utils.clip_grad_norm_(
            self.model.parameters(), 
            max_norm=1.0
        )
        
        # å‚æ•°æ›´æ–°
        self.optimizer.step()
        self.scheduler.step()  # å­¦ä¹ ç‡è°ƒåº¦
        
        # ç›‘æ§æŒ‡æ ‡
        if batch_idx % 100 == 0:
            current_lr = self.scheduler.get_last_lr()[0]
            print(f"Batch {batch_idx}: Loss={loss:.4f}, "
                  f"LR={current_lr:.6f}, GradNorm={grad_norm:.4f}")
        
        total_loss += loss.item()
    
    return total_loss / len(dataloader)
```

### ä¼˜åŒ–å™¨çŠ¶æ€ç›‘æ§

```python
def monitor_optimizer_state(optimizer, step):
    """ç›‘æ§ä¼˜åŒ–å™¨å†…éƒ¨çŠ¶æ€"""
    if hasattr(optimizer, 'state') and len(optimizer.state) > 0:
        # è·å–ç¬¬ä¸€ä¸ªå‚æ•°çš„çŠ¶æ€ä½œä¸ºä»£è¡¨
        first_param = next(iter(optimizer.param_groups[0]['params']))
        state = optimizer.state[first_param]
        
        if 'exp_avg' in state:  # Adamç±»ä¼˜åŒ–å™¨
            # ä¸€é˜¶åŠ¨é‡ç»Ÿè®¡
            m_norm = state['exp_avg'].norm().item()
            # äºŒé˜¶åŠ¨é‡ç»Ÿè®¡
            v_norm = state['exp_avg_sq'].norm().item()
            
            print(f"Step {step}: ä¸€é˜¶åŠ¨é‡èŒƒæ•°={m_norm:.6f}, "
                  f"äºŒé˜¶åŠ¨é‡èŒƒæ•°={v_norm:.6f}")
```

## å°ç»“ä¸æ€è€ƒ

æœ¬èŠ‚ä»‹ç»äº†æ·±åº¦å­¦ä¹ ä¼˜åŒ–çš„æ ¸å¿ƒç†è®ºå’Œå®è·µï¼š

1. **æ¢¯åº¦ä¸‹é™**æ˜¯å‚æ•°ä¼˜åŒ–çš„åŸºç¡€ç®—æ³•
2. **è‡ªé€‚åº”ç®—æ³•**ï¼ˆAdam/AdamWï¼‰é€‚åº”ä¸åŒå‚æ•°çš„æ›´æ–°éœ€æ±‚
3. **å­¦ä¹ ç‡è°ƒåº¦**æ§åˆ¶è®­ç»ƒçš„èŠ‚å¥å’Œæ”¶æ•›è´¨é‡
4. **æ¢¯åº¦è£å‰ª**ä¿è¯è®­ç»ƒçš„æ•°å€¼ç¨³å®šæ€§
5. **ä¼˜åŒ–é…ç½®**éœ€è¦æ ¹æ®å…·ä½“é—®é¢˜ç²¾å¿ƒè°ƒæ•´

**æ€è€ƒé¢˜**ï¼š
1. ä¸ºä»€ä¹ˆAdamWæ¯”Adamåœ¨å¤§æ¨¡å‹è®­ç»ƒä¸­è¡¨ç°æ›´å¥½ï¼Ÿ
2. å¦‚ä½•ä»æŸå¤±å‡½æ•°çš„å‡ ä½•æ€§è´¨ç†è§£ä¸åŒä¼˜åŒ–ç®—æ³•çš„è¡Œä¸ºï¼Ÿ
3. å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥å¦‚ä½•å½±å“æ¨¡å‹çš„æœ€ç»ˆæ€§èƒ½ï¼Ÿ

**ä¸‹ä¸€èŠ‚é¢„å‘Š**ï¼šæˆ‘ä»¬å°†å­¦ä¹ ç»Ÿè®¡å­¦ä¹ ç†è®ºï¼Œç†è§£æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„æ•°å­¦åŸºç¡€ã€‚

---

*ä¼˜åŒ–ç†è®ºä¸ºæ·±åº¦å­¦ä¹ æä¾›äº†å¯»æ‰¾æœ€ä¼˜è§£çš„æŒ‡å—é’ˆï¼Œè€Œå®è·µç»éªŒè®©æˆ‘ä»¬åœ¨å¤æ‚çš„å‚æ•°ç©ºé—´ä¸­æ‰¾åˆ°å‰è¿›çš„æ–¹å‘ã€‚* ğŸ¯