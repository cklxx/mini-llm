# 04 ç»Ÿè®¡å­¦ä¹ ç†è®º

> **ç†è§£æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„æ•°å­¦åŸºç¡€**

## æ ¸å¿ƒæ€æƒ³

ç»Ÿè®¡å­¦ä¹ ç†è®ºä¸ºæˆ‘ä»¬æä¾›äº†ç†è§£æœºå™¨å­¦ä¹ ç®—æ³•æ³›åŒ–èƒ½åŠ›çš„æ•°å­¦æ¡†æ¶ã€‚å®ƒå›ç­”äº†ä¸€ä¸ªæ ¹æœ¬é—®é¢˜ï¼šä¸ºä»€ä¹ˆåœ¨æœ‰é™è®­ç»ƒæ•°æ®ä¸Šå­¦åˆ°çš„æ¨¡å‹èƒ½å¤Ÿåœ¨æœªè§è¿‡çš„æµ‹è¯•æ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ï¼Ÿ

**å…³é”®æ´å¯Ÿ**ï¼š
- æ³›åŒ–èƒ½åŠ›æ¥è‡ªäºå‡è®¾ç©ºé—´çš„é€‚å½“çº¦æŸ
- åå·®-æ–¹å·®æƒè¡¡å†³å®šäº†æ¨¡å‹çš„æ€§èƒ½ä¸Šé™
- æ­£åˆ™åŒ–é€šè¿‡çº¦æŸæ¨¡å‹å¤æ‚åº¦æé«˜æ³›åŒ–èƒ½åŠ›
- æ ·æœ¬å¤æ‚åº¦ç†è®ºæŒ‡å¯¼æ•°æ®éœ€æ±‚çš„ä¼°è®¡

## 4.1 PACå­¦ä¹ æ¡†æ¶

### PACå­¦ä¹ çš„å®šä¹‰

**PAC (Probably Approximately Correct)** å­¦ä¹ æ¡†æ¶ç”±Valiantåœ¨1984å¹´æå‡ºï¼Œä¸ºå­¦ä¹ ç®—æ³•çš„ç†è®ºåˆ†ææä¾›äº†åŸºç¡€ã€‚

**å®šä¹‰**ï¼šè®¾ $X$ ä¸ºè¾“å…¥ç©ºé—´ï¼Œ$Y$ ä¸ºè¾“å‡ºç©ºé—´ï¼Œ$\mathcal{D}$ ä¸º $X$ ä¸Šçš„æœªçŸ¥åˆ†å¸ƒï¼Œ$c: X \rightarrow Y$ ä¸ºç›®æ ‡æ¦‚å¿µã€‚ä¸€ä¸ªç®—æ³• $A$ æ˜¯ PAC å¯å­¦ä¹ çš„ï¼Œå¦‚æœå¯¹äºä»»æ„ $\epsilon > 0, \delta > 0$ï¼Œå­˜åœ¨å¤šé¡¹å¼ $p(\cdot, \cdot, \cdot)$ å’Œæ ·æœ¬å¤æ‚åº¦ $m \geq p(1/\epsilon, 1/\delta, \text{size}(c))$ï¼Œä½¿å¾—å¯¹äºä»»æ„ç›®æ ‡æ¦‚å¿µ $c$ å’Œä»»æ„åˆ†å¸ƒ $\mathcal{D}$ï¼Œç®—æ³• $A$ åœ¨æ”¶åˆ° $m$ ä¸ªä» $\mathcal{D}$ ä¸­ç‹¬ç«‹åŒåˆ†å¸ƒé‡‡æ ·çš„æ ·æœ¬åï¼Œè¾“å‡ºå‡è®¾ $h$ æ»¡è¶³ï¼š

$$P[R(h) \leq \epsilon] \geq 1 - \delta$$

å…¶ä¸­ $R(h) = P_{(x,y) \sim \mathcal{D}}[h(x) \neq c(x)]$ æ˜¯æ³›åŒ–è¯¯å·®ã€‚

**è§£é‡Š**ï¼š
- $\epsilon$ï¼šå‡†ç¡®æ€§å‚æ•°ï¼ˆå…è®¸çš„è¯¯å·®ï¼‰
- $\delta$ï¼šç½®ä¿¡åº¦å‚æ•°ï¼ˆå¤±è´¥çš„æ¦‚ç‡ï¼‰
- ç®—æ³•ä»¥é«˜æ¦‚ç‡($1-\delta$)å­¦åˆ°è¿‘ä¼¼æ­£ç¡®($\epsilon$-å‡†ç¡®)çš„å‡è®¾

### åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨

```python
def pac_bound_analysis(model, train_dataset, test_dataset, epsilon=0.01, delta=0.01):
    """åˆ†ææ¨¡å‹çš„PACå­¦ä¹ ç‰¹æ€§"""
    
    # è®¡ç®—è®­ç»ƒè¯¯å·®
    train_error = evaluate_model(model, train_dataset)
    
    # è®¡ç®—æµ‹è¯•è¯¯å·®ï¼ˆæ³›åŒ–è¯¯å·®çš„ä¼°è®¡ï¼‰
    test_error = evaluate_model(model, test_dataset)
    
    # æ¨¡å‹å¤æ‚åº¦ï¼ˆå‚æ•°æ•°é‡çš„å¯¹æ•°ï¼‰
    num_params = sum(p.numel() for p in model.parameters())
    model_complexity = math.log(num_params)
    
    # æ ·æœ¬å¤æ‚åº¦çš„ç†è®ºä¼°è®¡ï¼ˆç®€åŒ–ç‰ˆï¼‰
    # å®é™…çš„ç•Œä¼šæ›´å¤æ‚ï¼Œè¿™é‡Œåªæ˜¯ç¤ºæ„
    theoretical_bound = model_complexity / (epsilon ** 2 * (1 - delta))
    
    print(f"è®­ç»ƒè¯¯å·®: {train_error:.4f}")
    print(f"æµ‹è¯•è¯¯å·®: {test_error:.4f}")
    print(f"æ¨¡å‹å¤æ‚åº¦ (logå‚æ•°æ•°): {model_complexity:.2f}")
    print(f"ç†è®ºæ ·æœ¬å¤æ‚åº¦ä¸‹ç•Œ: {theoretical_bound:.0f}")
    print(f"å®é™…è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}")
    
    # æ³›åŒ–é—´éš™
    generalization_gap = test_error - train_error
    print(f"æ³›åŒ–é—´éš™: {generalization_gap:.4f}")
    
    return {
        'train_error': train_error,
        'test_error': test_error,
        'generalization_gap': generalization_gap,
        'theoretical_bound': theoretical_bound
    }

def evaluate_model(model, dataset):
    """è¯„ä¼°æ¨¡å‹åœ¨æ•°æ®é›†ä¸Šçš„è¯¯å·®"""
    model.eval()
    total_loss = 0
    total_samples = 0
    
    with torch.no_grad():
        for batch in DataLoader(dataset, batch_size=32):
            if isinstance(batch, dict):
                input_ids, labels = batch['input_ids'], batch['labels']
            else:
                input_ids = batch
                labels = torch.cat([input_ids[:, 1:], 
                                  torch.zeros(input_ids.size(0), 1, dtype=torch.long)], dim=1)
            
            logits = model(input_ids)
            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), 
                                 labels.reshape(-1), ignore_index=0)
            
            total_loss += loss.item() * input_ids.size(0)
            total_samples += input_ids.size(0)
    
    return total_loss / total_samples
```

## 4.2 åå·®-æ–¹å·®åˆ†è§£

### ç†è®ºæ¡†æ¶

å¯¹äºå›å½’é—®é¢˜ï¼Œç»™å®šè¾“å…¥ $x$ï¼Œè®¾çœŸå®å‡½æ•°ä¸º $f(x)$ï¼Œå™ªå£°ä¸º $\epsilon \sim \mathcal{N}(0, \sigma^2)$ï¼Œè§‚æµ‹å€¼ä¸º $y = f(x) + \epsilon$ã€‚

å­¦ä¹ ç®—æ³•åœ¨è®­ç»ƒé›† $D$ ä¸Šå­¦åˆ°çš„æ¨¡å‹ä¸º $\hat{f}_D(x)$ï¼Œåˆ™æœŸæœ›å¹³æ–¹è¯¯å·®å¯ä»¥åˆ†è§£ä¸ºï¼š

$$\mathbb{E}_D[(y - \hat{f}_D(x))^2] = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}$$

å…¶ä¸­ï¼š
- **åå·®**ï¼š$\text{Bias}[\hat{f}_D(x)] = \mathbb{E}_D[\hat{f}_D(x)] - f(x)$
- **æ–¹å·®**ï¼š$\text{Var}[\hat{f}_D(x)] = \mathbb{E}_D[(\hat{f}_D(x) - \mathbb{E}_D[\hat{f}_D(x)])^2]$
- **ä¸å¯çº¦è¯¯å·®**ï¼š$\sigma^2$

### åå·®-æ–¹å·®æƒè¡¡

**é«˜åå·®ï¼Œä½æ–¹å·®**ï¼š
- æ¨¡å‹è¿‡äºç®€å•ï¼Œæ— æ³•æ•æ‰æ•°æ®çš„å¤æ‚æ¨¡å¼
- æ¬ æ‹Ÿåˆç°è±¡
- ä¾‹ï¼šçº¿æ€§æ¨¡å‹æ‹Ÿåˆéçº¿æ€§æ•°æ®

**ä½åå·®ï¼Œé«˜æ–¹å·®**ï¼š
- æ¨¡å‹è¿‡äºå¤æ‚ï¼Œå¯¹è®­ç»ƒæ•°æ®çš„éšæœºæ€§æ•æ„Ÿ
- è¿‡æ‹Ÿåˆç°è±¡
- ä¾‹ï¼šé«˜æ¬¡å¤šé¡¹å¼æ‹Ÿåˆå°‘é‡æ•°æ®

```python
def bias_variance_analysis(model_class, train_datasets, test_x, test_y, num_trials=50):
    """åå·®-æ–¹å·®åˆ†è§£åˆ†æ"""
    
    predictions = []
    
    # åœ¨ä¸åŒè®­ç»ƒé›†ä¸Šè®­ç»ƒå¤šä¸ªæ¨¡å‹
    for trial in range(num_trials):
        # éšæœºé‡‡æ ·è®­ç»ƒé›†
        train_dataset = train_datasets[trial % len(train_datasets)]
        
        # è®­ç»ƒæ¨¡å‹
        model = model_class()
        train_model(model, train_dataset)
        
        # åœ¨æµ‹è¯•ç‚¹ä¸Šé¢„æµ‹
        model.eval()
        with torch.no_grad():
            pred = model(test_x)
            predictions.append(pred.cpu().numpy())
    
    predictions = np.array(predictions)
    
    # è®¡ç®—åå·®å’Œæ–¹å·®
    mean_prediction = np.mean(predictions, axis=0)
    bias_squared = np.mean((mean_prediction - test_y.numpy()) ** 2)
    variance = np.mean(np.var(predictions, axis=0))
    
    # æ€»è¯¯å·®
    total_error = np.mean((predictions - test_y.numpy()) ** 2)
    
    print(f"åå·®Â²: {bias_squared:.6f}")
    print(f"æ–¹å·®: {variance:.6f}")
    print(f"æ€»è¯¯å·®: {total_error:.6f}")
    print(f"åå·®Â² + æ–¹å·®: {bias_squared + variance:.6f}")
    
    return {
        'bias_squared': bias_squared,
        'variance': variance,
        'total_error': total_error
    }
```

### åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„ä½“ç°

```python
def analyze_model_complexity_tradeoff(base_model, train_data, val_data, complexities):
    """åˆ†ææ¨¡å‹å¤æ‚åº¦ä¸åå·®-æ–¹å·®çš„å…³ç³»"""
    
    results = {'complexity': [], 'train_error': [], 'val_error': [], 
               'bias_estimate': [], 'variance_estimate': []}
    
    for complexity in complexities:
        print(f"\\nåˆ†æå¤æ‚åº¦: {complexity}")
        
        # åˆ›å»ºä¸åŒå¤æ‚åº¦çš„æ¨¡å‹
        if complexity == 'tiny':
            model = create_model(vocab_size=1000, model_size='tiny')
        elif complexity == 'small':
            model = create_model(vocab_size=1000, model_size='small')
        elif complexity == 'medium':
            model = create_model(vocab_size=1000, model_size='medium')
        
        # è®­ç»ƒå¤šæ¬¡ä»¥ä¼°è®¡æ–¹å·®
        train_errors = []
        val_errors = []
        
        for trial in range(5):  # 5æ¬¡ç‹¬ç«‹è®­ç»ƒ
            # é‡æ–°åˆå§‹åŒ–æ¨¡å‹
            model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)
            
            # è®­ç»ƒ
            trainer = create_trainer('pretrain', model, tokenizer, device='cpu')
            trainer.train(train_data, num_epochs=3)
            
            # è¯„ä¼°
            train_error = evaluate_model(model, train_data)
            val_error = evaluate_model(model, val_data)
            
            train_errors.append(train_error)
            val_errors.append(val_error)
        
        # ç»Ÿè®¡ç»“æœ
        avg_train_error = np.mean(train_errors)
        avg_val_error = np.mean(val_errors)
        val_variance = np.var(val_errors)
        
        # åå·®ä¼°è®¡ï¼ˆç®€åŒ–ï¼‰
        bias_estimate = avg_train_error
        
        results['complexity'].append(complexity)
        results['train_error'].append(avg_train_error)
        results['val_error'].append(avg_val_error)
        results['bias_estimate'].append(bias_estimate)
        results['variance_estimate'].append(val_variance)
        
        print(f"å¹³å‡è®­ç»ƒè¯¯å·®: {avg_train_error:.4f}")
        print(f"å¹³å‡éªŒè¯è¯¯å·®: {avg_val_error:.4f}")
        print(f"éªŒè¯è¯¯å·®æ–¹å·®: {val_variance:.6f}")
    
    return results
```

## 4.3 æ­£åˆ™åŒ–ä¸ç»“æ„é£é™©æœ€å°åŒ–

### ç»“æ„é£é™©æœ€å°åŒ–åŸç†

ä¼ ç»Ÿçš„ç»éªŒé£é™©æœ€å°åŒ–(ERM)åªè€ƒè™‘è®­ç»ƒè¯¯å·®ï¼š
$$\hat{f} = \arg\min_{f \in \mathcal{F}} \frac{1}{n}\sum_{i=1}^{n} L(y_i, f(x_i))$$

ç»“æ„é£é™©æœ€å°åŒ–(SRM)åœ¨ç»éªŒé£é™©çš„åŸºç¡€ä¸ŠåŠ å…¥æ¨¡å‹å¤æ‚åº¦æƒ©ç½šï¼š
$$\hat{f} = \arg\min_{f \in \mathcal{F}} \left[\frac{1}{n}\sum_{i=1}^{n} L(y_i, f(x_i)) + \lambda \Omega(f)\right]$$

å…¶ä¸­ $\Omega(f)$ æ˜¯å¤æ‚åº¦æƒ©ç½šé¡¹ï¼Œ$\lambda$ æ˜¯æ­£åˆ™åŒ–å¼ºåº¦ã€‚

### å¸¸è§æ­£åˆ™åŒ–æ–¹æ³•

#### 1. L1æ­£åˆ™åŒ–ï¼ˆLassoï¼‰

$$\Omega(f) = \|\mathbf{w}\|_1 = \sum_{i=1}^{d} |w_i|$$

**æ•ˆæœ**ï¼šäº§ç”Ÿç¨€ç–è§£ï¼Œå®ç°ç‰¹å¾é€‰æ‹©ã€‚

```python
class L1Regularization:
    def __init__(self, lambda_reg=0.01):
        self.lambda_reg = lambda_reg
    
    def __call__(self, model):
        """è®¡ç®—L1æ­£åˆ™åŒ–é¡¹"""
        l1_reg = 0
        for param in model.parameters():
            l1_reg += torch.sum(torch.abs(param))
        return self.lambda_reg * l1_reg
```

#### 2. L2æ­£åˆ™åŒ–ï¼ˆRidge/Weight Decayï¼‰

$$\Omega(f) = \|\mathbf{w}\|_2^2 = \sum_{i=1}^{d} w_i^2$$

**æ•ˆæœ**ï¼šé˜²æ­¢æƒé‡è¿‡å¤§ï¼Œæé«˜æ•°å€¼ç¨³å®šæ€§ã€‚

```python
class L2Regularization:
    def __init__(self, lambda_reg=0.01):
        self.lambda_reg = lambda_reg
    
    def __call__(self, model):
        """è®¡ç®—L2æ­£åˆ™åŒ–é¡¹"""
        l2_reg = 0
        for param in model.parameters():
            l2_reg += torch.sum(param ** 2)
        return self.lambda_reg * l2_reg
```

#### 3. Dropoutæ­£åˆ™åŒ–

åœ¨è®­ç»ƒæ—¶éšæœºå°†æŸäº›ç¥ç»å…ƒçš„è¾“å‡ºè®¾ä¸º0ï¼Œç›¸å½“äºè®­ç»ƒå¤šä¸ªå­ç½‘ç»œçš„é›†æˆã€‚

**æ•°å­¦è§£é‡Š**ï¼šè®¾ $r_i \sim \text{Bernoulli}(p)$ï¼Œåˆ™Dropoutåçš„è¾“å‡ºä¸ºï¼š
$$\tilde{y} = \mathbf{r} \odot \mathbf{y}$$

```python
class Dropout(nn.Module):
    def __init__(self, p=0.5):
        super().__init__()
        self.p = p
    
    def forward(self, x):
        if self.training:
            # ç”Ÿæˆéšæœºæ©ç 
            mask = torch.bernoulli(torch.full_like(x, 1 - self.p))
            # ç¼©æ”¾ä»¥ä¿æŒæœŸæœ›å€¼ä¸å˜
            return x * mask / (1 - self.p)
        else:
            return x
```

#### 4. æ‰¹å½’ä¸€åŒ–ï¼ˆBatch Normalizationï¼‰

é€šè¿‡å½’ä¸€åŒ–æ¿€æ´»å€¼çš„åˆ†å¸ƒæ¥æ­£åˆ™åŒ–æ¨¡å‹ï¼š

$$\hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$
$$y = \gamma \hat{x} + \beta$$

```python
class BatchNorm1d(nn.Module):
    def __init__(self, num_features, eps=1e-5, momentum=0.1):
        super().__init__()
        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum
        
        # å¯å­¦ä¹ å‚æ•°
        self.gamma = nn.Parameter(torch.ones(num_features))
        self.beta = nn.Parameter(torch.zeros(num_features))
        
        # ç§»åŠ¨å¹³å‡ç»Ÿè®¡é‡
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))
    
    def forward(self, x):
        if self.training:
            # è®¡ç®—æ‰¹ç»Ÿè®¡é‡
            mean = x.mean(0)
            var = x.var(0, unbiased=False)
            
            # æ›´æ–°ç§»åŠ¨å¹³å‡
            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean
            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var
        else:
            mean = self.running_mean
            var = self.running_var
        
        # å½’ä¸€åŒ–å’Œç¼©æ”¾
        x_norm = (x - mean) / torch.sqrt(var + self.eps)
        return self.gamma * x_norm + self.beta
```

## 4.4 æ³›åŒ–ç•Œä¸æ ·æœ¬å¤æ‚åº¦

### Rademacherå¤æ‚åº¦

Rademacherå¤æ‚åº¦è¡¡é‡å‡½æ•°ç±»åœ¨éšæœºæ•°æ®ä¸Šçš„æ‹Ÿåˆèƒ½åŠ›ï¼š

$$\mathfrak{R}_n(\mathcal{F}) = \mathbb{E}_{\sigma, S}\left[\sup_{f \in \mathcal{F}} \frac{1}{n}\sum_{i=1}^{n} \sigma_i f(x_i)\right]$$

å…¶ä¸­ $\sigma_i$ æ˜¯ç‹¬ç«‹çš„Rademacheréšæœºå˜é‡ï¼ˆç­‰æ¦‚ç‡å–Â±1ï¼‰ã€‚

**æ³›åŒ–ç•Œ**ï¼šä»¥é«˜æ¦‚ç‡($1-\delta$)ï¼Œå¯¹äºä»»æ„ $f \in \mathcal{F}$ï¼š
$$R(f) \leq \hat{R}_n(f) + 2\mathfrak{R}_n(\mathcal{F}) + \sqrt{\frac{\log(1/\delta)}{2n}}$$

```python
def estimate_rademacher_complexity(model, dataset, num_samples=1000):
    """ä¼°è®¡æ¨¡å‹çš„Rademacherå¤æ‚åº¦"""
    
    model.eval()
    rademacher_values = []
    
    # é‡‡æ ·æ•°æ®ç‚¹
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
    batch = next(iter(dataloader))
    
    if isinstance(batch, dict):
        x = batch['input_ids'][:num_samples]
    else:
        x = batch[:num_samples]
    
    for trial in range(100):  # å¤šæ¬¡è¯•éªŒ
        # ç”ŸæˆRademacherå˜é‡
        sigma = torch.randint(0, 2, (x.size(0),)) * 2 - 1  # {-1, 1}
        sigma = sigma.float()
        
        with torch.no_grad():
            # è®¡ç®—æ¨¡å‹è¾“å‡º
            outputs = model(x)
            
            # è®¡ç®— Rademacher ç›¸å…³æ€§
            if outputs.dim() > 2:
                outputs = outputs.mean(dim=1)  # å¹³å‡æ± åŒ–åˆ° (batch_size, d_model)
            
            rademacher_corr = torch.mean(sigma.unsqueeze(1) * outputs)
            rademacher_values.append(rademacher_corr.item())
    
    # ä¼°è®¡å¤æ‚åº¦
    estimated_complexity = np.mean(np.abs(rademacher_values))
    
    print(f"ä¼°è®¡çš„Rademacherå¤æ‚åº¦: {estimated_complexity:.6f}")
    return estimated_complexity
```

### VCç»´ä¸æ ·æœ¬å¤æ‚åº¦

VCç»´ï¼ˆVapnik-Chervonenkis dimensionï¼‰æ˜¯è¡¡é‡å‡½æ•°ç±»å¤æ‚åº¦çš„ç»å…¸æŒ‡æ ‡ã€‚

**å®šä¹‰**ï¼šå‡½æ•°ç±» $\mathcal{F}$ çš„VCç»´æ˜¯èƒ½è¢« $\mathcal{F}$ å®Œå…¨æ‰“æ•£ï¼ˆshatterï¼‰çš„æœ€å¤§æ ·æœ¬é›†åˆçš„å¤§å°ã€‚

**æ ·æœ¬å¤æ‚åº¦ç•Œ**ï¼šè¦è¾¾åˆ° $\epsilon$-å‡†ç¡®æ€§å’Œ $\delta$-ç½®ä¿¡åº¦ï¼Œéœ€è¦çš„æ ·æœ¬æ•°ä¸ºï¼š
$$m \geq O\left(\frac{d + \log(1/\delta)}{\epsilon^2}\right)$$

å…¶ä¸­ $d$ æ˜¯VCç»´ã€‚

```python
def estimate_sample_complexity(model_complexity, epsilon=0.01, delta=0.01):
    """ä¼°è®¡è¾¾åˆ°ç»™å®šç²¾åº¦æ‰€éœ€çš„æ ·æœ¬å¤æ‚åº¦"""
    
    # ç®€åŒ–çš„æ ·æœ¬å¤æ‚åº¦ä¼°è®¡
    # å®é™…åº”ç”¨ä¸­éœ€è¦æ›´ç²¾ç¡®çš„ç•Œ
    
    # å¯¹äºç¥ç»ç½‘ç»œï¼ŒVCç»´å¤§è‡´ä¸å‚æ•°æ•°é‡æˆæ­£æ¯”
    num_params = model_complexity
    vc_dimension = num_params  # ç®€åŒ–å‡è®¾
    
    # PACå­¦ä¹ çš„æ ·æœ¬å¤æ‚åº¦
    sample_complexity = (vc_dimension + math.log(1/delta)) / (epsilon ** 2)
    
    print(f"æ¨¡å‹å¤æ‚åº¦ (å‚æ•°æ•°): {num_params}")
    print(f"ä¼°è®¡VCç»´: {vc_dimension}")
    print(f"ç›®æ ‡ç²¾åº¦ Îµ: {epsilon}")
    print(f"ç½®ä¿¡åº¦ (1-Î´): {1-delta}")
    print(f"ç†è®ºæ ·æœ¬å¤æ‚åº¦: {sample_complexity:.0f}")
    
    return sample_complexity
```

## 4.5 å®è·µï¼šMiniGPTä¸­çš„æ³›åŒ–ç­–ç•¥

### æ­£åˆ™åŒ–æŠ€æœ¯çš„é›†æˆåº”ç”¨

```python
class RegularizedTrainer(PreTrainer):
    """é›†æˆå¤šç§æ­£åˆ™åŒ–æŠ€æœ¯çš„è®­ç»ƒå™¨"""
    
    def __init__(self, model, tokenizer, device='cpu', 
                 weight_decay=0.01, dropout_rate=0.1, 
                 l1_lambda=0.0, label_smoothing=0.1):
        
        super().__init__(model, tokenizer, device)
        
        # æ­£åˆ™åŒ–å‚æ•°
        self.l1_lambda = l1_lambda
        self.label_smoothing = label_smoothing
        
        # æ·»åŠ Dropoutå±‚
        self._add_dropout_layers(dropout_rate)
        
        # ä½¿ç”¨æƒé‡è¡°å‡çš„ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            model.parameters(), 
            lr=1e-4, 
            weight_decay=weight_decay
        )
    
    def _add_dropout_layers(self, dropout_rate):
        """ä¸ºæ¨¡å‹æ·»åŠ Dropoutå±‚"""
        for module in self.model.modules():
            if hasattr(module, 'dropout'):
                module.dropout.p = dropout_rate
    
    def compute_loss(self, logits, labels):
        """è®¡ç®—å¸¦æ­£åˆ™åŒ–çš„æŸå¤±"""
        # åŸºç¡€äº¤å‰ç†µæŸå¤±
        if self.label_smoothing > 0:
            # æ ‡ç­¾å¹³æ»‘
            ce_loss = self._label_smoothing_loss(logits, labels)
        else:
            ce_loss = F.cross_entropy(
                logits.reshape(-1, logits.size(-1)),
                labels.reshape(-1),
                ignore_index=self.tokenizer.pad_id
            )
        
        # L1æ­£åˆ™åŒ–
        l1_reg = 0
        if self.l1_lambda > 0:
            for param in self.model.parameters():
                l1_reg += torch.sum(torch.abs(param))
        
        total_loss = ce_loss + self.l1_lambda * l1_reg
        
        return total_loss
    
    def _label_smoothing_loss(self, logits, labels):
        """æ ‡ç­¾å¹³æ»‘æŸå¤±"""
        vocab_size = logits.size(-1)
        
        # åˆ›å»ºå¹³æ»‘æ ‡ç­¾
        smooth_labels = torch.zeros_like(logits)
        smooth_labels.fill_(self.label_smoothing / (vocab_size - 1))
        
        # è®¾ç½®çœŸå®æ ‡ç­¾çš„æ¦‚ç‡
        labels_expanded = labels.unsqueeze(-1)
        smooth_labels.scatter_(-1, labels_expanded, 1 - self.label_smoothing)
        
        # è®¡ç®—KLæ•£åº¦æŸå¤±
        log_probs = F.log_softmax(logits, dim=-1)
        loss = F.kl_div(log_probs, smooth_labels, reduction='batchmean')
        
        return loss
```

### æ³›åŒ–èƒ½åŠ›ç›‘æ§

```python
def monitor_generalization(trainer, train_loader, val_loader, test_loader):
    """ç›‘æ§æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›"""
    
    history = {
        'epoch': [],
        'train_loss': [],
        'val_loss': [],
        'test_loss': [],
        'generalization_gap': [],
        'overfitting_score': []
    }
    
    for epoch in range(trainer.num_epochs):
        # è®­ç»ƒ
        train_loss = trainer.train_epoch(train_loader)
        
        # è¯„ä¼°
        val_loss = trainer.validate(val_loader)
        test_loss = trainer.validate(test_loader)
        
        # è®¡ç®—æ³›åŒ–æŒ‡æ ‡
        generalization_gap = val_loss - train_loss
        overfitting_score = max(0, val_loss - train_loss) / train_loss
        
        # è®°å½•å†å²
        history['epoch'].append(epoch)
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['test_loss'].append(test_loss)
        history['generalization_gap'].append(generalization_gap)
        history['overfitting_score'].append(overfitting_score)
        
        # è¿‡æ‹Ÿåˆæ£€æµ‹
        if generalization_gap > 0.5:  # é˜ˆå€¼å¯è°ƒ
            print(f"âš ï¸  Epoch {epoch}: æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆï¼")
            print(f"   æ³›åŒ–é—´éš™: {generalization_gap:.4f}")
            print(f"   è¿‡æ‹Ÿåˆåˆ†æ•°: {overfitting_score:.4f}")
        
        # æ‰“å°ç›‘æ§ä¿¡æ¯
        print(f"Epoch {epoch}: Train={train_loss:.4f}, "
              f"Val={val_loss:.4f}, Test={test_loss:.4f}, "
              f"Gap={generalization_gap:.4f}")
    
    return history
```

### æ¨¡å‹é›†æˆæé«˜æ³›åŒ–

```python
class ModelEnsemble:
    """æ¨¡å‹é›†æˆä»¥æé«˜æ³›åŒ–èƒ½åŠ›"""
    
    def __init__(self, models):
        self.models = models
        
    def predict(self, x):
        """é›†æˆé¢„æµ‹"""
        predictions = []
        
        for model in self.models:
            model.eval()
            with torch.no_grad():
                pred = model(x)
                predictions.append(pred)
        
        # å¹³å‡é›†æˆ
        ensemble_pred = torch.stack(predictions).mean(dim=0)
        return ensemble_pred
    
    def predict_with_uncertainty(self, x):
        """å¸¦ä¸ç¡®å®šæ€§çš„é›†æˆé¢„æµ‹"""
        predictions = []
        
        for model in self.models:
            model.eval()
            with torch.no_grad():
                pred = F.softmax(model(x), dim=-1)
                predictions.append(pred)
        
        predictions = torch.stack(predictions)
        
        # å¹³å‡é¢„æµ‹
        mean_pred = predictions.mean(dim=0)
        
        # é¢„æµ‹ä¸ç¡®å®šæ€§ï¼ˆæ–¹å·®ï¼‰
        uncertainty = predictions.var(dim=0).mean(dim=-1)
        
        return mean_pred, uncertainty
```

## å°ç»“ä¸æ€è€ƒ

æœ¬èŠ‚ä»‹ç»äº†ç»Ÿè®¡å­¦ä¹ ç†è®ºçš„æ ¸å¿ƒæ¦‚å¿µåŠå…¶åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨ï¼š

1. **PACå­¦ä¹ æ¡†æ¶**ä¸ºç®—æ³•çš„å¯å­¦ä¹ æ€§æä¾›äº†ç†è®ºä¿è¯
2. **åå·®-æ–¹å·®åˆ†è§£**æ­ç¤ºäº†æ¨¡å‹æ€§èƒ½çš„æ ¹æœ¬æƒè¡¡
3. **æ­£åˆ™åŒ–æŠ€æœ¯**é€šè¿‡çº¦æŸæ¨¡å‹å¤æ‚åº¦æé«˜æ³›åŒ–èƒ½åŠ›
4. **æ³›åŒ–ç•Œç†è®º**æŒ‡å¯¼æ ·æœ¬å¤æ‚åº¦çš„ä¼°è®¡
5. **å®è·µç­–ç•¥**é›†æˆå¤šç§æŠ€æœ¯æé«˜æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½

**æ€è€ƒé¢˜**ï¼š
1. ä¸ºä»€ä¹ˆå¤§æ¨¡å‹åœ¨æœ‰é™æ•°æ®ä¸Šä»èƒ½è¡¨ç°è‰¯å¥½ï¼Ÿè¿™ä¸ä¼ ç»Ÿçš„åå·®-æ–¹å·®ç†è®ºæœ‰ä½•å†²çªï¼Ÿ
2. å¦‚ä½•ä»ç»Ÿè®¡å­¦ä¹ ç†è®ºçš„è§’åº¦ç†è§£é¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼çš„æœ‰æ•ˆæ€§ï¼Ÿ
3. ç°ä»£æ·±åº¦å­¦ä¹ ä¸­çš„"åŒé‡ä¸‹é™"ç°è±¡å¦‚ä½•ç”¨ç»Ÿè®¡å­¦ä¹ ç†è®ºè§£é‡Šï¼Ÿ

**ç¬¬ä¸€ç« æ€»ç»“**ï¼šè‡³æ­¤ï¼Œæˆ‘ä»¬å®Œæˆäº†æ·±åº¦å­¦ä¹ æ•°å­¦åŸºç¡€çš„å­¦ä¹ ï¼Œä¸ºç†è§£Transformeræ¶æ„å’Œè®­ç»ƒç®—æ³•å¥ å®šäº†åšå®çš„ç†è®ºåŸºç¡€ã€‚

**ä¸‹ç« é¢„å‘Š**ï¼šç¬¬äºŒç« å°†æ·±å…¥Transformerçš„æ ¸å¿ƒæ¶æ„ï¼Œä»æ•°å­¦è§’åº¦ç†è§£æ³¨æ„åŠ›æœºåˆ¶çš„å·¥ä½œåŸç†ã€‚

---

*ç»Ÿè®¡å­¦ä¹ ç†è®ºä¸ºæˆ‘ä»¬æä¾›äº†ç†è§£æ™ºèƒ½çš„ç»Ÿè®¡åŸºç¡€ï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬ä¸ºä»€ä¹ˆæœºå™¨èƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ï¼Œä»¥åŠå¦‚ä½•è®©å­¦ä¹ æ›´åŠ å¯é ã€‚* ğŸ“Š