# 03 åˆ†è¯ç­–ç•¥ä¸ä¿¡æ¯å‹ç¼©

> **ä»å­—ç¬¦åˆ°å­è¯ï¼šè¯­è¨€çš„ç¦»æ•£åŒ–ä¸ä¿¡æ¯è®ºä¼˜åŒ–**

## æ ¸å¿ƒæ€æƒ³

åˆ†è¯(Tokenization)æ˜¯è¯­è¨€æ¨¡å‹çš„ç¬¬ä¸€æ­¥ï¼Œå®ƒå°†è¿ç»­çš„æ–‡æœ¬è½¬æ¢ä¸ºç¦»æ•£çš„ç¬¦å·åºåˆ—ã€‚è¿™ä¸ªçœ‹ä¼¼ç®€å•çš„é¢„å¤„ç†æ­¥éª¤ï¼Œå®é™…ä¸Šæ·±åˆ»å½±å“ç€æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€è®¡ç®—æ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ã€‚

**å…³é”®æ´å¯Ÿ**ï¼š
- **ä¿¡æ¯å‹ç¼©**ï¼šå¥½çš„åˆ†è¯ç­–ç•¥èƒ½å¤Ÿç”¨æ›´å°‘çš„tokenè¡¨è¾¾æ›´å¤šçš„è¯­ä¹‰ä¿¡æ¯
- **è¯æ±‡å¹³è¡¡**ï¼šåœ¨è¯æ±‡è¡¨å¤§å°ä¸åºåˆ—é•¿åº¦ä¹‹é—´æ‰¾åˆ°æœ€ä¼˜å¹³è¡¡ç‚¹
- **è¯­è¨€ç»Ÿè®¡**ï¼šåˆ†è¯ç®—æ³•å®é™…ä¸Šæ˜¯åœ¨å­¦ä¹ è¯­è¨€çš„ç»Ÿè®¡ç»“æ„
- **OOVé—®é¢˜**ï¼šå­è¯åˆ†è§£å½»åº•è§£å†³äº†æœªç™»å½•è¯çš„é—®é¢˜

ä»ä¿¡æ¯è®ºè§’åº¦çœ‹ï¼Œåˆ†è¯æ˜¯åœ¨å¯»æ‰¾**æœ€ä¼˜çš„ç¼–ç æ–¹æ¡ˆ**ï¼Œä½¿å¾—æ–‡æœ¬çš„å¹³å‡ç¼–ç é•¿åº¦æœ€çŸ­ï¼ŒåŒæ—¶ä¿æŒè¯­ä¹‰çš„å®Œæ•´æ€§ã€‚

## 3.1 ä»å­—ç¬¦åˆ°è¯æ±‡çš„ç¼–ç æ¼”è¿›

### ç¼–ç ç­–ç•¥çš„ä¿¡æ¯è®ºåˆ†æ

**å­—ç¬¦çº§ç¼–ç **ï¼š
- è¯æ±‡è¡¨å¤§å°ï¼š~100ï¼ˆASCIIï¼‰æˆ–~65,000ï¼ˆUnicodeï¼‰
- åºåˆ—é•¿åº¦ï¼šå¾ˆé•¿ï¼ˆæ¯ä¸ªå­—ç¬¦ä¸€ä¸ªtokenï¼‰
- OOVç‡ï¼š0%ï¼ˆä»»ä½•å­—ç¬¦éƒ½èƒ½è¡¨ç¤ºï¼‰

**è¯çº§ç¼–ç **ï¼š
- è¯æ±‡è¡¨å¤§å°ï¼š~50,000-100,000
- åºåˆ—é•¿åº¦ï¼šè¾ƒçŸ­ï¼ˆæ¯ä¸ªè¯ä¸€ä¸ªtokenï¼‰
- OOVç‡ï¼šé«˜ï¼ˆæ–°è¯ã€å˜å½¢ã€é”™è¯¯ç­‰ï¼‰

**å­è¯ç¼–ç **ï¼š
- è¯æ±‡è¡¨å¤§å°ï¼šå¯é…ç½®ï¼ˆé€šå¸¸32K-64Kï¼‰
- åºåˆ—é•¿åº¦ï¼šé€‚ä¸­
- OOVç‡ï¼š0%ï¼ˆä»»ä½•è¯éƒ½èƒ½åˆ†è§£ï¼‰

```python
def compare_encoding_strategies(text_corpus, strategies):
    """æ¯”è¾ƒä¸åŒç¼–ç ç­–ç•¥çš„æ•ˆæœ"""
    
    results = {}
    
    for strategy_name, tokenizer in strategies.items():
        total_tokens = 0
        total_chars = 0
        vocab_usage = set()
        oov_count = 0
        
        for text in text_corpus:
            # ç¼–ç æ–‡æœ¬
            if strategy_name == 'char':
                tokens = list(text)
            elif strategy_name == 'word':
                tokens = text.split()
                # æ£€æŸ¥OOV
                for token in tokens:
                    if token not in tokenizer.vocab:
                        oov_count += 1
            elif strategy_name == 'subword':
                tokens = tokenizer.encode(text)
            
            total_tokens += len(tokens)
            total_chars += len(text)
            vocab_usage.update(tokens if strategy_name == 'char' else 
                             [str(t) for t in tokens])
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        compression_ratio = total_chars / total_tokens if total_tokens > 0 else 0
        vocab_coverage = len(vocab_usage)
        oov_rate = oov_count / total_tokens if total_tokens > 0 else 0
        
        results[strategy_name] = {
            'total_tokens': total_tokens,
            'total_chars': total_chars,
            'compression_ratio': compression_ratio,
            'vocab_coverage': vocab_coverage,
            'oov_rate': oov_rate
        }
        
        print(f"{strategy_name:10s}: tokens={total_tokens:,}, å‹ç¼©æ¯”={compression_ratio:.2f}, "
              f"è¯æ±‡è¦†ç›–={vocab_coverage:,}, OOVç‡={oov_rate:.2%}")
    
    return results

def analyze_information_content(text, tokenizer):
    """åˆ†ææ–‡æœ¬çš„ä¿¡æ¯å«é‡"""
    
    # ç¼–ç æ–‡æœ¬
    tokens = tokenizer.encode(text)
    
    # è®¡ç®—tokené¢‘ç‡
    token_counts = {}
    for token in tokens:
        token_counts[token] = token_counts.get(token, 0) + 1
    
    # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
    total_tokens = len(tokens)
    token_probs = {token: count/total_tokens for token, count in token_counts.items()}
    
    # è®¡ç®—ç†µ
    entropy = -sum(p * math.log2(p) for p in token_probs.values())
    
    # è®¡ç®—ç†è®ºæœ€å°ç¼–ç é•¿åº¦
    min_bits = entropy * total_tokens
    actual_bits = total_tokens * math.log2(len(tokenizer.vocab))
    
    print(f"ä¿¡æ¯è®ºåˆ†æ:")
    print(f"  æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
    print(f"  Tokenæ•°é‡: {total_tokens}")
    print(f"  å”¯ä¸€token: {len(token_counts)}")
    print(f"  ç†µ: {entropy:.4f} bits/token")
    print(f"  ç†è®ºæœ€å°ç¼–ç : {min_bits:.0f} bits")
    print(f"  å®é™…ç¼–ç : {actual_bits:.0f} bits")
    print(f"  ç¼–ç æ•ˆç‡: {min_bits/actual_bits:.2%}")
    
    return {
        'entropy': entropy,
        'compression_efficiency': min_bits/actual_bits,
        'vocab_utilization': len(token_counts)/len(tokenizer.vocab)
    }
```

## 3.2 BPEç®—æ³•çš„æ•°å­¦åŸç†

### å­—èŠ‚å¯¹ç¼–ç çš„ç»Ÿè®¡å­¦ä¹ 

**BPEç®—æ³•æ ¸å¿ƒæ€æƒ³**ï¼šè¿­ä»£åœ°åˆå¹¶æœ€é¢‘ç¹çš„å­—ç¬¦å¯¹ï¼Œç›´åˆ°è¾¾åˆ°ç›®æ ‡è¯æ±‡è¡¨å¤§å°ã€‚

**ç®—æ³•æ­¥éª¤**ï¼š
1. åˆå§‹åŒ–ï¼šæ¯ä¸ªå­—ç¬¦ä½œä¸ºåŸºæœ¬å•å…ƒ
2. ç»Ÿè®¡ï¼šè®¡ç®—æ‰€æœ‰ç›¸é‚»å­—ç¬¦å¯¹çš„é¢‘ç‡
3. åˆå¹¶ï¼šå°†æœ€é¢‘ç¹çš„å­—ç¬¦å¯¹åˆå¹¶ä¸ºæ–°çš„å­è¯
4. è¿­ä»£ï¼šé‡å¤æ­¥éª¤2-3ï¼Œç›´åˆ°è¾¾åˆ°ç›®æ ‡è¯æ±‡è¡¨å¤§å°

**æ•°å­¦è¡¨è¾¾**ï¼š
è®¾å½“å‰è¯æ±‡è¡¨ä¸º$V$ï¼Œæ–‡æœ¬è¯­æ–™ä¸º$C$ï¼Œåˆ™æ¯æ¬¡è¿­ä»£é€‰æ‹©çš„åˆå¹¶å¯¹ä¸ºï¼š
$$(s_1, s_2) = \arg\max_{(s_i, s_j)} \text{count}(s_i, s_j \text{ in } C)$$

```python
# MiniGPTä¸­BPEåˆ†è¯å™¨çš„æ ¸å¿ƒå®ç°åˆ†æ
class BPETokenizer:
    """BPEåˆ†è¯å™¨çš„æ•°å­¦å®ç°ä¸åˆ†æ"""
    
    def __init__(self, vocab_size=50000):
        self.vocab_size = vocab_size
        self.vocab = {}
        self.merges = {}
        self.word_freqs = {}
    
    def train(self, corpus):
        """è®­ç»ƒBPEåˆ†è¯å™¨"""
        
        print("=== BPEè®­ç»ƒè¿‡ç¨‹ ===")
        
        # 1. é¢„å¤„ç†ï¼šç»Ÿè®¡è¯é¢‘
        print("æ­¥éª¤1: ç»Ÿè®¡è¯é¢‘")
        for text in corpus:
            words = text.lower().split()
            for word in words:
                word_with_boundary = word + '</w>'  # æ·»åŠ è¯è¾¹ç•Œæ ‡è®°
                self.word_freqs[word_with_boundary] = self.word_freqs.get(word_with_boundary, 0) + 1
        
        print(f"  æ€»è¯æ±‡æ•°: {len(self.word_freqs)}")
        print(f"  æ€»tokenæ•°: {sum(self.word_freqs.values())}")
        
        # 2. åˆå§‹åŒ–ï¼šå­—ç¬¦çº§è¯æ±‡è¡¨
        print("\\næ­¥éª¤2: åˆå§‹åŒ–å­—ç¬¦çº§è¯æ±‡è¡¨")
        vocab = set()
        for word in self.word_freqs:
            for char in word:
                vocab.add(char)
        
        # è½¬æ¢ä¸ºå­—ç¬¦åºåˆ—è¡¨ç¤º
        word_splits = {}
        for word, freq in self.word_freqs.items():
            word_splits[word] = list(word)
        
        initial_vocab_size = len(vocab)
        print(f"  åˆå§‹è¯æ±‡è¡¨å¤§å°: {initial_vocab_size}")
        
        # 3. è¿­ä»£åˆå¹¶
        print("\\næ­¥éª¤3: è¿­ä»£åˆå¹¶æœ€é¢‘ç¹å­—ç¬¦å¯¹")
        merge_count = 0
        
        while len(vocab) < self.vocab_size:
            # ç»Ÿè®¡å­—ç¬¦å¯¹é¢‘ç‡
            pair_freqs = self._count_pairs(word_splits, self.word_freqs)
            
            if not pair_freqs:
                print("  æ²¡æœ‰æ›´å¤šå­—ç¬¦å¯¹å¯ä»¥åˆå¹¶")
                break
            
            # æ‰¾åˆ°æœ€é¢‘ç¹çš„å­—ç¬¦å¯¹
            best_pair = max(pair_freqs, key=pair_freqs.get)
            best_freq = pair_freqs[best_pair]
            
            # æ‰§è¡Œåˆå¹¶
            word_splits = self._merge_pair(best_pair, word_splits)
            self.merges[best_pair] = len(self.merges)
            
            # æ›´æ–°è¯æ±‡è¡¨
            new_subword = ''.join(best_pair)
            vocab.add(new_subword)
            
            merge_count += 1
            if merge_count % 1000 == 0 or merge_count <= 10:
                print(f"  åˆå¹¶ {merge_count:4d}: {best_pair} â†’ '{new_subword}' (é¢‘ç‡: {best_freq})")
        
        # 4. æ„å»ºæœ€ç»ˆè¯æ±‡è¡¨
        self.vocab = {token: i for i, token in enumerate(sorted(vocab))}
        
        print(f"\\nè®­ç»ƒå®Œæˆ:")
        print(f"  æœ€ç»ˆè¯æ±‡è¡¨å¤§å°: {len(self.vocab)}")
        print(f"  æ€»åˆå¹¶æ“ä½œ: {len(self.merges)}")
        
        return self
    
    def _count_pairs(self, word_splits, word_freqs):
        """ç»Ÿè®¡å­—ç¬¦å¯¹é¢‘ç‡"""
        pair_counts = {}
        
        for word, splits in word_splits.items():
            word_freq = word_freqs[word]
            
            # ç»Ÿè®¡è¯¥è¯ä¸­çš„æ‰€æœ‰ç›¸é‚»å­—ç¬¦å¯¹
            for i in range(len(splits) - 1):
                pair = (splits[i], splits[i + 1])
                pair_counts[pair] = pair_counts.get(pair, 0) + word_freq
        
        return pair_counts
    
    def _merge_pair(self, pair, word_splits):
        """åˆå¹¶æŒ‡å®šå­—ç¬¦å¯¹"""
        new_word_splits = {}
        
        for word, splits in word_splits.items():
            new_splits = []
            i = 0
            
            while i < len(splits):
                # æ£€æŸ¥æ˜¯å¦åŒ¹é…è¦åˆå¹¶çš„å­—ç¬¦å¯¹
                if (i < len(splits) - 1 and 
                    splits[i] == pair[0] and splits[i + 1] == pair[1]):
                    # åˆå¹¶å­—ç¬¦å¯¹
                    new_splits.append(pair[0] + pair[1])
                    i += 2
                else:
                    new_splits.append(splits[i])
                    i += 1
            
            new_word_splits[word] = new_splits
        
        return new_word_splits
    
    def encode(self, text):
        """ç¼–ç æ–‡æœ¬ä¸ºtokenåºåˆ—"""
        words = text.lower().split()
        encoded = []
        
        for word in words:
            word_with_boundary = word + '</w>'
            
            # åˆå§‹åŒ–ä¸ºå­—ç¬¦åºåˆ—
            word_tokens = list(word_with_boundary)
            
            # åº”ç”¨å­¦åˆ°çš„åˆå¹¶è§„åˆ™
            while len(word_tokens) > 1:
                # æ‰¾åˆ°å¯ä»¥åˆå¹¶çš„å­—ç¬¦å¯¹
                pairs = [(word_tokens[i], word_tokens[i + 1]) 
                        for i in range(len(word_tokens) - 1)]
                
                # æŒ‰åˆå¹¶ä¼˜å…ˆçº§æ’åº
                valid_pairs = [(pair, self.merges[pair]) for pair in pairs 
                              if pair in self.merges]
                
                if not valid_pairs:
                    break
                
                # é€‰æ‹©æœ€å…ˆå­¦åˆ°çš„åˆå¹¶ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
                best_pair = min(valid_pairs, key=lambda x: x[1])[0]
                
                # æ‰§è¡Œåˆå¹¶
                new_tokens = []
                i = 0
                while i < len(word_tokens):
                    if (i < len(word_tokens) - 1 and 
                        word_tokens[i] == best_pair[0] and 
                        word_tokens[i + 1] == best_pair[1]):
                        new_tokens.append(best_pair[0] + best_pair[1])
                        i += 2
                    else:
                        new_tokens.append(word_tokens[i])
                        i += 1
                
                word_tokens = new_tokens
            
            # è½¬æ¢ä¸ºè¯æ±‡è¡¨ID
            for token in word_tokens:
                if token in self.vocab:
                    encoded.append(self.vocab[token])
                else:
                    # å¤„ç†OOVï¼ˆç†è®ºä¸ŠBPEä¸åº”è¯¥æœ‰OOVï¼‰
                    encoded.append(self.vocab.get('<UNK>', 0))
        
        return encoded

def analyze_bpe_compression_efficiency():
    """åˆ†æBPEçš„å‹ç¼©æ•ˆç‡"""
    
    # åˆ›å»ºæµ‹è¯•è¯­æ–™
    test_corpus = [
        "the quick brown fox jumps over the lazy dog",
        "the dog is lazy and the fox is quick",
        "quick brown dogs and lazy foxes are jumping",
        "foxes jump quickly over lazy brown dogs"
    ]
    
    # è®­ç»ƒBPEåˆ†è¯å™¨
    bpe = BPETokenizer(vocab_size=100)
    bpe.train(test_corpus)
    
    # åˆ†æå‹ç¼©æ•ˆæœ
    print("\\n=== å‹ç¼©æ•ˆç‡åˆ†æ ===")
    
    original_chars = sum(len(text) for text in test_corpus)
    total_tokens = 0
    
    for text in test_corpus:
        tokens = bpe.encode(text)
        total_tokens += len(tokens)
        
        print(f"åŸæ–‡: '{text}'")
        print(f"ç¼–ç : {tokens}")
        print(f"é•¿åº¦: {len(text)} å­—ç¬¦ â†’ {len(tokens)} tokens")
        print()
    
    compression_ratio = original_chars / total_tokens
    print(f"æ€»ä½“å‹ç¼©æ¯”: {compression_ratio:.2f} å­—ç¬¦/token")
    print(f"å‹ç¼©æ•ˆç‡: {(1 - total_tokens/original_chars)*100:.1f}%")
    
    return bpe, compression_ratio
```

### ä¿¡æ¯è®ºè§†è§’ä¸‹çš„BPEä¼˜åŒ–

**å‹ç¼©æ•ˆç‡**ï¼šBPEè¯•å›¾æœ€å°åŒ–å¹³å‡ç¼–ç é•¿åº¦
$$L = \sum_{w \in V} P(w) \cdot |encode(w)|$$

å…¶ä¸­$P(w)$æ˜¯è¯$w$çš„æ¦‚ç‡ï¼Œ$|encode(w)|$æ˜¯ç¼–ç åçš„é•¿åº¦ã€‚

**è´ªå¿ƒç­–ç•¥çš„å±€é™æ€§**ï¼šBPEé‡‡ç”¨è´ªå¿ƒç­–ç•¥ï¼Œæ¯æ¬¡é€‰æ‹©é¢‘ç‡æœ€é«˜çš„å­—ç¬¦å¯¹ï¼Œä½†è¿™ä¸ä¸€å®šæ˜¯å…¨å±€æœ€ä¼˜çš„ã€‚

```python
def theoretical_vs_actual_compression():
    """ç†è®ºæœ€ä¼˜å‹ç¼©ä¸BPEå®é™…å‹ç¼©çš„æ¯”è¾ƒ"""
    
    # æ„é€ ç®€å•ä¾‹å­
    words = ['hello', 'world', 'hell', 'word', 'help', 'work']
    freqs = [100, 80, 60, 50, 40, 30]
    
    print("=== ç†è®ºæœ€ä¼˜ vs BPEå®é™…å‹ç¼© ===")
    
    # 1. è®¡ç®—ç†è®ºæœ€ä¼˜ç¼–ç ï¼ˆHuffmanç¼–ç æ€æƒ³ï¼‰
    total_freq = sum(freqs)
    word_probs = [f/total_freq for f in freqs]
    
    # è®¡ç®—ç†µï¼ˆç†è®ºæœ€ä¼˜ç¼–ç é•¿åº¦ä¸‹ç•Œï¼‰
    entropy = -sum(p * math.log2(p) for p in word_probs)
    theoretical_bits = entropy * total_freq
    
    print(f"ç†è®ºåˆ†æ:")
    print(f"  æ€»è¯é¢‘: {total_freq}")
    print(f"  ç†µ: {entropy:.4f} bits/word")
    print(f"  ç†è®ºæœ€ä¼˜ç¼–ç : {theoretical_bits:.0f} bits")
    
    # 2. BPEå®é™…ç¼–ç 
    # ç®€åŒ–çš„BPEæ¨¡æ‹Ÿ
    char_vocab = set(''.join(words))
    print(f"\\nBPEåˆ†æ:")
    print(f"  åˆå§‹å­—ç¬¦è¯æ±‡: {sorted(char_vocab)}")
    
    # ç»Ÿè®¡å­—ç¬¦å¯¹é¢‘ç‡
    pair_freqs = {}
    for word, freq in zip(words, freqs):
        for i in range(len(word) - 1):
            pair = (word[i], word[i+1])
            pair_freqs[pair] = pair_freqs.get(pair, 0) + freq
    
    print(f"  å­—ç¬¦å¯¹é¢‘ç‡: {sorted(pair_freqs.items(), key=lambda x: x[1], reverse=True)}")
    
    # æ¨¡æ‹Ÿå‡ æ¬¡åˆå¹¶
    merges = []
    current_splits = {word: list(word) for word in words}
    
    for merge_step in range(3):  # è¿›è¡Œ3æ¬¡åˆå¹¶
        if not pair_freqs:
            break
            
        # é€‰æ‹©æœ€é¢‘ç¹çš„å­—ç¬¦å¯¹
        best_pair = max(pair_freqs, key=pair_freqs.get)
        best_freq = pair_freqs[best_pair]
        merges.append((best_pair, best_freq))
        
        print(f"  åˆå¹¶æ­¥éª¤ {merge_step + 1}: {best_pair} (é¢‘ç‡: {best_freq})")
        
        # æ›´æ–°åˆ†è¯ç»“æœ
        for word in current_splits:
            splits = current_splits[word]
            new_splits = []
            i = 0
            while i < len(splits):
                if (i < len(splits) - 1 and 
                    splits[i] == best_pair[0] and splits[i + 1] == best_pair[1]):
                    new_splits.append(best_pair[0] + best_pair[1])
                    i += 2
                else:
                    new_splits.append(splits[i])
                    i += 1
            current_splits[word] = new_splits
        
        # é‡æ–°è®¡ç®—å­—ç¬¦å¯¹é¢‘ç‡
        pair_freqs = {}
        for word, freq in zip(words, freqs):
            splits = current_splits[word]
            for i in range(len(splits) - 1):
                pair = (splits[i], splits[i + 1])
                pair_freqs[pair] = pair_freqs.get(pair, 0) + freq
    
    # è®¡ç®—BPEç¼–ç é•¿åº¦
    bpe_tokens = sum(len(current_splits[word]) * freq for word, freq in zip(words, freqs))
    
    print(f"\\næœ€ç»ˆåˆ†è¯ç»“æœ:")
    for word, freq in zip(words, freqs):
        splits = current_splits[word]
        print(f"  '{word}' â†’ {splits} ({len(splits)} tokens, é¢‘ç‡: {freq})")
    
    print(f"\\nBPEæ€»tokenæ•°: {bpe_tokens}")
    print(f"å¹³å‡æ¯è¯tokenæ•°: {bpe_tokens/total_freq:.2f}")
    
    # æ•ˆç‡æ¯”è¾ƒ
    if theoretical_bits > 0:
        bpe_efficiency = theoretical_bits / (bpe_tokens * math.log2(len(char_vocab) + len(merges)))
        print(f"BPEç¼–ç æ•ˆç‡: {bpe_efficiency:.2%}")
    
    return current_splits, merges
```

## 3.3 è¯æ±‡è¡¨å¤§å°çš„æƒè¡¡åˆ†æ

### è®¡ç®—å¤æ‚åº¦ä¸è¡¨è¾¾èƒ½åŠ›çš„å¹³è¡¡

**è¯æ±‡è¡¨å¤§å°å¯¹æ€§èƒ½çš„å½±å“**ï¼š

1. **è®¡ç®—å¤æ‚åº¦**ï¼š
   - åµŒå…¥å±‚å‚æ•°ï¼š$O(|V| \times d_{model})$
   - è¾“å‡ºå±‚å‚æ•°ï¼š$O(|V| \times d_{model})$
   - Softmaxè®¡ç®—ï¼š$O(|V|)$

2. **åºåˆ—é•¿åº¦**ï¼š
   - å¤§è¯æ±‡è¡¨ â†’ çŸ­åºåˆ— â†’ å°‘çš„æ³¨æ„åŠ›è®¡ç®—
   - å°è¯æ±‡è¡¨ â†’ é•¿åºåˆ— â†’ å¤šçš„æ³¨æ„åŠ›è®¡ç®—

3. **è¡¨è¾¾èƒ½åŠ›**ï¼š
   - å¤§è¯æ±‡è¡¨ â†’ æ›´ç²¾ç¡®çš„è¯­ä¹‰è¡¨è¾¾
   - å°è¯æ±‡è¡¨ â†’ æ›´å¤šçš„ç»„åˆè¡¨è¾¾

```python
def analyze_vocab_size_tradeoffs(text_corpus, vocab_sizes=[1000, 5000, 10000, 30000, 50000]):
    """åˆ†æä¸åŒè¯æ±‡è¡¨å¤§å°çš„æƒè¡¡"""
    
    results = {}
    
    for vocab_size in vocab_sizes:
        print(f"\\n=== è¯æ±‡è¡¨å¤§å°: {vocab_size} ===")
        
        # è®­ç»ƒBPEåˆ†è¯å™¨
        bpe = BPETokenizer(vocab_size=vocab_size)
        bpe.train(text_corpus)
        
        # ç¼–ç è¯­æ–™åº“
        total_tokens = 0
        total_chars = 0
        unique_tokens = set()
        
        for text in text_corpus:
            tokens = bpe.encode(text)
            total_tokens += len(tokens)
            total_chars += len(text)
            unique_tokens.update(tokens)
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        compression_ratio = total_chars / total_tokens
        vocab_utilization = len(unique_tokens) / vocab_size
        
        # ä¼°ç®—è®¡ç®—æˆæœ¬
        embedding_params = vocab_size * 512  # å‡è®¾d_model=512
        output_params = vocab_size * 512
        total_vocab_params = embedding_params + output_params
        
        # ä¼°ç®—æ³¨æ„åŠ›è®¡ç®—æˆæœ¬ï¼ˆä¸åºåˆ—é•¿åº¦å¹³æ–¹æˆæ­£æ¯”ï¼‰
        avg_seq_len = total_tokens / len(text_corpus)
        attention_cost = avg_seq_len ** 2
        
        results[vocab_size] = {
            'compression_ratio': compression_ratio,
            'vocab_utilization': vocab_utilization,
            'avg_seq_len': avg_seq_len,
            'vocab_params': total_vocab_params,
            'attention_cost': attention_cost,
            'total_cost': total_vocab_params + attention_cost  # ç®€åŒ–çš„æ€»æˆæœ¬
        }
        
        print(f"  å‹ç¼©æ¯”: {compression_ratio:.2f} å­—ç¬¦/token")
        print(f"  è¯æ±‡åˆ©ç”¨ç‡: {vocab_utilization:.2%}")
        print(f"  å¹³å‡åºåˆ—é•¿åº¦: {avg_seq_len:.2f}")
        print(f"  è¯æ±‡å‚æ•°é‡: {total_vocab_params:,}")
        print(f"  æ³¨æ„åŠ›æˆæœ¬: {attention_cost:.0f}")
    
    # å¯»æ‰¾æœ€ä¼˜è¯æ±‡è¡¨å¤§å°
    print("\\n=== æƒè¡¡åˆ†æ ===")
    optimal_vocab = min(results.keys(), key=lambda v: results[v]['total_cost'])
    print(f"æœ€ä¼˜è¯æ±‡è¡¨å¤§å°ï¼ˆåŸºäºç®€åŒ–æˆæœ¬æ¨¡å‹ï¼‰: {optimal_vocab}")
    
    # ç»˜åˆ¶æƒè¡¡æ›²çº¿
    import matplotlib.pyplot as plt
    
    vocab_sizes_list = list(results.keys())
    compression_ratios = [results[v]['compression_ratio'] for v in vocab_sizes_list]
    vocab_utilizations = [results[v]['vocab_utilization'] for v in vocab_sizes_list]
    avg_seq_lens = [results[v]['avg_seq_len'] for v in vocab_sizes_list]
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))
    
    # å‹ç¼©æ¯”
    ax1.plot(vocab_sizes_list, compression_ratios, 'b-o')
    ax1.set_xlabel('è¯æ±‡è¡¨å¤§å°')
    ax1.set_ylabel('å‹ç¼©æ¯” (å­—ç¬¦/token)')
    ax1.set_title('å‹ç¼©æ•ˆç‡')
    ax1.grid(True)
    
    # è¯æ±‡åˆ©ç”¨ç‡
    ax2.plot(vocab_sizes_list, vocab_utilizations, 'r-o')
    ax2.set_xlabel('è¯æ±‡è¡¨å¤§å°')
    ax2.set_ylabel('è¯æ±‡åˆ©ç”¨ç‡')
    ax2.set_title('è¯æ±‡åˆ©ç”¨æ•ˆç‡')
    ax2.grid(True)
    
    # åºåˆ—é•¿åº¦
    ax3.plot(vocab_sizes_list, avg_seq_lens, 'g-o')
    ax3.set_xlabel('è¯æ±‡è¡¨å¤§å°')
    ax3.set_ylabel('å¹³å‡åºåˆ—é•¿åº¦')
    ax3.set_title('åºåˆ—é•¿åº¦å½±å“')
    ax3.grid(True)
    
    # æˆæœ¬å¯¹æ¯”
    vocab_params = [results[v]['vocab_params'] for v in vocab_sizes_list]
    attention_costs = [results[v]['attention_cost'] for v in vocab_sizes_list]
    
    ax4.plot(vocab_sizes_list, vocab_params, 'purple', label='è¯æ±‡å‚æ•°æˆæœ¬')
    ax4.plot(vocab_sizes_list, attention_costs, 'orange', label='æ³¨æ„åŠ›è®¡ç®—æˆæœ¬')
    ax4.set_xlabel('è¯æ±‡è¡¨å¤§å°')
    ax4.set_ylabel('è®¡ç®—æˆæœ¬')
    ax4.set_title('è®¡ç®—æˆæœ¬æƒè¡¡')
    ax4.legend()
    ax4.grid(True)
    
    plt.tight_layout()
    plt.show()
    
    return results

def memory_and_compute_analysis(d_model=512, seq_len=1024, batch_size=32):
    """è¯¦ç»†çš„å†…å­˜å’Œè®¡ç®—åˆ†æ"""
    
    vocab_sizes = [8000, 16000, 32000, 64000]
    
    print("=== å†…å­˜å’Œè®¡ç®—è¯¦ç»†åˆ†æ ===")
    print(f"æ¨¡å‹é…ç½®: d_model={d_model}, seq_len={seq_len}, batch_size={batch_size}")
    print()
    
    for vocab_size in vocab_sizes:
        print(f"è¯æ±‡è¡¨å¤§å°: {vocab_size:,}")
        
        # 1. å‚æ•°å†…å­˜
        embedding_params = vocab_size * d_model * 4  # 4 bytes per float32
        output_params = vocab_size * d_model * 4
        total_vocab_memory = embedding_params + output_params
        
        # 2. æ¿€æ´»å†…å­˜
        embedding_activations = batch_size * seq_len * d_model * 4
        output_activations = batch_size * seq_len * vocab_size * 4
        
        # 3. è®¡ç®—é‡ï¼ˆFLOPsï¼‰
        embedding_flops = batch_size * seq_len * d_model
        output_flops = batch_size * seq_len * d_model * vocab_size
        softmax_flops = batch_size * seq_len * vocab_size * 3  # exp + sum + div
        
        print(f"  å‚æ•°å†…å­˜:")
        print(f"    åµŒå…¥å±‚: {embedding_params/1e6:.1f} MB")
        print(f"    è¾“å‡ºå±‚: {output_params/1e6:.1f} MB")
        print(f"    æ€»è®¡: {total_vocab_memory/1e6:.1f} MB")
        
        print(f"  æ¿€æ´»å†…å­˜:")
        print(f"    åµŒå…¥æ¿€æ´»: {embedding_activations/1e6:.1f} MB")
        print(f"    è¾“å‡ºæ¿€æ´»: {output_activations/1e6:.1f} MB")
        
        print(f"  è®¡ç®—é‡:")
        print(f"    åµŒå…¥å±‚: {embedding_flops/1e9:.2f} GFLOPs")
        print(f"    è¾“å‡ºå±‚: {output_flops/1e9:.2f} GFLOPs")
        print(f"    Softmax: {softmax_flops/1e9:.2f} GFLOPs")
        print(f"    è¯æ±‡ç›¸å…³æ€»è®¡: {(output_flops + softmax_flops)/1e9:.2f} GFLOPs")
        print()
    
    # æ³¨æ„åŠ›è®¡ç®—æˆæœ¬ï¼ˆä¸è¯æ±‡è¡¨å¤§å°æ— å…³ï¼‰
    attention_memory = batch_size * seq_len * seq_len * 4  # attention weights
    attention_flops = batch_size * seq_len * seq_len * d_model * 4  # Q@K, softmax, @V, proj
    
    print("æ³¨æ„åŠ›æœºåˆ¶ï¼ˆä¸è¯æ±‡è¡¨å¤§å°æ— å…³ï¼‰:")
    print(f"  å†…å­˜: {attention_memory/1e6:.1f} MB")
    print(f"  è®¡ç®—é‡: {attention_flops/1e9:.2f} GFLOPs")
```

## 3.4 å¤šè¯­è¨€ä¸ç‰¹æ®Štokenå¤„ç†

### å­—ç¬¦é›†ä¸ç¼–ç æ–¹æ¡ˆ

**Unicodeæ”¯æŒ**ï¼šç°ä»£åˆ†è¯å™¨éœ€è¦å¤„ç†å¤šç§è¯­è¨€å’Œå­—ç¬¦é›†

**ç‰¹æ®Štoken**ï¼š
- `<PAD>`ï¼šå¡«å……tokenï¼Œç”¨äºæ‰¹é‡å¤„ç†
- `<UNK>`ï¼šæœªçŸ¥tokenï¼Œå¤„ç†OOVè¯
- `<BOS>`/`<EOS>`ï¼šåºåˆ—å¼€å§‹/ç»“æŸæ ‡è®°
- `<SEP>`ï¼šåˆ†éš”ç¬¦tokenï¼Œç”¨äºå¤šæ–‡æ¡£ä»»åŠ¡

```python
class MultilingualBPETokenizer:
    """æ”¯æŒå¤šè¯­è¨€çš„BPEåˆ†è¯å™¨"""
    
    def __init__(self, vocab_size=50000, special_tokens=None):
        self.vocab_size = vocab_size
        
        # å®šä¹‰ç‰¹æ®Štoken
        if special_tokens is None:
            special_tokens = ['<PAD>', '<UNK>', '<BOS>', '<EOS>', '<SEP>']
        
        self.special_tokens = special_tokens
        self.special_token_ids = {token: i for i, token in enumerate(special_tokens)}
        
        # ä¿ç•™è¯æ±‡è¡¨ç©ºé—´ç»™ç‰¹æ®Štoken
        self.regular_vocab_size = vocab_size - len(special_tokens)
        
    def preprocess_multilingual(self, text):
        """å¤šè¯­è¨€æ–‡æœ¬é¢„å¤„ç†"""
        
        # 1. Unicodeæ ‡å‡†åŒ–
        text = unicodedata.normalize('NFKC', text)
        
        # 2. è¯­è¨€æ£€æµ‹å’Œå¤„ç†
        languages = self._detect_languages(text)
        
        # 3. è¯­è¨€ç‰¹å®šçš„é¢„å¤„ç†
        processed_segments = []
        
        for segment, lang in self._segment_by_language(text, languages):
            if lang == 'zh':  # ä¸­æ–‡
                # ä¸­æ–‡åˆ†è¯é¢„å¤„ç†
                segment = self._preprocess_chinese(segment)
            elif lang == 'ar':  # é˜¿æ‹‰ä¼¯è¯­
                # é˜¿æ‹‰ä¼¯è¯­é¢„å¤„ç†ï¼ˆä»å³åˆ°å·¦ï¼‰
                segment = self._preprocess_arabic(segment)
            elif lang == 'ja':  # æ—¥è¯­
                # æ—¥è¯­é¢„å¤„ç†ï¼ˆæ··åˆå­—ç¬¦é›†ï¼‰
                segment = self._preprocess_japanese(segment)
            
            processed_segments.append(segment)
        
        return ' '.join(processed_segments)
    
    def _detect_languages(self, text):
        """ç®€åŒ–çš„è¯­è¨€æ£€æµ‹"""
        languages = set()
        
        for char in text:
            # ä¸­æ–‡å­—ç¬¦
            if '\u4e00' <= char <= '\u9fff':
                languages.add('zh')
            # é˜¿æ‹‰ä¼¯å­—ç¬¦
            elif '\u0600' <= char <= '\u06ff':
                languages.add('ar')
            # æ—¥æ–‡å¹³å‡å/ç‰‡å‡å
            elif '\u3040' <= char <= '\u309f' or '\u30a0' <= char <= '\u30ff':
                languages.add('ja')
            # é»˜è®¤æ‹‰ä¸å­—ç¬¦
            else:
                languages.add('en')
        
        return list(languages)
    
    def _segment_by_language(self, text, languages):
        """æŒ‰è¯­è¨€åˆ†å‰²æ–‡æœ¬"""
        # ç®€åŒ–å®ç°ï¼šè¿”å›æ•´ä¸ªæ–‡æœ¬å’Œä¸»è¦è¯­è¨€
        main_lang = languages[0] if languages else 'en'
        return [(text, main_lang)]
    
    def _preprocess_chinese(self, text):
        """ä¸­æ–‡é¢„å¤„ç†"""
        # å¤„ç†ä¸­æ–‡ç‰¹å®šçš„å­—ç¬¦å’Œæ ‡ç‚¹
        # è¿™é‡Œç®€åŒ–å¤„ç†
        return text
    
    def _preprocess_arabic(self, text):
        """é˜¿æ‹‰ä¼¯è¯­é¢„å¤„ç†"""
        # å¤„ç†é˜¿æ‹‰ä¼¯è¯­çš„ä»å³åˆ°å·¦ä¹¦å†™
        return text
    
    def _preprocess_japanese(self, text):
        """æ—¥è¯­é¢„å¤„ç†"""
        # å¤„ç†å¹³å‡åã€ç‰‡å‡åã€æ±‰å­—æ··åˆ
        return text
    
    def handle_special_cases(self, text):
        """å¤„ç†ç‰¹æ®Šæƒ…å†µ"""
        
        # 1. æ•°å­—å¤„ç†
        # å°†è¿ç»­æ•°å­—æ›¿æ¢ä¸ºç‰¹æ®Šæ ‡è®°
        import re
        text = re.sub(r'\\d+', '<NUM>', text)
        
        # 2. URLå’Œé‚®ç®±å¤„ç†
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '<URL>', text)
        text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '<EMAIL>', text)
        
        # 3. é•¿é‡å¤å­—ç¬¦å¤„ç†
        # å°†é•¿é‡å¤å­—ç¬¦å‹ç¼©
        text = re.sub(r'(.)\\1{3,}', r'\\1\\1\\1', text)
        
        return text

def analyze_tokenization_quality(tokenizer, test_texts, languages):
    """åˆ†æåˆ†è¯è´¨é‡"""
    
    results = {}
    
    for lang, texts in zip(languages, test_texts):
        print(f"\\n=== {lang.upper()} è¯­è¨€åˆ†è¯è´¨é‡åˆ†æ ===")
        
        total_chars = 0
        total_tokens = 0
        subword_ratios = []
        
        for text in texts:
            # é¢„å¤„ç†
            processed_text = tokenizer.preprocess_multilingual(text)
            processed_text = tokenizer.handle_special_cases(processed_text)
            
            # åˆ†è¯
            tokens = tokenizer.encode(processed_text)
            
            total_chars += len(text)
            total_tokens += len(tokens)
            
            # åˆ†æå­è¯æ¯”ä¾‹
            word_count = len(text.split())
            subword_ratio = len(tokens) / word_count if word_count > 0 else 0
            subword_ratios.append(subword_ratio)
            
            print(f"åŸæ–‡: {text[:50]}...")
            print(f"Tokenæ•°: {len(tokens)}, å­—ç¬¦æ•°: {len(text)}, å­è¯æ¯”: {subword_ratio:.2f}")
        
        # ç»Ÿè®¡ç»“æœ
        avg_compression = total_chars / total_tokens
        avg_subword_ratio = np.mean(subword_ratios)
        
        results[lang] = {
            'compression_ratio': avg_compression,
            'subword_ratio': avg_subword_ratio,
            'total_tokens': total_tokens,
            'total_chars': total_chars
        }
        
        print(f"\\n{lang} è¯­è¨€ç»Ÿè®¡:")
        print(f"  å¹³å‡å‹ç¼©æ¯”: {avg_compression:.2f} å­—ç¬¦/token")
        print(f"  å¹³å‡å­è¯æ¯”: {avg_subword_ratio:.2f} token/è¯")
    
    return results

def cross_lingual_evaluation():
    """è·¨è¯­è¨€è¯„ä¼°"""
    
    # å¤šè¯­è¨€æµ‹è¯•æ•°æ®
    multilingual_texts = {
        'en': [
            "The quick brown fox jumps over the lazy dog.",
            "Natural language processing is fascinating.",
            "Machine learning models require lots of data."
        ],
        'zh': [
            "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ã€‚",
            "æœºå™¨å­¦ä¹ æ¨¡å‹éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ã€‚",
            "æ·±åº¦å­¦ä¹ åœ¨è¿‘å¹´æ¥å–å¾—äº†é‡å¤§çªç ´ã€‚"
        ],
        'ja': [
            "è‡ªç„¶è¨€èªå‡¦ç†ã¯äººå·¥çŸ¥èƒ½ã®é‡è¦ãªåˆ†é‡ã§ã™ã€‚",
            "æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¯å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ã€‚",
            "æ·±å±¤å­¦ç¿’ã¯è¿‘å¹´å¤§ããªé€²æ­©ã‚’é‚ã’ã¦ã„ã¾ã™ã€‚"
        ],
        'ar': [
            "Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù‡ÙŠ ÙØ±Ø¹ Ù…Ù‡Ù… Ù…Ù† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.",
            "Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ØªØªØ·Ù„Ø¨ ÙƒÙ…ÙŠØ§Øª ÙƒØ¨ÙŠØ±Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.",
            "Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ø­Ù‚Ù‚ Ø§Ø®ØªØ±Ø§Ù‚Ø§Øª ÙƒØ¨ÙŠØ±Ø© ÙÙŠ Ø§Ù„Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø£Ø®ÙŠØ±Ø©."
        ]
    }
    
    # åˆ›å»ºå¤šè¯­è¨€åˆ†è¯å™¨
    multilingual_tokenizer = MultilingualBPETokenizer(vocab_size=50000)
    
    # åˆå¹¶æ‰€æœ‰è¯­è¨€çš„æ–‡æœ¬è¿›è¡Œè®­ç»ƒ
    all_texts = []
    for lang_texts in multilingual_texts.values():
        all_texts.extend(lang_texts)
    
    # è®­ç»ƒåˆ†è¯å™¨
    multilingual_tokenizer.train(all_texts)
    
    # è¯„ä¼°å„è¯­è¨€çš„åˆ†è¯è´¨é‡
    results = analyze_tokenization_quality(
        multilingual_tokenizer,
        list(multilingual_texts.values()),
        list(multilingual_texts.keys())
    )
    
    # è·¨è¯­è¨€ä¸€è‡´æ€§åˆ†æ
    print("\\n=== è·¨è¯­è¨€ä¸€è‡´æ€§åˆ†æ ===")
    compression_ratios = [results[lang]['compression_ratio'] for lang in results]
    consistency_score = 1 - (max(compression_ratios) - min(compression_ratios)) / np.mean(compression_ratios)
    
    print(f"å‹ç¼©æ¯”ä¸€è‡´æ€§è¯„åˆ†: {consistency_score:.3f}")
    print("(1.0 = å®Œå…¨ä¸€è‡´, 0.0 = å®Œå…¨ä¸ä¸€è‡´)")
    
    return results, multilingual_tokenizer
```

## 3.5 å®è·µï¼šMiniGPTä¸­çš„åˆ†è¯å®ç°

### ä¸MiniGPTä»£ç çš„å¯¹åº”åˆ†æ

```python
# MiniGPTåˆ†è¯å™¨å®ç°è§£æ (src/tokenizer/bpe_tokenizer.py)
def analyze_minigpt_tokenizer():
    """åˆ†æMiniGPTä¸­çš„åˆ†è¯å™¨å®ç°"""
    
    print("=== MiniGPTåˆ†è¯å™¨ä»£ç è§£æ ===")
    
    # ä»MiniGPTæºç ä¸­åˆ†æå…³é”®ç»„ä»¶
    tokenizer_components = {
        'vocab_construction': {
            'description': 'è¯æ±‡è¡¨æ„å»ºè¿‡ç¨‹',
            'key_methods': ['build_vocab', 'add_special_tokens'],
            'optimization': 'é¢‘ç‡ç»Ÿè®¡å’Œåˆå¹¶ä¼˜åŒ–'
        },
        'encoding_process': {
            'description': 'æ–‡æœ¬ç¼–ç è¿‡ç¨‹',
            'key_methods': ['encode', 'encode_batch'],
            'optimization': 'æ‰¹é‡å¤„ç†å’Œç¼“å­˜æœºåˆ¶'
        },
        'decoding_process': {
            'description': 'åºåˆ—è§£ç è¿‡ç¨‹', 
            'key_methods': ['decode', 'decode_batch'],
            'optimization': 'ç‰¹æ®Štokenå¤„ç†å’Œè¾¹ç•Œæ£€æµ‹'
        },
        'serialization': {
            'description': 'æ¨¡å‹åºåˆ—åŒ–',
            'key_methods': ['save', 'load'],
            'optimization': 'è¯æ±‡è¡¨å’Œåˆå¹¶è§„åˆ™çš„å‹ç¼©å­˜å‚¨'
        }
    }
    
    for component, details in tokenizer_components.items():
        print(f"\\n{component.upper()}:")
        print(f"  æè¿°: {details['description']}")
        print(f"  å…³é”®æ–¹æ³•: {', '.join(details['key_methods'])}")
        print(f"  ä¼˜åŒ–ç­–ç•¥: {details['optimization']}")
    
    # æ€§èƒ½åŸºå‡†æµ‹è¯•
    print("\\n=== æ€§èƒ½åŸºå‡†æµ‹è¯• ===")
    
    # æ¨¡æ‹Ÿä¸åŒå¤§å°çš„æ–‡æœ¬å¤„ç†
    text_sizes = [1000, 10000, 100000]  # å­—ç¬¦æ•°
    vocab_sizes = [8000, 16000, 32000]
    
    for vocab_size in vocab_sizes:
        print(f"\\nè¯æ±‡è¡¨å¤§å°: {vocab_size}")
        
        for text_size in text_sizes:
            # ç”Ÿæˆæµ‹è¯•æ–‡æœ¬
            test_text = "hello world " * (text_size // 12)
            
            # æ¨¡æ‹Ÿç¼–ç æ—¶é—´
            start_time = time.time()
            
            # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„ç¼–ç æ¨¡æ‹Ÿ
            # å®é™…MiniGPTå®ç°ä¼šæ›´å¤æ‚
            tokens = simple_bpe_encode(test_text, vocab_size)
            
            encoding_time = time.time() - start_time
            
            # è®¡ç®—ååé‡
            throughput = len(test_text) / encoding_time if encoding_time > 0 else float('inf')
            
            print(f"  æ–‡æœ¬å¤§å° {text_size:6d}: "
                  f"ç¼–ç æ—¶é—´ {encoding_time*1000:.2f}ms, "
                  f"ååé‡ {throughput:.0f} å­—ç¬¦/ç§’")

def simple_bpe_encode(text, vocab_size):
    """ç®€åŒ–çš„BPEç¼–ç å®ç°ï¼ˆç”¨äºæ€§èƒ½æµ‹è¯•ï¼‰"""
    # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…å®ç°ä¼šæ›´å¤æ‚
    words = text.split()
    tokens = []
    
    for word in words:
        # ç®€å•çš„å­è¯åˆ†è§£
        if len(word) <= 4:
            tokens.append(word)
        else:
            # åˆ†è§£ä¸º4å­—ç¬¦çš„å­è¯
            for i in range(0, len(word), 4):
                tokens.append(word[i:i+4])
    
    return tokens

def integration_test_with_model():
    """ä¸æ¨¡å‹é›†æˆæµ‹è¯•"""
    
    print("=== åˆ†è¯å™¨ä¸æ¨¡å‹é›†æˆæµ‹è¯• ===")
    
    # åˆ›å»ºæµ‹è¯•åˆ†è¯å™¨
    tokenizer = BPETokenizer(vocab_size=1000)
    
    # è®­ç»ƒæ•°æ®
    train_texts = [
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ã€‚",
        "æ·±åº¦å­¦ä¹ æ¨¡å‹éœ€è¦å¤§é‡æ•°æ®è®­ç»ƒã€‚",
        "è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨å¹¿æ³›ã€‚"
    ]
    
    tokenizer.train(train_texts)
    
    # æµ‹è¯•ä¸åŒé•¿åº¦çš„æ–‡æœ¬
    test_cases = [
        "çŸ­æ–‡æœ¬",
        "è¿™æ˜¯ä¸€ä¸ªä¸­ç­‰é•¿åº¦çš„æµ‹è¯•æ–‡æœ¬ï¼ŒåŒ…å«äº†å¤šç§è¯æ±‡å’Œè¡¨è¾¾æ–¹å¼ã€‚",
        "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„æµ‹è¯•æ–‡æœ¬ï¼Œ" * 20 + "ç”¨æ¥æµ‹è¯•åˆ†è¯å™¨åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶çš„æ€§èƒ½å’Œç¨³å®šæ€§ã€‚"
    ]
    
    for i, text in enumerate(test_cases):
        print(f"\\næµ‹è¯•ç”¨ä¾‹ {i+1}:")
        print(f"åŸæ–‡é•¿åº¦: {len(text)} å­—ç¬¦")
        
        # ç¼–ç 
        tokens = tokenizer.encode(text)
        print(f"Tokenæ•°é‡: {len(tokens)}")
        print(f"å‹ç¼©æ¯”: {len(text)/len(tokens):.2f}")
        
        # è§£ç éªŒè¯
        decoded_text = tokenizer.decode(tokens)
        print(f"è§£ç ä¸€è‡´æ€§: {'âœ“' if decoded_text.replace(' ', '') == text.replace(' ', '') else 'âœ—'}")
        
        # å†…å­˜ä½¿ç”¨ä¼°è®¡
        token_memory = len(tokens) * 4  # å‡è®¾æ¯ä¸ªtoken ID 4å­—èŠ‚
        original_memory = len(text.encode('utf-8'))
        memory_efficiency = original_memory / token_memory
        
        print(f"å†…å­˜æ•ˆç‡: {memory_efficiency:.2f} (åŸæ–‡/tokenå†…å­˜)")
```

## å°ç»“ä¸æ€è€ƒ

æœ¬èŠ‚æ·±å…¥æ¢è®¨äº†åˆ†è¯ç­–ç•¥ä¸ä¿¡æ¯å‹ç¼©ï¼š

1. **ç¼–ç æ¼”è¿›**ï¼šä»å­—ç¬¦çº§åˆ°å­è¯çº§ç¼–ç çš„ä¿¡æ¯è®ºä¼˜åŒ–
2. **BPEç®—æ³•**ï¼šé€šè¿‡ç»Ÿè®¡å­¦ä¹ å®ç°æœ€ä¼˜çš„å­—ç¬¦å¯¹åˆå¹¶
3. **è¯æ±‡è¡¨æƒè¡¡**ï¼šåœ¨è®¡ç®—æˆæœ¬å’Œè¡¨è¾¾èƒ½åŠ›ä¹‹é—´æ‰¾åˆ°å¹³è¡¡
4. **å¤šè¯­è¨€å¤„ç†**ï¼šè·¨è¯­è¨€åˆ†è¯çš„æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆ

**å…³é”®æ´å¯Ÿ**ï¼š
- åˆ†è¯æ˜¯è¯­è¨€çš„ä¿¡æ¯å‹ç¼©é—®é¢˜
- BPEé€šè¿‡è´ªå¿ƒç­–ç•¥è¿‘ä¼¼æœ€ä¼˜ç¼–ç 
- è¯æ±‡è¡¨å¤§å°éœ€è¦ç»¼åˆè€ƒè™‘å¤šä¸ªå› ç´ 
- ç‰¹æ®Štokenå’Œå¤šè¯­è¨€æ”¯æŒæ˜¯å®ç”¨ç³»ç»Ÿçš„å¿…è¦åŠŸèƒ½

**æ€è€ƒé¢˜**ï¼š
1. ä¸ºä»€ä¹ˆBPEæ¯”è¯çº§åˆ†è¯æ›´é€‚åˆç°ä»£è¯­è¨€æ¨¡å‹ï¼Ÿ
2. å¦‚ä½•è®¾è®¡æ›´å¥½çš„å­è¯åˆ†è§£ç®—æ³•ï¼Ÿ
3. ä¸åŒè¯­è¨€çš„åˆ†è¯ç­–ç•¥åº”è¯¥å¦‚ä½•å·®å¼‚åŒ–ï¼Ÿ
4. åˆ†è¯ç²’åº¦å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“æœºåˆ¶æ˜¯ä»€ä¹ˆï¼Ÿ

**ä¸‹ä¸€èŠ‚é¢„å‘Š**ï¼šæˆ‘ä»¬å°†å­¦ä¹ ä¼˜åŒ–ç®—æ³•æ·±åº¦è§£æï¼Œç†è§£å¦‚ä½•é«˜æ•ˆåœ°è®­ç»ƒå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚

---

*åˆ†è¯ä¸ä»…æ˜¯æŠ€æœ¯é—®é¢˜ï¼Œæ›´æ˜¯è¯­è¨€ç†è§£çš„å“²å­¦é—®é¢˜â€”â€”å¦‚ä½•åœ¨ç¦»æ•£çš„ç¬¦å·ä¸­ä¿æŒè¿ç»­çš„è¯­ä¹‰ã€‚* ğŸ”¤