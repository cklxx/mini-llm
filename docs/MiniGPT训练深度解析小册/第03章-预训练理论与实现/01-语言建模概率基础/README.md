# 01 è¯­è¨€å»ºæ¨¡æ¦‚ç‡åŸºç¡€

> **ä»ä¿¡æ¯ç†µåˆ°å›°æƒ‘åº¦ï¼šè¯­è¨€å»ºæ¨¡çš„ç»Ÿè®¡å­¦åŸºçŸ³**

## æ ¸å¿ƒæ€æƒ³

è¯­è¨€å»ºæ¨¡çš„æœ¬è´¨æ˜¯**æ¦‚ç‡å»ºæ¨¡**ï¼šç»™å®šä¸€ä¸ªè¯æ±‡åºåˆ—çš„å‰ç¼€ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒã€‚è¿™ä¸ªçœ‹ä¼¼ç®€å•çš„ä»»åŠ¡ï¼Œå®é™…ä¸ŠåŒ…å«äº†è¯­è¨€ç†è§£çš„å…¨éƒ¨å¤æ‚æ€§â€”â€”è¯­æ³•ã€è¯­ä¹‰ã€å¸¸è¯†ã€æ¨ç†ç­‰éƒ½éšå«åœ¨è¿™ä¸ªæ¡ä»¶æ¦‚ç‡åˆ†å¸ƒä¸­ã€‚

**å…³é”®æ´å¯Ÿ**ï¼š
- **æ¦‚ç‡é“¾å¼æ³•åˆ™**å°†å¤æ‚çš„åºåˆ—å»ºæ¨¡åˆ†è§£ä¸ºå¯å¤„ç†çš„æ¡ä»¶æ¦‚ç‡
- **æœ€å¤§ä¼¼ç„¶ä¼°è®¡**æä¾›äº†ä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å‹å‚æ•°çš„ç†è®ºæ¡†æ¶
- **ä¿¡æ¯ç†µ**åº¦é‡äº†è¯­è¨€çš„å›ºæœ‰ä¸ç¡®å®šæ€§å’Œæ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›
- **å›°æƒ‘åº¦**æ˜¯è¯„ä¼°è¯­è¨€æ¨¡å‹è´¨é‡çš„æ ¸å¿ƒæŒ‡æ ‡

ä»æ•°å­¦è§’åº¦çœ‹ï¼Œè¯­è¨€å»ºæ¨¡å°±æ˜¯åœ¨å­¦ä¹ çœŸå®è¯­è¨€åˆ†å¸ƒ$P_{data}$çš„ä¸€ä¸ªè¿‘ä¼¼$P_\theta$ï¼Œä½¿å¾—ä¸¤ä¸ªåˆ†å¸ƒå°½å¯èƒ½æ¥è¿‘ã€‚

## 1.1 æ¦‚ç‡é“¾å¼åˆ†è§£çš„æ•°å­¦åŸºç¡€

### ä»è”åˆæ¦‚ç‡åˆ°æ¡ä»¶æ¦‚ç‡

**åŸºæœ¬é—®é¢˜**ï¼šç»™å®šè¯æ±‡è¡¨$V$å’Œåºåˆ—é•¿åº¦$n$ï¼Œè¯­è¨€çš„è”åˆæ¦‚ç‡ç©ºé—´å¤§å°ä¸º$|V|^n$ã€‚å¯¹äºç°å®çš„è¯æ±‡è¡¨ï¼ˆ~50Kè¯ï¼‰å’Œåºåˆ—é•¿åº¦ï¼ˆ~2K tokensï¼‰ï¼Œè¿™ä¸ªç©ºé—´æ˜¯å¤©æ–‡æ•°å­—ï¼Œæ— æ³•ç›´æ¥å»ºæ¨¡ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šæ¦‚ç‡é“¾å¼æ³•åˆ™
$$P(x_1, x_2, ..., x_n) = P(x_1) \prod_{i=2}^{n} P(x_i | x_1, x_2, ..., x_{i-1})$$

è¿™å°†è”åˆæ¦‚ç‡åˆ†è§£ä¸ºä¸€ç³»åˆ—**æ¡ä»¶æ¦‚ç‡**çš„ä¹˜ç§¯ï¼Œæ¯ä¸ªæ¡ä»¶æ¦‚ç‡éƒ½å¯ä»¥é€šè¿‡ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼ã€‚

**æ•°å­¦è¡¨è¾¾**ï¼š
```python
# MiniGPTä¸­çš„æ¦‚ç‡è®¡ç®—å®ç°è§£æ
def compute_sequence_probability(model, input_ids, attention_mask=None):
    """è®¡ç®—åºåˆ—çš„å¯¹æ•°æ¦‚ç‡"""
    with torch.no_grad():
        # å‰å‘ä¼ æ’­è·å–logits
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs  # (batch_size, seq_len, vocab_size)
        
        # è®¡ç®—æ¯ä¸ªä½ç½®çš„æ¡ä»¶æ¦‚ç‡
        log_probs = F.log_softmax(logits, dim=-1)
        
        # è·å–çœŸå®tokençš„å¯¹æ•°æ¦‚ç‡
        # æ³¨æ„ï¼šé¢„æµ‹ä½ç½®iä½¿ç”¨çš„æ˜¯token i+1çš„æ¦‚ç‡
        target_ids = input_ids[:, 1:]  # ç§»é™¤<BOS>æˆ–ä½¿ç”¨ä¸‹ä¸€ä¸ªtoken
        target_log_probs = log_probs[:, :-1].gather(dim=-1, 
                                                   index=target_ids.unsqueeze(-1)).squeeze(-1)
        
        # è€ƒè™‘attention maskï¼ˆå¿½ç•¥paddingï¼‰
        if attention_mask is not None:
            mask = attention_mask[:, 1:]  # å¯¹åº”targetä½ç½®
            target_log_probs = target_log_probs * mask
            seq_log_prob = target_log_probs.sum(dim=-1)
            seq_length = mask.sum(dim=-1)
        else:
            seq_log_prob = target_log_probs.sum(dim=-1)
            seq_length = torch.tensor(target_ids.size(1))
        
        return seq_log_prob, seq_length
```

### æ¡ä»¶ç‹¬ç«‹æ€§å‡è®¾çš„å½±å“

**é©¬å°”å¯å¤«å‡è®¾**ï¼šåœ¨å®é™…å®ç°ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸å‡è®¾å½“å‰è¯åªä¾èµ–äºæœ‰é™çš„å†å²çª—å£ï¼š
$$P(x_i | x_1, ..., x_{i-1}) \approx P(x_i | x_{i-k}, ..., x_{i-1})$$

**Transformerçš„ä¼˜åŠ¿**ï¼šé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ï¼ŒTransformerå¯ä»¥å»ºæ¨¡**å…¨åºåˆ—çš„ä¾èµ–å…³ç³»**ï¼Œè€Œä¸å—å›ºå®šçª—å£é™åˆ¶ï¼š
```python
def analyze_dependency_range(attention_weights):
    """åˆ†ææ¨¡å‹çš„å®é™…ä¾èµ–èŒƒå›´"""
    # attention_weights: (batch_size, n_heads, seq_len, seq_len)
    
    batch_size, n_heads, seq_len, _ = attention_weights.shape
    
    # è®¡ç®—æ¯ä¸ªä½ç½®çš„æœ‰æ•ˆæ³¨æ„åŠ›èŒƒå›´
    effective_ranges = []
    
    for pos in range(seq_len):
        # è·å–ä½ç½®poså¯¹å†å²çš„æ³¨æ„åŠ›åˆ†å¸ƒ
        attn_dist = attention_weights[:, :, pos, :pos+1].mean(dim=(0,1))  # å¹³å‡æ‰€æœ‰å¤´å’Œbatch
        
        # è®¡ç®—æœ‰æ•ˆèŒƒå›´ï¼ˆç´¯ç§¯90%æ³¨æ„åŠ›æƒé‡çš„èŒƒå›´ï¼‰
        sorted_attn, indices = torch.sort(attn_dist, descending=True)
        cumsum_attn = torch.cumsum(sorted_attn, dim=0)
        effective_range = (cumsum_attn <= 0.9).sum().item() + 1
        
        effective_ranges.append(effective_range)
        
        print(f"ä½ç½® {pos}: æœ‰æ•ˆä¾èµ–èŒƒå›´ = {effective_range} tokens")
    
    avg_range = sum(effective_ranges) / len(effective_ranges)
    print(f"å¹³å‡æœ‰æ•ˆä¾èµ–èŒƒå›´: {avg_range:.2f} tokens")
    
    return effective_ranges
```

## 1.2 æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„ç†è®ºæ¡†æ¶

### ä¼¼ç„¶å‡½æ•°ä¸å¯¹æ•°ä¼¼ç„¶

ç»™å®šè®­ç»ƒæ•°æ®é›†$\mathcal{D} = \{(x^{(1)}, x^{(2)}, ..., x^{(N)})\}$ï¼Œæ¨¡å‹å‚æ•°$\theta$çš„**ä¼¼ç„¶å‡½æ•°**ä¸ºï¼š

$$L(\theta) = \prod_{i=1}^{N} P(x^{(i)}; \theta) = \prod_{i=1}^{N} \prod_{j=1}^{|x^{(i)}|} P(x^{(i)}_j | x^{(i)}_{<j}; \theta)$$

**å¯¹æ•°ä¼¼ç„¶**ï¼ˆæ›´æ•°å€¼ç¨³å®šï¼‰ï¼š
$$\ell(\theta) = \sum_{i=1}^{N} \sum_{j=1}^{|x^{(i)}|} \log P(x^{(i)}_j | x^{(i)}_{<j}; \theta)$$

**æœ€å¤§ä¼¼ç„¶ä¼°è®¡**ï¼š
$$\hat{\theta} = \arg\max_\theta \ell(\theta) = \arg\min_\theta \left(-\frac{1}{N}\ell(\theta)\right)$$

è¿™å°±æ˜¯æˆ‘ä»¬ç†Ÿæ‚‰çš„**äº¤å‰ç†µæŸå¤±å‡½æ•°**ï¼

```python
# MiniGPTä¸­çš„æŸå¤±å‡½æ•°å®ç° (src/training/trainer.py)
def compute_loss(self, outputs, targets, attention_mask=None):
    """è®¡ç®—è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼‰"""
    
    # outputs: (batch_size, seq_len, vocab_size)
    # targets: (batch_size, seq_len)
    
    # å°†è¾“å‡ºå’Œç›®æ ‡å±•å¹³
    logits = outputs.view(-1, outputs.size(-1))  # (batch_size * seq_len, vocab_size)
    targets = targets.view(-1)  # (batch_size * seq_len,)
    
    # è®¡ç®—äº¤å‰ç†µæŸå¤±
    loss = F.cross_entropy(logits, targets, ignore_index=self.tokenizer.pad_token_id, reduction='none')
    
    # å¦‚æœæœ‰attention maskï¼Œåº”ç”¨mask
    if attention_mask is not None:
        mask = attention_mask.view(-1)
        loss = loss * mask
        # è¿”å›å¹³å‡æŸå¤±ï¼ˆåªè€ƒè™‘épadding tokensï¼‰
        return loss.sum() / mask.sum()
    else:
        return loss.mean()
```

### Fisherä¿¡æ¯ä¸å‚æ•°ä¼°è®¡çš„æ–¹å·®

**Fisherä¿¡æ¯çŸ©é˜µ**å®šä¹‰ä¸ºï¼š
$$I(\theta) = -\mathbb{E}\left[\frac{\partial^2 \ell(\theta)}{\partial \theta^2}\right]$$

**CramÃ©râ€“Raoä¸‹ç•Œ**å‘Šè¯‰æˆ‘ä»¬ï¼Œå‚æ•°ä¼°è®¡çš„æ–¹å·®è‡³å°‘ä¸ºï¼š
$$\text{Var}(\hat{\theta}) \geq I(\theta)^{-1}$$

è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆ**æ›´å¤šçš„æ•°æ®**ï¼ˆæ›´å¤§çš„$N$ï¼‰èƒ½å¤Ÿæä¾›**æ›´ç²¾ç¡®çš„å‚æ•°ä¼°è®¡**ï¼š

```python
def analyze_parameter_estimation_quality(model, data_loader, num_samples=1000):
    """åˆ†æå‚æ•°ä¼°è®¡çš„è´¨é‡"""
    
    model.eval()
    log_likelihoods = []
    parameter_gradients = []
    
    with torch.no_grad():
        for i, batch in enumerate(data_loader):
            if i >= num_samples:
                break
                
            input_ids, targets = batch
            
            # è®¡ç®—å¯¹æ•°ä¼¼ç„¶
            outputs = model(input_ids)
            loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), 
                                 targets.view(-1), reduction='mean')
            log_likelihood = -loss.item()
            log_likelihoods.append(log_likelihood)
            
            # è®¡ç®—æ¢¯åº¦ï¼ˆç”¨äºä¼°è®¡Fisherä¿¡æ¯ï¼‰
            model.zero_grad()
            loss.backward()
            
            grad_norm = 0
            for param in model.parameters():
                if param.grad is not None:
                    grad_norm += param.grad.norm().item() ** 2
            parameter_gradients.append(math.sqrt(grad_norm))
    
    # ç»Ÿè®¡åˆ†æ
    ll_mean = np.mean(log_likelihoods)
    ll_std = np.std(log_likelihoods)
    grad_mean = np.mean(parameter_gradients)
    grad_std = np.std(parameter_gradients)
    
    print(f"å¯¹æ•°ä¼¼ç„¶ç»Ÿè®¡:")
    print(f"  å‡å€¼: {ll_mean:.4f}")
    print(f"  æ ‡å‡†å·®: {ll_std:.4f}")
    print(f"  ç½®ä¿¡åŒºé—´ (95%): [{ll_mean - 1.96*ll_std:.4f}, {ll_mean + 1.96*ll_std:.4f}]")
    
    print(f"\\næ¢¯åº¦èŒƒæ•°ç»Ÿè®¡:")
    print(f"  å‡å€¼: {grad_mean:.4f}")  
    print(f"  æ ‡å‡†å·®: {grad_std:.4f}")
    
    return {
        'log_likelihood': {'mean': ll_mean, 'std': ll_std},
        'gradient_norm': {'mean': grad_mean, 'std': grad_std}
    }
```

## 1.3 ä¿¡æ¯ç†µä¸è¯­è¨€å¤æ‚åº¦

### Shannonç†µçš„è¯­è¨€å­¦è§£é‡Š

**è¯­è¨€çš„ç†µ**è¡¡é‡äº†è¯­è¨€çš„å›ºæœ‰ä¸ç¡®å®šæ€§ï¼š
$$H(X) = -\sum_{x \in V} P(x) \log P(x)$$

å¯¹äºæ¡ä»¶åˆ†å¸ƒï¼ˆè¯­è¨€å»ºæ¨¡çš„æ ¸å¿ƒï¼‰ï¼š
$$H(X|Y) = -\sum_{y} P(y) \sum_{x} P(x|y) \log P(x|y)$$

**ç›´è§‰ç†è§£**ï¼š
- é«˜ç†µ â†’ è¯­è¨€æ›´éšæœºï¼Œæ›´éš¾é¢„æµ‹
- ä½ç†µ â†’ è¯­è¨€æ›´è§„å¾‹ï¼Œæ›´å®¹æ˜“é¢„æµ‹

```python
def compute_language_entropy(model, data_loader, vocabulary_size):
    """è®¡ç®—è¯­è¨€çš„æ¡ä»¶ç†µ"""
    
    model.eval()
    total_entropy = 0
    total_tokens = 0
    
    with torch.no_grad():
        for batch in data_loader:
            input_ids, targets = batch
            
            # è·å–é¢„æµ‹åˆ†å¸ƒ
            outputs = model(input_ids)  # (batch_size, seq_len, vocab_size)
            probs = F.softmax(outputs, dim=-1)
            
            # è®¡ç®—æ¯ä¸ªä½ç½®çš„ç†µ
            # H = -sum(p * log(p))
            log_probs = torch.log(probs + 1e-8)  # æ•°å€¼ç¨³å®šæ€§
            entropy = -(probs * log_probs).sum(dim=-1)  # (batch_size, seq_len)
            
            # ç´¯ç§¯ç»Ÿè®¡
            total_entropy += entropy.sum().item()
            total_tokens += entropy.numel()
    
    avg_entropy = total_entropy / total_tokens
    
    # è½¬æ¢ä¸ºä¸åŒå•ä½
    entropy_nat = avg_entropy  # è‡ªç„¶å¯¹æ•°å•ä½
    entropy_bit = avg_entropy / math.log(2)  # bitå•ä½
    entropy_normalized = avg_entropy / math.log(vocabulary_size)  # å½’ä¸€åŒ–åˆ°[0,1]
    
    print(f"è¯­è¨€æ¡ä»¶ç†µåˆ†æ:")
    print(f"  æ¡ä»¶ç†µ (nat): {entropy_nat:.4f}")
    print(f"  æ¡ä»¶ç†µ (bit): {entropy_bit:.4f}")
    print(f"  å½’ä¸€åŒ–ç†µ: {entropy_normalized:.4f}")
    print(f"  ç†è®ºæœ€å¤§ç†µ: {math.log(vocabulary_size):.4f} nat")
    print(f"  ç†µæ•ˆç‡: {entropy_normalized:.2%}")
    
    return {
        'entropy_nat': entropy_nat,
        'entropy_bit': entropy_bit,
        'entropy_normalized': entropy_normalized,
        'max_entropy': math.log(vocabulary_size)
    }
```

### äº¤å‰ç†µä¸KLæ•£åº¦

**äº¤å‰ç†µ**è¡¡é‡ä½¿ç”¨æ¨¡å‹åˆ†å¸ƒ$Q$æ¥ç¼–ç çœŸå®åˆ†å¸ƒ$P$çš„æˆæœ¬ï¼š
$$H(P, Q) = -\sum_{x} P(x) \log Q(x)$$

**KLæ•£åº¦**è¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒçš„å·®å¼‚ï¼š
$$D_{KL}(P||Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)} = H(P, Q) - H(P)$$

**å…³é”®æ´å¯Ÿ**ï¼š
$$\mathbb{E}_{x \sim P_{data}}[-\log P_\theta(x)] = H(P_{data}) + D_{KL}(P_{data}||P_\theta)$$

æœ€å°åŒ–äº¤å‰ç†µæŸå¤±ç­‰ä»·äºæœ€å°åŒ–æ¨¡å‹åˆ†å¸ƒä¸æ•°æ®åˆ†å¸ƒçš„KLæ•£åº¦ï¼

```python
def analyze_distribution_divergence(model, true_distribution, test_data):
    """åˆ†ææ¨¡å‹åˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒçš„å·®å¼‚"""
    
    model.eval()
    kl_divergences = []
    cross_entropies = []
    
    with torch.no_grad():
        for batch in test_data:
            input_ids, targets = batch
            
            # è·å–æ¨¡å‹é¢„æµ‹åˆ†å¸ƒ
            outputs = model(input_ids)
            model_probs = F.softmax(outputs, dim=-1)  # Q(x|context)
            
            # è®¡ç®—äº¤å‰ç†µ
            true_probs = true_distribution[targets]  # P(x|context) 
            cross_entropy = -(true_probs * torch.log(model_probs + 1e-8)).sum(dim=-1)
            
            # è®¡ç®—KLæ•£åº¦  
            kl_div = (true_probs * torch.log(true_probs / (model_probs + 1e-8) + 1e-8)).sum(dim=-1)
            
            cross_entropies.extend(cross_entropy.flatten().tolist())
            kl_divergences.extend(kl_div.flatten().tolist())
    
    # ç»Ÿè®¡åˆ†æ
    avg_ce = np.mean(cross_entropies)
    avg_kl = np.mean(kl_divergences)
    
    print(f"åˆ†å¸ƒåˆ†æ:")
    print(f"  å¹³å‡äº¤å‰ç†µ: {avg_ce:.4f}")
    print(f"  å¹³å‡KLæ•£åº¦: {avg_kl:.4f}")
    print(f"  æ•°æ®åˆ†å¸ƒç†µä¼°è®¡: {avg_ce - avg_kl:.4f}")
    
    return {
        'cross_entropy': avg_ce,
        'kl_divergence': avg_kl,
        'data_entropy_estimate': avg_ce - avg_kl
    }
```

## 1.4 å›°æƒ‘åº¦ï¼šè¯­è¨€æ¨¡å‹çš„æ ‡å‡†è¯„ä¼°æŒ‡æ ‡

### å›°æƒ‘åº¦çš„æ•°å­¦å®šä¹‰

**å›°æƒ‘åº¦(Perplexity)**æ˜¯äº¤å‰ç†µçš„æŒ‡æ•°ï¼š
$$\text{PPL} = \exp(H(P_{data}, P_\theta)) = \exp\left(-\frac{1}{N}\sum_{i=1}^{N} \log P_\theta(x_i)\right)$$

**å‡ ä½•è§£é‡Š**ï¼šå›°æƒ‘åº¦è¡¨ç¤ºæ¨¡å‹åœ¨æ¯ä¸ªä½ç½®"å›°æƒ‘"çš„é€‰æ‹©æ•°é‡ã€‚
- PPL = 1ï¼šæ¨¡å‹å®Œå…¨ç¡®å®šï¼ˆæ€»æ˜¯æ­£ç¡®é¢„æµ‹ï¼‰
- PPL = |V|ï¼šæ¨¡å‹å®Œå…¨éšæœºï¼ˆå‡åŒ€åˆ†å¸ƒé¢„æµ‹ï¼‰

**ä¿¡æ¯è®ºè§£é‡Š**ï¼šå›°æƒ‘åº¦æ˜¯**æœ‰æ•ˆè¯æ±‡é‡**ï¼Œå³æ¨¡å‹è®¤ä¸ºåœ¨å½“å‰ä¸Šä¸‹æ–‡ä¸­ç­‰å¯èƒ½å‡ºç°çš„è¯çš„æ•°é‡ã€‚

```python
def compute_perplexity(model, data_loader, tokenizer):
    """è®¡ç®—æ¨¡å‹å›°æƒ‘åº¦"""
    
    model.eval()
    total_loss = 0
    total_tokens = 0
    
    with torch.no_grad():
        for batch in tqdm(data_loader, desc="Computing perplexity"):
            input_ids = batch['input_ids']
            attention_mask = batch.get('attention_mask', None)
            
            # è®¡ç®—æŸå¤±
            outputs = model(input_ids)
            
            # åˆ›å»ºtargetsï¼ˆå‘å·¦åç§»ä¸€ä½ï¼‰
            targets = input_ids[:, 1:].contiguous()
            logits = outputs[:, :-1].contiguous()
            
            # è®¡ç®—äº¤å‰ç†µæŸå¤±
            loss = F.cross_entropy(
                logits.view(-1, logits.size(-1)),
                targets.view(-1),
                ignore_index=tokenizer.pad_token_id,
                reduction='sum'
            )
            
            # è®¡ç®—æœ‰æ•ˆtokenæ•°é‡
            if attention_mask is not None:
                mask = attention_mask[:, 1:].contiguous()
                num_tokens = mask.sum().item()
            else:
                num_tokens = (targets != tokenizer.pad_token_id).sum().item()
            
            total_loss += loss.item()
            total_tokens += num_tokens
    
    # è®¡ç®—å¹³å‡æŸå¤±å’Œå›°æƒ‘åº¦
    avg_loss = total_loss / total_tokens
    perplexity = math.exp(avg_loss)
    
    print(f"å›°æƒ‘åº¦è¯„ä¼°ç»“æœ:")
    print(f"  å¹³å‡æŸå¤±: {avg_loss:.4f}")
    print(f"  å›°æƒ‘åº¦: {perplexity:.2f}")
    print(f"  æ€»tokenæ•°: {total_tokens:,}")
    
    return perplexity, avg_loss
```

### å›°æƒ‘åº¦çš„åˆ†è§£åˆ†æ

å›°æƒ‘åº¦å¯ä»¥æŒ‰ä¸åŒç»´åº¦åˆ†è§£ï¼Œå¸®åŠ©ç†è§£æ¨¡å‹çš„è¡Œä¸ºï¼š

```python
def detailed_perplexity_analysis(model, data_loader, tokenizer):
    """è¯¦ç»†çš„å›°æƒ‘åº¦åˆ†æ"""
    
    model.eval()
    
    # æŒ‰ä½ç½®åˆ†æ
    position_losses = defaultdict(list)
    # æŒ‰tokené¢‘ç‡åˆ†æ  
    token_losses = defaultdict(list)
    # æŒ‰tokenç±»å‹åˆ†æ
    token_type_losses = defaultdict(list)
    
    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch['input_ids']
            
            outputs = model(input_ids)
            targets = input_ids[:, 1:]
            logits = outputs[:, :-1]
            
            # è®¡ç®—æ¯ä¸ªä½ç½®çš„æŸå¤±
            losses = F.cross_entropy(
                logits.view(-1, logits.size(-1)),
                targets.view(-1),
                ignore_index=tokenizer.pad_token_id,
                reduction='none'
            ).view(targets.shape)
            
            # æŒ‰ä½ç½®ç»Ÿè®¡
            for pos in range(targets.size(1)):
                valid_mask = targets[:, pos] != tokenizer.pad_token_id
                if valid_mask.sum() > 0:
                    pos_loss = losses[:, pos][valid_mask].mean().item()
                    position_losses[pos].append(pos_loss)
            
            # æŒ‰tokenç»Ÿè®¡  
            for i in range(targets.size(0)):
                for j in range(targets.size(1)):
                    token_id = targets[i, j].item()
                    if token_id != tokenizer.pad_token_id:
                        loss = losses[i, j].item()
                        token_losses[token_id].append(loss)
                        
                        # tokenç±»å‹åˆ†æ
                        token = tokenizer.decode([token_id])
                        if token.isalpha():
                            token_type = 'alphabetic'
                        elif token.isdigit():
                            token_type = 'numeric'
                        elif token.isspace():
                            token_type = 'whitespace'
                        else:
                            token_type = 'punctuation'
                        
                        token_type_losses[token_type].append(loss)
    
    # ç»Ÿè®¡åˆ†æ
    print("=== å›°æƒ‘åº¦åˆ†è§£åˆ†æ ===")
    
    # 1. ä½ç½®åˆ†æ
    print("\\n1. æŒ‰ä½ç½®åˆ†æ:")
    for pos in sorted(position_losses.keys())[:10]:  # å‰10ä¸ªä½ç½®
        losses = position_losses[pos]
        avg_loss = np.mean(losses)
        ppl = math.exp(avg_loss)
        print(f"  ä½ç½® {pos:2d}: æŸå¤±={avg_loss:.4f}, å›°æƒ‘åº¦={ppl:.2f}")
    
    # 2. æŒ‰tokenç±»å‹åˆ†æ
    print("\\n2. æŒ‰tokenç±»å‹åˆ†æ:")
    for token_type, losses in token_type_losses.items():
        avg_loss = np.mean(losses)
        ppl = math.exp(avg_loss)
        count = len(losses)
        print(f"  {token_type:12s}: æŸå¤±={avg_loss:.4f}, å›°æƒ‘åº¦={ppl:.2f}, æ ·æœ¬æ•°={count:,}")
    
    # 3. é«˜é¢‘vsä½é¢‘tokenåˆ†æ
    print("\\n3. æŒ‰tokené¢‘ç‡åˆ†æ:")
    token_freqs = [(token_id, len(losses)) for token_id, losses in token_losses.items()]
    token_freqs.sort(key=lambda x: x[1], reverse=True)
    
    # é«˜é¢‘token (top 10%)
    high_freq_tokens = token_freqs[:len(token_freqs)//10]
    high_freq_losses = []
    for token_id, _ in high_freq_tokens:
        high_freq_losses.extend(token_losses[token_id])
    
    # ä½é¢‘token (bottom 10%)  
    low_freq_tokens = token_freqs[-len(token_freqs)//10:]
    low_freq_losses = []
    for token_id, _ in low_freq_tokens:
        low_freq_losses.extend(token_losses[token_id])
    
    high_freq_ppl = math.exp(np.mean(high_freq_losses))
    low_freq_ppl = math.exp(np.mean(low_freq_losses))
    
    print(f"  é«˜é¢‘token (top 10%): å›°æƒ‘åº¦={high_freq_ppl:.2f}")
    print(f"  ä½é¢‘token (bottom 10%): å›°æƒ‘åº¦={low_freq_ppl:.2f}")
    print(f"  å›°æƒ‘åº¦æ¯”å€¼: {low_freq_ppl/high_freq_ppl:.2f}")
    
    return {
        'position_losses': dict(position_losses),
        'token_type_losses': dict(token_type_losses),
        'frequency_analysis': {
            'high_freq_ppl': high_freq_ppl,
            'low_freq_ppl': low_freq_ppl
        }
    }
```

### å›°æƒ‘åº¦ä¸äººç±»è¯­è¨€èƒ½åŠ›çš„å¯¹æ¯”

**ç»éªŒæ•°æ®**ï¼š
- éšæœºåŸºçº¿ï¼šå›°æƒ‘åº¦ â‰ˆ |V| (è¯æ±‡è¡¨å¤§å°)
- ä¼ ç»Ÿn-gramæ¨¡å‹ï¼šå›°æƒ‘åº¦ â‰ˆ 100-300
- æ—©æœŸç¥ç»è¯­è¨€æ¨¡å‹ï¼šå›°æƒ‘åº¦ â‰ˆ 50-100  
- ç°ä»£Transformerï¼šå›°æƒ‘åº¦ â‰ˆ 10-30
- äººç±»è¡¨ç°ä¼°è®¡ï¼šå›°æƒ‘åº¦ â‰ˆ 12

```python
def compare_with_baselines(model, test_data, tokenizer):
    """ä¸åŸºçº¿æ¨¡å‹æ¯”è¾ƒå›°æƒ‘åº¦"""
    
    # 1. è®¡ç®—ç›®æ ‡æ¨¡å‹å›°æƒ‘åº¦
    model_ppl, _ = compute_perplexity(model, test_data, tokenizer)
    
    # 2. è®¡ç®—éšæœºåŸºçº¿å›°æƒ‘åº¦
    vocab_size = len(tokenizer.vocab)
    random_ppl = vocab_size
    
    # 3. è®¡ç®—unigramåŸºçº¿å›°æƒ‘åº¦
    # ç»Ÿè®¡è®­ç»ƒæ•°æ®çš„tokené¢‘ç‡
    token_counts = defaultdict(int)
    total_tokens = 0
    
    for batch in test_data:
        input_ids = batch['input_ids']
        for token_id in input_ids.flatten():
            if token_id != tokenizer.pad_token_id:
                token_counts[token_id.item()] += 1
                total_tokens += 1
    
    # è®¡ç®—unigramæ¦‚ç‡å’Œå›°æƒ‘åº¦
    unigram_loss = 0
    for batch in test_data:
        input_ids = batch['input_ids']
        targets = input_ids[:, 1:]
        
        for token_id in targets.flatten():
            if token_id != tokenizer.pad_token_id:
                token_id = token_id.item()
                prob = token_counts[token_id] / total_tokens
                unigram_loss += -math.log(prob)
    
    unigram_ppl = math.exp(unigram_loss / sum(token_counts.values()))
    
    # 4. ç»“æœæ¯”è¾ƒ
    print("=== å›°æƒ‘åº¦åŸºçº¿æ¯”è¾ƒ ===")
    print(f"éšæœºåŸºçº¿:     {random_ppl:.2f}")  
    print(f"UnigramåŸºçº¿:  {unigram_ppl:.2f}")
    print(f"ç›®æ ‡æ¨¡å‹:     {model_ppl:.2f}")
    print(f"äººç±»ä¼°è®¡:     ~12")
    print()
    print("æ”¹è¿›åˆ†æ:")
    print(f"vs éšæœºåŸºçº¿:   {random_ppl/model_ppl:.1f}x æ”¹è¿›")
    print(f"vs Unigram:   {unigram_ppl/model_ppl:.1f}x æ”¹è¿›")
    print(f"vs äººç±»ä¼°è®¡:   {model_ppl/12:.1f}x å·®è·")
    
    return {
        'model_ppl': model_ppl,
        'random_ppl': random_ppl,
        'unigram_ppl': unigram_ppl,
        'human_estimate': 12
    }
```

## 1.5 å®è·µï¼šMiniGPTä¸­çš„æ¦‚ç‡è®¡ç®—

### å®Œæ•´çš„è®­ç»ƒæŸå¤±è®¡ç®—

```python
# MiniGPTè®­ç»ƒå¾ªç¯ä¸­çš„æŸå¤±è®¡ç®—å®ç°
class LanguageModelingLoss:
    """è¯­è¨€å»ºæ¨¡æŸå¤±å‡½æ•°çš„å®Œæ•´å®ç°"""
    
    def __init__(self, vocab_size, ignore_index=-100, label_smoothing=0.0):
        self.vocab_size = vocab_size
        self.ignore_index = ignore_index
        self.label_smoothing = label_smoothing
    
    def __call__(self, logits, targets, reduction='mean'):
        """
        è®¡ç®—è¯­è¨€å»ºæ¨¡æŸå¤±
        
        Args:
            logits: (batch_size, seq_len, vocab_size) æ¨¡å‹è¾“å‡º
            targets: (batch_size, seq_len) ç›®æ ‡token ids
            reduction: 'mean', 'sum', or 'none'
        
        Returns:
            loss: æ ‡é‡æŸå¤±å€¼
        """
        
        # å±•å¹³è¾“å…¥
        flat_logits = logits.view(-1, self.vocab_size)  # (N, vocab_size)
        flat_targets = targets.view(-1)  # (N,)
        
        if self.label_smoothing > 0:
            # æ ‡ç­¾å¹³æ»‘æ­£åˆ™åŒ–
            loss = self._label_smoothing_loss(flat_logits, flat_targets, reduction)
        else:
            # æ ‡å‡†äº¤å‰ç†µæŸå¤±
            loss = F.cross_entropy(
                flat_logits, 
                flat_targets, 
                ignore_index=self.ignore_index,
                reduction=reduction
            )
        
        return loss
    
    def _label_smoothing_loss(self, logits, targets, reduction):
        """æ ‡ç­¾å¹³æ»‘æŸå¤±å®ç°"""
        
        # è®¡ç®—å¯¹æ•°æ¦‚ç‡
        log_probs = F.log_softmax(logits, dim=-1)
        
        # åˆ›å»ºå¹³æ»‘æ ‡ç­¾
        smooth_targets = torch.zeros_like(log_probs)
        
        # æœ‰æ•ˆç›®æ ‡æ©ç 
        valid_mask = targets != self.ignore_index
        valid_targets = targets[valid_mask]
        
        if valid_targets.numel() > 0:
            # çœŸå®æ ‡ç­¾æ¦‚ç‡
            true_prob = 1.0 - self.label_smoothing
            # å…¶ä»–æ ‡ç­¾æ¦‚ç‡
            smooth_prob = self.label_smoothing / (self.vocab_size - 1)
            
            # å¡«å……å¹³æ»‘æ ‡ç­¾
            smooth_targets[valid_mask] = smooth_prob
            smooth_targets[valid_mask, valid_targets] = true_prob
            
            # è®¡ç®—KLæ•£åº¦æŸå¤±
            loss = -torch.sum(smooth_targets * log_probs, dim=-1)
            
            if reduction == 'mean':
                return loss[valid_mask].mean()
            elif reduction == 'sum':
                return loss[valid_mask].sum()
            else:
                return loss
        else:
            return torch.tensor(0.0, device=logits.device)

# ä½¿ç”¨ç¤ºä¾‹
def training_step_with_detailed_loss(model, batch, tokenizer):
    """è®­ç»ƒæ­¥éª¤ä¸­çš„è¯¦ç»†æŸå¤±è®¡ç®—"""
    
    input_ids = batch['input_ids']
    attention_mask = batch.get('attention_mask', None)
    
    # å‰å‘ä¼ æ’­
    outputs = model(input_ids, attention_mask=attention_mask)
    
    # å‡†å¤‡targetsï¼ˆè‡ªå›å½’ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼‰
    targets = input_ids[:, 1:].contiguous()
    logits = outputs[:, :-1].contiguous()
    
    # è®¡ç®—æŸå¤±
    loss_fn = LanguageModelingLoss(
        vocab_size=len(tokenizer.vocab),
        ignore_index=tokenizer.pad_token_id,
        label_smoothing=0.1
    )
    
    loss = loss_fn(logits, targets)
    
    # è¯¦ç»†ç»Ÿè®¡
    with torch.no_grad():
        # è®¡ç®—å›°æƒ‘åº¦
        ppl = torch.exp(loss)
        
        # è®¡ç®—å‡†ç¡®ç‡ï¼ˆä¸‹ä¸€ä¸ªtokené¢„æµ‹æ­£ç¡®ç‡ï¼‰
        predictions = logits.argmax(dim=-1)
        valid_mask = targets != tokenizer.pad_token_id
        correct = (predictions == targets) & valid_mask
        accuracy = correct.sum().float() / valid_mask.sum().float()
        
        # è®¡ç®—top-kå‡†ç¡®ç‡
        _, top5_preds = logits.topk(5, dim=-1)
        top5_correct = (top5_preds == targets.unsqueeze(-1)).any(dim=-1) & valid_mask
        top5_accuracy = top5_correct.sum().float() / valid_mask.sum().float()
        
        print(f"è®­ç»ƒç»Ÿè®¡:")
        print(f"  æŸå¤±: {loss.item():.4f}")
        print(f"  å›°æƒ‘åº¦: {ppl.item():.2f}")
        print(f"  Top-1å‡†ç¡®ç‡: {accuracy.item():.2%}")
        print(f"  Top-5å‡†ç¡®ç‡: {top5_accuracy.item():.2%}")
    
    return loss, {
        'perplexity': ppl.item(),
        'accuracy': accuracy.item(),
        'top5_accuracy': top5_accuracy.item()
    }
```

## å°ç»“ä¸æ€è€ƒ

æœ¬èŠ‚æ·±å…¥æ¢è®¨äº†è¯­è¨€å»ºæ¨¡çš„æ¦‚ç‡åŸºç¡€ï¼š

1. **æ¦‚ç‡é“¾å¼åˆ†è§£**ï¼šå°†å¤æ‚çš„åºåˆ—å»ºæ¨¡é—®é¢˜åˆ†è§£ä¸ºå¯å¤„ç†çš„æ¡ä»¶æ¦‚ç‡é¢„æµ‹
2. **æœ€å¤§ä¼¼ç„¶ä¼°è®¡**ï¼šæä¾›äº†ä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å‹å‚æ•°çš„ç†è®ºæ¡†æ¶
3. **ä¿¡æ¯ç†µç†è®º**ï¼šæ­ç¤ºäº†è¯­è¨€çš„å›ºæœ‰å¤æ‚åº¦å’Œæ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›
4. **å›°æƒ‘åº¦æŒ‡æ ‡**ï¼šæˆä¸ºè¯„ä¼°è¯­è¨€æ¨¡å‹è´¨é‡çš„æ ‡å‡†å·¥å…·

**å…³é”®æ´å¯Ÿ**ï¼š
- è¯­è¨€å»ºæ¨¡æœ¬è´¨ä¸Šæ˜¯æ¦‚ç‡åˆ†å¸ƒçš„å­¦ä¹ å’Œè¿‘ä¼¼
- äº¤å‰ç†µæŸå¤±ç­‰ä»·äºæœ€å¤§åŒ–æ•°æ®ä¼¼ç„¶å’Œæœ€å°åŒ–KLæ•£åº¦
- å›°æƒ‘åº¦ç›´è§‚åœ°åæ˜ äº†æ¨¡å‹çš„"ç¡®ä¿¡ç¨‹åº¦"
- ä¿¡æ¯è®ºæä¾›äº†ç†è§£å’Œåˆ†æè¯­è¨€æ¨¡å‹çš„æ•°å­¦å·¥å…·

**æ€è€ƒé¢˜**ï¼š
1. ä¸ºä»€ä¹ˆè‡ªå›å½’åˆ†è§£æ˜¯å¤„ç†å˜é•¿åºåˆ—çš„æœ‰æ•ˆæ–¹æ³•ï¼Ÿ
2. æ ‡ç­¾å¹³æ»‘å¦‚ä½•ä»ä¿¡æ¯è®ºè§’åº¦æ”¹å–„æ¨¡å‹æ€§èƒ½ï¼Ÿ
3. å›°æƒ‘åº¦ä¸äººç±»è¯­è¨€ç†è§£èƒ½åŠ›çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ
4. å¦‚ä½•ä»æ¦‚ç‡è§’åº¦ç†è§£æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Ÿ

**ä¸‹ä¸€èŠ‚é¢„å‘Š**ï¼šæˆ‘ä»¬å°†å­¦ä¹ è‡ªå›å½’å»ºæ¨¡ä¸å› æœæ©ç ï¼Œç†è§£å¦‚ä½•åœ¨Transformerä¸­å®ç°æ—¶é—´åºåˆ—çš„å› æœæ€§çº¦æŸã€‚

---

*æ¦‚ç‡è®ºæ˜¯è¯­è¨€å»ºæ¨¡çš„æ•°å­¦åŸºçŸ³ï¼Œå®ƒä¸ä»…å‘Šè¯‰æˆ‘ä»¬å¦‚ä½•è®­ç»ƒæ¨¡å‹ï¼Œæ›´å¸®åŠ©æˆ‘ä»¬ç†è§£è¯­è¨€çš„æœ¬è´¨å’Œæ¨¡å‹çš„è¡Œä¸ºã€‚* ğŸ“Š