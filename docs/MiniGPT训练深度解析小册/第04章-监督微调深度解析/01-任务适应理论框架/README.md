# 01 ä»»åŠ¡é€‚åº”ç†è®ºæ¡†æ¶

> **ä»è¿ç§»å­¦ä¹ åˆ°å‚æ•°é«˜æ•ˆå¾®è°ƒï¼šç†è®ºä¸å®è·µçš„å®Œç¾ç»“åˆ**

## æ ¸å¿ƒæ€æƒ³

ä»»åŠ¡é€‚åº”æ˜¯å°†é€šç”¨é¢„è®­ç»ƒæ¨¡å‹è½¬åŒ–ä¸ºç‰¹å®šä»»åŠ¡ä¸“å®¶çš„è¿‡ç¨‹ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸ä»…ä»…æ˜¯ç®€å•çš„å‚æ•°è°ƒæ•´ï¼Œè€Œæ˜¯ä¸€ä¸ªå¤æ‚çš„çŸ¥è¯†è¿ç§»å’Œé‡ç»„è¿‡ç¨‹ï¼Œæ¶‰åŠæºåŸŸçŸ¥è¯†çš„ä¿æŒã€ç›®æ ‡åŸŸç‰¹å¾çš„å­¦ä¹ ï¼Œä»¥åŠä¸¤è€…ä¹‹é—´çš„å¹³è¡¡ã€‚

**å…³é”®æ´å¯Ÿ**ï¼š
- **çŸ¥è¯†å±‚æ¬¡æ€§**ï¼šä¸åŒå±‚çº§çš„çŸ¥è¯†æœ‰ä¸åŒçš„è¿ç§»ç‰¹æ€§
- **å‚æ•°æ•æ„Ÿæ€§**ï¼šä¸åŒå‚æ•°å¯¹ä»»åŠ¡é€‚åº”çš„è´¡çŒ®å·®å¼‚å·¨å¤§
- **é—å¿˜æœºåˆ¶**ï¼šæ–°çŸ¥è¯†å­¦ä¹ ä¸æ—§çŸ¥è¯†ä¿æŒçš„åŠ¨æ€å¹³è¡¡
- **æ•ˆç‡çº¦æŸ**ï¼šåœ¨è®¡ç®—èµ„æºé™åˆ¶ä¸‹å®ç°æœ€ä¼˜é€‚åº”

ä»æ•°å­¦è§’åº¦çœ‹ï¼Œä»»åŠ¡é€‚åº”æ˜¯åœ¨é¢„è®­ç»ƒå‚æ•°ç©ºé—´ä¸­å¯»æ‰¾ä¸€ä¸ªæ–°çš„å‚æ•°ç‚¹ï¼Œä½¿å…¶åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä¼˜ï¼ŒåŒæ—¶ä¿æŒå¯¹æºä»»åŠ¡çš„åˆç†æ€§èƒ½ã€‚

## 1.1 è¿ç§»å­¦ä¹ çš„æ•°å­¦æ¨¡å‹

### æºåŸŸä¸ç›®æ ‡åŸŸçš„æ¦‚ç‡æ¡†æ¶

**æºåŸŸ**ï¼ˆé¢„è®­ç»ƒï¼‰ï¼š
- æ•°æ®åˆ†å¸ƒï¼š$\mathcal{D}_s = \{(x_i^s, y_i^s)\}_{i=1}^{N_s}$ï¼Œå…¶ä¸­$x_i^s \sim P_s(X), y_i^s \sim P_s(Y|X)$
- ä»»åŠ¡ï¼š$\mathcal{T}_s = \{Y_s, P_s(Y|X)\}$ï¼Œé€šå¸¸æ˜¯è¯­è¨€å»ºæ¨¡
- å­¦ä¹ åˆ°çš„å‚æ•°ï¼š$\theta_s^* = \arg\min_\theta \mathcal{L}_s(\theta)$

**ç›®æ ‡åŸŸ**ï¼ˆå¾®è°ƒï¼‰ï¼š
- æ•°æ®åˆ†å¸ƒï¼š$\mathcal{D}_t = \{(x_i^t, y_i^t)\}_{i=1}^{N_t}$ï¼Œå…¶ä¸­$x_i^t \sim P_t(X), y_i^t \sim P_t(Y|X)$
- ä»»åŠ¡ï¼š$\mathcal{T}_t = \{Y_t, P_t(Y|X)\}$ï¼Œå¦‚é—®ç­”ã€å¯¹è¯ç­‰
- ç›®æ ‡å‚æ•°ï¼š$\theta_t^* = \arg\min_\theta \mathcal{L}_t(\theta)$

**è¿ç§»å­¦ä¹ ç›®æ ‡**ï¼š
$$\theta_{SFT}^* = \arg\min_\theta \left[ \mathcal{L}_t(\theta) + \lambda \Omega(\theta, \theta_s^*) \right]$$

å…¶ä¸­$\Omega(\theta, \theta_s^*)$æ˜¯æ­£åˆ™åŒ–é¡¹ï¼Œé˜²æ­¢å‚æ•°åç¦»é¢„è®­ç»ƒçŠ¶æ€å¤ªè¿œã€‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import numpy as np
from typing import Dict, List, Tuple, Optional

def analyze_domain_shift():
    """åˆ†ææºåŸŸä¸ç›®æ ‡åŸŸçš„åˆ†å¸ƒå·®å¼‚"""
    
    print("=== æºåŸŸä¸ç›®æ ‡åŸŸåˆ†æ ===")
    
    # æ¨¡æ‹Ÿä¸åŒåŸŸçš„æ•°æ®ç‰¹å¾
    domains = {
        'pretraining': {
            'description': 'é¢„è®­ç»ƒåŸŸï¼ˆé€šç”¨æ–‡æœ¬ï¼‰',
            'characteristics': {
                'vocab_diversity': 0.95,  # è¯æ±‡å¤šæ ·æ€§
                'sentence_length': 50.2,  # å¹³å‡å¥é•¿
                'topic_consistency': 0.3,  # ä¸»é¢˜ä¸€è‡´æ€§
                'formality_level': 0.5,   # æ­£å¼ç¨‹åº¦
                'task_specificity': 0.1   # ä»»åŠ¡ç‰¹å¼‚æ€§
            }
        },
        'qa_domain': {
            'description': 'é—®ç­”åŸŸ',
            'characteristics': {
                'vocab_diversity': 0.7,
                'sentence_length': 15.8,
                'topic_consistency': 0.8,
                'formality_level': 0.7,
                'task_specificity': 0.9
            }
        },
        'dialogue_domain': {
            'description': 'å¯¹è¯åŸŸ',
            'characteristics': {
                'vocab_diversity': 0.6,
                'sentence_length': 12.4,
                'topic_consistency': 0.6,
                'formality_level': 0.3,
                'task_specificity': 0.85
            }
        },
        'code_domain': {
            'description': 'ä»£ç åŸŸ',
            'characteristics': {
                'vocab_diversity': 0.4,
                'sentence_length': 8.9,
                'topic_consistency': 0.95,
                'formality_level': 0.9,
                'task_specificity': 0.95
            }
        }
    }
    
    # è®¡ç®—åŸŸé—´è·ç¦»
    def compute_domain_distance(domain1, domain2):
        """è®¡ç®—ä¸¤ä¸ªåŸŸä¹‹é—´çš„ç‰¹å¾è·ç¦»"""
        chars1 = domain1['characteristics']
        chars2 = domain2['characteristics']
        
        distance = 0
        for key in chars1.keys():
            distance += (chars1[key] - chars2[key]) ** 2
        
        return math.sqrt(distance)
    
    print("åŸŸé—´ç‰¹å¾è·ç¦»çŸ©é˜µ:")
    domain_names = list(domains.keys())
    print(f"{'':15s}", end="")
    for name in domain_names:
        print(f"{name:12s}", end="")
    print()
    
    distance_matrix = {}
    for i, domain1 in enumerate(domain_names):
        distance_matrix[domain1] = {}
        print(f"{domain1:15s}", end="")
        for j, domain2 in enumerate(domain_names):
            if i <= j:
                dist = compute_domain_distance(domains[domain1], domains[domain2])
                distance_matrix[domain1][domain2] = dist
                print(f"{dist:12.3f}", end="")
            else:
                dist = distance_matrix[domain2][domain1]
                print(f"{dist:12.3f}", end="")
        print()
    
    # åˆ†æè¿ç§»éš¾åº¦
    print("\\n=== è¿ç§»éš¾åº¦åˆ†æ ===")
    pretraining_domain = domains['pretraining']
    
    for domain_name, domain_info in domains.items():
        if domain_name == 'pretraining':
            continue
        
        distance = compute_domain_distance(pretraining_domain, domain_info)
        
        # æ ¹æ®è·ç¦»é¢„æµ‹è¿ç§»éš¾åº¦
        if distance < 0.5:
            difficulty = "å®¹æ˜“"
        elif distance < 1.0:
            difficulty = "ä¸­ç­‰"
        elif distance < 1.5:
            difficulty = "å›°éš¾"
        else:
            difficulty = "æéš¾"
        
        print(f"{domain_info['description']:10s}: è·ç¦»={distance:.3f}, è¿ç§»éš¾åº¦={difficulty}")
    
    return distance_matrix

class TransferLearningAnalyzer:
    """è¿ç§»å­¦ä¹ æ•ˆæœåˆ†æå™¨"""
    
    def __init__(self, source_model, target_task_data):
        self.source_model = source_model
        self.target_data = target_task_data
        self.transfer_metrics = {}
    
    def compute_transferability_score(self, layer_idx=None):
        """è®¡ç®—å±‚çº§å¯è¿ç§»æ€§è¯„åˆ†"""
        
        if layer_idx is None:
            # åˆ†ææ‰€æœ‰å±‚
            layers_to_analyze = range(len(self.source_model.transformer_blocks))
        else:
            layers_to_analyze = [layer_idx]
        
        transferability_scores = {}
        
        for layer in layers_to_analyze:
            # 1. ç‰¹å¾è¡¨ç¤ºåˆ†æ
            source_features = self._extract_layer_features(layer, is_source=True)
            target_features = self._extract_layer_features(layer, is_source=False)
            
            # 2. è®¡ç®—ç‰¹å¾ç›¸ä¼¼åº¦
            feature_similarity = self._compute_feature_similarity(
                source_features, target_features
            )
            
            # 3. è®¡ç®—æ¢¯åº¦ç›¸ä¼¼åº¦
            gradient_similarity = self._compute_gradient_similarity(layer)
            
            # 4. è®¡ç®—æƒé‡å˜åŒ–å¹…åº¦
            weight_change_magnitude = self._compute_weight_change(layer)
            
            # 5. ç»¼åˆè¯„åˆ†
            transferability = (
                0.4 * feature_similarity +
                0.3 * gradient_similarity +
                0.3 * (1 - weight_change_magnitude)  # å˜åŒ–è¶Šå°ï¼Œå¯è¿ç§»æ€§è¶Šé«˜
            )
            
            transferability_scores[layer] = {
                'overall_score': transferability,
                'feature_similarity': feature_similarity,
                'gradient_similarity': gradient_similarity,
                'weight_stability': 1 - weight_change_magnitude
            }
        
        return transferability_scores
    
    def _extract_layer_features(self, layer_idx, is_source=True):
        """æå–æŒ‡å®šå±‚çš„ç‰¹å¾è¡¨ç¤º"""
        
        # ç®€åŒ–å®ç°ï¼šè¿”å›æ¨¡æ‹Ÿçš„ç‰¹å¾å‘é‡
        if is_source:
            # æºåŸŸç‰¹å¾ï¼ˆé¢„è®­ç»ƒï¼‰
            return torch.randn(100, 512)  # (samples, feature_dim)
        else:
            # ç›®æ ‡åŸŸç‰¹å¾
            return torch.randn(50, 512)   # ç›®æ ‡åŸŸæ ·æœ¬è¾ƒå°‘
    
    def _compute_feature_similarity(self, source_features, target_features):
        """è®¡ç®—ç‰¹å¾ç©ºé—´ç›¸ä¼¼åº¦"""
        
        # ä½¿ç”¨CKA (Centered Kernel Alignment) åº¦é‡
        def centered_kernel_alignment(X, Y):
            # çº¿æ€§æ ¸çš„CKA
            X_centered = X - X.mean(dim=0, keepdim=True)
            Y_centered = Y - Y.mean(dim=0, keepdim=True)
            
            # è®¡ç®—GramçŸ©é˜µ
            K_X = torch.mm(X_centered, X_centered.t())
            K_Y = torch.mm(Y_centered, Y_centered.t())
            
            # CKAè®¡ç®—
            numerator = torch.trace(torch.mm(K_X, K_Y))
            denominator = torch.sqrt(
                torch.trace(torch.mm(K_X, K_X)) * torch.trace(torch.mm(K_Y, K_Y))
            )
            
            return (numerator / denominator).item()
        
        # ç”±äºç»´åº¦ä¸åŒï¼Œä½¿ç”¨éšæœºé‡‡æ ·å¯¹é½
        min_samples = min(source_features.size(0), target_features.size(0))
        source_sample = source_features[:min_samples]
        target_sample = target_features[:min_samples]
        
        return centered_kernel_alignment(source_sample, target_sample)
    
    def _compute_gradient_similarity(self, layer_idx):
        """è®¡ç®—æ¢¯åº¦ç©ºé—´ç›¸ä¼¼åº¦"""
        
        # ç®€åŒ–å®ç°ï¼šè¿”å›æ¨¡æ‹Ÿçš„æ¢¯åº¦ç›¸ä¼¼åº¦
        # å®é™…å®ç°éœ€è¦è®¡ç®—æºä»»åŠ¡å’Œç›®æ ‡ä»»åŠ¡çš„æ¢¯åº¦ï¼Œç„¶åè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        return np.random.uniform(0.3, 0.9)  # æ¨¡æ‹Ÿæ¢¯åº¦ç›¸ä¼¼åº¦
    
    def _compute_weight_change(self, layer_idx):
        """è®¡ç®—æƒé‡å˜åŒ–å¹…åº¦"""
        
        # ç®€åŒ–å®ç°ï¼šè¿”å›æ¨¡æ‹Ÿçš„æƒé‡å˜åŒ–
        # å®é™…å®ç°éœ€è¦æ¯”è¾ƒå¾®è°ƒå‰åçš„æƒé‡å·®å¼‚
        return np.random.uniform(0.1, 0.5)  # æ¨¡æ‹Ÿæƒé‡å˜åŒ–å¹…åº¦
    
    def analyze_layer_wise_transferability(self):
        """åˆ†æå„å±‚çš„å¯è¿ç§»æ€§"""
        
        print("=== åˆ†å±‚å¯è¿ç§»æ€§åˆ†æ ===")
        
        num_layers = 12  # å‡è®¾12å±‚Transformer
        layer_scores = {}
        
        for layer in range(num_layers):
            scores = self.compute_transferability_score(layer)
            layer_scores[layer] = scores[layer]
            
            overall = scores[layer]['overall_score']
            feature_sim = scores[layer]['feature_similarity']
            gradient_sim = scores[layer]['gradient_similarity']
            weight_stab = scores[layer]['weight_stability']
            
            print(f"Layer {layer:2d}: æ€»è¯„åˆ†={overall:.3f} "
                  f"(ç‰¹å¾={feature_sim:.3f}, æ¢¯åº¦={gradient_sim:.3f}, ç¨³å®š={weight_stab:.3f})")
        
        # åˆ†æè¶‹åŠ¿
        overall_scores = [layer_scores[i]['overall_score'] for i in range(num_layers)]
        
        print(f"\\nå¯è¿ç§»æ€§è¶‹åŠ¿åˆ†æ:")
        print(f"  æœ€é«˜å¯è¿ç§»æ€§: Layer {np.argmax(overall_scores)} (è¯„åˆ†: {max(overall_scores):.3f})")
        print(f"  æœ€ä½å¯è¿ç§»æ€§: Layer {np.argmin(overall_scores)} (è¯„åˆ†: {min(overall_scores):.3f})")
        print(f"  å¹³å‡å¯è¿ç§»æ€§: {np.mean(overall_scores):.3f}")
        
        # å±‚çº§æ¨¡å¼åˆ†æ
        early_layers = overall_scores[:4]
        middle_layers = overall_scores[4:8]
        late_layers = overall_scores[8:]
        
        print(f"\\nå±‚çº§æ¨¡å¼:")
        print(f"  æ—©æœŸå±‚ (0-3):   å¹³å‡è¯„åˆ† {np.mean(early_layers):.3f}")
        print(f"  ä¸­é—´å±‚ (4-7):   å¹³å‡è¯„åˆ† {np.mean(middle_layers):.3f}")
        print(f"  åæœŸå±‚ (8-11):  å¹³å‡è¯„åˆ† {np.mean(late_layers):.3f}")
        
        return layer_scores

def theoretical_transfer_analysis():
    """ç†è®ºè¿ç§»å­¦ä¹ åˆ†æ"""
    
    print("=== è¿ç§»å­¦ä¹ ç†è®ºåˆ†æ ===")
    
    # 1. è¿ç§»å­¦ä¹ çš„æ•°å­¦æ¡†æ¶
    print("1. æ•°å­¦æ¡†æ¶:")
    print("   æºåŸŸæŸå¤±: L_s(Î¸) = E_{(x,y)~D_s}[â„“(f_Î¸(x), y)]")
    print("   ç›®æ ‡åŸŸæŸå¤±: L_t(Î¸) = E_{(x,y)~D_t}[â„“(f_Î¸(x), y)]")
    print("   è¿ç§»ç›®æ ‡: min_Î¸ L_t(Î¸) + Î»Î©(Î¸, Î¸_s*)")
    
    # 2. ç†è®ºä¿è¯åˆ†æ
    print("\\n2. ç†è®ºä¿è¯:")
    
    # Ben-Davidç­‰äººçš„è¿ç§»å­¦ä¹ ç†è®º
    def analyze_transfer_bound(source_error, target_samples, domain_divergence, 
                              combined_error, lambda_reg):
        """åˆ†æè¿ç§»å­¦ä¹ çš„ç†è®ºç•Œé™"""
        
        # ç›®æ ‡åŸŸæœŸæœ›è¯¯å·®çš„ä¸Šç•Œ
        # R_t(Î¸) â‰¤ R_s(Î¸) + d_H(D_s, D_t) + Î»_combined
        target_error_bound = (
            source_error +                    # æºåŸŸè¯¯å·®
            domain_divergence +               # åŸŸé—´æ•£åº¦
            combined_error +                  # ç»„åˆè¯¯å·®
            math.sqrt(math.log(1/0.05) / (2 * target_samples))  # æ ·æœ¬å¤æ‚åº¦é¡¹
        )
        
        return target_error_bound
    
    # ä¸åŒåœºæ™¯çš„ç†è®ºåˆ†æ
    scenarios = {
        'similar_domains': {
            'source_error': 0.05,
            'domain_divergence': 0.1,
            'combined_error': 0.02,
            'target_samples': 1000
        },
        'different_domains': {
            'source_error': 0.05,
            'domain_divergence': 0.4,
            'combined_error': 0.1,
            'target_samples': 1000
        },
        'few_shot': {
            'source_error': 0.05,
            'domain_divergence': 0.2,
            'combined_error': 0.05,
            'target_samples': 100
        }
    }
    
    for scenario_name, params in scenarios.items():
        bound = analyze_transfer_bound(lambda_reg=0.01, **params)
        print(f"   {scenario_name:15s}: ç›®æ ‡è¯¯å·®ä¸Šç•Œ â‰¤ {bound:.3f}")
    
    # 3. è¿ç§»æ•ˆæœçš„å½±å“å› ç´ 
    print("\\n3. å…³é”®å½±å“å› ç´ :")
    factors = {
        'domain_similarity': 'åŸŸç›¸ä¼¼æ€§è¶Šé«˜ï¼Œè¿ç§»æ•ˆæœè¶Šå¥½',
        'source_data_quality': 'æºåŸŸæ•°æ®è´¨é‡ç›´æ¥å½±å“è¿ç§»ä¸Šé™',
        'target_data_size': 'ç›®æ ‡åŸŸæ•°æ®é‡å½±å“è¿‡æ‹Ÿåˆé£é™©',
        'model_capacity': 'æ¨¡å‹å®¹é‡éœ€è¦åŒ¹é…ä»»åŠ¡å¤æ‚åº¦',
        'regularization': 'æ­£åˆ™åŒ–å¼ºåº¦æ§åˆ¶æ–°æ—§çŸ¥è¯†å¹³è¡¡'
    }
    
    for factor, description in factors.items():
        print(f"   {factor:18s}: {description}")
    
    return scenarios
```

## 1.2 ç¾éš¾æ€§é—å¿˜çš„æ•°å­¦åˆ†æ

### é—å¿˜æœºåˆ¶çš„ç¥ç»ç§‘å­¦è§†è§’

**ç¾éš¾æ€§é—å¿˜**æ˜¯æŒ‡åœ¨å­¦ä¹ æ–°ä»»åŠ¡æ—¶ï¼Œç¥ç»ç½‘ç»œå€¾å‘äº"å¿˜è®°"ä¹‹å‰å­¦åˆ°çš„çŸ¥è¯†ã€‚è¿™åœ¨æ·±åº¦å­¦ä¹ ä¸­æ˜¯ä¸€ä¸ªæ ¹æœ¬æ€§é—®é¢˜ã€‚

**æ•°å­¦è¡¨è¾¾**ï¼š
è®¾$\theta_0$ä¸ºé¢„è®­ç»ƒå‚æ•°ï¼Œ$\theta_t$ä¸ºå¾®è°ƒ$t$æ­¥åçš„å‚æ•°ï¼Œåˆ™é—å¿˜ç¨‹åº¦å¯ä»¥é‡åŒ–ä¸ºï¼š
$$\text{Forgetting}(t) = \mathcal{L}_{\text{pretrain}}(\theta_t) - \mathcal{L}_{\text{pretrain}}(\theta_0)$$

```python
class CatastrophicForgettingAnalyzer:
    """ç¾éš¾æ€§é—å¿˜åˆ†æå™¨"""
    
    def __init__(self, model, pretrain_data, finetune_data):
        self.model = model
        self.pretrain_data = pretrain_data
        self.finetune_data = finetune_data
        self.forgetting_history = []
        self.initial_pretrain_loss = None
    
    def measure_forgetting_during_training(self, num_steps=1000, eval_interval=50):
        """æµ‹é‡è®­ç»ƒè¿‡ç¨‹ä¸­çš„é—å¿˜ç¨‹åº¦"""
        
        print("=== ç¾éš¾æ€§é—å¿˜åŠ¨æ€åˆ†æ ===")
        
        # è®°å½•åˆå§‹é¢„è®­ç»ƒæ€§èƒ½
        self.initial_pretrain_loss = self._evaluate_on_pretrain_data()
        print(f"åˆå§‹é¢„è®­ç»ƒæŸå¤±: {self.initial_pretrain_loss:.4f}")
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-4)
        
        forgetting_curve = []
        finetune_curve = []
        steps = []
        
        for step in range(num_steps):
            # å¾®è°ƒä¸€æ­¥
            self.model.train()
            batch = self._get_finetune_batch()
            
            optimizer.zero_grad()
            loss = self._compute_finetune_loss(batch)
            loss.backward()
            optimizer.step()
            
            # å®šæœŸè¯„ä¼°
            if step % eval_interval == 0:
                self.model.eval()
                with torch.no_grad():
                    # è¯„ä¼°é¢„è®­ç»ƒä»»åŠ¡æ€§èƒ½ï¼ˆæµ‹é‡é—å¿˜ï¼‰
                    current_pretrain_loss = self._evaluate_on_pretrain_data()
                    forgetting = current_pretrain_loss - self.initial_pretrain_loss
                    
                    # è¯„ä¼°å¾®è°ƒä»»åŠ¡æ€§èƒ½
                    current_finetune_loss = self._evaluate_on_finetune_data()
                    
                    forgetting_curve.append(forgetting)
                    finetune_curve.append(current_finetune_loss)
                    steps.append(step)
                    
                    print(f"æ­¥éª¤ {step:4d}: é—å¿˜åº¦={forgetting:+.4f}, "
                          f"å¾®è°ƒæŸå¤±={current_finetune_loss:.4f}")
        
        # åˆ†æé—å¿˜æ¨¡å¼
        self._analyze_forgetting_patterns(steps, forgetting_curve, finetune_curve)
        
        return {
            'steps': steps,
            'forgetting_curve': forgetting_curve,
            'finetune_curve': finetune_curve
        }
    
    def _evaluate_on_pretrain_data(self):
        """åœ¨é¢„è®­ç»ƒæ•°æ®ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        self.model.eval()
        total_loss = 0
        num_batches = 0
        
        with torch.no_grad():
            for _ in range(10):  # è¯„ä¼°10ä¸ªæ‰¹æ¬¡
                batch = self._get_pretrain_batch()
                loss = self._compute_pretrain_loss(batch)
                total_loss += loss.item()
                num_batches += 1
        
        return total_loss / num_batches
    
    def _evaluate_on_finetune_data(self):
        """åœ¨å¾®è°ƒæ•°æ®ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        self.model.eval()
        total_loss = 0
        num_batches = 0
        
        with torch.no_grad():
            for _ in range(5):  # è¯„ä¼°5ä¸ªæ‰¹æ¬¡
                batch = self._get_finetune_batch()
                loss = self._compute_finetune_loss(batch)
                total_loss += loss.item()
                num_batches += 1
        
        return total_loss / num_batches
    
    def _get_pretrain_batch(self):
        """è·å–é¢„è®­ç»ƒæ•°æ®æ‰¹æ¬¡"""
        # ç®€åŒ–å®ç°ï¼šè¿”å›æ¨¡æ‹Ÿæ•°æ®
        vocab_size = 10000
        seq_len = 128
        batch_size = 32
        
        input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
        return {'input_ids': input_ids}
    
    def _get_finetune_batch(self):
        """è·å–å¾®è°ƒæ•°æ®æ‰¹æ¬¡"""
        # ç®€åŒ–å®ç°ï¼šè¿”å›æ¨¡æ‹Ÿçš„é—®ç­”æ•°æ®
        vocab_size = 10000
        batch_size = 16
        
        # é—®ç­”æ ¼å¼ï¼š[instruction][sep][response]
        instruction_len = 32
        response_len = 64
        
        instructions = torch.randint(0, vocab_size, (batch_size, instruction_len))
        responses = torch.randint(0, vocab_size, (batch_size, response_len))
        
        return {
            'instructions': instructions,
            'responses': responses
        }
    
    def _compute_pretrain_loss(self, batch):
        """è®¡ç®—é¢„è®­ç»ƒæŸå¤±ï¼ˆè¯­è¨€å»ºæ¨¡ï¼‰"""
        input_ids = batch['input_ids']
        
        # å‰å‘ä¼ æ’­
        outputs = self.model(input_ids)
        
        # è®¡ç®—è¯­è¨€æ¨¡å‹æŸå¤±
        targets = input_ids[:, 1:].contiguous()
        logits = outputs[:, :-1].contiguous()
        
        loss = F.cross_entropy(
            logits.view(-1, logits.size(-1)),
            targets.view(-1),
            ignore_index=-100
        )
        
        return loss
    
    def _compute_finetune_loss(self, batch):
        """è®¡ç®—å¾®è°ƒæŸå¤±ï¼ˆæŒ‡ä»¤è·Ÿéšï¼‰"""
        instructions = batch['instructions']
        responses = batch['responses']
        
        # æ‹¼æ¥æŒ‡ä»¤å’Œå“åº”
        input_ids = torch.cat([instructions, responses], dim=1)
        
        # å‰å‘ä¼ æ’­
        outputs = self.model(input_ids)
        
        # åªå¯¹å“åº”éƒ¨åˆ†è®¡ç®—æŸå¤±
        instruction_len = instructions.size(1)
        response_logits = outputs[:, instruction_len-1:-1]
        response_targets = responses
        
        loss = F.cross_entropy(
            response_logits.contiguous().view(-1, response_logits.size(-1)),
            response_targets.contiguous().view(-1),
            ignore_index=-100
        )
        
        return loss
    
    def _analyze_forgetting_patterns(self, steps, forgetting_curve, finetune_curve):
        """åˆ†æé—å¿˜æ¨¡å¼"""
        
        print("\\n=== é—å¿˜æ¨¡å¼åˆ†æ ===")
        
        # 1. é—å¿˜é˜¶æ®µåˆ†æ
        max_forgetting = max(forgetting_curve)
        max_forgetting_step = steps[forgetting_curve.index(max_forgetting)]
        
        print(f"æœ€å¤§é—å¿˜åº¦: {max_forgetting:.4f} (æ­¥éª¤ {max_forgetting_step})")
        
        # 2. é—å¿˜vsæ”¶ç›Šæƒè¡¡
        final_forgetting = forgetting_curve[-1]
        initial_finetune_loss = finetune_curve[0]
        final_finetune_loss = finetune_curve[-1]
        finetune_improvement = initial_finetune_loss - final_finetune_loss
        
        print(f"æœ€ç»ˆé—å¿˜åº¦: {final_forgetting:.4f}")
        print(f"å¾®è°ƒæ”¹è¿›åº¦: {finetune_improvement:.4f}")
        print(f"æƒè¡¡æ¯”ç‡: {finetune_improvement/abs(final_forgetting):.2f}")
        
        # 3. é—å¿˜é€Ÿåº¦åˆ†æ
        if len(forgetting_curve) > 1:
            forgetting_velocity = np.diff(forgetting_curve)
            avg_forgetting_speed = np.mean(forgetting_velocity[forgetting_velocity > 0])
            print(f"å¹³å‡é—å¿˜é€Ÿåº¦: {avg_forgetting_speed:.6f}/æ­¥")
        
        # 4. å…³é”®æ—¶é—´ç‚¹è¯†åˆ«
        rapid_forgetting_threshold = 0.01  # å¿«é€Ÿé—å¿˜é˜ˆå€¼
        rapid_forgetting_steps = []
        
        for i, forgetting in enumerate(forgetting_curve):
            if forgetting > rapid_forgetting_threshold:
                rapid_forgetting_steps.append(steps[i])
        
        if rapid_forgetting_steps:
            print(f"å¿«é€Ÿé—å¿˜å¼€å§‹: æ­¥éª¤ {rapid_forgetting_steps[0]}")

def analyze_forgetting_mechanisms():
    """åˆ†æé—å¿˜çš„å†…åœ¨æœºåˆ¶"""
    
    print("=== é—å¿˜æœºåˆ¶æ·±åº¦åˆ†æ ===")
    
    # 1. æƒé‡ç©ºé—´è§†è§’
    print("1. æƒé‡ç©ºé—´åˆ†æ:")
    
    def weight_space_analysis():
        """æƒé‡ç©ºé—´ä¸­çš„é—å¿˜åˆ†æ"""
        
        # æ¨¡æ‹Ÿæƒé‡å˜åŒ–
        original_weights = torch.randn(1000, 512)  # é¢„è®­ç»ƒæƒé‡
        
        forgetting_scenarios = {
            'gentle_adaptation': {
                'learning_rate': 1e-5,
                'steps': 1000,
                'description': 'æ¸©å’Œé€‚åº”'
            },
            'aggressive_adaptation': {
                'learning_rate': 1e-3,
                'steps': 1000,
                'description': 'æ¿€è¿›é€‚åº”'
            },
            'short_aggressive': {
                'learning_rate': 1e-3,
                'steps': 100,
                'description': 'çŸ­æœŸæ¿€è¿›'
            }
        }
        
        for scenario_name, config in forgetting_scenarios.items():
            # æ¨¡æ‹Ÿæƒé‡æ›´æ–°
            current_weights = original_weights.clone()
            weight_changes = []
            
            for step in range(config['steps']):
                # æ¨¡æ‹Ÿæ¢¯åº¦
                gradient = torch.randn_like(current_weights) * 0.1
                
                # æ›´æ–°æƒé‡
                current_weights -= config['learning_rate'] * gradient
                
                # è®°å½•æƒé‡å˜åŒ–
                if step % (config['steps'] // 10) == 0:
                    weight_change = torch.norm(current_weights - original_weights)
                    weight_changes.append(weight_change.item())
            
            final_change = weight_changes[-1]
            print(f"   {config['description']:8s}: æœ€ç»ˆæƒé‡å˜åŒ– = {final_change:.4f}")
    
    weight_space_analysis()
    
    # 2. æ¢¯åº¦å†²çªåˆ†æ
    print("\\n2. æ¢¯åº¦å†²çªåˆ†æ:")
    
    def gradient_conflict_analysis():
        """åˆ†æé¢„è®­ç»ƒä»»åŠ¡å’Œå¾®è°ƒä»»åŠ¡çš„æ¢¯åº¦å†²çª"""
        
        # æ¨¡æ‹Ÿä¸åŒä»»åŠ¡çš„æ¢¯åº¦
        model_dim = 1000
        
        # é¢„è®­ç»ƒä»»åŠ¡æ¢¯åº¦ï¼ˆè¯­è¨€å»ºæ¨¡ï¼‰
        pretrain_gradients = []
        for _ in range(100):  # 100ä¸ªé¢„è®­ç»ƒæ ·æœ¬
            grad = torch.randn(model_dim) * 0.5
            pretrain_gradients.append(grad)
        
        # å¾®è°ƒä»»åŠ¡æ¢¯åº¦ï¼ˆé—®ç­”ï¼‰
        finetune_gradients = []
        for _ in range(20):   # 20ä¸ªå¾®è°ƒæ ·æœ¬
            grad = torch.randn(model_dim) * 0.8  # å¾®è°ƒæ¢¯åº¦é€šå¸¸æ›´å¤§
            finetune_gradients.append(grad)
        
        # è®¡ç®—æ¢¯åº¦æ–¹å‘å†²çª
        pretrain_avg_grad = torch.stack(pretrain_gradients).mean(dim=0)
        finetune_avg_grad = torch.stack(finetune_gradients).mean(dim=0)
        
        # ä½™å¼¦ç›¸ä¼¼åº¦
        cosine_sim = F.cosine_similarity(
            pretrain_avg_grad.unsqueeze(0),
            finetune_avg_grad.unsqueeze(0)
        ).item()
        
        # æ¢¯åº¦å¹…åº¦æ¯”
        pretrain_magnitude = torch.norm(pretrain_avg_grad).item()
        finetune_magnitude = torch.norm(finetune_avg_grad).item()
        magnitude_ratio = finetune_magnitude / pretrain_magnitude
        
        print(f"   æ¢¯åº¦æ–¹å‘ç›¸ä¼¼åº¦: {cosine_sim:.3f}")
        print(f"   æ¢¯åº¦å¹…åº¦æ¯”: {magnitude_ratio:.3f}")
        
        # å†²çªç¨‹åº¦è¯„ä¼°
        if cosine_sim < 0:
            conflict_level = "ä¸¥é‡å†²çª"
        elif cosine_sim < 0.5:
            conflict_level = "ä¸­ç­‰å†²çª"
        elif cosine_sim < 0.8:
            conflict_level = "è½»å¾®å†²çª"
        else:
            conflict_level = "åŸºæœ¬ä¸€è‡´"
        
        print(f"   å†²çªç¨‹åº¦: {conflict_level}")
        
        return cosine_sim, magnitude_ratio
    
    gradient_conflict_analysis()
    
    # 3. è®°å¿†å®¹é‡ç†è®º
    print("\\n3. è®°å¿†å®¹é‡åˆ†æ:")
    
    def memory_capacity_analysis():
        """åŸºäºè®°å¿†å®¹é‡ç†è®ºçš„åˆ†æ"""
        
        # æ¨¡å‹å‚æ•°
        total_params = 125e6  # 125Må‚æ•°
        effective_capacity = total_params * 0.1  # æœ‰æ•ˆè®°å¿†å®¹é‡çº¦10%
        
        # ä¸åŒä»»åŠ¡çš„è®°å¿†éœ€æ±‚
        tasks = {
            'language_modeling': {
                'knowledge_bits': 1e9,    # è¯­è¨€çŸ¥è¯†éœ€æ±‚
                'description': 'è¯­è¨€å»ºæ¨¡'
            },
            'qa_task': {
                'knowledge_bits': 1e7,    # é—®ç­”çŸ¥è¯†éœ€æ±‚
                'description': 'é—®ç­”ä»»åŠ¡'
            },
            'dialogue_task': {
                'knowledge_bits': 5e6,    # å¯¹è¯çŸ¥è¯†éœ€æ±‚
                'description': 'å¯¹è¯ä»»åŠ¡'
            }
        }
        
        print(f"   æ¨¡å‹æœ‰æ•ˆè®°å¿†å®¹é‡: {effective_capacity:.1e} bits")
        
        for task_name, task_info in tasks.items():
            knowledge_demand = task_info['knowledge_bits']
            capacity_ratio = knowledge_demand / effective_capacity
            
            if capacity_ratio > 1:
                memory_status = "å®¹é‡ä¸è¶³ï¼Œå¿…ç„¶é—å¿˜"
            elif capacity_ratio > 0.8:
                memory_status = "æ¥è¿‘é¥±å’Œï¼Œå¯èƒ½é—å¿˜"
            else:
                memory_status = "å®¹é‡å……è¶³"
            
            print(f"   {task_info['description']:8s}: éœ€æ±‚={knowledge_demand:.1e}, "
                  f"æ¯”ç‡={capacity_ratio:.2f}, {memory_status}")
    
    memory_capacity_analysis()
    
    # 4. é—å¿˜çš„æ•°å­¦å»ºæ¨¡
    print("\\n4. é—å¿˜æ•°å­¦æ¨¡å‹:")
    
    def forgetting_mathematical_model():
        """é—å¿˜è¿‡ç¨‹çš„æ•°å­¦å»ºæ¨¡"""
        
        print("   æŒ‡æ•°é—å¿˜æ¨¡å‹: F(t) = Fâ‚€ Ã— exp(-Î»t)")
        print("   å…¶ä¸­ F(t) æ˜¯tæ—¶åˆ»çš„é—å¿˜é‡ï¼ŒÎ»æ˜¯é—å¿˜ç‡")
        
        # ä¸åŒå­¦ä¹ ç‡ä¸‹çš„é—å¿˜å»ºæ¨¡
        learning_rates = [1e-5, 1e-4, 1e-3]
        forgetting_rates = []
        
        for lr in learning_rates:
            # é—å¿˜ç‡ä¸å­¦ä¹ ç‡æ­£ç›¸å…³
            forgetting_rate = lr * 1000  # ç®€åŒ–å…³ç³»
            forgetting_rates.append(forgetting_rate)
            
            # é¢„æµ‹1000æ­¥åçš„é—å¿˜é‡
            steps = 1000
            predicted_forgetting = 0.1 * math.exp(forgetting_rate * steps / 1000)
            
            print(f"   å­¦ä¹ ç‡={lr:.1e}: é—å¿˜ç‡Î»={forgetting_rate:.3f}, "
                  f"é¢„æµ‹é—å¿˜={predicted_forgetting:.4f}")
    
    forgetting_mathematical_model()
```

## 1.3 å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯

### LoRAçš„æ•°å­¦åŸç†

**LoRA (Low-Rank Adaptation)**åŸºäºä¸€ä¸ªå…³é”®æ´å¯Ÿï¼šå¾®è°ƒè¿‡ç¨‹ä¸­çš„æƒé‡å˜åŒ–å…·æœ‰ä½ç§©ç»“æ„ã€‚

**æ•°å­¦è¡¨è¾¾**ï¼š
åŸå§‹æƒé‡æ›´æ–°ï¼š$W = W_0 + \Delta W$
LoRAåˆ†è§£ï¼š$\Delta W = AB^T$ï¼Œå…¶ä¸­$A \in \mathbb{R}^{d \times r}, B \in \mathbb{R}^{r \times k}, r \ll \min(d,k)$

**å‚æ•°å‡å°‘é‡**ï¼š
- åŸå§‹ï¼š$d \times k$ä¸ªå‚æ•°
- LoRAï¼š$(d + k) \times r$ä¸ªå‚æ•°
- å‹ç¼©æ¯”ï¼š$\frac{dk}{(d+k)r}$

```python
class LoRALayer(nn.Module):
    """LoRAå±‚çš„æ•°å­¦å®ç°ä¸åˆ†æ"""
    
    def __init__(self, in_features, out_features, rank=16, alpha=32, dropout=0.1):
        super().__init__()
        
        self.rank = rank
        self.alpha = alpha
        self.scaling = alpha / rank
        
        # LoRAæƒé‡
        self.lora_A = nn.Linear(in_features, rank, bias=False)
        self.lora_B = nn.Linear(rank, out_features, bias=False)
        self.lora_dropout = nn.Dropout(dropout)
        
        # åˆå§‹åŒ–
        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))
        nn.init.zeros_(self.lora_B.weight)
        
        # å†»ç»“çš„é¢„è®­ç»ƒæƒé‡
        self.base_layer = None
        
    def forward(self, x):
        """LoRAå‰å‘ä¼ æ’­"""
        
        # åŸºç¡€å±‚è¾“å‡º
        if self.base_layer is not None:
            base_output = self.base_layer(x)
        else:
            base_output = 0
        
        # LoRAå¢é‡è¾“å‡º
        lora_output = self.lora_B(self.lora_dropout(self.lora_A(x))) * self.scaling
        
        return base_output + lora_output
    
    def merge_weights(self):
        """åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€å±‚"""
        if self.base_layer is not None:
            # è®¡ç®—LoRAå¢é‡
            delta_weight = self.lora_B.weight @ self.lora_A.weight * self.scaling
            
            # åˆå¹¶åˆ°åŸºç¡€å±‚
            self.base_layer.weight.data += delta_weight
            
            # æ¸…é›¶LoRAæƒé‡
            nn.init.zeros_(self.lora_A.weight)
            nn.init.zeros_(self.lora_B.weight)
    
    def get_parameter_info(self):
        """è·å–å‚æ•°ä¿¡æ¯"""
        if self.base_layer is not None:
            base_params = self.base_layer.weight.numel()
        else:
            base_params = self.lora_A.in_features * self.lora_B.out_features
        
        lora_params = self.lora_A.weight.numel() + self.lora_B.weight.numel()
        
        return {
            'base_params': base_params,
            'lora_params': lora_params,
            'total_params': base_params + lora_params,
            'trainable_params': lora_params,
            'compression_ratio': base_params / lora_params if lora_params > 0 else float('inf')
        }

class AdapterLayer(nn.Module):
    """Adapterå±‚çš„å®ç°"""
    
    def __init__(self, d_model, bottleneck_dim=64, dropout=0.1):
        super().__init__()
        
        self.d_model = d_model
        self.bottleneck_dim = bottleneck_dim
        
        # ä¸‹æŠ•å½±
        self.down_proj = nn.Linear(d_model, bottleneck_dim)
        # éçº¿æ€§æ¿€æ´»
        self.activation = nn.ReLU()
        # ä¸ŠæŠ•å½±
        self.up_proj = nn.Linear(bottleneck_dim, d_model)
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
        # åˆå§‹åŒ–ï¼šè¾“å‡ºæ¥è¿‘é›¶
        nn.init.normal_(self.down_proj.weight, std=1e-3)
        nn.init.normal_(self.up_proj.weight, std=1e-3)
        nn.init.zeros_(self.down_proj.bias)
        nn.init.zeros_(self.up_proj.bias)
    
    def forward(self, x):
        """Adapterå‰å‘ä¼ æ’­"""
        
        # æ®‹å·®è¿æ¥
        residual = x
        
        # Adapterå˜æ¢
        x = self.down_proj(x)
        x = self.activation(x)
        x = self.dropout(x)
        x = self.up_proj(x)
        
        return residual + x
    
    def get_parameter_info(self):
        """è·å–å‚æ•°ä¿¡æ¯"""
        adapter_params = (
            self.down_proj.weight.numel() + self.down_proj.bias.numel() +
            self.up_proj.weight.numel() + self.up_proj.bias.numel()
        )
        
        # ç›¸å¯¹äºå…¨å±‚å‚æ•°çš„æ¯”ä¾‹
        full_layer_params = self.d_model * self.d_model  # å‡è®¾å…¨è¿æ¥å±‚
        
        return {
            'adapter_params': adapter_params,
            'full_layer_params': full_layer_params,
            'parameter_ratio': adapter_params / full_layer_params
        }

def compare_parameter_efficient_methods():
    """æ¯”è¾ƒä¸åŒå‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•"""
    
    print("=== å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•æ¯”è¾ƒ ===")
    
    # æ¨¡å‹é…ç½®
    d_model = 768
    n_layers = 12
    vocab_size = 50000
    
    # è®¡ç®—å…¨é‡å¾®è°ƒå‚æ•°
    full_params = {
        'embedding': vocab_size * d_model,
        'attention': n_layers * (4 * d_model * d_model),  # Q,K,V,O
        'ffn': n_layers * (2 * d_model * d_model * 4),    # ä¸¤ä¸ªçº¿æ€§å±‚ï¼Œä¸­é—´ç»´åº¦4å€
        'layer_norm': n_layers * 2 * d_model,             # æ¯å±‚ä¸¤ä¸ªLN
        'output': vocab_size * d_model
    }
    
    total_full_params = sum(full_params.values())
    
    print(f"å…¨é‡å¾®è°ƒå‚æ•°ç»Ÿè®¡:")
    for component, params in full_params.items():
        ratio = params / total_full_params
        print(f"  {component:12s}: {params:10,} ({ratio:.1%})")
    print(f"  {'æ€»è®¡':12s}: {total_full_params:10,}")
    
    # ä¸åŒæ–¹æ³•çš„å‚æ•°è®¡ç®—
    methods = {}
    
    # 1. LoRA
    lora_ranks = [8, 16, 32, 64]
    methods['LoRA'] = {}
    
    for rank in lora_ranks:
        # åªå¯¹attentionå±‚åº”ç”¨LoRA
        lora_params = n_layers * 4 * (d_model + d_model) * rank  # Q,K,V,O
        trainable_params = lora_params
        
        methods['LoRA'][f'rank_{rank}'] = {
            'trainable_params': trainable_params,
            'total_params': total_full_params,  # é¢„è®­ç»ƒæƒé‡ä¸å˜
            'ratio': trainable_params / total_full_params
        }
    
    # 2. Adapter
    adapter_dims = [32, 64, 128, 256]
    methods['Adapter'] = {}
    
    for dim in adapter_dims:
        # æ¯å±‚ä¸¤ä¸ªadapterï¼ˆattentionåå’ŒFFNåï¼‰
        adapter_params = n_layers * 2 * (d_model * dim + dim * d_model + 2 * dim)
        trainable_params = adapter_params
        
        methods['Adapter'][f'dim_{dim}'] = {
            'trainable_params': trainable_params,
            'total_params': total_full_params + adapter_params,
            'ratio': trainable_params / total_full_params
        }
    
    # 3. Prompt Tuning
    prompt_lengths = [10, 50, 100, 200]
    methods['Prompt'] = {}
    
    for length in prompt_lengths:
        prompt_params = length * d_model
        trainable_params = prompt_params
        
        methods['Prompt'][f'len_{length}'] = {
            'trainable_params': trainable_params,
            'total_params': total_full_params,
            'ratio': trainable_params / total_full_params
        }
    
    # è¾“å‡ºæ¯”è¾ƒç»“æœ
    print(f"\\nå‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•æ¯”è¾ƒ:")
    print(f"{'æ–¹æ³•':12s} {'é…ç½®':12s} {'å¯è®­ç»ƒå‚æ•°':>12s} {'å‚æ•°æ¯”ä¾‹':>10s} {'å‹ç¼©æ¯”':>8s}")
    print("-" * 70)
    
    for method_name, configs in methods.items():
        for config_name, stats in configs.items():
            trainable = stats['trainable_params']
            ratio = stats['ratio']
            compression = 1 / ratio
            
            print(f"{method_name:12s} {config_name:12s} {trainable:12,} "
                  f"{ratio:.3%} {compression:8.0f}x")
    
    return methods

def analyze_lora_rank_selection():
    """åˆ†æLoRAç§©é€‰æ‹©çš„æ•°å­¦åŸç†"""
    
    print("\\n=== LoRAç§©é€‰æ‹©åˆ†æ ===")
    
    # 1. ç†è®ºåˆ†æ
    print("1. ç†è®ºåŸºç¡€:")
    print("   - å¾®è°ƒæƒé‡å˜åŒ–çš„å†…åœ¨ç»´åº¦é€šå¸¸å¾ˆä½")
    print("   - ç§©ræ§åˆ¶äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å’Œå‚æ•°æ•ˆç‡çš„æƒè¡¡")
    print("   - ç»éªŒä¸Šr=16å¯¹å¤§å¤šæ•°ä»»åŠ¡å·²è¶³å¤Ÿ")
    
    # 2. ä¸åŒç§©çš„æ€§èƒ½åˆ†æ
    def rank_performance_analysis():
        """ä¸åŒç§©çš„æ€§èƒ½ç†è®ºåˆ†æ"""
        
        d_model = 768
        ranks = [1, 2, 4, 8, 16, 32, 64, 128]
        
        print("\\n2. ä¸åŒç§©çš„ç†è®ºåˆ†æ:")
        print(f"{'ç§©':>4s} {'å‚æ•°é‡':>8s} {'è¡¨è¾¾èƒ½åŠ›':>8s} {'è¿‡æ‹Ÿåˆé£é™©':>12s} {'æ¨èåœºæ™¯':>20s}")
        print("-" * 60)
        
        for rank in ranks:
            # å‚æ•°é‡ï¼ˆä»¥attentionå±‚ä¸ºä¾‹ï¼‰
            params_per_layer = 2 * d_model * rank
            
            # è¡¨è¾¾èƒ½åŠ›ï¼ˆç®€åŒ–ä¸ºç§©ä¸æ€»ç»´åº¦çš„æ¯”å€¼ï¼‰
            expressiveness = min(rank / d_model, 1.0)
            
            # è¿‡æ‹Ÿåˆé£é™©ï¼ˆå‚æ•°é‡è¶Šå¤šé£é™©è¶Šé«˜ï¼‰
            overfitting_risk = params_per_layer / (d_model * d_model)
            
            # æ¨èåœºæ™¯
            if rank <= 4:
                scenario = "ç®€å•ä»»åŠ¡ï¼Œæ•°æ®å°‘"
            elif rank <= 16:
                scenario = "ä¸­ç­‰ä»»åŠ¡ï¼Œå¹³è¡¡æ€§èƒ½"
            elif rank <= 64:
                scenario = "å¤æ‚ä»»åŠ¡ï¼Œæ•°æ®å¤š"
            else:
                scenario = "æå¤æ‚ä»»åŠ¡"
            
            print(f"{rank:4d} {params_per_layer:8,} {expressiveness:8.3f} "
                  f"{overfitting_risk:12.6f} {scenario:>20s}")
    
    rank_performance_analysis()
    
    # 3. ç§©çš„æ•°å­¦æ€§è´¨åˆ†æ
    print("\\n3. ç§©çš„æ•°å­¦æ€§è´¨:")
    
    def rank_mathematical_properties():
        """åˆ†æä¸åŒç§©çš„æ•°å­¦æ€§è´¨"""
        
        # æ¨¡æ‹Ÿæƒé‡çŸ©é˜µçš„å¥‡å¼‚å€¼åˆ†è§£
        d_in, d_out = 768, 768
        full_rank = min(d_in, d_out)
        
        # åˆ›å»ºéšæœºæƒé‡å˜åŒ–çŸ©é˜µ
        np.random.seed(42)
        delta_W = np.random.randn(d_out, d_in) * 0.1
        
        # SVDåˆ†è§£
        U, S, Vt = np.linalg.svd(delta_W, full_matrices=False)
        
        # åˆ†æä¸åŒç§©çš„ä¿¡æ¯ä¿ç•™
        ranks_to_analyze = [1, 4, 16, 64, 256]
        
        print("   å¥‡å¼‚å€¼åˆ†æï¼ˆæƒé‡å˜åŒ–çš„å†…åœ¨ç»“æ„ï¼‰:")
        print(f"   {'ç§©':>4s} {'ä¿¡æ¯ä¿ç•™':>10s} {'å‹ç¼©æŸå¤±':>10s} {'ç´¯ç§¯è´¡çŒ®':>10s}")
        
        total_energy = np.sum(S**2)  # æ€»ä¿¡æ¯é‡
        
        for rank in ranks_to_analyze:
            if rank <= len(S):
                # å‰rä¸ªå¥‡å¼‚å€¼çš„èƒ½é‡
                retained_energy = np.sum(S[:rank]**2)
                information_retention = retained_energy / total_energy
                compression_loss = 1 - information_retention
                cumulative_ratio = np.sum(S[:rank]) / np.sum(S)
                
                print(f"   {rank:4d} {information_retention:10.3%} "
                      f"{compression_loss:10.3%} {cumulative_ratio:10.3%}")
        
        # æ‰¾åˆ°åˆç†çš„ç§©
        info_threshold = 0.9  # ä¿ç•™90%ä¿¡æ¯
        for rank in range(1, len(S) + 1):
            retained_energy = np.sum(S[:rank]**2) / total_energy
            if retained_energy >= info_threshold:
                optimal_rank = rank
                break
        
        print(f"   ä¿ç•™{info_threshold:.0%}ä¿¡æ¯çš„æœ€å°ç§©: {optimal_rank}")
    
    rank_mathematical_properties()
    
    return ranks

def implement_minigpt_peft():
    """å®ç°MiniGPTçš„å‚æ•°é«˜æ•ˆå¾®è°ƒ"""
    
    print("\\n=== MiniGPTå‚æ•°é«˜æ•ˆå¾®è°ƒå®ç° ===")
    
    class MiniGPTWithLoRA(nn.Module):
        """é›†æˆLoRAçš„MiniGPTæ¨¡å‹"""
        
        def __init__(self, base_model, lora_config):
            super().__init__()
            
            self.base_model = base_model
            self.lora_config = lora_config
            
            # å†»ç»“é¢„è®­ç»ƒå‚æ•°
            for param in self.base_model.parameters():
                param.requires_grad = False
            
            # æ·»åŠ LoRAå±‚
            self._add_lora_layers()
            
            print(f"LoRAé…ç½®: rank={lora_config['rank']}, alpha={lora_config['alpha']}")
            self._print_parameter_stats()
        
        def _add_lora_layers(self):
            """ä¸ºattentionå±‚æ·»åŠ LoRA"""
            
            rank = self.lora_config['rank']
            alpha = self.lora_config['alpha']
            
            # ä¸ºæ¯ä¸ªTransformerå±‚çš„attentionæ·»åŠ LoRA
            for layer_idx, transformer_block in enumerate(self.base_model.transformer_blocks):
                # è·å–attentionå±‚
                attention = transformer_block.attention
                
                # ä¸ºQ, K, V, OæŠ•å½±æ·»åŠ LoRA
                if hasattr(attention, 'w_q'):
                    d_model = attention.w_q.in_features
                    
                    # åˆ›å»ºLoRAå±‚
                    attention.lora_q = LoRALayer(d_model, d_model, rank, alpha)
                    attention.lora_k = LoRALayer(d_model, d_model, rank, alpha)
                    attention.lora_v = LoRALayer(d_model, d_model, rank, alpha)
                    attention.lora_o = LoRALayer(d_model, d_model, rank, alpha)
                    
                    # è¿æ¥åŸºç¡€å±‚
                    attention.lora_q.base_layer = attention.w_q
                    attention.lora_k.base_layer = attention.w_k
                    attention.lora_v.base_layer = attention.w_v
                    attention.lora_o.base_layer = attention.w_o
        
        def _print_parameter_stats(self):
            """æ‰“å°å‚æ•°ç»Ÿè®¡"""
            
            total_params = 0
            trainable_params = 0
            lora_params = 0
            
            for name, param in self.named_parameters():
                total_params += param.numel()
                
                if param.requires_grad:
                    trainable_params += param.numel()
                    
                    if 'lora' in name:
                        lora_params += param.numel()
            
            print(f"å‚æ•°ç»Ÿè®¡:")
            print(f"  æ€»å‚æ•°é‡: {total_params:,}")
            print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({trainable_params/total_params:.2%})")
            print(f"  LoRAå‚æ•°: {lora_params:,}")
            print(f"  å‚æ•°æ•ˆç‡: {total_params/trainable_params:.1f}x å‹ç¼©")
    
    # ä½¿ç”¨ç¤ºä¾‹
    print("MiniGPT LoRAå¾®è°ƒå®ç°ç‰¹ç‚¹:")
    print("1. åªå¯¹attentionå±‚åº”ç”¨LoRA")
    print("2. é¢„è®­ç»ƒå‚æ•°å®Œå…¨å†»ç»“")
    print("3. æ”¯æŒä¸åŒçš„rankå’Œalphaé…ç½®")
    print("4. å¯ä»¥è½»æ¾åˆå¹¶å’Œåˆ†ç¦»LoRAæƒé‡")
    print("5. æ˜¾è‘—å‡å°‘æ˜¾å­˜å ç”¨å’Œè®­ç»ƒæ—¶é—´")
    
    # é…ç½®æ¨è
    recommended_configs = {
        'small_task': {'rank': 8, 'alpha': 16, 'description': 'ç®€å•ä»»åŠ¡ï¼Œå°‘é‡æ•°æ®'},
        'medium_task': {'rank': 16, 'alpha': 32, 'description': 'ä¸­ç­‰ä»»åŠ¡ï¼Œå¹³è¡¡è®¾ç½®'},
        'complex_task': {'rank': 32, 'alpha': 64, 'description': 'å¤æ‚ä»»åŠ¡ï¼Œå¤§é‡æ•°æ®'},
        'specialized_domain': {'rank': 64, 'alpha': 128, 'description': 'ä¸“ä¸šé¢†åŸŸï¼Œéœ€è¦æ›´å¤šè¡¨è¾¾èƒ½åŠ›'}
    }
    
    print("\\næ¨èé…ç½®:")
    for config_name, config in recommended_configs.items():
        print(f"  {config['description']:20s}: rank={config['rank']:2d}, alpha={config['alpha']:3d}")
    
    return MiniGPTWithLoRA
```

## å°ç»“ä¸æ€è€ƒ

æœ¬èŠ‚æ·±å…¥æ¢è®¨äº†ä»»åŠ¡é€‚åº”çš„ç†è®ºæ¡†æ¶ï¼š

1. **è¿ç§»å­¦ä¹ æ•°å­¦æ¨¡å‹**ï¼šä»æºåŸŸåˆ°ç›®æ ‡åŸŸçš„æ¦‚ç‡æ¡†æ¶å’Œç†è®ºä¿è¯
2. **ç¾éš¾æ€§é—å¿˜åˆ†æ**ï¼šé—å¿˜æœºåˆ¶çš„æ•°å­¦å»ºæ¨¡å’ŒåŠ¨æ€åˆ†æ
3. **å‚æ•°é«˜æ•ˆå¾®è°ƒ**ï¼šLoRAã€Adapterç­‰æ–¹æ³•çš„æ•°å­¦åŸç†å’Œå®ç°ç»†èŠ‚

**å…³é”®æ´å¯Ÿ**ï¼š
- ä»»åŠ¡é€‚åº”æ˜¯çŸ¥è¯†è¿ç§»å’Œé‡ç»„çš„å¤æ‚è¿‡ç¨‹
- ç¾éš¾æ€§é—å¿˜æœ‰å…¶å†…åœ¨çš„æ•°å­¦æœºåˆ¶ï¼Œå¯ä»¥é€šè¿‡åˆç†è®¾è®¡ç¼“è§£
- å‚æ•°é«˜æ•ˆå¾®è°ƒé€šè¿‡ä½ç§©å‡è®¾å¤§å¹…å‡å°‘è®­ç»ƒæˆæœ¬
- ä¸åŒæ–¹æ³•é€‚ç”¨äºä¸åŒçš„ä»»åŠ¡å¤æ‚åº¦å’Œèµ„æºçº¦æŸ

**æ€è€ƒé¢˜**ï¼š
1. å¦‚ä½•è®¾è®¡æ›´å¥½çš„æ­£åˆ™åŒ–æ–¹æ³•æ¥å¹³è¡¡æ–°æ—§çŸ¥è¯†ï¼Ÿ
2. LoRAçš„ç§©é€‰æ‹©é™¤äº†ç»éªŒæ³•åˆ™å¤–ï¼Œæ˜¯å¦æœ‰ç†è®ºæŒ‡å¯¼ï¼Ÿ
3. ä¸åŒç±»å‹çš„ä»»åŠ¡éœ€è¦æ€æ ·çš„é€‚åº”ç­–ç•¥ï¼Ÿ
4. å¦‚ä½•é‡åŒ–å’Œé¢„æµ‹ä»»åŠ¡é€‚åº”çš„éš¾åº¦ï¼Ÿ

**ä¸‹ä¸€èŠ‚é¢„å‘Š**ï¼šæˆ‘ä»¬å°†å­¦ä¹ æŒ‡ä»¤è·Ÿéšä¸å¯¹è¯å»ºæ¨¡ï¼Œç†è§£å¦‚ä½•è®©æ¨¡å‹å‡†ç¡®ç†è§£å’Œæ‰§è¡Œäººç±»æŒ‡ä»¤ã€‚

---

*ä»»åŠ¡é€‚åº”æ˜¯AIä»é€šç”¨èµ°å‘ä¸“ä¸šçš„å¿…ç»ä¹‹è·¯ã€‚ç†è§£å…¶æ•°å­¦åŸç†ï¼Œå°±èƒ½è®¾è®¡å‡ºæ›´é«˜æ•ˆã€æ›´ç¨³å®šçš„å¾®è°ƒç­–ç•¥ã€‚* ğŸ¯
