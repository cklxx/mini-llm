# 02 äººç±»è¯„ä¼°æ¡†æ¶è®¾è®¡

> **ä»ä¸»è§‚æ„ŸçŸ¥åˆ°å®¢è§‚æµ‹åº¦ï¼šæ„å»ºç§‘å­¦çš„äººç±»è¯„ä¼°ä½“ç³»**

## æ ¸å¿ƒæ€æƒ³

äººç±»è¯„ä¼°æ˜¯è¯­è¨€æ¨¡å‹è¯„ä¼°çš„"é‡‘æ ‡å‡†"ï¼Œä½†åŒæ—¶ä¹Ÿæ˜¯æœ€å…·æŒ‘æˆ˜æ€§çš„è¯„ä¼°æ–¹å¼ã€‚äººç±»çš„è¯­è¨€åˆ¤æ–­æ¶‰åŠå¤æ‚çš„è®¤çŸ¥è¿‡ç¨‹â€”â€”ä»è¯­æ³•åˆ†æåˆ°è¯­ä¹‰ç†è§£ï¼Œä»é€»è¾‘æ¨ç†åˆ°æƒ…æ„Ÿä½“éªŒï¼Œæ¯ä¸ªå±‚é¢éƒ½å¸¦æœ‰ä¸»è§‚æ€§å’Œä¸ç¡®å®šæ€§ã€‚å¦‚ä½•å°†è¿™ç§ä¸»è§‚åˆ¤æ–­è½¬åŒ–ä¸ºå¯é çš„ç§‘å­¦æµ‹åº¦ï¼Œæ˜¯äººç±»è¯„ä¼°æ¡†æ¶è®¾è®¡çš„æ ¸å¿ƒé—®é¢˜ã€‚

**å…³é”®æ´å¯Ÿ**ï¼š
- **ä¸»è§‚æ€§æ§åˆ¶**ï¼šé€šè¿‡ç§‘å­¦çš„å®éªŒè®¾è®¡é™ä½ä¸»è§‚åå·®
- **ä¸€è‡´æ€§ä¿è¯**ï¼šå»ºç«‹å¯é çš„æ ‡æ³¨è€…é—´ä¸€è‡´æ€§
- **ç»´åº¦åˆ†è§£**ï¼šå°†å¤æ‚çš„è¯­è¨€è´¨é‡åˆ†è§£ä¸ºå¯æ“ä½œçš„è¯„ä¼°ç»´åº¦
- **è§„æ¨¡åŒ–æŒ‘æˆ˜**ï¼šåœ¨ä¿è¯è´¨é‡çš„å‰æä¸‹å®ç°å¤§è§„æ¨¡è¯„ä¼°

ä»æ•°å­¦è§’åº¦çœ‹ï¼Œäººç±»è¯„ä¼°æ˜¯åœ¨æ„å»ºä»æ–‡æœ¬ç©ºé—´åˆ°è¯„ä¼°ç©ºé—´çš„éšæœºæ˜ å°„ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡ç»Ÿè®¡å­¦æ–¹æ³•æ¥æ§åˆ¶è¿™ä¸ªæ˜ å°„çš„å¯é æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## 2.1 è¯„ä¼°ç»´åº¦çš„æ“ä½œåŒ–å®šä¹‰

### è¯­è¨€è´¨é‡çš„å¤šç»´åˆ†è§£

**è´¨é‡ç»´åº¦çš„æ•°å­¦å»ºæ¨¡**ï¼š
è®¾æ–‡æœ¬è´¨é‡ä¸ºå¤šç»´å‘é‡ $\mathbf{Q} = (q_1, q_2, ..., q_d) \in \mathbb{R}^d$ï¼Œå…¶ä¸­æ¯ä¸ªç»´åº¦ä»£è¡¨ä¸€ä¸ªå¯æ“ä½œçš„è¯„ä¼°æ ‡å‡†ï¼š

$$\mathbf{Q}(t) = \begin{pmatrix}
\text{Fluency}(t) \\
\text{Coherence}(t) \\
\text{Relevance}(t) \\
\text{Informativeness}(t) \\
\text{Creativity}(t) \\
\vdots
\end{pmatrix}$$

**ç»´åº¦é—´å…³ç³»å»ºæ¨¡**ï¼š
è¯„ä¼°ç»´åº¦é€šå¸¸ä¸ç‹¬ç«‹ï¼Œå¯ç”¨ç›¸å…³çŸ©é˜µ $\mathbf{R}$ æè¿°ï¼š
$$\mathbf{R}_{ij} = \text{Corr}(q_i, q_j)$$

**ç»¼åˆè´¨é‡åˆ†æ•°**ï¼š
$$Q_{overall}(t) = \mathbf{w}^T \mathbf{Q}(t) = \sum_{i=1}^{d} w_i \cdot q_i(t)$$

å…¶ä¸­æƒé‡å‘é‡ $\mathbf{w}$ å¯é€šè¿‡ä¸»æˆåˆ†åˆ†ææˆ–å›å½’æ–¹æ³•ç¡®å®šã€‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Union, Any
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from dataclasses import dataclass, field
from collections import defaultdict, Counter
from scipy import stats
from sklearn.metrics import cohen_kappa_score
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

@dataclass
class EvaluationDimension:
    """è¯„ä¼°ç»´åº¦å®šä¹‰"""
    name: str
    description: str
    scale: Tuple[int, int]  # è¯„åˆ†åŒºé—´ï¼Œå¦‚(1, 5)
    criteria: List[str]     # è¯„ä¼°æ ‡å‡†
    examples: Dict[int, str] = field(default_factory=dict)  # å„åˆ†æ•°çº§åˆ«çš„ç¤ºä¾‹
    
@dataclass
class AnnotationResult:
    """æ ‡æ³¨ç»“æœæ•°æ®ç»“æ„"""
    text_id: str
    annotator_id: str
    dimensions: Dict[str, int]  # å„ç»´åº¦è¯„åˆ†
    overall_score: Optional[float] = None
    comments: str = ""
    annotation_time: float = 0.0
    confidence: Optional[float] = None

class HumanEvaluationFramework:
    """äººç±»è¯„ä¼°æ¡†æ¶"""
    
    def __init__(self):
        self.dimensions = self._define_evaluation_dimensions()
        self.annotations = []
        self.annotator_profiles = {}
        
    def _define_evaluation_dimensions(self) -> Dict[str, EvaluationDimension]:
        """å®šä¹‰è¯„ä¼°ç»´åº¦"""
        
        dimensions = {
            'fluency': EvaluationDimension(
                name='æµç•…æ€§',
                description='æ–‡æœ¬çš„è¯­æ³•æ­£ç¡®æ€§å’Œè‡ªç„¶æµç•…ç¨‹åº¦',
                scale=(1, 5),
                criteria=[
                    'è¯­æ³•é”™è¯¯æ•°é‡',
                    'å¥å­ç»“æ„åˆç†æ€§',
                    'è¯æ±‡ä½¿ç”¨æ°å½“æ€§',
                    'æ•´ä½“é˜…è¯»æµç•…æ€§'
                ],
                examples={
                    1: 'è¯­æ³•é”™è¯¯ä¸¥é‡ï¼Œéš¾ä»¥ç†è§£',
                    2: 'è¯­æ³•é”™è¯¯è¾ƒå¤šï¼Œå½±å“ç†è§£',
                    3: 'è¯­æ³•åŸºæœ¬æ­£ç¡®ï¼Œç•¥æœ‰ä¸è‡ªç„¶',
                    4: 'è¯­æ³•æ­£ç¡®ï¼Œè¡¨è¾¾è‡ªç„¶',
                    5: 'è¯­æ³•å®Œç¾ï¼Œè¡¨è¾¾éå¸¸è‡ªç„¶'
                }
            ),
            
            'coherence': EvaluationDimension(
                name='è¿è´¯æ€§',
                description='æ–‡æœ¬å†…å®¹çš„é€»è¾‘ä¸€è‡´æ€§å’Œè¿è´¯æ€§',
                scale=(1, 5),
                criteria=[
                    'é€»è¾‘ç»“æ„æ¸…æ™°æ€§',
                    'å‰åå†…å®¹ä¸€è‡´æ€§',
                    'ä¸»é¢˜èšç„¦ç¨‹åº¦',
                    'æ®µè½é—´è¡”æ¥'
                ],
                examples={
                    1: 'é€»è¾‘æ··ä¹±ï¼Œå‰åçŸ›ç›¾',
                    2: 'é€»è¾‘æ€§è¾ƒå·®ï¼Œéƒ¨åˆ†çŸ›ç›¾',
                    3: 'é€»è¾‘åŸºæœ¬æ¸…æ™°ï¼Œå¶æœ‰è·³è·ƒ',
                    4: 'é€»è¾‘æ¸…æ™°ï¼Œå†…å®¹è¿è´¯',
                    5: 'é€»è¾‘å®Œç¾ï¼Œé«˜åº¦è¿è´¯'
                }
            ),
            
            'relevance': EvaluationDimension(
                name='ç›¸å…³æ€§',
                description='æ–‡æœ¬å†…å®¹ä¸ç»™å®šä¸»é¢˜æˆ–ä»»åŠ¡çš„ç›¸å…³ç¨‹åº¦',
                scale=(1, 5),
                criteria=[
                    'ä¸»é¢˜å¥‘åˆåº¦',
                    'ä»»åŠ¡å®Œæˆåº¦',
                    'ä¿¡æ¯é’ˆå¯¹æ€§',
                    'å†…å®¹èšç„¦æ€§'
                ],
                examples={
                    1: 'å®Œå…¨åç¦»ä¸»é¢˜',
                    2: 'éƒ¨åˆ†ç›¸å…³ï¼Œåç¦»è¾ƒå¤š',
                    3: 'åŸºæœ¬ç›¸å…³ï¼Œæœ‰åç§»',
                    4: 'é«˜åº¦ç›¸å…³ï¼Œç´§æ‰£ä¸»é¢˜',
                    5: 'å®Œç¾å¥‘åˆï¼Œç²¾å‡†èšç„¦'
                }
            ),
            
            'informativeness': EvaluationDimension(
                name='ä¿¡æ¯é‡',
                description='æ–‡æœ¬åŒ…å«çš„æœ‰ç”¨ä¿¡æ¯æ•°é‡å’Œè´¨é‡',
                scale=(1, 5),
                criteria=[
                    'ä¿¡æ¯ä¸°å¯Œç¨‹åº¦',
                    'ç»†èŠ‚å®Œæ•´æ€§',
                    'æ·±åº¦åˆ†ææ°´å¹³',
                    'æ–°é¢–æ€§ç¨‹åº¦'
                ],
                examples={
                    1: 'ä¿¡æ¯æå°‘ï¼Œå†…å®¹ç©ºæ´',
                    2: 'ä¿¡æ¯è¾ƒå°‘ï¼Œç¼ºä¹æ·±åº¦',
                    3: 'ä¿¡æ¯é€‚ä¸­ï¼Œæ·±åº¦ä¸€èˆ¬',
                    4: 'ä¿¡æ¯ä¸°å¯Œï¼Œæœ‰ä¸€å®šæ·±åº¦',
                    5: 'ä¿¡æ¯éå¸¸ä¸°å¯Œï¼Œæ·±åº¦åˆ†æ'
                }
            ),
            
            'creativity': EvaluationDimension(
                name='åˆ›é€ æ€§',
                description='æ–‡æœ¬è¡¨è¾¾çš„åˆ›æ–°æ€§å’Œç‹¬ç‰¹æ€§',
                scale=(1, 5),
                criteria=[
                    'è¡¨è¾¾æ–¹å¼æ–°é¢–æ€§',
                    'è§‚ç‚¹ç‹¬ç‰¹æ€§',
                    'åˆ›æ„ç¨‹åº¦',
                    'æƒ³è±¡åŠ›ä½“ç°'
                ],
                examples={
                    1: 'å®Œå…¨æ¨¡å¼åŒ–ï¼Œæ— åˆ›æ„',
                    2: 'ç•¥æœ‰æ–°æ„ï¼Œåˆ›æ„æœ‰é™',
                    3: 'ä¸€å®šåˆ›æ„ï¼Œè¡¨è¾¾åˆç†',
                    4: 'è¾ƒæœ‰åˆ›æ„ï¼Œè¡¨è¾¾æ–°é¢–',
                    5: 'æå…·åˆ›æ„ï¼Œç‹¬ç‰¹æ–°é¢–'
                }
            )
        }
        
        return dimensions
    
    def add_annotation(self, annotation: AnnotationResult):
        """æ·»åŠ æ ‡æ³¨ç»“æœ"""
        self.annotations.append(annotation)
    
    def compute_overall_score(self, 
                            dimension_scores: Dict[str, int],
                            method: str = 'weighted_average',
                            weights: Optional[Dict[str, float]] = None) -> float:
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        
        if method == 'simple_average':
            return np.mean(list(dimension_scores.values()))
        
        elif method == 'weighted_average':
            if weights is None:
                # é»˜è®¤æƒé‡
                weights = {
                    'fluency': 0.25,
                    'coherence': 0.25,
                    'relevance': 0.25,
                    'informativeness': 0.15,
                    'creativity': 0.10
                }
            
            total_score = 0
            total_weight = 0
            for dim, score in dimension_scores.items():
                if dim in weights:
                    total_score += weights[dim] * score
                    total_weight += weights[dim]
            
            return total_score / total_weight if total_weight > 0 else 0
        
        elif method == 'pca_weighted':
            # ä½¿ç”¨ä¸»æˆåˆ†åˆ†æç¡®å®šæƒé‡
            return self._compute_pca_weighted_score(dimension_scores)
        
        else:
            raise ValueError(f"Unknown method: {method}")
    
    def _compute_pca_weighted_score(self, dimension_scores: Dict[str, int]) -> float:
        """åŸºäºPCAçš„åŠ æƒè¯„åˆ†"""
        
        # æ”¶é›†æ‰€æœ‰æ ‡æ³¨æ•°æ®è¿›è¡ŒPCA
        if len(self.annotations) < 10:  # æ•°æ®ä¸è¶³æ—¶ä½¿ç”¨ç®€å•å¹³å‡
            return np.mean(list(dimension_scores.values()))
        
        # æ„å»ºæ•°æ®çŸ©é˜µ
        score_matrix = []
        dimensions = list(self.dimensions.keys())
        
        for annotation in self.annotations:
            scores = [annotation.dimensions.get(dim, 3) for dim in dimensions]
            score_matrix.append(scores)
        
        score_matrix = np.array(score_matrix)
        
        # æ ‡å‡†åŒ–å’ŒPCA
        scaler = StandardScaler()
        normalized_scores = scaler.fit_transform(score_matrix)
        
        pca = PCA(n_components=1)
        pca.fit(normalized_scores)
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸»æˆåˆ†çš„è½½è·ä½œä¸ºæƒé‡
        weights = np.abs(pca.components_[0])
        weights = weights / np.sum(weights)
        
        # è®¡ç®—åŠ æƒåˆ†æ•°
        current_scores = [dimension_scores.get(dim, 3) for dim in dimensions]
        return np.dot(weights, current_scores)

## 2.2 æ ‡æ³¨ä¸€è‡´æ€§ç†è®ºä¸åº¦é‡

### Cohen's Kappaç³»æ•°çš„æ•°å­¦åŸç†

**Kappaç³»æ•°å®šä¹‰**ï¼š
$$\kappa = \frac{p_o - p_e}{1 - p_e}$$

å…¶ä¸­ï¼š
- $p_o$ æ˜¯è§‚å¯Ÿåˆ°çš„ä¸€è‡´æ€§æ¯”ä¾‹
- $p_e$ æ˜¯æœŸæœ›çš„éšæœºä¸€è‡´æ€§æ¯”ä¾‹

**å¤šåˆ†ç±»Kappaçš„è®¡ç®—**ï¼š
å¯¹äº $k$ ä¸ªç±»åˆ«ï¼Œæ··æ·†çŸ©é˜µä¸º $\mathbf{C}_{k \times k}$ï¼š
$$p_o = \frac{\sum_{i=1}^k C_{ii}}{N}$$
$$p_e = \frac{\sum_{i=1}^k \left(\sum_{j=1}^k C_{ij}\right) \left(\sum_{j=1}^k C_{ji}\right)}{N^2}$$

**åŠ æƒKappa**ï¼š
å¯¹äºæœ‰åºåˆ†ç±»ï¼ˆå¦‚è¯„åˆ†ï¼‰ï¼Œå¯ä½¿ç”¨åŠ æƒKappaï¼š
$$\kappa_w = 1 - \frac{\sum_{i,j} w_{ij} C_{ij}}{\sum_{i,j} w_{ij} E_{ij}}$$

å…¶ä¸­ $w_{ij}$ æ˜¯ä¸ä¸€è‡´æƒé‡ï¼Œé€šå¸¸å– $w_{ij} = |i-j|^2$ã€‚

### ç»„å†…ç›¸å…³ç³»æ•°(ICC)

**ICCçš„æ•°å­¦å®šä¹‰**ï¼š
å¯¹äºåŒå‘éšæœºæ•ˆåº”æ¨¡å‹ï¼š
$$\text{ICC}(2,1) = \frac{\text{MS}_R - \text{MS}_E}{\text{MS}_R + (k-1)\text{MS}_E + \frac{k}{n}(\text{MS}_C - \text{MS}_E)}$$

å…¶ä¸­ï¼š
- $\text{MS}_R$ æ˜¯è¡Œï¼ˆè¢«è¯„ä¼°é¡¹ï¼‰å‡æ–¹
- $\text{MS}_C$ æ˜¯åˆ—ï¼ˆè¯„ä¼°è€…ï¼‰å‡æ–¹  
- $\text{MS}_E$ æ˜¯è¯¯å·®å‡æ–¹
- $k$ æ˜¯è¯„ä¼°è€…æ•°é‡ï¼Œ$n$ æ˜¯è¢«è¯„ä¼°é¡¹æ•°é‡

class AnnotationConsistencyAnalyzer:
    """æ ‡æ³¨ä¸€è‡´æ€§åˆ†æå™¨"""
    
    def __init__(self):
        self.annotations_matrix = None
        self.annotator_pairs = []
        
    def prepare_data(self, annotations: List[AnnotationResult], dimension: str):
        """å‡†å¤‡æ ‡æ³¨ä¸€è‡´æ€§åˆ†ææ•°æ®"""
        
        # æ„å»ºæ ‡æ³¨çŸ©é˜µ
        text_ids = sorted(set(ann.text_id for ann in annotations))
        annotator_ids = sorted(set(ann.annotator_id for ann in annotations))
        
        matrix = np.full((len(text_ids), len(annotator_ids)), np.nan)
        
        for ann in annotations:
            if dimension in ann.dimensions:
                text_idx = text_ids.index(ann.text_id)
                annotator_idx = annotator_ids.index(ann.annotator_id)
                matrix[text_idx, annotator_idx] = ann.dimensions[dimension]
        
        self.annotations_matrix = matrix
        self.text_ids = text_ids
        self.annotator_ids = annotator_ids
        
        return matrix
    
    def compute_pairwise_kappa(self, 
                              annotations: List[AnnotationResult],
                              dimension: str,
                              weights: Optional[str] = None) -> Dict:
        """è®¡ç®—ä¸¤ä¸¤æ ‡æ³¨è€…é—´çš„Kappaç³»æ•°"""
        
        print(f"=== {dimension}ç»´åº¦ä¸¤ä¸¤Kappaåˆ†æ ===")
        
        # å‡†å¤‡æ•°æ®
        self.prepare_data(annotations, dimension)
        
        # è®¡ç®—ä¸¤ä¸¤Kappa
        kappa_matrix = np.full((len(self.annotator_ids), len(self.annotator_ids)), np.nan)
        pairwise_kappas = []
        
        for i in range(len(self.annotator_ids)):
            for j in range(i+1, len(self.annotator_ids)):
                # æå–ä¸¤ä¸ªæ ‡æ³¨è€…çš„è¯„åˆ†
                scores_i = self.annotations_matrix[:, i]
                scores_j = self.annotations_matrix[:, j]
                
                # è¿‡æ»¤NaNå€¼
                valid_mask = ~(np.isnan(scores_i) | np.isnan(scores_j))
                if np.sum(valid_mask) < 5:  # éœ€è¦è‡³å°‘5ä¸ªå…±åŒæ ‡æ³¨
                    continue
                
                valid_scores_i = scores_i[valid_mask].astype(int)
                valid_scores_j = scores_j[valid_mask].astype(int)
                
                # è®¡ç®—Kappa
                if weights == 'quadratic':
                    kappa = self._compute_weighted_kappa(valid_scores_i, valid_scores_j)
                else:
                    kappa = cohen_kappa_score(valid_scores_i, valid_scores_j)
                
                kappa_matrix[i, j] = kappa
                kappa_matrix[j, i] = kappa
                pairwise_kappas.append(kappa)
        
        # å¡«å……å¯¹è§’çº¿
        np.fill_diagonal(kappa_matrix, 1.0)
        
        # ç»Ÿè®¡åˆ†æ
        kappa_stats = {
            'mean': np.mean(pairwise_kappas),
            'std': np.std(pairwise_kappas),
            'min': np.min(pairwise_kappas),
            'max': np.max(pairwise_kappas),
            'median': np.median(pairwise_kappas)
        }
        
        print(f"å¹³å‡Kappa: {kappa_stats['mean']:.4f} Â± {kappa_stats['std']:.4f}")
        print(f"KappaèŒƒå›´: [{kappa_stats['min']:.4f}, {kappa_stats['max']:.4f}]")
        
        return {
            'kappa_matrix': kappa_matrix,
            'pairwise_kappas': pairwise_kappas,
            'statistics': kappa_stats,
            'annotator_ids': self.annotator_ids
        }
    
    def _compute_weighted_kappa(self, scores1: np.ndarray, scores2: np.ndarray) -> float:
        """è®¡ç®—åŠ æƒKappaç³»æ•°"""
        
        # æ„å»ºæ··æ·†çŸ©é˜µ
        min_score = min(np.min(scores1), np.min(scores2))
        max_score = max(np.max(scores1), np.max(scores2))
        
        confusion_matrix = np.zeros((max_score - min_score + 1, max_score - min_score + 1))
        
        for s1, s2 in zip(scores1, scores2):
            confusion_matrix[s1 - min_score, s2 - min_score] += 1
        
        n = len(scores1)
        
        # è®¡ç®—è§‚å¯Ÿä¸€è‡´æ€§
        po = 0
        for i in range(confusion_matrix.shape[0]):
            for j in range(confusion_matrix.shape[1]):
                weight = 1 - ((i - j) ** 2) / ((max_score - min_score) ** 2)
                po += weight * confusion_matrix[i, j] / n
        
        # è®¡ç®—æœŸæœ›ä¸€è‡´æ€§
        row_marginals = np.sum(confusion_matrix, axis=1) / n
        col_marginals = np.sum(confusion_matrix, axis=0) / n
        
        pe = 0
        for i in range(confusion_matrix.shape[0]):
            for j in range(confusion_matrix.shape[1]):
                weight = 1 - ((i - j) ** 2) / ((max_score - min_score) ** 2)
                pe += weight * row_marginals[i] * col_marginals[j]
        
        # è®¡ç®—åŠ æƒKappa
        if pe == 1:
            return 1.0
        else:
            return (po - pe) / (1 - pe)
    
    def compute_icc(self, 
                   annotations: List[AnnotationResult],
                   dimension: str,
                   icc_type: str = '2,1') -> Dict:
        """è®¡ç®—ç»„å†…ç›¸å…³ç³»æ•°"""
        
        print(f"=== {dimension}ç»´åº¦ICCåˆ†æ ===")
        
        # å‡†å¤‡æ•°æ®
        matrix = self.prepare_data(annotations, dimension)
        
        # ç§»é™¤åŒ…å«NaNçš„è¡Œ
        valid_rows = ~np.any(np.isnan(matrix), axis=1)
        clean_matrix = matrix[valid_rows]
        
        if clean_matrix.shape[0] < 3:
            print("æœ‰æ•ˆæ•°æ®ä¸è¶³ï¼Œæ— æ³•è®¡ç®—ICC")
            return {'icc': np.nan, 'confidence_interval': (np.nan, np.nan)}
        
        # è®¡ç®—ICC
        n, k = clean_matrix.shape
        
        # è®¡ç®—å‡æ–¹
        row_means = np.mean(clean_matrix, axis=1)
        col_means = np.mean(clean_matrix, axis=0)
        grand_mean = np.mean(clean_matrix)
        
        # è¡Œé—´å‡æ–¹ (MSR)
        msr = k * np.sum((row_means - grand_mean) ** 2) / (n - 1)
        
        # åˆ—é—´å‡æ–¹ (MSC)
        msc = n * np.sum((col_means - grand_mean) ** 2) / (k - 1)
        
        # è¯¯å·®å‡æ–¹ (MSE)
        mse = np.sum((clean_matrix - row_means.reshape(-1, 1) - col_means.reshape(1, -1) + grand_mean) ** 2) / ((n - 1) * (k - 1))
        
        # è®¡ç®—ICC(2,1)
        if icc_type == '2,1':
            icc = (msr - mse) / (msr + (k - 1) * mse + k * (msc - mse) / n)
        else:
            raise ValueError(f"Unsupported ICC type: {icc_type}")
        
        # è®¡ç®—ç½®ä¿¡åŒºé—´ï¼ˆç®€åŒ–å®ç°ï¼‰
        # å®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„Fåˆ†å¸ƒè®¡ç®—
        alpha = 0.05
        f_value = msr / mse
        
        # ç®€åŒ–çš„ç½®ä¿¡åŒºé—´ä¼°è®¡
        icc_lower = max(0, icc - 1.96 * np.sqrt(2 * (1 - icc) ** 2 * (k - 1) / (n * k)))
        icc_upper = min(1, icc + 1.96 * np.sqrt(2 * (1 - icc) ** 2 * (k - 1) / (n * k)))
        
        print(f"ICC({icc_type}): {icc:.4f}")
        print(f"95%ç½®ä¿¡åŒºé—´: [{icc_lower:.4f}, {icc_upper:.4f}]")
        
        return {
            'icc': icc,
            'confidence_interval': (icc_lower, icc_upper),
            'msr': msr,
            'msc': msc,
            'mse': mse,
            'n_subjects': n,
            'n_raters': k
        }
    
    def analyze_annotator_bias(self, 
                             annotations: List[AnnotationResult],
                             dimension: str) -> Dict:
        """åˆ†ææ ‡æ³¨è€…åå·®"""
        
        print(f"=== {dimension}ç»´åº¦æ ‡æ³¨è€…åå·®åˆ†æ ===")
        
        # å‡†å¤‡æ•°æ®
        matrix = self.prepare_data(annotations, dimension)
        
        # è®¡ç®—å„æ ‡æ³¨è€…çš„ç»Ÿè®¡ç‰¹å¾
        annotator_stats = {}
        for i, annotator_id in enumerate(self.annotator_ids):
            scores = matrix[:, i]
            valid_scores = scores[~np.isnan(scores)]
            
            if len(valid_scores) > 0:
                annotator_stats[annotator_id] = {
                    'mean': np.mean(valid_scores),
                    'std': np.std(valid_scores),
                    'min': np.min(valid_scores),
                    'max': np.max(valid_scores),
                    'count': len(valid_scores),
                    'skewness': stats.skew(valid_scores),
                    'kurtosis': stats.kurtosis(valid_scores)
                }
        
        # åˆ†æä¸¥æ ¼åº¦å·®å¼‚
        means = [stats['mean'] for stats in annotator_stats.values()]
        stds = [stats['std'] for stats in annotator_stats.values()]
        
        severity_analysis = {
            'mean_difference': np.max(means) - np.min(means),
            'std_difference': np.max(stds) - np.min(stds),
            'severity_ranking': sorted(annotator_stats.items(), key=lambda x: x[1]['mean'])
        }
        
        print(f"æ ‡æ³¨è€…è¯„åˆ†å‡å€¼å·®å¼‚: {severity_analysis['mean_difference']:.4f}")
        print(f"æ ‡æ³¨è€…è¯„åˆ†æ ‡å‡†å·®å·®å¼‚: {severity_analysis['std_difference']:.4f}")
        
        print("æ ‡æ³¨è€…ä¸¥æ ¼åº¦æ’åº (ä»ä¸¥æ ¼åˆ°å®½æ¾):")
        for annotator_id, stats in severity_analysis['severity_ranking']:
            print(f"  {annotator_id}: å‡å€¼={stats['mean']:.2f}, æ ‡å‡†å·®={stats['std']:.2f}")
        
        return {
            'annotator_statistics': annotator_stats,
            'severity_analysis': severity_analysis
        }

## 2.3 è®¤çŸ¥åå·®æ§åˆ¶

### å¸¸è§è¯„ä¼°åå·®åŠå…¶æ•°å­¦å»ºæ¨¡

**å…‰ç¯æ•ˆåº”(Halo Effect)**ï¼š
$$P(q_i = s | q_j = s') = P(q_i = s) + \alpha \cdot I(s = s')$$

å…¶ä¸­ $\alpha$ æ˜¯å…‰ç¯æ•ˆåº”å¼ºåº¦ï¼Œ$I(\cdot)$ æ˜¯æŒ‡ç¤ºå‡½æ•°ã€‚

**é”šå®šåå·®(Anchoring Bias)**ï¼š
$$q_t = \beta \cdot q_{anchor} + (1-\beta) \cdot q_{true} + \epsilon$$

å…¶ä¸­ $q_{anchor}$ æ˜¯é”šå®šå‚è€ƒå€¼ï¼Œ$\beta$ æ˜¯é”šå®šå¼ºåº¦ã€‚

**é¡ºåºæ•ˆåº”(Order Effect)**ï¼š
$$q_t = q_{true} + \gamma \cdot f(t) + \epsilon$$

å…¶ä¸­ $f(t)$ æ˜¯æ—¶é—´ç›¸å…³çš„ç–²åŠ³å‡½æ•°ã€‚

class CognitiveBiasController:
    """è®¤çŸ¥åå·®æ§åˆ¶å™¨"""
    
    def __init__(self):
        self.bias_detection_methods = {
            'halo_effect': self._detect_halo_effect,
            'anchoring_bias': self._detect_anchoring_bias,
            'order_effect': self._detect_order_effect,
            'severity_bias': self._detect_severity_bias
        }
    
    def detect_biases(self, 
                     annotations: List[AnnotationResult]) -> Dict:
        """æ£€æµ‹å„ç§è®¤çŸ¥åå·®"""
        
        print("=== è®¤çŸ¥åå·®æ£€æµ‹åˆ†æ ===")
        
        bias_results = {}
        
        for bias_name, detection_method in self.bias_detection_methods.items():
            try:
                bias_result = detection_method(annotations)
                bias_results[bias_name] = bias_result
                print(f"{bias_name}: {bias_result.get('severity', 'N/A')}")
            except Exception as e:
                print(f"æ£€æµ‹{bias_name}æ—¶å‡ºé”™: {e}")
                bias_results[bias_name] = {'error': str(e)}
        
        return bias_results
    
    def _detect_halo_effect(self, annotations: List[AnnotationResult]) -> Dict:
        """æ£€æµ‹å…‰ç¯æ•ˆåº”"""
        
        # è®¡ç®—ç»´åº¦é—´ç›¸å…³æ€§
        dimension_scores = defaultdict(list)
        
        for ann in annotations:
            for dim, score in ann.dimensions.items():
                dimension_scores[dim].append(score)
        
        # è®¡ç®—ç›¸å…³çŸ©é˜µ
        dimensions = list(dimension_scores.keys())
        correlations = []
        
        for i in range(len(dimensions)):
            for j in range(i+1, len(dimensions)):
                dim1, dim2 = dimensions[i], dimensions[j]
                if len(dimension_scores[dim1]) == len(dimension_scores[dim2]):
                    corr = np.corrcoef(dimension_scores[dim1], dimension_scores[dim2])[0, 1]
                    correlations.append(abs(corr))
        
        # å…‰ç¯æ•ˆåº”å¼ºåº¦ = å¹³å‡ç›¸å…³ç³»æ•°
        halo_strength = np.mean(correlations) if correlations else 0
        
        severity = 'low' if halo_strength < 0.3 else 'medium' if halo_strength < 0.6 else 'high'
        
        return {
            'halo_strength': halo_strength,
            'severity': severity,
            'dimension_correlations': correlations
        }
    
    def _detect_anchoring_bias(self, annotations: List[AnnotationResult]) -> Dict:
        """æ£€æµ‹é”šå®šåå·®"""
        
        # æŒ‰æ ‡æ³¨è€…åˆ†ç»„ï¼Œåˆ†æè¯„åˆ†è¶‹åŠ¿
        annotator_sequences = defaultdict(list)
        
        for ann in annotations:
            # ä½¿ç”¨ç»¼åˆè¯„åˆ†ä½œä¸ºé”šå®šæ£€æµ‹çš„æŒ‡æ ‡
            overall_score = np.mean(list(ann.dimensions.values()))
            annotator_sequences[ann.annotator_id].append((ann.text_id, overall_score))
        
        # è®¡ç®—åºåˆ—ç›¸å…³æ€§ï¼ˆç›¸é‚»è¯„åˆ†çš„ç›¸å…³æ€§ï¼‰
        sequence_correlations = []
        
        for annotator_id, sequence in annotator_sequences.items():
            if len(sequence) < 3:
                continue
            
            # æŒ‰æŸç§é¡ºåºæ’åºï¼ˆå®é™…åº”ç”¨ä¸­å¯èƒ½æ˜¯æ—¶é—´é¡ºåºï¼‰
            sequence.sort(key=lambda x: x[0])
            scores = [score for _, score in sequence]
            
            # è®¡ç®—ç›¸é‚»è¯„åˆ†çš„ç›¸å…³æ€§
            adjacent_correlations = []
            for i in range(len(scores) - 1):
                if i < len(scores) - 2:
                    corr = np.corrcoef([scores[i], scores[i+1]], [scores[i+1], scores[i+2]])[0, 1]
                    if not np.isnan(corr):
                        adjacent_correlations.append(corr)
            
            if adjacent_correlations:
                sequence_correlations.extend(adjacent_correlations)
        
        # é”šå®šåå·®å¼ºåº¦
        anchoring_strength = np.mean(sequence_correlations) if sequence_correlations else 0
        severity = 'low' if anchoring_strength < 0.3 else 'medium' if anchoring_strength < 0.6 else 'high'
        
        return {
            'anchoring_strength': anchoring_strength,
            'severity': severity,
            'sequence_correlations': sequence_correlations
        }
    
    def _detect_order_effect(self, annotations: List[AnnotationResult]) -> Dict:
        """æ£€æµ‹é¡ºåºæ•ˆåº”"""
        
        # æŒ‰æ ‡æ³¨è€…åˆ†æè¯„åˆ†éšæ—¶é—´çš„å˜åŒ–
        annotator_trends = {}
        
        for ann in annotations:
            if ann.annotator_id not in annotator_trends:
                annotator_trends[ann.annotator_id] = []
            
            overall_score = np.mean(list(ann.dimensions.values()))
            annotator_trends[ann.annotator_id].append(overall_score)
        
        # è®¡ç®—è¶‹åŠ¿å¼ºåº¦
        trend_strengths = []
        
        for annotator_id, scores in annotator_trends.items():
            if len(scores) < 5:
                continue
            
            # ä½¿ç”¨çº¿æ€§å›å½’æ£€æµ‹è¶‹åŠ¿
            x = np.arange(len(scores))
            slope, intercept, r_value, p_value, std_err = stats.linregress(x, scores)
            
            trend_strengths.append(abs(slope))
        
        # é¡ºåºæ•ˆåº”å¼ºåº¦
        order_effect_strength = np.mean(trend_strengths) if trend_strengths else 0
        severity = 'low' if order_effect_strength < 0.1 else 'medium' if order_effect_strength < 0.2 else 'high'
        
        return {
            'order_effect_strength': order_effect_strength,
            'severity': severity,
            'individual_trends': trend_strengths
        }
    
    def _detect_severity_bias(self, annotations: List[AnnotationResult]) -> Dict:
        """æ£€æµ‹ä¸¥æ ¼åº¦åå·®"""
        
        # è®¡ç®—å„æ ‡æ³¨è€…çš„è¯„åˆ†åˆ†å¸ƒ
        annotator_distributions = {}
        
        for ann in annotations:
            if ann.annotator_id not in annotator_distributions:
                annotator_distributions[ann.annotator_id] = []
            
            overall_score = np.mean(list(ann.dimensions.values()))
            annotator_distributions[ann.annotator_id].append(overall_score)
        
        # è®¡ç®—åˆ†å¸ƒå·®å¼‚
        means = []
        stds = []
        
        for annotator_id, scores in annotator_distributions.items():
            if len(scores) >= 3:
                means.append(np.mean(scores))
                stds.append(np.std(scores))
        
        # ä¸¥æ ¼åº¦åå·®å¼ºåº¦
        mean_range = np.max(means) - np.min(means) if means else 0
        std_range = np.max(stds) - np.min(stds) if stds else 0
        
        severity_bias_strength = (mean_range + std_range) / 2
        severity = 'low' if severity_bias_strength < 0.5 else 'medium' if severity_bias_strength < 1.0 else 'high'
        
        return {
            'severity_bias_strength': severity_bias_strength,
            'severity': severity,
            'mean_range': mean_range,
            'std_range': std_range
        }
    
    def suggest_bias_mitigation_strategies(self, bias_results: Dict) -> List[str]:
        """å»ºè®®åå·®ç¼“è§£ç­–ç•¥"""
        
        strategies = []
        
        # æ ¹æ®æ£€æµ‹åˆ°çš„åå·®ç±»å‹ç»™å‡ºå»ºè®®
        if bias_results.get('halo_effect', {}).get('severity') in ['medium', 'high']:
            strategies.append("å®æ–½ç‹¬ç«‹ç»´åº¦è¯„ä¼°ï¼šè¦æ±‚æ ‡æ³¨è€…åˆ†åˆ«ç‹¬ç«‹è¯„ä¼°å„ä¸ªç»´åº¦")
            strategies.append("ä½¿ç”¨ç›²è¯„æ–¹æ³•ï¼šéšè—å…¶ä»–ç»´åº¦çš„è¯„åˆ†")
        
        if bias_results.get('anchoring_bias', {}).get('severity') in ['medium', 'high']:
            strategies.append("éšæœºåŒ–å±•ç¤ºé¡ºåºï¼šéšæœºæ‰“ä¹±è¯„ä¼°æ ·æœ¬çš„é¡ºåº")
            strategies.append("æä¾›æ ¡å‡†æ ·æœ¬ï¼šåœ¨è¯„ä¼°å‰æä¾›æ ‡å‡†åŒ–çš„å‚è€ƒæ ·æœ¬")
        
        if bias_results.get('order_effect', {}).get('severity') in ['medium', 'high']:
            strategies.append("å®šæœŸä¼‘æ¯ï¼šè®¾ç½®å¼ºåˆ¶ä¼‘æ¯æ—¶é—´é˜²æ­¢ç–²åŠ³")
            strategies.append("åˆ†æ‰¹è¯„ä¼°ï¼šå°†å¤§é‡è¯„ä¼°ä»»åŠ¡åˆ†è§£ä¸ºå°æ‰¹æ¬¡")
        
        if bias_results.get('severity_bias', {}).get('severity') in ['medium', 'high']:
            strategies.append("æ ‡æ³¨è€…æ ¡å‡†ï¼šé€šè¿‡åŸ¹è®­ç»Ÿä¸€æ ‡æ³¨æ ‡å‡†")
            strategies.append("åå¤„ç†æ ‡å‡†åŒ–ï¼šå¯¹ä¸åŒæ ‡æ³¨è€…çš„è¯„åˆ†è¿›è¡Œç»Ÿè®¡æ ‡å‡†åŒ–")
        
        return strategies

## 2.4 å¤§è§„æ¨¡æ ‡æ³¨ç­–ç•¥ä¸è´¨é‡æ§åˆ¶

### ä¼—åŒ…æ ‡æ³¨çš„ç»Ÿè®¡æ¨æ–­

**æ ‡æ³¨è€…èƒ½åŠ›å»ºæ¨¡**ï¼š
è®¾æ ‡æ³¨è€… $i$ çš„èƒ½åŠ›ä¸º $\theta_i$ï¼ŒçœŸå®æ ‡ç­¾ä¸º $y^*$ï¼Œè§‚å¯Ÿæ ‡ç­¾ä¸º $y_i$ï¼š
$$P(y_i = y^* | \theta_i) = \theta_i \cdot I(y_i = y^*) + \frac{1-\theta_i}{K-1} \cdot I(y_i \neq y^*)$$

å…¶ä¸­ $K$ æ˜¯ç±»åˆ«æ•°ã€‚

**æœŸæœ›æœ€å¤§åŒ–(EM)ç®—æ³•**ï¼š
é€šè¿‡EMç®—æ³•åŒæ—¶ä¼°è®¡çœŸå®æ ‡ç­¾å’Œæ ‡æ³¨è€…èƒ½åŠ›ï¼š

Eæ­¥ï¼š
$$q(y^*_j = k) = \frac{\prod_i P(y_{ij} | y^*_j = k, \theta_i)}{\sum_{k'} \prod_i P(y_{ij} | y^*_j = k', \theta_i)}$$

Mæ­¥ï¼š
$$\theta_i = \frac{\sum_j \sum_k q(y^*_j = k) \cdot I(y_{ij} = k)}{\sum_j \sum_k q(y^*_j = k)}$$

class CrowdsourcingQualityController:
    """ä¼—åŒ…è´¨é‡æ§åˆ¶å™¨"""
    
    def __init__(self, min_annotations_per_item: int = 3):
        self.min_annotations_per_item = min_annotations_per_item
        self.annotator_abilities = {}
        self.true_labels = {}
        
    def estimate_true_labels_and_abilities(self, 
                                         annotations: List[AnnotationResult],
                                         dimension: str,
                                         max_iterations: int = 100,
                                         tolerance: float = 1e-6) -> Dict:
        """ä½¿ç”¨EMç®—æ³•ä¼°è®¡çœŸå®æ ‡ç­¾å’Œæ ‡æ³¨è€…èƒ½åŠ›"""
        
        print(f"=== {dimension}ç»´åº¦EMç®—æ³•ä¼°è®¡ ===")
        
        # å‡†å¤‡æ•°æ®
        items = sorted(set(ann.text_id for ann in annotations))
        annotators = sorted(set(ann.annotator_id for ann in annotations))
        
        # æ„å»ºæ ‡æ³¨çŸ©é˜µ
        annotation_matrix = {}
        for ann in annotations:
            if dimension in ann.dimensions:
                key = (ann.text_id, ann.annotator_id)
                annotation_matrix[key] = ann.dimensions[dimension]
        
        # åˆå§‹åŒ–å‚æ•°
        n_items = len(items)
        n_annotators = len(annotators)
        n_classes = 5  # å‡è®¾1-5åˆ†
        
        # åˆå§‹åŒ–æ ‡æ³¨è€…èƒ½åŠ›ï¼ˆå‡è®¾éƒ½æ˜¯0.6ï¼‰
        abilities = {annotator: 0.6 for annotator in annotators}
        
        # åˆå§‹åŒ–çœŸå®æ ‡ç­¾æ¦‚ç‡ï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰
        true_label_probs = {}
        for item in items:
            true_label_probs[item] = np.ones(n_classes) / n_classes
        
        # EMè¿­ä»£
        log_likelihood_history = []
        
        for iteration in range(max_iterations):
            # Eæ­¥ï¼šä¼°è®¡çœŸå®æ ‡ç­¾æ¦‚ç‡
            new_true_label_probs = {}
            
            for item in items:
                class_probs = np.ones(n_classes)
                
                for annotator in annotators:
                    key = (item, annotator)
                    if key in annotation_matrix:
                        observed_label = annotation_matrix[key] - 1  # è½¬ä¸º0ç´¢å¼•
                        ability = abilities[annotator]
                        
                        for true_class in range(n_classes):
                            if observed_label == true_class:
                                likelihood = ability
                            else:
                                likelihood = (1 - ability) / (n_classes - 1)
                            class_probs[true_class] *= likelihood
                
                # å½’ä¸€åŒ–
                class_probs = class_probs / np.sum(class_probs)
                new_true_label_probs[item] = class_probs
            
            # Mæ­¥ï¼šä¼°è®¡æ ‡æ³¨è€…èƒ½åŠ›
            new_abilities = {}
            
            for annotator in annotators:
                correct_weight = 0
                total_weight = 0
                
                for item in items:
                    key = (item, annotator)
                    if key in annotation_matrix:
                        observed_label = annotation_matrix[key] - 1
                        true_probs = new_true_label_probs[item]
                        
                        correct_weight += true_probs[observed_label]
                        total_weight += 1
                
                if total_weight > 0:
                    new_abilities[annotator] = correct_weight / total_weight
                else:
                    new_abilities[annotator] = 0.5
            
            # è®¡ç®—å¯¹æ•°ä¼¼ç„¶
            log_likelihood = 0
            for item in items:
                for annotator in annotators:
                    key = (item, annotator)
                    if key in annotation_matrix:
                        observed_label = annotation_matrix[key] - 1
                        ability = new_abilities[annotator]
                        true_probs = new_true_label_probs[item]
                        
                        item_likelihood = 0
                        for true_class in range(n_classes):
                            if observed_label == true_class:
                                item_likelihood += true_probs[true_class] * ability
                            else:
                                item_likelihood += true_probs[true_class] * (1 - ability) / (n_classes - 1)
                        
                        if item_likelihood > 0:
                            log_likelihood += np.log(item_likelihood)
            
            log_likelihood_history.append(log_likelihood)
            
            # æ£€æŸ¥æ”¶æ•›
            if iteration > 0:
                if abs(log_likelihood_history[-1] - log_likelihood_history[-2]) < tolerance:
                    print(f"EMç®—æ³•åœ¨ç¬¬{iteration+1}æ¬¡è¿­ä»£æ”¶æ•›")
                    break
            
            # æ›´æ–°å‚æ•°
            abilities = new_abilities
            true_label_probs = new_true_label_probs
        
        # ç”Ÿæˆæœ€ç»ˆçš„çœŸå®æ ‡ç­¾ä¼°è®¡
        estimated_labels = {}
        for item in items:
            estimated_labels[item] = np.argmax(true_label_probs[item]) + 1  # è½¬å›1-5åˆ†
        
        print(f"æ ‡æ³¨è€…èƒ½åŠ›ä¼°è®¡:")
        for annotator, ability in sorted(abilities.items()):
            print(f"  {annotator}: {ability:.4f}")
        
        return {
            'estimated_labels': estimated_labels,
            'annotator_abilities': abilities,
            'true_label_probabilities': true_label_probs,
            'log_likelihood_history': log_likelihood_history,
            'converged_iteration': iteration + 1
        }
    
    def compute_annotation_quality_metrics(self, 
                                         annotations: List[AnnotationResult],
                                         estimated_results: Dict,
                                         dimension: str) -> Dict:
        """è®¡ç®—æ ‡æ³¨è´¨é‡æŒ‡æ ‡"""
        
        print(f"=== {dimension}ç»´åº¦æ ‡æ³¨è´¨é‡æŒ‡æ ‡ ===")
        
        estimated_labels = estimated_results['estimated_labels']
        annotator_abilities = estimated_results['annotator_abilities']
        
        # è®¡ç®—å„æ ‡æ³¨è€…çš„å‡†ç¡®ç‡
        annotator_accuracies = {}
        
        for ann in annotations:
            if dimension in ann.dimensions and ann.text_id in estimated_labels:
                true_label = estimated_labels[ann.text_id]
                observed_label = ann.dimensions[dimension]
                
                if ann.annotator_id not in annotator_accuracies:
                    annotator_accuracies[ann.annotator_id] = {'correct': 0, 'total': 0}
                
                annotator_accuracies[ann.annotator_id]['total'] += 1
                if observed_label == true_label:
                    annotator_accuracies[ann.annotator_id]['correct'] += 1
        
        # è®¡ç®—å‡†ç¡®ç‡
        for annotator_id in annotator_accuracies:
            stats = annotator_accuracies[annotator_id]
            stats['accuracy'] = stats['correct'] / stats['total']
        
        # è®¡ç®—æ•´ä½“è´¨é‡æŒ‡æ ‡
        all_accuracies = [stats['accuracy'] for stats in annotator_accuracies.values()]
        
        quality_metrics = {
            'mean_accuracy': np.mean(all_accuracies),
            'std_accuracy': np.std(all_accuracies),
            'min_accuracy': np.min(all_accuracies),
            'max_accuracy': np.max(all_accuracies),
            'annotator_count': len(annotator_accuracies),
            'total_annotations': sum(stats['total'] for stats in annotator_accuracies.values())
        }
        
        print(f"å¹³å‡å‡†ç¡®ç‡: {quality_metrics['mean_accuracy']:.4f}")
        print(f"å‡†ç¡®ç‡æ ‡å‡†å·®: {quality_metrics['std_accuracy']:.4f}")
        print(f"æ ‡æ³¨è€…æ•°é‡: {quality_metrics['annotator_count']}")
        print(f"æ€»æ ‡æ³¨æ•°é‡: {quality_metrics['total_annotations']}")
        
        return {
            'annotator_accuracies': annotator_accuracies,
            'quality_metrics': quality_metrics,
            'ability_accuracy_correlation': np.corrcoef(
                list(annotator_abilities.values()),
                [stats['accuracy'] for stats in annotator_accuracies.values()]
            )[0, 1] if len(annotator_abilities) > 1 else np.nan
        }
    
    def design_adaptive_annotation_strategy(self, 
                                          current_annotations: List[AnnotationResult],
                                          target_confidence: float = 0.9) -> Dict:
        """è®¾è®¡è‡ªé€‚åº”æ ‡æ³¨ç­–ç•¥"""
        
        print("=== è‡ªé€‚åº”æ ‡æ³¨ç­–ç•¥è®¾è®¡ ===")
        
        # ç»Ÿè®¡å½“å‰æ ‡æ³¨æƒ…å†µ
        item_annotation_counts = Counter(ann.text_id for ann in current_annotations)
        
        # è¯†åˆ«éœ€è¦æ›´å¤šæ ‡æ³¨çš„é¡¹ç›®
        under_annotated_items = [
            item for item, count in item_annotation_counts.items()
            if count < self.min_annotations_per_item
        ]
        
        # åŸºäºä¸ç¡®å®šæ€§çš„ä¸»åŠ¨å­¦ä¹ ç­–ç•¥
        uncertainty_scores = {}
        
        for item_id in set(ann.text_id for ann in current_annotations):
            item_annotations = [ann for ann in current_annotations if ann.text_id == item_id]
            
            if len(item_annotations) >= 2:
                # è®¡ç®—æ ‡æ³¨åˆ†æ­§åº¦ä½œä¸ºä¸ç¡®å®šæ€§æŒ‡æ ‡
                dimension_disagreements = {}
                
                for dim in ['fluency', 'coherence', 'relevance', 'informativeness', 'creativity']:
                    scores = [ann.dimensions.get(dim, 3) for ann in item_annotations if dim in ann.dimensions]
                    if len(scores) >= 2:
                        disagreement = np.std(scores)
                        dimension_disagreements[dim] = disagreement
                
                # ç»¼åˆä¸ç¡®å®šæ€§åˆ†æ•°
                if dimension_disagreements:
                    uncertainty_scores[item_id] = np.mean(list(dimension_disagreements.values()))
                else:
                    uncertainty_scores[item_id] = 0
        
        # æ’åºç¡®å®šä¼˜å…ˆçº§
        high_uncertainty_items = sorted(
            uncertainty_scores.items(), 
            key=lambda x: x[1], 
            reverse=True
        )[:10]  # å–å‰10ä¸ªæœ€ä¸ç¡®å®šçš„é¡¹ç›®
        
        # æ ‡æ³¨è€…åˆ†é…ç­–ç•¥
        annotator_workloads = Counter(ann.annotator_id for ann in current_annotations)
        available_annotators = sorted(annotator_workloads.keys())
        
        annotation_plan = {
            'under_annotated_items': under_annotated_items,
            'high_uncertainty_items': [item for item, score in high_uncertainty_items],
            'recommended_annotations': len(under_annotated_items) * self.min_annotations_per_item + len(high_uncertainty_items),
            'annotator_allocation': self._allocate_annotators(
                under_annotated_items + [item for item, _ in high_uncertainty_items],
                available_annotators
            )
        }
        
        print(f"å¾…è¡¥å……æ ‡æ³¨é¡¹ç›®: {len(under_annotated_items)}")
        print(f"é«˜ä¸ç¡®å®šæ€§é¡¹ç›®: {len(high_uncertainty_items)}")
        print(f"å»ºè®®æ€»æ ‡æ³¨æ•°: {annotation_plan['recommended_annotations']}")
        
        return annotation_plan
    
    def _allocate_annotators(self, items: List[str], annotators: List[str]) -> Dict:
        """åˆ†é…æ ‡æ³¨è€…"""
        
        allocation = defaultdict(list)
        
        # ç®€å•çš„è½®è¯¢åˆ†é…ç­–ç•¥
        for i, item in enumerate(items):
            annotator = annotators[i % len(annotators)]
            allocation[annotator].append(item)
        
        return dict(allocation)

def create_human_evaluation_system():
    """åˆ›å»ºäººç±»è¯„ä¼°ç³»ç»Ÿ"""
    
    framework = HumanEvaluationFramework()
    consistency_analyzer = AnnotationConsistencyAnalyzer()
    bias_controller = CognitiveBiasController()
    quality_controller = CrowdsourcingQualityController()
    
    return {
        'framework': framework,
        'consistency_analyzer': consistency_analyzer,
        'bias_controller': bias_controller,
        'quality_controller': quality_controller
    }

# æ¼”ç¤ºå®Œæ•´çš„äººç±»è¯„ä¼°ç³»ç»Ÿ
def demonstrate_human_evaluation_system():
    """æ¼”ç¤ºäººç±»è¯„ä¼°ç³»ç»Ÿ"""
    
    print("=== MiniGPTäººç±»è¯„ä¼°æ¡†æ¶æ¼”ç¤º ===\n")
    
    # åˆ›å»ºè¯„ä¼°ç³»ç»Ÿ
    eval_system = create_human_evaluation_system()
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ ‡æ³¨æ•°æ®
    np.random.seed(42)
    
    texts = [f"text_{i:03d}" for i in range(50)]
    annotators = [f"annotator_{i}" for i in range(8)]
    dimensions = ['fluency', 'coherence', 'relevance', 'informativeness', 'creativity']
    
    annotations = []
    
    # æ¨¡æ‹Ÿä¸åŒè´¨é‡çš„æ ‡æ³¨è€…
    annotator_qualities = {
        'annotator_0': 0.9,  # é«˜è´¨é‡
        'annotator_1': 0.8,
        'annotator_2': 0.7,
        'annotator_3': 0.6,
        'annotator_4': 0.5,  # ä¸­ç­‰è´¨é‡
        'annotator_5': 0.4,
        'annotator_6': 0.3,
        'annotator_7': 0.2   # ä½è´¨é‡
    }
    
    for text_id in texts:
        # æ¯ä¸ªæ–‡æœ¬ç”±3-5ä¸ªæ ‡æ³¨è€…è¯„ä¼°
        n_annotators = np.random.randint(3, 6)
        selected_annotators = np.random.choice(annotators, n_annotators, replace=False)
        
        # ç”Ÿæˆ"çœŸå®"è´¨é‡åˆ†æ•°
        true_scores = {
            'fluency': np.random.randint(2, 6),
            'coherence': np.random.randint(2, 6),
            'relevance': np.random.randint(2, 6),
            'informativeness': np.random.randint(2, 6),
            'creativity': np.random.randint(1, 5)
        }
        
        for annotator_id in selected_annotators:
            quality = annotator_qualities[annotator_id]
            
            # æ ¹æ®æ ‡æ³¨è€…è´¨é‡ç”Ÿæˆè§‚å¯Ÿåˆ†æ•°
            observed_scores = {}
            for dim, true_score in true_scores.items():
                if np.random.random() < quality:
                    # æ­£ç¡®æ ‡æ³¨
                    observed_scores[dim] = true_score
                else:
                    # é”™è¯¯æ ‡æ³¨
                    observed_scores[dim] = np.random.randint(1, 6)
            
            # æ·»åŠ ä¸€äº›ç³»ç»Ÿæ€§åå·®
            if 'strict' in annotator_id or annotator_id in ['annotator_1', 'annotator_3']:
                # ä¸¥æ ¼æ ‡æ³¨è€…
                for dim in observed_scores:
                    observed_scores[dim] = max(1, observed_scores[dim] - 1)
            elif 'lenient' in annotator_id or annotator_id in ['annotator_6', 'annotator_7']:
                # å®½æ¾æ ‡æ³¨è€…
                for dim in observed_scores:
                    observed_scores[dim] = min(5, observed_scores[dim] + 1)
            
            annotation = AnnotationResult(
                text_id=text_id,
                annotator_id=annotator_id,
                dimensions=observed_scores,
                annotation_time=np.random.uniform(30, 300),  # 30ç§’åˆ°5åˆ†é’Ÿ
                confidence=np.random.uniform(0.6, 1.0)
            )
            
            annotations.append(annotation)
    
    # 1. æ ‡æ³¨ä¸€è‡´æ€§åˆ†æ
    print("1. æ ‡æ³¨ä¸€è‡´æ€§åˆ†æ")
    for dimension in dimensions:
        kappa_results = eval_system['consistency_analyzer'].compute_pairwise_kappa(
            annotations, dimension, weights='quadratic'
        )
        
        icc_results = eval_system['consistency_analyzer'].compute_icc(
            annotations, dimension
        )
        
        bias_analysis = eval_system['consistency_analyzer'].analyze_annotator_bias(
            annotations, dimension
        )
    
    # 2. è®¤çŸ¥åå·®æ£€æµ‹
    print("\n2. è®¤çŸ¥åå·®æ£€æµ‹")
    bias_results = eval_system['bias_controller'].detect_biases(annotations)
    
    strategies = eval_system['bias_controller'].suggest_bias_mitigation_strategies(bias_results)
    print("\nå»ºè®®çš„åå·®ç¼“è§£ç­–ç•¥:")
    for strategy in strategies:
        print(f"- {strategy}")
    
    # 3. ä¼—åŒ…è´¨é‡æ§åˆ¶
    print("\n3. ä¼—åŒ…è´¨é‡æ§åˆ¶")
    for dimension in dimensions[:2]:  # åªæ¼”ç¤ºå‰ä¸¤ä¸ªç»´åº¦
        em_results = eval_system['quality_controller'].estimate_true_labels_and_abilities(
            annotations, dimension
        )
        
        quality_metrics = eval_system['quality_controller'].compute_annotation_quality_metrics(
            annotations, em_results, dimension
        )
    
    # 4. è‡ªé€‚åº”æ ‡æ³¨ç­–ç•¥
    print("\n4. è‡ªé€‚åº”æ ‡æ³¨ç­–ç•¥")
    annotation_plan = eval_system['quality_controller'].design_adaptive_annotation_strategy(
        annotations
    )
    
    return {
        'annotations': annotations,
        'consistency_results': kappa_results,
        'bias_results': bias_results,
        'quality_results': quality_metrics,
        'annotation_plan': annotation_plan,
        'evaluation_system': eval_system
    }

# è¿è¡Œæ¼”ç¤º
if __name__ == "__main__":
    results = demonstrate_human_evaluation_system()
    
    print("\n=== äººç±»è¯„ä¼°æ¡†æ¶åˆ†æå®Œæˆ ===")
    print(f"ç³»ç»Ÿæ€§èƒ½æ€»ç»“:")
    print(f"- æ ‡æ³¨ä¸€è‡´æ€§: éœ€è¦æ”¹è¿›")
    print(f"- è®¤çŸ¥åå·®æ§åˆ¶: è‰¯å¥½")
    print(f"- è´¨é‡æ§åˆ¶æœºåˆ¶: æœ‰æ•ˆ")
    print(f"- è‡ªé€‚åº”ç­–ç•¥: å®ç”¨")
```

## ç†è®ºæ€»ç»“

### 2.5 äººç±»è¯„ä¼°çš„ç»Ÿä¸€ç†è®ºæ¡†æ¶

**è¯„ä¼°è¿‡ç¨‹çš„æ¦‚ç‡æ¨¡å‹**ï¼š
äººç±»è¯„ä¼°å¯å»ºæ¨¡ä¸ºä»çœŸå®è´¨é‡åˆ°è§‚å¯Ÿè¯„åˆ†çš„æ¦‚ç‡æ˜ å°„ï¼š
$$P(\text{observed} | \text{true}, \text{annotator}) = f(\text{ability}, \text{bias}, \text{context})$$

**è´å¶æ–¯æ¨æ–­æ¡†æ¶**ï¼š
ç»“åˆå…ˆéªŒçŸ¥è¯†è¿›è¡Œè¯„ä¼°æ¨æ–­ï¼š
$$P(\text{true} | \text{observed}) \propto P(\text{observed} | \text{true}) \cdot P(\text{true})$$

**ä¿¡æ¯èåˆç†è®º**ï¼š
å¤šæ ‡æ³¨è€…ä¿¡æ¯çš„æœ€ä¼˜èåˆï¼š
$$\hat{y} = \arg\max_y \sum_i w_i \log P(y_i | y, \theta_i)$$

å…¶ä¸­ $w_i$ æ˜¯æ ‡æ³¨è€…æƒé‡ï¼Œ$\theta_i$ æ˜¯èƒ½åŠ›å‚æ•°ã€‚

## åº”ç”¨æŒ‡å¯¼

### å®è·µå»ºè®®

1. **è¯„ä¼°ç»´åº¦è®¾è®¡**ï¼š
   - ç¡®ä¿ç»´åº¦ç‹¬ç«‹æ€§å’Œå®Œæ•´æ€§
   - æä¾›æ¸…æ™°çš„æ“ä½œåŒ–å®šä¹‰
   - è®¾è®¡é€‚å½“çš„è¯„åˆ†å°ºåº¦

2. **æ ‡æ³¨è€…ç®¡ç†**ï¼š
   - å®æ–½ä¸¥æ ¼çš„æ ‡æ³¨è€…åŸ¹è®­
   - å®šæœŸè¿›è¡Œä¸€è‡´æ€§æ£€æŸ¥
   - å»ºç«‹æ ‡æ³¨è€…ç»©æ•ˆæ¡£æ¡ˆ

3. **è´¨é‡ä¿è¯æœºåˆ¶**ï¼š
   - å¤šé‡æ ‡æ³¨å’Œäº¤å‰éªŒè¯
   - è‡ªåŠ¨åŒ–è´¨é‡æ£€æµ‹
   - æŒç»­çš„åé¦ˆå’Œæ”¹è¿›

äººç±»è¯„ä¼°æ¡†æ¶çš„è®¾è®¡éœ€è¦åœ¨ç§‘å­¦æ€§å’Œå®ç”¨æ€§ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ï¼Œé€šè¿‡ä¸¥æ ¼çš„ç»Ÿè®¡å­¦æ–¹æ³•ç¡®ä¿è¯„ä¼°ç»“æœçš„å¯é æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## æ‰©å±•é˜…è¯»

- ã€ŠInter-rater Reliability: The Kappa Statisticã€‹- ä¸€è‡´æ€§åˆ†æç»å…¸æ–‡çŒ®
- ã€ŠCrowdsourcing for NLP: What's Next?ã€‹- ä¼—åŒ…æ ‡æ³¨æ–¹æ³•ç»¼è¿°
- ã€ŠCognitive Biases in Natural Language Processingã€‹- è®¤çŸ¥åå·®ç ”ç©¶
- ã€ŠQuality Control in Crowdsourcingã€‹- ä¼—åŒ…è´¨é‡æ§åˆ¶ç†è®º

---

*"äººç±»çš„åˆ¤æ–­æ˜¯å¤æ‚çš„ï¼Œä½†é€šè¿‡ç§‘å­¦çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™ç§å¤æ‚æ€§è½¬åŒ–ä¸ºå¯é çš„çŸ¥è¯†ã€‚"* ğŸ¯