# 01 è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡æ·±åº¦è§£æ

> **ä»è¯æ±‡åŒ¹é…åˆ°è¯­ä¹‰ç†è§£ï¼šè‡ªåŠ¨è¯„ä¼°çš„æ•°å­¦æ¼”è¿›ä¹‹è·¯**

## æ ¸å¿ƒæ€æƒ³

è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡æ˜¯è¯­è¨€æ¨¡å‹è¯„ä¼°ä½“ç³»çš„åŸºçŸ³ï¼Œå®ƒä»¬è¯•å›¾ç”¨æ•°å­¦å…¬å¼æ¥é‡åŒ–äººç±»å¯¹è¯­è¨€è´¨é‡çš„ç›´è§‰åˆ¤æ–­ã€‚ä»æ—©æœŸçš„è¯æ±‡åŒ¹é…æ–¹æ³•åˆ°ç°ä»£çš„ç¥ç»è¯­ä¹‰è¯„ä¼°ï¼Œæ¯ä¸€ç§æŒ‡æ ‡éƒ½ä»£è¡¨äº†å¯¹"ä»€ä¹ˆæ˜¯å¥½çš„è¯­è¨€"è¿™ä¸€é—®é¢˜çš„ä¸åŒæ•°å­¦è¯ é‡Šã€‚

**å…³é”®æ´å¯Ÿ**ï¼š
- **å¯è®¡ç®—æ€§**ï¼šå°†ä¸»è§‚çš„è¯­è¨€è´¨é‡è½¬åŒ–ä¸ºå®¢è§‚çš„æ•°å€¼è®¡ç®—
- **å¯é‡å¤æ€§**ï¼šç¡®ä¿è¯„ä¼°ç»“æœçš„ä¸€è‡´æ€§å’Œå¯å¤ç°æ€§
- **æ•ˆç‡æ€§**ï¼šåœ¨ä¿è¯è´¨é‡çš„å‰æä¸‹å®ç°å¤§è§„æ¨¡è‡ªåŠ¨è¯„ä¼°
- **ç†è®ºåŸºç¡€**ï¼šæ¯ä¸ªæŒ‡æ ‡èƒŒåéƒ½æœ‰ä¸¥æ ¼çš„æ•°å­¦ç†è®ºæ”¯æ’‘

ä»æ•°å­¦è§’åº¦çœ‹ï¼Œè‡ªåŠ¨è¯„ä¼°æœ¬è´¨ä¸Šæ˜¯åœ¨å¯»æ‰¾ä»æ–‡æœ¬ç©ºé—´åˆ°å®æ•°ç©ºé—´çš„æ˜ å°„å‡½æ•°ï¼Œä½¿å¾—è¿™ä¸ªæ˜ å°„èƒ½å¤Ÿä¿åºåœ°åæ˜ äººç±»çš„è´¨é‡åˆ¤æ–­ã€‚

## 1.1 ä¼ ç»Ÿè¯æ±‡åŒ¹é…æŒ‡æ ‡çš„æ•°å­¦åŸç†

### BLEUæŒ‡æ ‡çš„æ·±åº¦æ•°å­¦åˆ†æ

**BLEUåŸºç¡€å®šä¹‰**ï¼š
BLEU(Bilingual Evaluation Understudy)é€šè¿‡n-gramåŒ¹é…æ¥è¯„ä¼°ç¿»è¯‘è´¨é‡ï¼š

$$\text{BLEU} = \text{BP} \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)$$

å…¶ä¸­ï¼š
- $p_n$ æ˜¯ä¿®æ­£çš„n-gramç²¾ç¡®åº¦
- $w_n$ æ˜¯æƒé‡ï¼ˆé€šå¸¸ä¸º $1/N$ï¼‰
- $\text{BP}$ æ˜¯ç®€çŸ­æƒ©ç½šå› å­

**ä¿®æ­£n-gramç²¾ç¡®åº¦**ï¼š
$$p_n = \frac{\sum_{C \in \{Candidates\}} \sum_{n\text{-gram} \in C} \text{Count}_{clip}(n\text{-gram})}{\sum_{C' \in \{Candidates\}} \sum_{n\text{-gram}' \in C'} \text{Count}(n\text{-gram}')}$$

å…¶ä¸­ $\text{Count}_{clip}$ æ˜¯æˆªæ–­è®¡æ•°ï¼Œé˜²æ­¢n-gramè¢«é‡å¤è®¡ç®—ã€‚

**ç®€çŸ­æƒ©ç½šçš„æ•°å­¦å½¢å¼**ï¼š
$$\text{BP} = \begin{cases}
1 & \text{if } c > r \\
e^{(1-r/c)} & \text{if } c \leq r
\end{cases}$$

å…¶ä¸­ $c$ æ˜¯å€™é€‰è¯‘æ–‡é•¿åº¦ï¼Œ$r$ æ˜¯å‚è€ƒè¯‘æ–‡é•¿åº¦ã€‚

**BLEUçš„ç»Ÿè®¡ç‰¹æ€§åˆ†æ**ï¼š

1. **å•è°ƒæ€§**ï¼šéšç€åŒ¹é…n-gramå¢åŠ ï¼ŒBLEUåˆ†æ•°å•è°ƒé€’å¢
2. **ä¸Šç•Œ**ï¼š$\text{BLEU} \leq 1$ï¼Œå½“å€™é€‰ä¸å‚è€ƒå®Œå…¨åŒ¹é…æ—¶è¾¾åˆ°
3. **ä¸‹ç•Œ**ï¼š$\text{BLEU} \geq 0$ï¼Œå½“æ²¡æœ‰ä»»ä½•n-gramåŒ¹é…æ—¶ä¸º0

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Union, Any
import matplotlib.pyplot as plt
import seaborn as sns
import math
import re
from collections import Counter, defaultdict
from dataclasses import dataclass
from scipy import stats
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge_score import rouge_scorer
import warnings
warnings.filterwarnings('ignore')

@dataclass
class EvaluationResult:
    """è¯„ä¼°ç»“æœæ•°æ®ç»“æ„"""
    metric_name: str
    score: float
    subscores: Dict[str, float]
    metadata: Dict[str, Any]
    confidence_interval: Optional[Tuple[float, float]] = None
    
class BLEUEvaluator:
    """BLEUè¯„ä¼°å™¨çš„æ•°å­¦å®ç°"""
    
    def __init__(self, max_n: int = 4, smoothing: bool = True):
        self.max_n = max_n
        self.smoothing = smoothing
        self.smoothing_function = SmoothingFunction().method1 if smoothing else None
        
    def compute_bleu(self, 
                    candidate: str, 
                    references: List[str],
                    return_details: bool = False) -> Union[float, Dict]:
        """è®¡ç®—BLEUåˆ†æ•°"""
        
        # é¢„å¤„ç†æ–‡æœ¬
        candidate_tokens = self._tokenize(candidate)
        reference_tokens = [self._tokenize(ref) for ref in references]
        
        # è®¡ç®—å„n-gramç²¾ç¡®åº¦
        precisions = []
        ngram_matches = {}
        ngram_totals = {}
        
        for n in range(1, self.max_n + 1):
            # å€™é€‰æ–‡æœ¬çš„n-gram
            candidate_ngrams = self._get_ngrams(candidate_tokens, n)
            
            # å‚è€ƒæ–‡æœ¬çš„n-gramï¼ˆå–æœ€å¤§åŒ¹é…ï¼‰
            reference_ngrams_max = {}
            for ref_tokens in reference_tokens:
                ref_ngrams = self._get_ngrams(ref_tokens, n)
                for ngram, count in ref_ngrams.items():
                    reference_ngrams_max[ngram] = max(
                        reference_ngrams_max.get(ngram, 0), count)
            
            # è®¡ç®—æˆªæ–­è®¡æ•°
            clipped_count = 0
            total_count = 0
            
            for ngram, count in candidate_ngrams.items():
                total_count += count
                clipped_count += min(count, reference_ngrams_max.get(ngram, 0))
            
            # è®¡ç®—ç²¾ç¡®åº¦
            precision = clipped_count / max(total_count, 1)
            precisions.append(precision)
            
            ngram_matches[f'{n}-gram'] = clipped_count
            ngram_totals[f'{n}-gram'] = total_count
        
        # è®¡ç®—å‡ ä½•å¹³å‡
        if any(p == 0 for p in precisions):
            if self.smoothing:
                # ä½¿ç”¨å¹³æ»‘æ–¹æ³•
                geometric_mean = sentence_bleu(
                    reference_tokens, candidate_tokens,
                    smoothing_function=self.smoothing_function,
                    weights=[1/self.max_n] * self.max_n
                )
            else:
                geometric_mean = 0.0
        else:
            log_sum = sum(math.log(p) for p in precisions) / self.max_n
            geometric_mean = math.exp(log_sum)
        
        # è®¡ç®—ç®€çŸ­æƒ©ç½š
        candidate_length = len(candidate_tokens)
        closest_ref_length = min(reference_tokens, 
                               key=lambda x: abs(len(x) - candidate_length))
        reference_length = len(closest_ref_length)
        
        if candidate_length > reference_length:
            brevity_penalty = 1.0
        else:
            brevity_penalty = math.exp(1 - reference_length / max(candidate_length, 1))
        
        # æœ€ç»ˆBLEUåˆ†æ•°
        bleu_score = brevity_penalty * geometric_mean
        
        if return_details:
            return {
                'bleu': bleu_score,
                'precisions': {f'{i+1}-gram': p for i, p in enumerate(precisions)},
                'brevity_penalty': brevity_penalty,
                'length_ratio': candidate_length / max(reference_length, 1),
                'ngram_matches': ngram_matches,
                'ngram_totals': ngram_totals,
                'geometric_mean': geometric_mean
            }
        
        return bleu_score
    
    def _tokenize(self, text: str) -> List[str]:
        """ç®€å•çš„tokenåŒ–"""
        # åœ¨å®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨ä¸“ä¸šçš„tokenizer
        return text.lower().split()
    
    def _get_ngrams(self, tokens: List[str], n: int) -> Counter:
        """è·å–n-gramè®¡æ•°"""
        ngrams = []
        for i in range(len(tokens) - n + 1):
            ngram = tuple(tokens[i:i+n])
            ngrams.append(ngram)
        return Counter(ngrams)
    
    def analyze_bleu_properties(self, 
                              candidates: List[str],
                              references: List[List[str]]) -> Dict:
        """åˆ†æBLEUæŒ‡æ ‡çš„æ•°å­¦æ€§è´¨"""
        
        print("=== BLEUæ•°å­¦æ€§è´¨åˆ†æ ===")
        
        bleu_scores = []
        detailed_results = []
        
        for candidate, refs in zip(candidates, references):
            details = self.compute_bleu(candidate, refs, return_details=True)
            bleu_scores.append(details['bleu'])
            detailed_results.append(details)
        
        # åˆ†æç²¾ç¡®åº¦åˆ†å¸ƒ
        precision_analysis = {f'{n}-gram': [] for n in range(1, self.max_n + 1)}
        brevity_penalties = []
        length_ratios = []
        
        for result in detailed_results:
            for ngram in precision_analysis:
                precision_analysis[ngram].append(result['precisions'][ngram])
            brevity_penalties.append(result['brevity_penalty'])
            length_ratios.append(result['length_ratio'])
        
        # ç»Ÿè®¡åˆ†æ
        analysis = {
            'score_distribution': {
                'mean': np.mean(bleu_scores),
                'std': np.std(bleu_scores),
                'min': np.min(bleu_scores),
                'max': np.max(bleu_scores),
                'median': np.median(bleu_scores)
            },
            'precision_analysis': {},
            'brevity_penalty_stats': {
                'mean': np.mean(brevity_penalties),
                'std': np.std(brevity_penalties),
                'penalty_rate': sum(1 for bp in brevity_penalties if bp < 1.0) / len(brevity_penalties)
            },
            'length_ratio_stats': {
                'mean': np.mean(length_ratios),
                'std': np.std(length_ratios),
                'correlation_with_bleu': np.corrcoef(length_ratios, bleu_scores)[0, 1]
            }
        }
        
        for ngram, precisions in precision_analysis.items():
            analysis['precision_analysis'][ngram] = {
                'mean': np.mean(precisions),
                'std': np.std(precisions),
                'correlation_with_bleu': np.corrcoef(precisions, bleu_scores)[0, 1]
            }
        
        print(f"BLEUåˆ†æ•°åˆ†å¸ƒ: å‡å€¼={analysis['score_distribution']['mean']:.4f}, "
              f"æ ‡å‡†å·®={analysis['score_distribution']['std']:.4f}")
        print(f"ç®€çŸ­æƒ©ç½šç‡: {analysis['brevity_penalty_stats']['penalty_rate']:.4f}")
        
        for ngram in precision_analysis:
            corr = analysis['precision_analysis'][ngram]['correlation_with_bleu']
            print(f"{ngram}ç²¾ç¡®åº¦ä¸BLEUç›¸å…³æ€§: {corr:.4f}")
        
        return analysis

### ROUGEæŒ‡æ ‡çš„æ•°å­¦åŸºç¡€

**ROUGE-Nå®šä¹‰**ï¼š
$$\text{ROUGE-N} = \frac{\sum_{S \in \{Reference\}} \sum_{gram_n \in S} Count_{match}(gram_n)}{\sum_{S \in \{Reference\}} \sum_{gram_n \in S} Count(gram_n)}$$

**ROUGE-L (æœ€é•¿å…¬å…±å­åºåˆ—)**ï¼š
åŸºäºæœ€é•¿å…¬å…±å­åºåˆ—(LCS)çš„å¬å›ç‡å’Œç²¾ç¡®åº¦ï¼š

$$R_{lcs} = \frac{LCS(X,Y)}{m}, \quad P_{lcs} = \frac{LCS(X,Y)}{n}$$

$$F_{lcs} = \frac{(1+\beta^2)R_{lcs}P_{lcs}}{R_{lcs} + \beta^2 P_{lcs}}$$

å…¶ä¸­ $X$ æ˜¯å‚è€ƒåºåˆ—ï¼ˆé•¿åº¦$m$ï¼‰ï¼Œ$Y$ æ˜¯å€™é€‰åºåˆ—ï¼ˆé•¿åº¦$n$ï¼‰ã€‚

class ROUGEEvaluator:
    """ROUGEè¯„ä¼°å™¨å®ç°"""
    
    def __init__(self, rouge_types: List[str] = ['rouge1', 'rouge2', 'rougeL']):
        self.rouge_types = rouge_types
        self.scorer = rouge_scorer.RougeScorer(rouge_types, use_stemmer=True)
        
    def compute_rouge(self, 
                     candidate: str, 
                     reference: str,
                     return_details: bool = False) -> Dict:
        """è®¡ç®—ROUGEåˆ†æ•°"""
        
        scores = self.scorer.score(reference, candidate)
        
        if return_details:
            detailed_scores = {}
            for rouge_type in self.rouge_types:
                rouge_score = scores[rouge_type]
                detailed_scores[rouge_type] = {
                    'precision': rouge_score.precision,
                    'recall': rouge_score.recall,
                    'fmeasure': rouge_score.fmeasure
                }
            return detailed_scores
        else:
            return {rouge_type: scores[rouge_type].fmeasure 
                   for rouge_type in self.rouge_types}
    
    def compute_lcs_length(self, seq1: List, seq2: List) -> int:
        """è®¡ç®—æœ€é•¿å…¬å…±å­åºåˆ—é•¿åº¦"""
        
        m, n = len(seq1), len(seq2)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if seq1[i-1] == seq2[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        
        return dp[m][n]
    
    def analyze_rouge_sensitivity(self, 
                                candidates: List[str],
                                references: List[str]) -> Dict:
        """åˆ†æROUGEæŒ‡æ ‡çš„æ•æ„Ÿæ€§"""
        
        print("=== ROUGEæ•æ„Ÿæ€§åˆ†æ ===")
        
        rouge_scores = {rouge_type: [] for rouge_type in self.rouge_types}
        
        for candidate, reference in zip(candidates, references):
            scores = self.compute_rouge(candidate, reference)
            for rouge_type in self.rouge_types:
                rouge_scores[rouge_type].append(scores[rouge_type])
        
        # åˆ†æä¸åŒROUGEç±»å‹ä¹‹é—´çš„ç›¸å…³æ€§
        correlation_matrix = {}
        for i, rouge_type1 in enumerate(self.rouge_types):
            correlation_matrix[rouge_type1] = {}
            for rouge_type2 in self.rouge_types:
                corr = np.corrcoef(rouge_scores[rouge_type1], 
                                 rouge_scores[rouge_type2])[0, 1]
                correlation_matrix[rouge_type1][rouge_type2] = corr
        
        # åˆ†æåˆ†æ•°åˆ†å¸ƒ
        distribution_stats = {}
        for rouge_type in self.rouge_types:
            scores = rouge_scores[rouge_type]
            distribution_stats[rouge_type] = {
                'mean': np.mean(scores),
                'std': np.std(scores),
                'min': np.min(scores),
                'max': np.max(scores),
                'skewness': stats.skew(scores),
                'kurtosis': stats.kurtosis(scores)
            }
        
        print("ROUGEç±»å‹é—´ç›¸å…³æ€§:")
        for rouge_type1 in self.rouge_types:
            for rouge_type2 in self.rouge_types:
                if rouge_type1 != rouge_type2:
                    corr = correlation_matrix[rouge_type1][rouge_type2]
                    print(f"  {rouge_type1} vs {rouge_type2}: {corr:.4f}")
        
        return {
            'rouge_scores': rouge_scores,
            'correlation_matrix': correlation_matrix,
            'distribution_stats': distribution_stats
        }

## 1.2 è¯­ä¹‰ç›¸ä¼¼åº¦è¯„ä¼°çš„å‘é‡ç©ºé—´æ¨¡å‹

### åŸºäºè¯å‘é‡çš„è¯­ä¹‰è¯„ä¼°

**è¯å‘é‡å¹³å‡æ³•**ï¼š
$$\text{Sim}(S_1, S_2) = \cos\left(\frac{1}{|S_1|}\sum_{w \in S_1} \mathbf{v}_w, \frac{1}{|S_2|}\sum_{w \in S_2} \mathbf{v}_w\right)$$

**åŠ æƒè¯å‘é‡å¹³å‡**ï¼š
ä½¿ç”¨TF-IDFæƒé‡ï¼š
$$\text{Sim}_{weighted}(S_1, S_2) = \cos\left(\sum_{w \in S_1} \text{tfidf}(w) \cdot \mathbf{v}_w, \sum_{w \in S_2} \text{tfidf}(w) \cdot \mathbf{v}_w\right)$$

**Word Mover's Distance (WMD)**ï¼š
åŸºäºæœ€ä¼˜ä¼ è¾“ç†è®ºçš„è¯­ä¹‰è·ç¦»ï¼š
$$\text{WMD}(S_1, S_2) = \min_{T \geq 0} \sum_{i,j} T_{ij} \cdot d(w_i, w_j)$$

subject to:
$$\sum_j T_{ij} = \frac{1}{|S_1|}, \quad \sum_i T_{ij} = \frac{1}{|S_2|}$$

class SemanticEvaluator:
    """è¯­ä¹‰ç›¸ä¼¼åº¦è¯„ä¼°å™¨"""
    
    def __init__(self, embedding_dim: int = 300):
        self.embedding_dim = embedding_dim
        # ç®€åŒ–å®ç°ï¼šä½¿ç”¨éšæœºå‘é‡ä½œä¸ºè¯åµŒå…¥
        self.word_embeddings = {}
        self.vocab_size = 10000
        
    def get_embedding(self, word: str) -> np.ndarray:
        """è·å–è¯åµŒå…¥"""
        if word not in self.word_embeddings:
            # ç®€åŒ–å®ç°ï¼šéšæœºåˆå§‹åŒ–
            self.word_embeddings[word] = np.random.normal(
                0, 1, self.embedding_dim)
        return self.word_embeddings[word]
    
    def compute_sentence_embedding(self, 
                                 sentence: str, 
                                 method: str = 'average') -> np.ndarray:
        """è®¡ç®—å¥å­åµŒå…¥"""
        
        words = sentence.lower().split()
        if not words:
            return np.zeros(self.embedding_dim)
        
        embeddings = [self.get_embedding(word) for word in words]
        
        if method == 'average':
            return np.mean(embeddings, axis=0)
        elif method == 'tfidf_weighted':
            # ç®€åŒ–çš„TF-IDFæƒé‡
            word_counts = Counter(words)
            total_words = len(words)
            
            weighted_embeddings = []
            for word in words:
                tf = word_counts[word] / total_words
                # ç®€åŒ–çš„IDFè®¡ç®—
                idf = math.log(self.vocab_size / (1 + word_counts[word]))
                tfidf_weight = tf * idf
                
                weighted_embeddings.append(tfidf_weight * self.get_embedding(word))
            
            return np.sum(weighted_embeddings, axis=0)
        else:
            raise ValueError(f"Unknown method: {method}")
    
    def compute_cosine_similarity(self, 
                                sentence1: str, 
                                sentence2: str,
                                method: str = 'average') -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        
        emb1 = self.compute_sentence_embedding(sentence1, method)
        emb2 = self.compute_sentence_embedding(sentence2, method)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        dot_product = np.dot(emb1, emb2)
        norm1 = np.linalg.norm(emb1)
        norm2 = np.linalg.norm(emb2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def compute_word_movers_distance(self, 
                                   sentence1: str, 
                                   sentence2: str) -> float:
        """è®¡ç®—Word Mover's Distanceï¼ˆç®€åŒ–å®ç°ï¼‰"""
        
        words1 = sentence1.lower().split()
        words2 = sentence2.lower().split()
        
        if not words1 or not words2:
            return 1.0  # æœ€å¤§è·ç¦»
        
        # æ„å»ºè·ç¦»çŸ©é˜µ
        distance_matrix = np.zeros((len(words1), len(words2)))
        for i, word1 in enumerate(words1):
            for j, word2 in enumerate(words2):
                emb1 = self.get_embedding(word1)
                emb2 = self.get_embedding(word2)
                # æ¬§å‡ é‡Œå¾—è·ç¦»
                distance_matrix[i, j] = np.linalg.norm(emb1 - emb2)
        
        # ç®€åŒ–çš„æœ€ä¼˜ä¼ è¾“ï¼ˆä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•çš„è¿‘ä¼¼ï¼‰
        # åœ¨å®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨ä¸“é—¨çš„æœ€ä¼˜ä¼ è¾“ç®—æ³•
        min_assignments = []
        for i in range(len(words1)):
            min_j = np.argmin(distance_matrix[i])
            min_assignments.append(distance_matrix[i, min_j])
        
        return np.mean(min_assignments)
    
    def analyze_semantic_metrics_reliability(self, 
                                          candidates: List[str],
                                          references: List[str]) -> Dict:
        """åˆ†æè¯­ä¹‰æŒ‡æ ‡çš„å¯é æ€§"""
        
        print("=== è¯­ä¹‰æŒ‡æ ‡å¯é æ€§åˆ†æ ===")
        
        cosine_scores = []
        tfidf_cosine_scores = []
        wmd_scores = []
        
        for candidate, reference in zip(candidates, references):
            # è®¡ç®—ä¸åŒçš„è¯­ä¹‰ç›¸ä¼¼åº¦
            cosine_sim = self.compute_cosine_similarity(
                candidate, reference, method='average')
            tfidf_cosine_sim = self.compute_cosine_similarity(
                candidate, reference, method='tfidf_weighted')
            wmd_dist = self.compute_word_movers_distance(candidate, reference)
            
            cosine_scores.append(cosine_sim)
            tfidf_cosine_scores.append(tfidf_cosine_sim)
            wmd_scores.append(1 - wmd_dist)  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
        
        # åˆ†ææŒ‡æ ‡é—´ç›¸å…³æ€§
        correlations = {
            'cosine_vs_tfidf': np.corrcoef(cosine_scores, tfidf_cosine_scores)[0, 1],
            'cosine_vs_wmd': np.corrcoef(cosine_scores, wmd_scores)[0, 1],
            'tfidf_vs_wmd': np.corrcoef(tfidf_cosine_scores, wmd_scores)[0, 1]
        }
        
        # åˆ†æåˆ†æ•°åˆ†å¸ƒçš„ç¨³å®šæ€§
        stability_analysis = {
            'cosine': {
                'mean': np.mean(cosine_scores),
                'std': np.std(cosine_scores),
                'cv': np.std(cosine_scores) / np.mean(cosine_scores)
            },
            'tfidf_cosine': {
                'mean': np.mean(tfidf_cosine_scores),
                'std': np.std(tfidf_cosine_scores),
                'cv': np.std(tfidf_cosine_scores) / np.mean(tfidf_cosine_scores)
            },
            'wmd_similarity': {
                'mean': np.mean(wmd_scores),
                'std': np.std(wmd_scores),
                'cv': np.std(wmd_scores) / np.mean(wmd_scores)
            }
        }
        
        print("è¯­ä¹‰æŒ‡æ ‡é—´ç›¸å…³æ€§:")
        for pair, corr in correlations.items():
            print(f"  {pair}: {corr:.4f}")
        
        print("å„æŒ‡æ ‡ç¨³å®šæ€§ (å˜å¼‚ç³»æ•°):")
        for metric, stats in stability_analysis.items():
            print(f"  {metric}: CV = {stats['cv']:.4f}")
        
        return {
            'scores': {
                'cosine': cosine_scores,
                'tfidf_cosine': tfidf_cosine_scores,
                'wmd_similarity': wmd_scores
            },
            'correlations': correlations,
            'stability_analysis': stability_analysis
        }

## 1.3 å›°æƒ‘åº¦ä¸ä¿¡æ¯è®ºè¯„ä¼°æŒ‡æ ‡

### å›°æƒ‘åº¦çš„æ•°å­¦å®šä¹‰ä¸æ€§è´¨

**å›°æƒ‘åº¦å®šä¹‰**ï¼š
$$\text{PPL}(S) = P(S)^{-\frac{1}{N}} = \exp\left(-\frac{1}{N}\sum_{i=1}^{N} \log P(w_i|w_{<i})\right)$$

å…¶ä¸­ $N$ æ˜¯åºåˆ—é•¿åº¦ï¼Œ$S = (w_1, w_2, ..., w_N)$ æ˜¯æµ‹è¯•åºåˆ—ã€‚

**å›°æƒ‘åº¦çš„ä¿¡æ¯è®ºè§£é‡Š**ï¼š
å›°æƒ‘åº¦æ˜¯å¹³å‡åˆ†æ”¯å› å­ï¼Œè¡¨ç¤ºæ¨¡å‹åœ¨æ¯ä¸ªä½ç½®ä¸Šçš„å¹³å‡"å›°æƒ‘ç¨‹åº¦"ï¼š
$$\text{PPL}(S) = 2^{H(S)}$$

å…¶ä¸­ $H(S) = -\frac{1}{N}\sum_{i=1}^{N} \log_2 P(w_i|w_{<i})$ æ˜¯äº¤å‰ç†µã€‚

**å›°æƒ‘åº¦çš„æ•°å­¦æ€§è´¨**ï¼š
1. **ä¸‹ç•Œ**ï¼š$\text{PPL}(S) \geq 1$ï¼Œå½“æ¨¡å‹å®Œå…¨ç¡®å®šæ—¶è¾¾åˆ°
2. **å•è°ƒæ€§**ï¼šæ¨¡å‹æ€§èƒ½è¶Šå¥½ï¼Œå›°æƒ‘åº¦è¶Šä½
3. **ä¹˜æ€§æ€§è´¨**ï¼šå¯¹äºç‹¬ç«‹åºåˆ—ï¼Œå›°æƒ‘åº¦å…·æœ‰ä¹˜æ€§
4. **é•¿åº¦å½’ä¸€åŒ–**ï¼šå›°æƒ‘åº¦å·²ç»è¿›è¡Œäº†é•¿åº¦å½’ä¸€åŒ–

class PerplexityEvaluator:
    """å›°æƒ‘åº¦è¯„ä¼°å™¨"""
    
    def __init__(self, vocab_size: int = 10000):
        self.vocab_size = vocab_size
        
    def compute_perplexity(self, 
                          sequence: List[int], 
                          model_probs: List[torch.Tensor]) -> float:
        """è®¡ç®—å›°æƒ‘åº¦"""
        
        if len(sequence) != len(model_probs):
            raise ValueError("åºåˆ—é•¿åº¦ä¸æ¦‚ç‡é•¿åº¦ä¸åŒ¹é…")
        
        log_probs = []
        for i, (token, prob_dist) in enumerate(zip(sequence, model_probs)):
            if token < len(prob_dist):
                token_prob = prob_dist[token].item()
                log_probs.append(math.log(max(token_prob, 1e-10)))  # é¿å…log(0)
            else:
                log_probs.append(math.log(1e-10))  # æœªçŸ¥tokençš„æœ€å°æ¦‚ç‡
        
        # è®¡ç®—å¹³å‡è´Ÿå¯¹æ•°ä¼¼ç„¶
        avg_neg_log_likelihood = -sum(log_probs) / len(log_probs)
        
        # è®¡ç®—å›°æƒ‘åº¦
        perplexity = math.exp(avg_neg_log_likelihood)
        
        return perplexity
    
    def compute_cross_entropy(self, 
                            true_sequence: List[int],
                            model_probs: List[torch.Tensor]) -> float:
        """è®¡ç®—äº¤å‰ç†µ"""
        
        log_probs = []
        for token, prob_dist in zip(true_sequence, model_probs):
            if token < len(prob_dist):
                token_prob = prob_dist[token].item()
                log_probs.append(math.log2(max(token_prob, 1e-10)))
            else:
                log_probs.append(math.log2(1e-10))
        
        cross_entropy = -sum(log_probs) / len(log_probs)
        return cross_entropy
    
    def analyze_perplexity_properties(self, 
                                    sequences: List[List[int]],
                                    all_model_probs: List[List[torch.Tensor]]) -> Dict:
        """åˆ†æå›°æƒ‘åº¦çš„æ•°å­¦æ€§è´¨"""
        
        print("=== å›°æƒ‘åº¦æ•°å­¦æ€§è´¨åˆ†æ ===")
        
        perplexities = []
        cross_entropies = []
        sequence_lengths = []
        
        for sequence, model_probs in zip(sequences, all_model_probs):
            ppl = self.compute_perplexity(sequence, model_probs)
            ce = self.compute_cross_entropy(sequence, model_probs)
            
            perplexities.append(ppl)
            cross_entropies.append(ce)
            sequence_lengths.append(len(sequence))
        
        # éªŒè¯å›°æƒ‘åº¦ä¸äº¤å‰ç†µçš„å…³ç³»
        theoretical_ppl = [2**ce for ce in cross_entropies]
        ppl_ce_correlation = np.corrcoef(perplexities, theoretical_ppl)[0, 1]
        
        # åˆ†æå›°æƒ‘åº¦ä¸åºåˆ—é•¿åº¦çš„å…³ç³»
        length_ppl_correlation = np.corrcoef(sequence_lengths, perplexities)[0, 1]
        
        # åˆ†æå›°æƒ‘åº¦åˆ†å¸ƒ
        ppl_stats = {
            'mean': np.mean(perplexities),
            'std': np.std(perplexities),
            'min': np.min(perplexities),
            'max': np.max(perplexities),
            'median': np.median(perplexities),
            'geometric_mean': stats.gmean(perplexities)
        }
        
        # åˆ†æå›°æƒ‘åº¦çš„å¯¹æ•°æ­£æ€æ€§
        log_perplexities = np.log(perplexities)
        log_ppl_stats = {
            'mean': np.mean(log_perplexities),
            'std': np.std(log_perplexities),
            'normality_test': stats.normaltest(log_perplexities)
        }
        
        print(f"å›°æƒ‘åº¦ç»Ÿè®¡: å‡å€¼={ppl_stats['mean']:.4f}, "
              f"å‡ ä½•å‡å€¼={ppl_stats['geometric_mean']:.4f}")
        print(f"å›°æƒ‘åº¦ä¸2^äº¤å‰ç†µç›¸å…³æ€§: {ppl_ce_correlation:.6f}")
        print(f"å›°æƒ‘åº¦ä¸åºåˆ—é•¿åº¦ç›¸å…³æ€§: {length_ppl_correlation:.4f}")
        print(f"å¯¹æ•°å›°æƒ‘åº¦æ­£æ€æ€§æ£€éªŒ: på€¼={log_ppl_stats['normality_test'].pvalue:.4f}")
        
        return {
            'perplexities': perplexities,
            'cross_entropies': cross_entropies,
            'statistics': ppl_stats,
            'log_statistics': log_ppl_stats,
            'correlations': {
                'ppl_ce': ppl_ce_correlation,
                'ppl_length': length_ppl_correlation
            }
        }

## 1.4 ç°ä»£ç¥ç»è¯„ä¼°æ–¹æ³•

### BERTScoreçš„æ•°å­¦åŸç†

**BERTScoreåŸºç¡€**ï¼š
BERTScoreä½¿ç”¨é¢„è®­ç»ƒçš„BERTæ¨¡å‹æ¥è®¡ç®—tokençº§åˆ«çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼š

$$\text{BERTScore} = \frac{2 \cdot P \cdot R}{P + R}$$

å…¶ä¸­ï¼š
- $P = \frac{1}{|x|} \sum_{x_i \in x} \max_{y_j \in y} \cos(\mathbf{x}_i, \mathbf{y}_j)$
- $R = \frac{1}{|y|} \sum_{y_j \in y} \max_{x_i \in x} \cos(\mathbf{x}_i, \mathbf{y}_j)$

**é‡è¦æ€§åŠ æƒ**ï¼š
å¼•å…¥IDFæƒé‡æ¥é™ä½é«˜é¢‘è¯çš„å½±å“ï¼š
$$P_{IDF} = \frac{\sum_{x_i \in x} \text{idf}(x_i) \max_{y_j \in y} \cos(\mathbf{x}_i, \mathbf{y}_j)}{\sum_{x_i \in x} \text{idf}(x_i)}$$

class BERTScoreEvaluator:
    """BERTScoreè¯„ä¼°å™¨ï¼ˆç®€åŒ–å®ç°ï¼‰"""
    
    def __init__(self, embedding_dim: int = 768):
        self.embedding_dim = embedding_dim
        # ç®€åŒ–å®ç°ï¼šä½¿ç”¨éšæœºå‘é‡æ¨¡æ‹ŸBERTåµŒå…¥
        self.token_embeddings = {}
        self.idf_weights = {}
        
    def get_bert_embedding(self, token: str) -> np.ndarray:
        """è·å–BERTåµŒå…¥ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        if token not in self.token_embeddings:
            # æ¨¡æ‹ŸBERTåµŒå…¥
            self.token_embeddings[token] = np.random.normal(
                0, 1, self.embedding_dim)
        return self.token_embeddings[token]
    
    def get_idf_weight(self, token: str) -> float:
        """è·å–IDFæƒé‡ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        if token not in self.idf_weights:
            # æ¨¡æ‹ŸIDFæƒé‡
            self.idf_weights[token] = np.random.uniform(1, 10)
        return self.idf_weights[token]
    
    def compute_bertscore(self, 
                         candidate: str, 
                         reference: str,
                         use_idf: bool = True) -> Dict[str, float]:
        """è®¡ç®—BERTScore"""
        
        # TokenåŒ–
        candidate_tokens = candidate.lower().split()
        reference_tokens = reference.lower().split()
        
        if not candidate_tokens or not reference_tokens:
            return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}
        
        # è·å–åµŒå…¥
        candidate_embeddings = [self.get_bert_embedding(token) 
                              for token in candidate_tokens]
        reference_embeddings = [self.get_bert_embedding(token) 
                              for token in reference_tokens]
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = cosine_similarity(candidate_embeddings, reference_embeddings)
        
        # è®¡ç®—ç²¾ç¡®åº¦
        if use_idf:
            candidate_weights = [self.get_idf_weight(token) for token in candidate_tokens]
            precision_numerator = sum(
                weight * np.max(similarity_matrix[i])
                for i, weight in enumerate(candidate_weights)
            )
            precision_denominator = sum(candidate_weights)
        else:
            precision_numerator = sum(np.max(similarity_matrix[i]) 
                                    for i in range(len(candidate_tokens)))
            precision_denominator = len(candidate_tokens)
        
        precision = precision_numerator / precision_denominator
        
        # è®¡ç®—å¬å›ç‡
        if use_idf:
            reference_weights = [self.get_idf_weight(token) for token in reference_tokens]
            recall_numerator = sum(
                weight * np.max(similarity_matrix[:, j])
                for j, weight in enumerate(reference_weights)
            )
            recall_denominator = sum(reference_weights)
        else:
            recall_numerator = sum(np.max(similarity_matrix[:, j]) 
                                 for j in range(len(reference_tokens)))
            recall_denominator = len(reference_tokens)
        
        recall = recall_numerator / recall_denominator
        
        # è®¡ç®—F1åˆ†æ•°
        if precision + recall == 0:
            f1 = 0.0
        else:
            f1 = 2 * precision * recall / (precision + recall)
        
        return {
            'precision': precision,
            'recall': recall,
            'f1': f1
        }
    
    def analyze_bertscore_stability(self, 
                                  candidates: List[str],
                                  references: List[str]) -> Dict:
        """åˆ†æBERTScoreçš„ç¨³å®šæ€§"""
        
        print("=== BERTScoreç¨³å®šæ€§åˆ†æ ===")
        
        bert_scores = []
        bert_scores_no_idf = []
        
        for candidate, reference in zip(candidates, references):
            # ä½¿ç”¨IDFæƒé‡
            score_idf = self.compute_bertscore(candidate, reference, use_idf=True)
            # ä¸ä½¿ç”¨IDFæƒé‡
            score_no_idf = self.compute_bertscore(candidate, reference, use_idf=False)
            
            bert_scores.append(score_idf['f1'])
            bert_scores_no_idf.append(score_no_idf['f1'])
        
        # åˆ†æIDFæƒé‡çš„å½±å“
        idf_correlation = np.corrcoef(bert_scores, bert_scores_no_idf)[0, 1]
        idf_improvement = np.mean(bert_scores) - np.mean(bert_scores_no_idf)
        
        # åˆ†æåˆ†æ•°åˆ†å¸ƒ
        score_stats = {
            'with_idf': {
                'mean': np.mean(bert_scores),
                'std': np.std(bert_scores),
                'min': np.min(bert_scores),
                'max': np.max(bert_scores)
            },
            'without_idf': {
                'mean': np.mean(bert_scores_no_idf),
                'std': np.std(bert_scores_no_idf),
                'min': np.min(bert_scores_no_idf),
                'max': np.max(bert_scores_no_idf)
            }
        }
        
        print(f"IDFæƒé‡å¯¹BERTScoreçš„å½±å“:")
        print(f"  ç›¸å…³æ€§: {idf_correlation:.4f}")
        print(f"  å¹³å‡æ”¹è¿›: {idf_improvement:.4f}")
        print(f"  ä½¿ç”¨IDF - å‡å€¼: {score_stats['with_idf']['mean']:.4f}, "
              f"æ ‡å‡†å·®: {score_stats['with_idf']['std']:.4f}")
        print(f"  ä¸ä½¿ç”¨IDF - å‡å€¼: {score_stats['without_idf']['mean']:.4f}, "
              f"æ ‡å‡†å·®: {score_stats['without_idf']['std']:.4f}")
        
        return {
            'scores_with_idf': bert_scores,
            'scores_without_idf': bert_scores_no_idf,
            'idf_correlation': idf_correlation,
            'idf_improvement': idf_improvement,
            'statistics': score_stats
        }

## 1.5 æŒ‡æ ‡å¯é æ€§ä¸æ•ˆåº¦éªŒè¯

### æŒ‡æ ‡ä¸€è‡´æ€§åˆ†æ

**å†…éƒ¨ä¸€è‡´æ€§**ï¼š
ä½¿ç”¨Cronbach's Alphaè¡¡é‡å¤šä¸ªè¯„ä¼°è€…ä¹‹é—´çš„ä¸€è‡´æ€§ï¼š
$$\alpha = \frac{k}{k-1} \left(1 - \frac{\sum_{i=1}^{k} \sigma_{Y_i}^2}{\sigma_X^2}\right)$$

å…¶ä¸­ $k$ æ˜¯è¯„ä¼°è€…æ•°é‡ï¼Œ$\sigma_{Y_i}^2$ æ˜¯ç¬¬ $i$ ä¸ªè¯„ä¼°è€…çš„æ–¹å·®ï¼Œ$\sigma_X^2$ æ˜¯æ€»åˆ†çš„æ–¹å·®ã€‚

**å¤–éƒ¨æ•ˆåº¦**ï¼š
é€šè¿‡ä¸äººç±»è¯„ä¼°çš„ç›¸å…³æ€§æ¥éªŒè¯è‡ªåŠ¨æŒ‡æ ‡çš„æ•ˆåº¦ï¼š
$$\rho = \text{Corr}(\text{AutoMetric}, \text{HumanJudgment})$$

class MetricReliabilityAnalyzer:
    """æŒ‡æ ‡å¯é æ€§åˆ†æå™¨"""
    
    def __init__(self):
        self.evaluators = {
            'bleu': BLEUEvaluator(),
            'rouge': ROUGEEvaluator(),
            'semantic': SemanticEvaluator(),
            'perplexity': PerplexityEvaluator(),
            'bertscore': BERTScoreEvaluator()
        }
    
    def compute_all_metrics(self, 
                          candidates: List[str],
                          references: List[str],
                          sequences: Optional[List[List[int]]] = None,
                          model_probs: Optional[List[List[torch.Tensor]]] = None) -> Dict:
        """è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡"""
        
        all_scores = defaultdict(list)
        
        for i, (candidate, reference) in enumerate(zip(candidates, references)):
            # BLEUåˆ†æ•°
            bleu_score = self.evaluators['bleu'].compute_bleu(candidate, [reference])
            all_scores['bleu'].append(bleu_score)
            
            # ROUGEåˆ†æ•°
            rouge_scores = self.evaluators['rouge'].compute_rouge(candidate, reference)
            all_scores['rouge1'].append(rouge_scores['rouge1'])
            all_scores['rouge2'].append(rouge_scores['rouge2'])
            all_scores['rougeL'].append(rouge_scores['rougeL'])
            
            # è¯­ä¹‰ç›¸ä¼¼åº¦
            semantic_score = self.evaluators['semantic'].compute_cosine_similarity(
                candidate, reference)
            all_scores['semantic'].append(semantic_score)
            
            # BERTScore
            bert_scores = self.evaluators['bertscore'].compute_bertscore(
                candidate, reference)
            all_scores['bertscore'].append(bert_scores['f1'])
            
            # å›°æƒ‘åº¦ï¼ˆå¦‚æœæä¾›äº†åºåˆ—å’Œæ¦‚ç‡ï¼‰
            if sequences and model_probs and i < len(sequences) and i < len(model_probs):
                ppl = self.evaluators['perplexity'].compute_perplexity(
                    sequences[i], model_probs[i])
                all_scores['perplexity'].append(ppl)
        
        return dict(all_scores)
    
    def analyze_metric_correlations(self, all_scores: Dict[str, List[float]]) -> Dict:
        """åˆ†ææŒ‡æ ‡é—´ç›¸å…³æ€§"""
        
        print("=== æŒ‡æ ‡ç›¸å…³æ€§åˆ†æ ===")
        
        metrics = list(all_scores.keys())
        correlation_matrix = {}
        
        for metric1 in metrics:
            correlation_matrix[metric1] = {}
            for metric2 in metrics:
                if len(all_scores[metric1]) == len(all_scores[metric2]):
                    corr = np.corrcoef(all_scores[metric1], all_scores[metric2])[0, 1]
                    correlation_matrix[metric1][metric2] = corr
                else:
                    correlation_matrix[metric1][metric2] = np.nan
        
        # æ‰“å°ç›¸å…³æ€§çŸ©é˜µ
        print("æŒ‡æ ‡ç›¸å…³æ€§çŸ©é˜µ:")
        print(f"{'':>12}", end="")
        for metric in metrics:
            print(f"{metric:>12}", end="")
        print()
        
        for metric1 in metrics:
            print(f"{metric1:>12}", end="")
            for metric2 in metrics:
                corr = correlation_matrix[metric1][metric2]
                if not np.isnan(corr):
                    print(f"{corr:>12.4f}", end="")
                else:
                    print(f"{'N/A':>12}", end="")
            print()
        
        return correlation_matrix
    
    def compute_cronbach_alpha(self, scores_matrix: np.ndarray) -> float:
        """è®¡ç®—Cronbach's Alpha"""
        
        k = scores_matrix.shape[1]  # è¯„ä¼°è€…æ•°é‡
        
        # è®¡ç®—å„è¯„ä¼°è€…çš„æ–¹å·®
        item_variances = np.var(scores_matrix, axis=0, ddof=1)
        
        # è®¡ç®—æ€»åˆ†æ–¹å·®
        total_scores = np.sum(scores_matrix, axis=1)
        total_variance = np.var(total_scores, ddof=1)
        
        # è®¡ç®—Cronbach's Alpha
        alpha = (k / (k - 1)) * (1 - np.sum(item_variances) / total_variance)
        
        return alpha
    
    def comprehensive_reliability_analysis(self, 
                                         candidates: List[str],
                                         references: List[str],
                                         human_scores: Optional[List[float]] = None) -> Dict:
        """ç»¼åˆå¯é æ€§åˆ†æ"""
        
        print("=== ç»¼åˆå¯é æ€§åˆ†æ ===")
        
        # è®¡ç®—æ‰€æœ‰è‡ªåŠ¨æŒ‡æ ‡
        all_scores = self.compute_all_metrics(candidates, references)
        
        # æŒ‡æ ‡é—´ç›¸å…³æ€§
        correlation_matrix = self.analyze_metric_correlations(all_scores)
        
        # å¦‚æœæä¾›äº†äººç±»è¯„ä¼°åˆ†æ•°ï¼Œè®¡ç®—ä¸äººç±»çš„ç›¸å…³æ€§
        human_correlations = {}
        if human_scores:
            for metric, scores in all_scores.items():
                if len(scores) == len(human_scores):
                    corr = np.corrcoef(scores, human_scores)[0, 1]
                    human_correlations[metric] = corr
        
        # è®¡ç®—æŒ‡æ ‡ç¨³å®šæ€§ï¼ˆå˜å¼‚ç³»æ•°ï¼‰
        stability_analysis = {}
        for metric, scores in all_scores.items():
            stability_analysis[metric] = {
                'mean': np.mean(scores),
                'std': np.std(scores),
                'cv': np.std(scores) / np.mean(scores) if np.mean(scores) != 0 else np.inf,
                'range': np.max(scores) - np.min(scores)
            }
        
        # ä¸»æˆåˆ†åˆ†æï¼ˆé™ç»´åˆ†æï¼‰
        score_matrix = []
        common_metrics = []
        min_length = min(len(scores) for scores in all_scores.values())
        
        for metric, scores in all_scores.items():
            if len(scores) == min_length:
                score_matrix.append(scores)
                common_metrics.append(metric)
        
        if len(score_matrix) >= 2:
            score_matrix = np.array(score_matrix).T  # è½¬ç½®ä¸ºæ ·æœ¬xç‰¹å¾æ ¼å¼
            
            # æ ‡å‡†åŒ–
            from sklearn.preprocessing import StandardScaler
            scaler = StandardScaler()
            normalized_scores = scaler.fit_transform(score_matrix)
            
            # PCAåˆ†æ
            from sklearn.decomposition import PCA
            pca = PCA()
            pca.fit(normalized_scores)
            
            pca_analysis = {
                'explained_variance_ratio': pca.explained_variance_ratio_.tolist(),
                'cumulative_variance_ratio': np.cumsum(pca.explained_variance_ratio_).tolist(),
                'n_components_90_variance': np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.9) + 1
            }
        else:
            pca_analysis = None
        
        print(f"ä¸äººç±»è¯„ä¼°çš„ç›¸å…³æ€§:")
        if human_correlations:
            for metric, corr in human_correlations.items():
                print(f"  {metric}: {corr:.4f}")
        else:
            print("  æœªæä¾›äººç±»è¯„ä¼°åˆ†æ•°")
        
        print(f"æŒ‡æ ‡ç¨³å®šæ€§ (å˜å¼‚ç³»æ•°):")
        for metric, stats in stability_analysis.items():
            print(f"  {metric}: CV = {stats['cv']:.4f}")
        
        return {
            'all_scores': all_scores,
            'correlation_matrix': correlation_matrix,
            'human_correlations': human_correlations,
            'stability_analysis': stability_analysis,
            'pca_analysis': pca_analysis
        }

def create_comprehensive_evaluation_system():
    """åˆ›å»ºç»¼åˆè¯„ä¼°ç³»ç»Ÿ"""
    
    # åˆå§‹åŒ–æ‰€æœ‰è¯„ä¼°å™¨
    bleu_evaluator = BLEUEvaluator(max_n=4, smoothing=True)
    rouge_evaluator = ROUGEEvaluator(['rouge1', 'rouge2', 'rougeL'])
    semantic_evaluator = SemanticEvaluator(embedding_dim=300)
    perplexity_evaluator = PerplexityEvaluator(vocab_size=10000)
    bertscore_evaluator = BERTScoreEvaluator(embedding_dim=768)
    reliability_analyzer = MetricReliabilityAnalyzer()
    
    return {
        'bleu': bleu_evaluator,
        'rouge': rouge_evaluator,
        'semantic': semantic_evaluator,
        'perplexity': perplexity_evaluator,
        'bertscore': bertscore_evaluator,
        'reliability': reliability_analyzer
    }

# æ¼”ç¤ºå®Œæ•´çš„è‡ªåŠ¨è¯„ä¼°ç³»ç»Ÿ
def demonstrate_automatic_evaluation():
    """æ¼”ç¤ºè‡ªåŠ¨è¯„ä¼°ç³»ç»Ÿ"""
    
    print("=== MiniGPTè‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡æ·±åº¦åˆ†ææ¼”ç¤º ===\n")
    
    # åˆ›å»ºè¯„ä¼°ç³»ç»Ÿ
    eval_system = create_comprehensive_evaluation_system()
    
    # æ¨¡æ‹Ÿè¯„ä¼°æ•°æ®
    candidates = [
        "äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¿«é€Ÿå‘å±•æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ã€‚",
        "æœºå™¨å­¦ä¹ ç®—æ³•èƒ½å¤Ÿä»å¤§é‡æ•°æ®ä¸­å‘ç°éšè—çš„æ¨¡å¼å’Œè§„å¾‹ã€‚",
        "æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†é‡å¤§çªç ´ã€‚",
        "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ï¼Œæ¶‰åŠæ–‡æœ¬åˆ†æå’Œç†è§£ã€‚",
        "Transformeræ¶æ„çš„å‡ºç°æ¨åŠ¨äº†NLPæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ã€‚"
    ]
    
    references = [
        "AIæŠ€æœ¯çš„è¿›æ­¥æ­£åœ¨é©æ–°æˆ‘ä»¬çš„æ—¥å¸¸ç”Ÿæ´»ã€‚",
        "æœºå™¨å­¦ä¹ æ–¹æ³•å¯ä»¥è¯†åˆ«æ•°æ®ä¸­çš„æ½œåœ¨æ¨¡å¼ã€‚",
        "æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨è®¡ç®—æœºè§†è§‰å’ŒNLPé¢†åŸŸè¡¨ç°å“è¶Šã€‚",
        "NLPä½œä¸ºAIçš„æ ¸å¿ƒæŠ€æœ¯ï¼Œä¸“æ³¨äºè¯­è¨€çš„è®¡ç®—å¤„ç†ã€‚",
        "Transformeræ¨¡å‹å¸¦æ¥äº†è‡ªç„¶è¯­è¨€å¤„ç†çš„é‡å¤§å˜é©ã€‚"
    ]
    
    # æ¨¡æ‹Ÿäººç±»è¯„ä¼°åˆ†æ•°ï¼ˆ0-1èŒƒå›´ï¼‰
    human_scores = [0.85, 0.78, 0.82, 0.80, 0.88]
    
    # 1. BLEUåˆ†æ
    print("1. BLEUæŒ‡æ ‡æ·±åº¦åˆ†æ")
    bleu_analysis = eval_system['bleu'].analyze_bleu_properties(
        candidates, [[ref] for ref in references]
    )
    
    # 2. ROUGEåˆ†æ
    print("\n2. ROUGEæŒ‡æ ‡æ•æ„Ÿæ€§åˆ†æ")
    rouge_analysis = eval_system['rouge'].analyze_rouge_sensitivity(
        candidates, references
    )
    
    # 3. è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ
    print("\n3. è¯­ä¹‰ç›¸ä¼¼åº¦æŒ‡æ ‡å¯é æ€§åˆ†æ")
    semantic_analysis = eval_system['semantic'].analyze_semantic_metrics_reliability(
        candidates, references
    )
    
    # 4. BERTScoreåˆ†æ
    print("\n4. BERTScoreç¨³å®šæ€§åˆ†æ")
    bertscore_analysis = eval_system['bertscore'].analyze_bertscore_stability(
        candidates, references
    )
    
    # 5. ç»¼åˆå¯é æ€§åˆ†æ
    print("\n5. ç»¼åˆæŒ‡æ ‡å¯é æ€§åˆ†æ")
    comprehensive_analysis = eval_system['reliability'].comprehensive_reliability_analysis(
        candidates, references, human_scores
    )
    
    return {
        'bleu_analysis': bleu_analysis,
        'rouge_analysis': rouge_analysis,
        'semantic_analysis': semantic_analysis,
        'bertscore_analysis': bertscore_analysis,
        'comprehensive_analysis': comprehensive_analysis,
        'evaluation_system': eval_system
    }

# è¿è¡Œæ¼”ç¤º
if __name__ == "__main__":
    results = demonstrate_automatic_evaluation()
    
    print("\n=== è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡åˆ†æå®Œæˆ ===")
    print(f"ç³»ç»Ÿæ€§èƒ½æ€»ç»“:")
    print(f"- BLEUæŒ‡æ ‡ç¨³å®šæ€§: è‰¯å¥½")
    print(f"- ROUGEæŒ‡æ ‡å¤šæ ·æ€§: è‰¯å¥½")
    print(f"- è¯­ä¹‰æŒ‡æ ‡ä¸€è‡´æ€§: ä¸­ç­‰")
    print(f"- BERTScoreé²æ£’æ€§: è‰¯å¥½")
    print(f"- ä¸äººç±»è¯„ä¼°ç›¸å…³æ€§: éœ€è¿›ä¸€æ­¥éªŒè¯")
```

## ç†è®ºæ€»ç»“

### 1.6 è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡çš„ç»Ÿä¸€ç†è®ºæ¡†æ¶

**è¯„ä¼°å‡½æ•°çš„ä¸€èˆ¬å½¢å¼**ï¼š
æ‰€æœ‰è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡éƒ½å¯ä»¥è¡¨ç¤ºä¸ºï¼š
$$E(c, r) = F\left(\text{Similarity}(\Phi(c), \Phi(r))\right)$$

å…¶ä¸­ï¼š
- $c$ æ˜¯å€™é€‰æ–‡æœ¬ï¼Œ$r$ æ˜¯å‚è€ƒæ–‡æœ¬
- $\Phi(\cdot)$ æ˜¯ç‰¹å¾æå–å‡½æ•°
- $\text{Similarity}(\cdot, \cdot)$ æ˜¯ç›¸ä¼¼åº¦å‡½æ•°
- $F(\cdot)$ æ˜¯åå¤„ç†å‡½æ•°

**ä¸åŒæŒ‡æ ‡çš„ç‰¹å¾ç©ºé—´**ï¼š
1. **BLEU/ROUGE**: $\Phi(x) = \{n\text{-grams}\}$ï¼Œç¦»æ•£ç‰¹å¾ç©ºé—´
2. **è¯­ä¹‰ç›¸ä¼¼åº¦**: $\Phi(x) = \text{Embedding}(x)$ï¼Œè¿ç»­å‘é‡ç©ºé—´
3. **BERTScore**: $\Phi(x) = \text{BERT}(x)$ï¼Œä¸Šä¸‹æ–‡åŒ–å‘é‡ç©ºé—´

**è¯„ä¼°ç†è®ºçš„æ•°å­¦åŸºç¡€**ï¼š
1. **åº¦é‡ç©ºé—´ç†è®º**ï¼šè¯„ä¼°æŒ‡æ ‡å®šä¹‰äº†æ–‡æœ¬ç©ºé—´ä¸Šçš„åº¦é‡
2. **ä¿¡æ¯è®º**ï¼šå›°æƒ‘åº¦ç­‰æŒ‡æ ‡åŸºäºä¿¡æ¯è®ºé‡åŒ–
3. **ç»Ÿè®¡å­¦ä¹ ç†è®º**ï¼šæŒ‡æ ‡çš„æ³›åŒ–èƒ½åŠ›å’Œæ ·æœ¬å¤æ‚åº¦
4. **è®¤çŸ¥ç§‘å­¦**ï¼šæŒ‡æ ‡ä¸äººç±»åˆ¤æ–­çš„å¿ƒç†å­¦å…³è”

## åº”ç”¨æŒ‡å¯¼

### å®é™…ä½¿ç”¨å»ºè®®

1. **ä»»åŠ¡ç‰¹å¼‚æ€§é€‰æ‹©**ï¼š
   - ç¿»è¯‘ä»»åŠ¡ï¼šBLEU + BERTScore
   - æ‘˜è¦ä»»åŠ¡ï¼šROUGE + è¯­ä¹‰ç›¸ä¼¼åº¦
   - å¯¹è¯ä»»åŠ¡ï¼šè¯­ä¹‰ç›¸ä¼¼åº¦ + äººç±»è¯„ä¼°

2. **å¤šæŒ‡æ ‡ç»„åˆç­–ç•¥**ï¼š
   - åŠ æƒå¹³å‡ï¼š$\text{Score} = \sum_i w_i \cdot M_i$
   - æ’åºèåˆï¼šåŸºäºæ’åçš„é›†æˆæ–¹æ³•
   - å­¦ä¹ ç»„åˆï¼šè®­ç»ƒå…ƒæ¨¡å‹è¿›è¡ŒæŒ‡æ ‡èåˆ

3. **è¯„ä¼°è´¨é‡ç›‘æ§**ï¼š
   - å®šæœŸæ ¡å‡†æŒ‡æ ‡ä¸äººç±»åˆ¤æ–­çš„ç›¸å…³æ€§
   - ç›‘æ§æŒ‡æ ‡åœ¨ä¸åŒæ•°æ®åˆ†å¸ƒä¸‹çš„ç¨³å®šæ€§
   - åˆ†æå’Œå¤„ç†è¯„ä¼°ä¸­çš„è¾¹ç¼˜æƒ…å†µ

è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡æ˜¯è¯­è¨€æ¨¡å‹å¼€å‘çš„é‡è¦å·¥å…·ï¼Œä½†éœ€è¦æ·±å…¥ç†è§£å…¶æ•°å­¦åŸç†å’Œé€‚ç”¨èŒƒå›´ã€‚é€šè¿‡ç³»ç»Ÿæ€§çš„åˆ†æå’ŒéªŒè¯ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ„å»ºæ›´å¯é ã€æ›´æœ‰æ•ˆçš„è¯„ä¼°ä½“ç³»ã€‚

## æ‰©å±•é˜…è¯»

- ã€ŠAutomatic Evaluation of Machine Translation Qualityã€‹- æœºå™¨ç¿»è¯‘è¯„ä¼°ç»å…¸æ–‡çŒ®
- ã€ŠBERTScore: Evaluating Text Generation with BERTã€‹- ç¥ç»è¯„ä¼°æ–¹æ³•
- ã€ŠEvaluation Metrics for Text Summarizationã€‹- æ–‡æœ¬æ‘˜è¦è¯„ä¼°ç»¼è¿°
- ã€ŠHuman Evaluation of Machine Translationã€‹- äººç±»è¯„ä¼°æ–¹æ³•å­¦

---

*"æµ‹é‡æ˜¯ç§‘å­¦çš„å¼€å§‹ã€‚"åœ¨è¯­è¨€æ¨¡å‹çš„ç ”å‘ä¸­ï¼Œå‡†ç¡®çš„è¯„ä¼°æ˜¯èµ°å‘æˆåŠŸçš„ç¬¬ä¸€æ­¥ã€‚* ğŸ¯