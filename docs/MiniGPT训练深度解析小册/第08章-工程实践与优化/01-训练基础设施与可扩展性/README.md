# 01 è®­ç»ƒåŸºç¡€è®¾æ–½ä¸å¯æ‰©å±•æ€§

> **ä»å•æœºåˆ°é›†ç¾¤ï¼šæ„å»ºå¯æ‰©å±•çš„å¤§è§„æ¨¡è®­ç»ƒç³»ç»Ÿ**

## æ ¸å¿ƒæ€æƒ³

è®­ç»ƒåŸºç¡€è®¾æ–½æ˜¯æ”¯æ’‘å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®­ç»ƒçš„æŠ€æœ¯åŸºçŸ³ã€‚éšç€æ¨¡å‹è§„æ¨¡çš„æŒ‡æ•°çº§å¢é•¿ï¼Œå•æœºè®­ç»ƒå·²æ— æ³•æ»¡è¶³éœ€æ±‚ï¼Œåˆ†å¸ƒå¼è®­ç»ƒæˆä¸ºå¿…ç„¶é€‰æ‹©ã€‚ç„¶è€Œï¼Œåˆ†å¸ƒå¼ç³»ç»Ÿçš„å¤æ‚æ€§è¿œè¶…å•æœºç¯å¢ƒâ€”â€”ä»æ•°æ®åˆ†å‰²åˆ°æ¢¯åº¦åŒæ­¥ï¼Œä»æ•…éšœå¤„ç†åˆ°èµ„æºè°ƒåº¦ï¼Œæ¯ä¸ªç¯èŠ‚éƒ½éœ€è¦ç²¾å¿ƒè®¾è®¡å’Œä¼˜åŒ–ã€‚

**å…³é”®æ´å¯Ÿ**ï¼š
- **å¹¶è¡ŒåŒ–ç­–ç•¥**ï¼šæ•°æ®å¹¶è¡Œã€æ¨¡å‹å¹¶è¡Œã€æµæ°´çº¿å¹¶è¡Œçš„æ•°å­¦åŸç†ä¸å·¥ç¨‹å®ç°
- **é€šä¿¡ä¼˜åŒ–**ï¼šå‡å°‘é€šä¿¡å¼€é”€ï¼Œæé«˜å¸¦å®½åˆ©ç”¨ç‡
- **å®¹é”™æœºåˆ¶**ï¼šåœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­ä¿è¯è®­ç»ƒçš„ç¨³å®šæ€§å’Œå¯æ¢å¤æ€§
- **èµ„æºç®¡ç†**ï¼šæ™ºèƒ½è°ƒåº¦å’Œé«˜æ•ˆåˆ©ç”¨æ˜‚è´µçš„GPUèµ„æº

ä»æ•°å­¦è§’åº¦çœ‹ï¼Œåˆ†å¸ƒå¼è®­ç»ƒæ˜¯å°†å•æœºä¼˜åŒ–é—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªå­é—®é¢˜çš„å¹¶è¡Œæ±‚è§£ï¼Œå…¶æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºä¿è¯æ”¶æ•›æ€§çš„åŒæ—¶æœ€å¤§åŒ–å¹¶è¡Œæ•ˆç‡ã€‚

## 1.1 åˆ†å¸ƒå¼è®­ç»ƒæ¶æ„çš„æ•°å­¦ç†è®º

### æ•°æ®å¹¶è¡Œçš„æ•°å­¦å»ºæ¨¡

**æ•°æ®å¹¶è¡Œçš„åŸºæœ¬æ€æƒ³**ï¼š
å°†æ‰¹æ•°æ® $\mathcal{B}$ åˆ†å‰²ä¸º $N$ ä¸ªå­æ‰¹æ¬¡ï¼š$\mathcal{B} = \bigcup_{i=1}^{N} \mathcal{B}_i$ï¼Œæ¯ä¸ªè®¾å¤‡è®¡ç®—å±€éƒ¨æ¢¯åº¦ï¼š
$$\mathbf{g}_i = \frac{1}{|\mathcal{B}_i|} \sum_{x \in \mathcal{B}_i} \nabla_\theta L(f_\theta(x), y)$$

**å…¨å±€æ¢¯åº¦èšåˆ**ï¼š
$$\mathbf{g}_{global} = \frac{1}{N} \sum_{i=1}^{N} \mathbf{g}_i$$

**æ”¶æ•›æ€§åˆ†æ**ï¼š
è®¾çœŸå®æ¢¯åº¦ä¸º $\mathbf{g}^*$ï¼Œæ•°æ®å¹¶è¡Œæ¢¯åº¦ä¸º $\mathbf{g}_{dp}$ï¼Œåˆ™æ–¹å·®ä¸ºï¼š
$$\text{Var}[\mathbf{g}_{dp}] = \frac{1}{N} \text{Var}[\mathbf{g}^*] + \frac{N-1}{N} \text{Bias}^2$$

å…¶ä¸­åå·®é¡¹æ¥æºäºä¸åŒè®¾å¤‡ä¸Šæ•°æ®åˆ†å¸ƒçš„å·®å¼‚ã€‚

### æ¨¡å‹å¹¶è¡Œçš„å¼ é‡åˆ†å‰²ç†è®º

**çº¿æ€§å±‚çš„åˆ†å‰²**ï¼š
å¯¹äºçº¿æ€§å˜æ¢ $\mathbf{y} = \mathbf{x} \mathbf{W}$ï¼Œå¯ä»¥æŒ‰è¡Œæˆ–åˆ—åˆ†å‰²æƒé‡çŸ©é˜µï¼š

**åˆ—å¹¶è¡Œ**ï¼š$\mathbf{W} = [\mathbf{W}_1, \mathbf{W}_2, ..., \mathbf{W}_N]$
$$\mathbf{y} = \sum_{i=1}^{N} \mathbf{x} \mathbf{W}_i$$

**è¡Œå¹¶è¡Œ**ï¼š$\mathbf{W} = \begin{bmatrix} \mathbf{W}_1 \\ \mathbf{W}_2 \\ \vdots \\ \mathbf{W}_N \end{bmatrix}$
$$\mathbf{y}_i = \mathbf{x}_i \mathbf{W}_i, \quad \mathbf{y} = \text{concat}([\mathbf{y}_1, \mathbf{y}_2, ..., \mathbf{y}_N])$$

**æ³¨æ„åŠ›æœºåˆ¶çš„åˆ†å‰²**ï¼š
å¤šå¤´æ³¨æ„åŠ›å¤©ç„¶æ”¯æŒå¹¶è¡Œï¼š
$$\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, ..., \text{head}_h) \mathbf{W}^O$$

æ¯ä¸ªå¤´å¯ä»¥åˆ†é…åˆ°ä¸åŒè®¾å¤‡å¹¶è¡Œè®¡ç®—ã€‚

```python
import torch
import torch.nn as nn
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
import numpy as np
from typing import Dict, List, Tuple, Optional, Union, Any
import time
import os
import logging
from dataclasses import dataclass, field
from enum import Enum
import threading
import queue
from concurrent.futures import ThreadPoolExecutor, as_completed
import psutil
import GPUtil

class ParallelismType(Enum):
    """å¹¶è¡Œç±»å‹æšä¸¾"""
    DATA_PARALLEL = "data_parallel"
    MODEL_PARALLEL = "model_parallel"
    PIPELINE_PARALLEL = "pipeline_parallel"
    HYBRID_PARALLEL = "hybrid_parallel"

@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®"""
    # æ¨¡å‹é…ç½®
    model_size: str = "small"
    vocab_size: int = 10000
    hidden_size: int = 512
    num_layers: int = 6
    num_heads: int = 8
    
    # è®­ç»ƒé…ç½®
    batch_size: int = 32
    learning_rate: float = 1e-4
    max_steps: int = 10000
    gradient_accumulation_steps: int = 1
    
    # åˆ†å¸ƒå¼é…ç½®
    world_size: int = 1
    local_rank: int = 0
    parallelism_type: ParallelismType = ParallelismType.DATA_PARALLEL
    
    # ç³»ç»Ÿé…ç½®
    mixed_precision: bool = True
    gradient_checkpointing: bool = False
    
    # æ•…éšœæ¢å¤é…ç½®
    checkpoint_interval: int = 1000
    max_checkpoints: int = 3

class DistributedTrainingManager:
    """åˆ†å¸ƒå¼è®­ç»ƒç®¡ç†å™¨"""
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.logger = self._setup_logging()
        self.device = None
        self.process_group = None
        self.performance_metrics = {
            'throughput': [],
            'memory_usage': [],
            'communication_time': [],
            'computation_time': []
        }
        
    def _setup_logging(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format=f'[Rank {self.config.local_rank}] %(asctime)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger(__name__)
    
    def initialize_distributed(self, backend: str = 'nccl'):
        """åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ"""
        
        self.logger.info(f"åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ: backend={backend}, world_size={self.config.world_size}")
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['MASTER_ADDR'] = 'localhost'
        os.environ['MASTER_PORT'] = '12355'
        os.environ['WORLD_SIZE'] = str(self.config.world_size)
        os.environ['RANK'] = str(self.config.local_rank)
        
        # åˆå§‹åŒ–è¿›ç¨‹ç»„
        if self.config.world_size > 1:
            dist.init_process_group(
                backend=backend,
                rank=self.config.local_rank,
                world_size=self.config.world_size
            )
            self.process_group = dist.group.WORLD
        
        # è®¾ç½®è®¾å¤‡
        if torch.cuda.is_available():
            self.device = torch.device(f'cuda:{self.config.local_rank}')
            torch.cuda.set_device(self.device)
        else:
            self.device = torch.device('cpu')
        
        self.logger.info(f"åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ: device={self.device}")
    
    def create_data_parallel_model(self, model: nn.Module) -> nn.Module:
        """åˆ›å»ºæ•°æ®å¹¶è¡Œæ¨¡å‹"""
        
        model = model.to(self.device)
        
        if self.config.world_size > 1:
            # ä½¿ç”¨DistributedDataParallel
            model = DDP(
                model,
                device_ids=[self.config.local_rank],
                output_device=self.config.local_rank,
                find_unused_parameters=False
            )
            self.logger.info("ä½¿ç”¨DistributedDataParallelåŒ…è£…æ¨¡å‹")
        
        return model
    
    def create_model_parallel_model(self, model: nn.Module) -> nn.Module:
        """åˆ›å»ºæ¨¡å‹å¹¶è¡Œæ¨¡å‹"""
        
        # ç®€åŒ–å®ç°ï¼šå°†æ¨¡å‹çš„ä¸åŒå±‚åˆ†é…åˆ°ä¸åŒè®¾å¤‡
        if self.config.world_size > 1:
            layers_per_device = len(model.layers) // self.config.world_size
            start_layer = self.config.local_rank * layers_per_device
            end_layer = min((self.config.local_rank + 1) * layers_per_device, 
                           len(model.layers))
            
            # åªä¿ç•™åˆ†é…ç»™å½“å‰è®¾å¤‡çš„å±‚
            device_layers = model.layers[start_layer:end_layer]
            
            class ModelParallelWrapper(nn.Module):
                def __init__(self, layers, device, rank, world_size):
                    super().__init__()
                    self.layers = nn.ModuleList(layers)
                    self.device = device
                    self.rank = rank
                    self.world_size = world_size
                    
                def forward(self, x):
                    x = x.to(self.device)
                    
                    for layer in self.layers:
                        x = layer(x)
                    
                    # å‘é€åˆ°ä¸‹ä¸€ä¸ªè®¾å¤‡
                    if self.rank < self.world_size - 1:
                        dist.send(x.contiguous(), dst=self.rank + 1)
                        return None
                    else:
                        return x
            
            wrapped_model = ModelParallelWrapper(
                device_layers, self.device, 
                self.config.local_rank, self.config.world_size
            )
            
            self.logger.info(f"æ¨¡å‹å¹¶è¡Œ: è®¾å¤‡{self.config.local_rank}è´Ÿè´£å±‚{start_layer}-{end_layer}")
            return wrapped_model
        
        return model.to(self.device)

class PipelineParallelTrainer:
    """æµæ°´çº¿å¹¶è¡Œè®­ç»ƒå™¨"""
    
    def __init__(self, config: TrainingConfig, num_microbatches: int = 4):
        self.config = config
        self.num_microbatches = num_microbatches
        self.logger = logging.getLogger(__name__)
        
    def create_pipeline_schedule(self, num_stages: int, num_microbatches: int) -> List[List[str]]:
        """åˆ›å»ºæµæ°´çº¿è°ƒåº¦"""
        
        # GPipeè°ƒåº¦ç­–ç•¥
        schedule = [[] for _ in range(num_stages)]
        
        # Forward pass
        for micro_batch in range(num_microbatches):
            for stage in range(num_stages):
                schedule[stage].append(f'F{micro_batch}')
        
        # Backward pass (åå‘)
        for micro_batch in range(num_microbatches-1, -1, -1):
            for stage in range(num_stages-1, -1, -1):
                schedule[stage].append(f'B{micro_batch}')
        
        return schedule
    
    def execute_pipeline_stage(self, 
                             stage_model: nn.Module,
                             schedule: List[str],
                             input_queue: queue.Queue,
                             output_queue: queue.Queue):
        """æ‰§è¡Œæµæ°´çº¿é˜¶æ®µ"""
        
        activations = {}  # ä¿å­˜æ¿€æ´»å€¼ç”¨äºåå‘ä¼ æ’­
        
        for operation in schedule:
            op_type = operation[0]  # 'F' æˆ– 'B'
            micro_batch_id = int(operation[1:])
            
            if op_type == 'F':
                # Forward pass
                if not input_queue.empty():
                    input_data = input_queue.get()
                    
                    with torch.no_grad() if not stage_model.training else torch.enable_grad():
                        output = stage_model(input_data)
                        activations[micro_batch_id] = output.detach()
                        
                        if not output_queue.full():
                            output_queue.put(output)
            
            elif op_type == 'B':
                # Backward pass
                if micro_batch_id in activations:
                    activation = activations[micro_batch_id]
                    # æ‰§è¡Œåå‘ä¼ æ’­
                    activation.backward()
                    del activations[micro_batch_id]
    
    def analyze_pipeline_efficiency(self, 
                                  num_stages: int, 
                                  num_microbatches: int,
                                  stage_times: List[float]) -> Dict:
        """åˆ†ææµæ°´çº¿æ•ˆç‡"""
        
        # è®¡ç®—ç†è®ºæœ€ä¼˜æ—¶é—´
        max_stage_time = max(stage_times)
        sequential_time = sum(stage_times) * num_microbatches
        
        # æµæ°´çº¿æ‰§è¡Œæ—¶é—´
        pipeline_time = (num_stages - 1) * max_stage_time + num_microbatches * max_stage_time
        
        # æ•ˆç‡åˆ†æ
        speedup = sequential_time / pipeline_time
        efficiency = speedup / num_stages
        
        # æ°”æ³¡æ—¶é—´åˆ†æ
        bubble_time = (num_stages - 1) * max_stage_time
        bubble_ratio = bubble_time / pipeline_time
        
        return {
            'speedup': speedup,
            'efficiency': efficiency,
            'bubble_ratio': bubble_ratio,
            'pipeline_time': pipeline_time,
            'sequential_time': sequential_time,
            'max_stage_time': max_stage_time
        }

## 1.2 é€šä¿¡ä¼˜åŒ–ä¸å¸¦å®½ç®¡ç†

### All-Reduceç®—æ³•çš„æ•°å­¦åˆ†æ

**Ring All-Reduceç®—æ³•**ï¼š
å¯¹äº $N$ ä¸ªè®¾å¤‡ï¼Œæ¯ä¸ªè®¾å¤‡æœ‰å‚æ•°å‘é‡ $\mathbf{p}_i$ï¼ŒRing All-Reduceçš„é€šä¿¡å¤æ‚åº¦ä¸ºï¼š
$$T_{comm} = 2 \cdot \frac{N-1}{N} \cdot \frac{M}{B}$$

å…¶ä¸­ $M$ æ˜¯å‚æ•°æ€»é‡ï¼Œ$B$ æ˜¯å¸¦å®½ã€‚

**Butterfly All-Reduceç®—æ³•**ï¼š
é€šä¿¡å¤æ‚åº¦ä¸ºï¼š
$$T_{comm} = \log_2(N) \cdot \frac{M}{B}$$

é€‚ç”¨äºè®¾å¤‡æ•°é‡è¾ƒå°‘çš„åœºæ™¯ã€‚

class CommunicationOptimizer:
    """é€šä¿¡ä¼˜åŒ–å™¨"""
    
    def __init__(self, world_size: int, bandwidth_mbps: float = 1000):
        self.world_size = world_size
        self.bandwidth_mbps = bandwidth_mbps
        self.communication_stats = {
            'total_bytes': 0,
            'total_time': 0.0,
            'operation_count': 0
        }
        self.logger = logging.getLogger(__name__)
    
    def estimate_allreduce_time(self, 
                              tensor_size_bytes: int,
                              algorithm: str = 'ring') -> float:
        """ä¼°ç®—All-Reduceé€šä¿¡æ—¶é—´"""
        
        bandwidth_bps = self.bandwidth_mbps * 1024 * 1024  # è½¬æ¢ä¸ºbps
        
        if algorithm == 'ring':
            # Ring All-Reduce: 2 * (N-1)/N * M/B
            comm_time = 2 * (self.world_size - 1) / self.world_size * tensor_size_bytes / bandwidth_bps
        
        elif algorithm == 'butterfly':
            # Butterfly All-Reduce: log2(N) * M/B
            comm_time = np.log2(self.world_size) * tensor_size_bytes / bandwidth_bps
        
        elif algorithm == 'tree':
            # Tree All-Reduce: 2 * log2(N) * M/B
            comm_time = 2 * np.log2(self.world_size) * tensor_size_bytes / bandwidth_bps
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")
        
        return comm_time
    
    def optimize_gradient_communication(self, 
                                      gradients: Dict[str, torch.Tensor],
                                      compression_ratio: float = 0.1) -> Dict[str, torch.Tensor]:
        """ä¼˜åŒ–æ¢¯åº¦é€šä¿¡"""
        
        start_time = time.time()
        
        optimized_gradients = {}
        total_original_size = 0
        total_compressed_size = 0
        
        for name, grad in gradients.items():
            total_original_size += grad.numel() * grad.element_size()
            
            # æ¢¯åº¦å‹ç¼©
            if compression_ratio < 1.0:
                compressed_grad = self._compress_gradient(grad, compression_ratio)
                optimized_gradients[name] = compressed_grad
                total_compressed_size += compressed_grad.numel() * compressed_grad.element_size()
            else:
                optimized_gradients[name] = grad
                total_compressed_size += grad.numel() * grad.element_size()
        
        # æ‰§è¡ŒAll-Reduce
        for name, grad in optimized_gradients.items():
            if self.world_size > 1:
                dist.all_reduce(grad, op=dist.ReduceOp.SUM)
                grad.div_(self.world_size)
        
        comm_time = time.time() - start_time
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.communication_stats['total_bytes'] += total_compressed_size
        self.communication_stats['total_time'] += comm_time
        self.communication_stats['operation_count'] += 1
        
        compression_rate = total_compressed_size / total_original_size
        self.logger.info(f"æ¢¯åº¦é€šä¿¡å®Œæˆ: å‹ç¼©ç‡={compression_rate:.3f}, æ—¶é—´={comm_time:.3f}s")
        
        return optimized_gradients
    
    def _compress_gradient(self, gradient: torch.Tensor, ratio: float) -> torch.Tensor:
        """æ¢¯åº¦å‹ç¼©"""
        
        # Top-Kå‹ç¼©
        if ratio < 1.0:
            k = max(1, int(gradient.numel() * ratio))
            
            # å±•å¹³æ¢¯åº¦
            flat_grad = gradient.view(-1)
            
            # é€‰æ‹©top-kå…ƒç´ 
            _, top_k_indices = torch.topk(torch.abs(flat_grad), k)
            
            # åˆ›å»ºç¨€ç–æ¢¯åº¦
            compressed_grad = torch.zeros_like(flat_grad)
            compressed_grad[top_k_indices] = flat_grad[top_k_indices]
            
            return compressed_grad.view(gradient.shape)
        
        return gradient
    
    def analyze_communication_pattern(self) -> Dict:
        """åˆ†æé€šä¿¡æ¨¡å¼"""
        
        if self.communication_stats['operation_count'] == 0:
            return {}
        
        avg_bytes_per_op = self.communication_stats['total_bytes'] / self.communication_stats['operation_count']
        avg_time_per_op = self.communication_stats['total_time'] / self.communication_stats['operation_count']
        effective_bandwidth = avg_bytes_per_op / avg_time_per_op if avg_time_per_op > 0 else 0
        
        return {
            'total_operations': self.communication_stats['operation_count'],
            'total_bytes_transferred': self.communication_stats['total_bytes'],
            'total_communication_time': self.communication_stats['total_time'],
            'average_bytes_per_operation': avg_bytes_per_op,
            'average_time_per_operation': avg_time_per_op,
            'effective_bandwidth_mbps': effective_bandwidth / (1024 * 1024),
            'bandwidth_utilization': (effective_bandwidth / (1024 * 1024)) / self.bandwidth_mbps
        }

## 1.3 èµ„æºç®¡ç†ä¸è°ƒåº¦ç³»ç»Ÿ

### GPUé›†ç¾¤èµ„æºå»ºæ¨¡

**èµ„æºçº¦æŸä¼˜åŒ–**ï¼š
$$\max \sum_{i=1}^{N} u_i \cdot x_i \quad \text{s.t.} \begin{cases}
\sum_{i=1}^{N} r_i \cdot x_i \leq R \\
\sum_{i=1}^{N} m_i \cdot x_i \leq M \\
x_i \in \{0, 1\}
\end{cases}$$

å…¶ä¸­ $u_i$ æ˜¯ä»»åŠ¡æ•ˆç”¨ï¼Œ$r_i, m_i$ æ˜¯èµ„æºéœ€æ±‚ï¼Œ$R, M$ æ˜¯æ€»èµ„æºã€‚

**è´Ÿè½½å‡è¡¡æŒ‡æ ‡**ï¼š
$$\text{Load Balance} = 1 - \frac{\text{std}(\{L_1, L_2, ..., L_N\})}{\text{mean}(\{L_1, L_2, ..., L_N\})}$$

å…¶ä¸­ $L_i$ æ˜¯ç¬¬ $i$ ä¸ªè®¾å¤‡çš„è´Ÿè½½ã€‚

class ResourceManager:
    """èµ„æºç®¡ç†å™¨"""
    
    def __init__(self):
        self.gpu_info = self._get_gpu_info()
        self.resource_history = []
        self.allocation_strategy = "greedy"
        self.logger = logging.getLogger(__name__)
        
    def _get_gpu_info(self) -> List[Dict]:
        """è·å–GPUä¿¡æ¯"""
        
        gpu_info = []
        
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                info = {
                    'device_id': i,
                    'name': torch.cuda.get_device_name(i),
                    'memory_total': torch.cuda.get_device_properties(i).total_memory,
                    'memory_available': torch.cuda.get_device_properties(i).total_memory,
                    'compute_capability': torch.cuda.get_device_properties(i).major,
                    'utilization': 0.0,
                    'temperature': 0.0
                }
                gpu_info.append(info)
        
        return gpu_info
    
    def monitor_resource_usage(self) -> Dict:
        """ç›‘æ§èµ„æºä½¿ç”¨æƒ…å†µ"""
        
        usage_info = {
            'timestamp': time.time(),
            'cpu_usage': psutil.cpu_percent(),
            'memory_usage': psutil.virtual_memory().percent,
            'gpu_usage': []
        }
        
        # è·å–GPUä½¿ç”¨ä¿¡æ¯
        try:
            gpus = GPUtil.getGPUs()
            for gpu in gpus:
                gpu_usage = {
                    'device_id': gpu.id,
                    'utilization': gpu.load * 100,
                    'memory_usage': gpu.memoryUtil * 100,
                    'temperature': gpu.temperature,
                    'power_usage': getattr(gpu, 'powerDraw', 0)
                }
                usage_info['gpu_usage'].append(gpu_usage)
        except:
            # å¦‚æœGPUtilä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–ä¿¡æ¯
            for i in range(torch.cuda.device_count()):
                memory_allocated = torch.cuda.memory_allocated(i)
                memory_total = torch.cuda.get_device_properties(i).total_memory
                
                gpu_usage = {
                    'device_id': i,
                    'utilization': 0.0,  # æ— æ³•è·å–
                    'memory_usage': (memory_allocated / memory_total) * 100,
                    'temperature': 0.0,  # æ— æ³•è·å–
                    'power_usage': 0.0   # æ— æ³•è·å–
                }
                usage_info['gpu_usage'].append(gpu_usage)
        
        # è®°å½•å†å²
        self.resource_history.append(usage_info)
        
        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        if len(self.resource_history) > 1000:
            self.resource_history = self.resource_history[-1000:]
        
        return usage_info
    
    def allocate_resources(self, 
                         task_requirements: List[Dict],
                         strategy: str = "first_fit") -> Dict:
        """èµ„æºåˆ†é…"""
        
        self.logger.info(f"å¼€å§‹èµ„æºåˆ†é…: {len(task_requirements)}ä¸ªä»»åŠ¡, ç­–ç•¥={strategy}")
        
        allocation_result = {
            'allocated_tasks': [],
            'failed_tasks': [],
            'resource_utilization': {}
        }
        
        available_resources = {
            'gpu_memory': [gpu['memory_available'] for gpu in self.gpu_info],
            'gpu_utilization': [gpu['utilization'] for gpu in self.gpu_info]
        }
        
        for task_id, requirements in enumerate(task_requirements):
            allocated = False
            
            if strategy == "first_fit":
                # ç¬¬ä¸€ä¸ªæ»¡è¶³æ¡ä»¶çš„GPU
                for gpu_id, gpu in enumerate(self.gpu_info):
                    if self._can_allocate(gpu_id, requirements, available_resources):
                        self._allocate_to_gpu(task_id, gpu_id, requirements, available_resources)
                        allocation_result['allocated_tasks'].append({
                            'task_id': task_id,
                            'gpu_id': gpu_id,
                            'requirements': requirements
                        })
                        allocated = True
                        break
            
            elif strategy == "best_fit":
                # é€‰æ‹©æœ€é€‚åˆçš„GPUï¼ˆå‰©ä½™èµ„æºæœ€å°‘ä½†è¶³å¤Ÿï¼‰
                best_gpu = -1
                min_waste = float('inf')
                
                for gpu_id, gpu in enumerate(self.gpu_info):
                    if self._can_allocate(gpu_id, requirements, available_resources):
                        waste = available_resources['gpu_memory'][gpu_id] - requirements.get('memory', 0)
                        if waste < min_waste:
                            min_waste = waste
                            best_gpu = gpu_id
                
                if best_gpu != -1:
                    self._allocate_to_gpu(task_id, best_gpu, requirements, available_resources)
                    allocation_result['allocated_tasks'].append({
                        'task_id': task_id,
                        'gpu_id': best_gpu,
                        'requirements': requirements
                    })
                    allocated = True
            
            elif strategy == "load_balance":
                # è´Ÿè½½å‡è¡¡ï¼šé€‰æ‹©åˆ©ç”¨ç‡æœ€ä½çš„å¯ç”¨GPU
                best_gpu = -1
                min_utilization = float('inf')
                
                for gpu_id, gpu in enumerate(self.gpu_info):
                    if (self._can_allocate(gpu_id, requirements, available_resources) and
                        available_resources['gpu_utilization'][gpu_id] < min_utilization):
                        min_utilization = available_resources['gpu_utilization'][gpu_id]
                        best_gpu = gpu_id
                
                if best_gpu != -1:
                    self._allocate_to_gpu(task_id, best_gpu, requirements, available_resources)
                    allocation_result['allocated_tasks'].append({
                        'task_id': task_id,
                        'gpu_id': best_gpu,
                        'requirements': requirements
                    })
                    allocated = True
            
            if not allocated:
                allocation_result['failed_tasks'].append({
                    'task_id': task_id,
                    'requirements': requirements,
                    'reason': 'insufficient_resources'
                })
        
        # è®¡ç®—èµ„æºåˆ©ç”¨ç‡
        total_memory = sum(gpu['memory_total'] for gpu in self.gpu_info)
        used_memory = total_memory - sum(available_resources['gpu_memory'])
        allocation_result['resource_utilization'] = {
            'memory_utilization': used_memory / total_memory if total_memory > 0 else 0,
            'gpu_utilization': sum(available_resources['gpu_utilization']) / len(self.gpu_info)
        }
        
        self.logger.info(f"èµ„æºåˆ†é…å®Œæˆ: {len(allocation_result['allocated_tasks'])}ä¸ªä»»åŠ¡æˆåŠŸåˆ†é…")
        
        return allocation_result
    
    def _can_allocate(self, 
                     gpu_id: int, 
                     requirements: Dict, 
                     available_resources: Dict) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥åˆ†é…"""
        
        required_memory = requirements.get('memory', 0)
        available_memory = available_resources['gpu_memory'][gpu_id]
        
        # ç®€åŒ–æ£€æŸ¥ï¼šåªæ£€æŸ¥å†…å­˜
        return available_memory >= required_memory
    
    def _allocate_to_gpu(self, 
                        task_id: int, 
                        gpu_id: int, 
                        requirements: Dict, 
                        available_resources: Dict):
        """åˆ†é…ä»»åŠ¡åˆ°GPU"""
        
        required_memory = requirements.get('memory', 0)
        required_utilization = requirements.get('utilization', 10.0)
        
        # æ›´æ–°å¯ç”¨èµ„æº
        available_resources['gpu_memory'][gpu_id] -= required_memory
        available_resources['gpu_utilization'][gpu_id] += required_utilization
    
    def optimize_resource_allocation(self, 
                                   tasks: List[Dict],
                                   optimization_objective: str = "utilization") -> Dict:
        """ä¼˜åŒ–èµ„æºåˆ†é…"""
        
        if optimization_objective == "utilization":
            return self._optimize_for_utilization(tasks)
        elif optimization_objective == "latency":
            return self._optimize_for_latency(tasks)
        elif optimization_objective == "throughput":
            return self._optimize_for_throughput(tasks)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–ç›®æ ‡: {optimization_objective}")
    
    def _optimize_for_utilization(self, tasks: List[Dict]) -> Dict:
        """ä¼˜åŒ–GPUåˆ©ç”¨ç‡"""
        
        # ä½¿ç”¨è´ªå¿ƒç®—æ³•ï¼šæŒ‰èµ„æºéœ€æ±‚é™åºæ’åºï¼Œä¼˜å…ˆåˆ†é…å¤§ä»»åŠ¡
        sorted_tasks = sorted(enumerate(tasks), 
                            key=lambda x: x[1].get('memory', 0), 
                            reverse=True)
        
        reordered_tasks = [task for _, task in sorted_tasks]
        
        return self.allocate_resources(reordered_tasks, strategy="best_fit")
    
    def _optimize_for_latency(self, tasks: List[Dict]) -> Dict:
        """ä¼˜åŒ–å»¶è¿Ÿ"""
        
        # è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼šå°½é‡å‡åŒ€åˆ†é…
        return self.allocate_resources(tasks, strategy="load_balance")
    
    def _optimize_for_throughput(self, tasks: List[Dict]) -> Dict:
        """ä¼˜åŒ–ååé‡"""
        
        # å…ˆåˆ†é…å°ä»»åŠ¡ï¼Œæé«˜å¹¶è¡Œåº¦
        sorted_tasks = sorted(enumerate(tasks), 
                            key=lambda x: x[1].get('memory', 0))
        
        reordered_tasks = [task for _, task in sorted_tasks]
        
        return self.allocate_resources(reordered_tasks, strategy="first_fit")

## 1.4 å®¹é”™ä¸æ¢å¤æœºåˆ¶

### æ•…éšœæ£€æµ‹çš„æ•°å­¦æ¨¡å‹

**æ•…éšœæ¦‚ç‡å»ºæ¨¡**ï¼š
è®¾è®¾å¤‡ $i$ åœ¨æ—¶é—´ $t$ å†…å‘ç”Ÿæ•…éšœçš„æ¦‚ç‡ä¸ºï¼š
$$P_{\text{fail}}(i, t) = 1 - e^{-\lambda_i t}$$

å…¶ä¸­ $\lambda_i$ æ˜¯è®¾å¤‡ $i$ çš„æ•…éšœç‡ã€‚

**ç³»ç»Ÿå¯é æ€§**ï¼š
å¯¹äºæœ‰ $N$ ä¸ªè®¾å¤‡çš„ç³»ç»Ÿï¼Œè‡³å°‘æœ‰ $k$ ä¸ªè®¾å¤‡æ­£å¸¸å·¥ä½œçš„æ¦‚ç‡ï¼š
$$R(t, k) = \sum_{i=k}^{N} \binom{N}{i} p^i (1-p)^{N-i}$$

å…¶ä¸­ $p = e^{-\lambda t}$ æ˜¯å•è®¾å¤‡å¯é æ€§ã€‚

class FaultToleranceManager:
    """å®¹é”™ç®¡ç†å™¨"""
    
    def __init__(self, 
                 checkpoint_interval: int = 1000,
                 max_checkpoints: int = 3,
                 backup_strategy: str = "redundant"):
        
        self.checkpoint_interval = checkpoint_interval
        self.max_checkpoints = max_checkpoints
        self.backup_strategy = backup_strategy
        self.checkpoint_history = []
        self.failure_history = []
        self.logger = logging.getLogger(__name__)
        
    def create_checkpoint(self, 
                         model: nn.Module, 
                         optimizer: torch.optim.Optimizer,
                         step: int,
                         checkpoint_dir: str = "./checkpoints") -> str:
        """åˆ›å»ºæ£€æŸ¥ç‚¹"""
        
        checkpoint_path = os.path.join(checkpoint_dir, f"checkpoint_step_{step}.pt")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        checkpoint_data = {
            'step': step,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'timestamp': time.time(),
            'random_state': torch.get_rng_state(),
            'cuda_random_state': torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None
        }
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        start_time = time.time()
        torch.save(checkpoint_data, checkpoint_path)
        save_time = time.time() - start_time
        
        # è®°å½•æ£€æŸ¥ç‚¹ä¿¡æ¯
        checkpoint_info = {
            'path': checkpoint_path,
            'step': step,
            'timestamp': checkpoint_data['timestamp'],
            'save_time': save_time,
            'file_size': os.path.getsize(checkpoint_path)
        }
        
        self.checkpoint_history.append(checkpoint_info)
        
        # æ¸…ç†æ—§æ£€æŸ¥ç‚¹
        self._cleanup_old_checkpoints()
        
        self.logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}, è€—æ—¶{save_time:.2f}s")
        
        return checkpoint_path
    
    def load_checkpoint(self, 
                       checkpoint_path: str,
                       model: nn.Module,
                       optimizer: torch.optim.Optimizer) -> int:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        
        start_time = time.time()
        checkpoint_data = torch.load(checkpoint_path, map_location=self.device)
        load_time = time.time() - start_time
        
        # æ¢å¤æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€
        model.load_state_dict(checkpoint_data['model_state_dict'])
        optimizer.load_state_dict(checkpoint_data['optimizer_state_dict'])
        
        # æ¢å¤éšæœºæ•°çŠ¶æ€
        torch.set_rng_state(checkpoint_data['random_state'])
        if checkpoint_data['cuda_random_state'] and torch.cuda.is_available():
            torch.cuda.set_rng_state_all(checkpoint_data['cuda_random_state'])
        
        step = checkpoint_data['step']
        
        self.logger.info(f"æ£€æŸ¥ç‚¹å·²åŠ è½½: {checkpoint_path}, æ¢å¤åˆ°æ­¥éª¤{step}, è€—æ—¶{load_time:.2f}s")
        
        return step
    
    def detect_failure(self, 
                      devices: List[int],
                      timeout: float = 30.0) -> List[int]:
        """æ£€æµ‹è®¾å¤‡æ•…éšœ"""
        
        failed_devices = []
        
        for device_id in devices:
            try:
                # å¥åº·æ£€æŸ¥ï¼šå°è¯•åœ¨è®¾å¤‡ä¸Šæ‰§è¡Œç®€å•æ“ä½œ
                if torch.cuda.is_available() and device_id < torch.cuda.device_count():
                    with torch.cuda.device(device_id):
                        # åˆ›å»ºæµ‹è¯•å¼ é‡
                        test_tensor = torch.randn(100, 100, device=device_id)
                        result = test_tensor @ test_tensor.T
                        
                        # æ£€æŸ¥ç»“æœæ˜¯å¦æ­£å¸¸
                        if torch.isnan(result).any() or torch.isinf(result).any():
                            failed_devices.append(device_id)
                            self.logger.warning(f"è®¾å¤‡{device_id}è®¡ç®—ç»“æœå¼‚å¸¸")
                
            except Exception as e:
                failed_devices.append(device_id)
                self.logger.error(f"è®¾å¤‡{device_id}æ•…éšœæ£€æµ‹å¤±è´¥: {e}")
                
                # è®°å½•æ•…éšœ
                failure_info = {
                    'device_id': device_id,
                    'timestamp': time.time(),
                    'error': str(e),
                    'error_type': type(e).__name__
                }
                self.failure_history.append(failure_info)
        
        if failed_devices:
            self.logger.warning(f"æ£€æµ‹åˆ°æ•…éšœè®¾å¤‡: {failed_devices}")
        
        return failed_devices
    
    def handle_device_failure(self, 
                            failed_devices: List[int],
                            available_devices: List[int]) -> Dict:
        """å¤„ç†è®¾å¤‡æ•…éšœ"""
        
        recovery_plan = {
            'strategy': self.backup_strategy,
            'device_mapping': {},
            'recovery_actions': []
        }
        
        if self.backup_strategy == "redundant":
            # å†—ä½™ç­–ç•¥ï¼šä½¿ç”¨å¤‡ç”¨è®¾å¤‡æ›¿æ¢æ•…éšœè®¾å¤‡
            for failed_device in failed_devices:
                if available_devices:
                    backup_device = available_devices.pop(0)
                    recovery_plan['device_mapping'][failed_device] = backup_device
                    recovery_plan['recovery_actions'].append(
                        f"å°†è®¾å¤‡{failed_device}çš„ä»»åŠ¡è¿ç§»åˆ°è®¾å¤‡{backup_device}"
                    )
                else:
                    recovery_plan['recovery_actions'].append(
                        f"æ— å¯ç”¨å¤‡ç”¨è®¾å¤‡æ›¿æ¢æ•…éšœè®¾å¤‡{failed_device}"
                    )
        
        elif self.backup_strategy == "checkpoint_restart":
            # æ£€æŸ¥ç‚¹é‡å¯ç­–ç•¥ï¼šä»æœ€è¿‘çš„æ£€æŸ¥ç‚¹æ¢å¤
            if self.checkpoint_history:
                latest_checkpoint = self.checkpoint_history[-1]
                recovery_plan['recovery_actions'].append(
                    f"ä»æ£€æŸ¥ç‚¹{latest_checkpoint['path']}æ¢å¤è®­ç»ƒ"
                )
            else:
                recovery_plan['recovery_actions'].append(
                    "æ²¡æœ‰å¯ç”¨æ£€æŸ¥ç‚¹ï¼Œéœ€è¦é‡æ–°å¼€å§‹è®­ç»ƒ"
                )
        
        elif self.backup_strategy == "graceful_degradation":
            # ä¼˜é›…é™çº§ï¼šè°ƒæ•´æ‰¹å¤§å°æˆ–æ¨¡å‹å¹¶è¡Œåº¦
            recovery_plan['recovery_actions'].append(
                "è°ƒæ•´è®­ç»ƒé…ç½®ä»¥é€‚åº”å‡å°‘çš„è®¾å¤‡æ•°é‡"
            )
        
        self.logger.info(f"æ•…éšœæ¢å¤è®¡åˆ’: {recovery_plan}")
        
        return recovery_plan
    
    def _cleanup_old_checkpoints(self):
        """æ¸…ç†æ—§æ£€æŸ¥ç‚¹"""
        
        if len(self.checkpoint_history) > self.max_checkpoints:
            # åˆ é™¤æœ€æ—§çš„æ£€æŸ¥ç‚¹
            old_checkpoints = self.checkpoint_history[:-self.max_checkpoints]
            
            for checkpoint_info in old_checkpoints:
                try:
                    if os.path.exists(checkpoint_info['path']):
                        os.remove(checkpoint_info['path'])
                        self.logger.info(f"å·²åˆ é™¤æ—§æ£€æŸ¥ç‚¹: {checkpoint_info['path']}")
                except Exception as e:
                    self.logger.error(f"åˆ é™¤æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            
            # æ›´æ–°å†å²è®°å½•
            self.checkpoint_history = self.checkpoint_history[-self.max_checkpoints:]
    
    def analyze_failure_patterns(self) -> Dict:
        """åˆ†ææ•…éšœæ¨¡å¼"""
        
        if not self.failure_history:
            return {}
        
        # æ•…éšœé¢‘ç‡åˆ†æ
        device_failure_counts = Counter([f['device_id'] for f in self.failure_history])
        error_type_counts = Counter([f['error_type'] for f in self.failure_history])
        
        # æ—¶é—´åˆ†æ
        timestamps = [f['timestamp'] for f in self.failure_history]
        if len(timestamps) > 1:
            intervals = [timestamps[i] - timestamps[i-1] for i in range(1, len(timestamps))]
            avg_interval = np.mean(intervals)
            std_interval = np.std(intervals)
        else:
            avg_interval = 0
            std_interval = 0
        
        return {
            'total_failures': len(self.failure_history),
            'device_failure_counts': dict(device_failure_counts),
            'error_type_counts': dict(error_type_counts),
            'failure_rate': len(self.failure_history) / (time.time() - timestamps[0]) if timestamps else 0,
            'average_failure_interval': avg_interval,
            'failure_interval_std': std_interval,
            'most_unreliable_device': device_failure_counts.most_common(1)[0][0] if device_failure_counts else None,
            'most_common_error': error_type_counts.most_common(1)[0][0] if error_type_counts else None
        }

def create_distributed_training_system(config: TrainingConfig):
    """åˆ›å»ºåˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿ"""
    
    # åˆå§‹åŒ–ç®¡ç†å™¨
    training_manager = DistributedTrainingManager(config)
    resource_manager = ResourceManager()
    comm_optimizer = CommunicationOptimizer(config.world_size)
    fault_manager = FaultToleranceManager(
        checkpoint_interval=config.checkpoint_interval,
        max_checkpoints=config.max_checkpoints
    )
    
    return {
        'training_manager': training_manager,
        'resource_manager': resource_manager,
        'communication_optimizer': comm_optimizer,
        'fault_tolerance_manager': fault_manager
    }

# æ¼”ç¤ºå®Œæ•´çš„åˆ†å¸ƒå¼è®­ç»ƒåŸºç¡€è®¾æ–½
def demonstrate_distributed_infrastructure():
    """æ¼”ç¤ºåˆ†å¸ƒå¼è®­ç»ƒåŸºç¡€è®¾æ–½"""
    
    print("=== MiniGPTåˆ†å¸ƒå¼è®­ç»ƒåŸºç¡€è®¾æ–½æ¼”ç¤º ===\n")
    
    # åˆ›å»ºè®­ç»ƒé…ç½®
    config = TrainingConfig(
        model_size="small",
        batch_size=64,
        world_size=4,
        parallelism_type=ParallelismType.DATA_PARALLEL,
        mixed_precision=True,
        checkpoint_interval=100
    )
    
    # åˆ›å»ºåˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿ
    system = create_distributed_training_system(config)
    
    # 1. åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    print("1. åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ")
    system['training_manager'].initialize_distributed()
    
    # 2. èµ„æºç›‘æ§å’Œåˆ†é…
    print("\n2. èµ„æºç›‘æ§å’Œç®¡ç†")
    
    # ç›‘æ§èµ„æºä½¿ç”¨
    resource_usage = system['resource_manager'].monitor_resource_usage()
    print(f"å½“å‰CPUä½¿ç”¨ç‡: {resource_usage['cpu_usage']:.1f}%")
    print(f"å½“å‰å†…å­˜ä½¿ç”¨ç‡: {resource_usage['memory_usage']:.1f}%")
    
    # æ¨¡æ‹Ÿä»»åŠ¡åˆ†é…
    tasks = [
        {'memory': 2048, 'utilization': 20.0},  # 2GBå†…å­˜éœ€æ±‚
        {'memory': 4096, 'utilization': 35.0},  # 4GBå†…å­˜éœ€æ±‚
        {'memory': 1024, 'utilization': 15.0},  # 1GBå†…å­˜éœ€æ±‚
        {'memory': 3072, 'utilization': 25.0},  # 3GBå†…å­˜éœ€æ±‚
    ]
    
    allocation = system['resource_manager'].allocate_resources(tasks, strategy="best_fit")
    print(f"æˆåŠŸåˆ†é…ä»»åŠ¡æ•°: {len(allocation['allocated_tasks'])}")
    print(f"èµ„æºåˆ©ç”¨ç‡: {allocation['resource_utilization']['memory_utilization']:.3f}")
    
    # 3. é€šä¿¡ä¼˜åŒ–åˆ†æ
    print("\n3. é€šä¿¡ä¼˜åŒ–åˆ†æ")
    
    # æ¨¡æ‹Ÿæ¢¯åº¦å¼ é‡
    gradients = {
        'layer1.weight': torch.randn(512, 256),
        'layer1.bias': torch.randn(512),
        'layer2.weight': torch.randn(256, 128),
        'layer2.bias': torch.randn(256)
    }
    
    # ä¼°ç®—é€šä¿¡æ—¶é—´
    total_size = sum(grad.numel() * grad.element_size() for grad in gradients.values())
    estimated_time = system['communication_optimizer'].estimate_allreduce_time(total_size, 'ring')
    print(f"ä¼°ç®—Ring All-Reduceæ—¶é—´: {estimated_time:.4f}s")
    
    # 4. å®¹é”™æœºåˆ¶æµ‹è¯•
    print("\n4. å®¹é”™å’Œæ¢å¤æœºåˆ¶")
    
    # æ¨¡æ‹Ÿæ•…éšœæ£€æµ‹
    devices = list(range(config.world_size))
    failed_devices = system['fault_tolerance_manager'].detect_failure(devices)
    
    if failed_devices:
        recovery_plan = system['fault_tolerance_manager'].handle_device_failure(
            failed_devices, [4, 5, 6]  # å¯ç”¨çš„å¤‡ç”¨è®¾å¤‡
        )
        print(f"æ•…éšœæ¢å¤è®¡åˆ’: {recovery_plan['strategy']}")
    else:
        print("æ‰€æœ‰è®¾å¤‡è¿è¡Œæ­£å¸¸")
    
    # 5. æ€§èƒ½åˆ†æ
    print("\n5. ç³»ç»Ÿæ€§èƒ½åˆ†æ")
    
    # æµæ°´çº¿æ•ˆç‡åˆ†æ
    pipeline_trainer = PipelineParallelTrainer(config, num_microbatches=8)
    stage_times = [0.1, 0.12, 0.11, 0.13]  # å„é˜¶æ®µæ—¶é—´
    efficiency = pipeline_trainer.analyze_pipeline_efficiency(
        num_stages=4, num_microbatches=8, stage_times=stage_times
    )
    
    print(f"æµæ°´çº¿åŠ é€Ÿæ¯”: {efficiency['speedup']:.2f}")
    print(f"æµæ°´çº¿æ•ˆç‡: {efficiency['efficiency']:.2f}")
    print(f"æ°”æ³¡æ—¶é—´æ¯”ä¾‹: {efficiency['bubble_ratio']:.2f}")
    
    return {
        'config': config,
        'system': system,
        'resource_usage': resource_usage,
        'allocation_result': allocation,
        'pipeline_efficiency': efficiency
    }

# è¿è¡Œæ¼”ç¤º
if __name__ == "__main__":
    results = demonstrate_distributed_infrastructure()
    
    print("\n=== åˆ†å¸ƒå¼è®­ç»ƒåŸºç¡€è®¾æ–½è¯„ä¼°å®Œæˆ ===")
    print(f"ç³»ç»Ÿé…ç½®æ€»ç»“:")
    print(f"- å¹¶è¡Œç­–ç•¥: {results['config'].parallelism_type.value}")
    print(f"- è®¾å¤‡æ•°é‡: {results['config'].world_size}")
    print(f"- æ‰¹å¤§å°: {results['config'].batch_size}")
    print(f"- æ··åˆç²¾åº¦: {results['config'].mixed_precision}")
    print(f"- æµæ°´çº¿æ•ˆç‡: {results['pipeline_efficiency']['efficiency']:.2f}")
```

## ç†è®ºæ€»ç»“

### 1.5 åˆ†å¸ƒå¼è®­ç»ƒçš„ç»Ÿä¸€ç†è®ºæ¡†æ¶

**æ‰©å±•æ€§å®šå¾‹**ï¼š
ç†æƒ³æƒ…å†µä¸‹ï¼Œä½¿ç”¨ $N$ ä¸ªè®¾å¤‡çš„åŠ é€Ÿæ¯”ä¸ºï¼š
$$S(N) = \frac{T_1}{T_N} = \frac{T_1}{T_1/N + T_{comm} + T_{sync}}$$

å…¶ä¸­ $T_{comm}$ æ˜¯é€šä¿¡æ—¶é—´ï¼Œ$T_{sync}$ æ˜¯åŒæ­¥æ—¶é—´ã€‚

**Amdahlå®šå¾‹çš„æ¨å¹¿**ï¼š
$$S(N) = \frac{1}{f + \frac{1-f}{N}}$$

å…¶ä¸­ $f$ æ˜¯ä¸å¯å¹¶è¡Œéƒ¨åˆ†çš„æ¯”ä¾‹ã€‚

**é€šä¿¡å¤æ‚åº¦ç†è®º**ï¼š
å¯¹äºå‚æ•°é‡ä¸º $P$ çš„æ¨¡å‹ï¼Œä¸åŒå¹¶è¡Œç­–ç•¥çš„é€šä¿¡å¤æ‚åº¦ï¼š
- æ•°æ®å¹¶è¡Œï¼š$O(P)$
- æ¨¡å‹å¹¶è¡Œï¼š$O(\sqrt{P})$
- æµæ°´çº¿å¹¶è¡Œï¼š$O(1)$

## åº”ç”¨æŒ‡å¯¼

### å®è·µå»ºè®®

1. **å¹¶è¡Œç­–ç•¥é€‰æ‹©**ï¼š
   - å°æ¨¡å‹ï¼šä¼˜å…ˆæ•°æ®å¹¶è¡Œ
   - å¤§æ¨¡å‹ï¼šæ··åˆå¹¶è¡Œç­–ç•¥
   - è¶…å¤§æ¨¡å‹ï¼š3Då¹¶è¡Œï¼ˆæ•°æ®+æ¨¡å‹+æµæ°´çº¿ï¼‰

2. **é€šä¿¡ä¼˜åŒ–**ï¼š
   - ä½¿ç”¨é«˜é€Ÿäº’è”ï¼ˆInfiniBandã€NVLinkï¼‰
   - å®æ–½æ¢¯åº¦å‹ç¼©å’Œé‡åŒ–
   - é‡å è®¡ç®—ä¸é€šä¿¡

3. **å®¹é”™è®¾è®¡**ï¼š
   - å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
   - å®ç°å¿«é€Ÿæ•…éšœæ£€æµ‹
   - è®¾è®¡ä¼˜é›…é™çº§æœºåˆ¶

è®­ç»ƒåŸºç¡€è®¾æ–½æ˜¯å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æˆåŠŸçš„å…³é”®æ”¯æ’‘ï¼Œéœ€è¦åœ¨æ€§èƒ½ã€å¯é æ€§ã€å¯æ‰©å±•æ€§ä¹‹é—´æ‰¾åˆ°æœ€ä¼˜å¹³è¡¡ã€‚

## æ‰©å±•é˜…è¯»

- ã€ŠDistributed Deep Learning: A Surveyã€‹- åˆ†å¸ƒå¼æ·±åº¦å­¦ä¹ ç»¼è¿°
- ã€ŠEfficient Large-Scale Language Model Trainingã€‹- å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®­ç»ƒæŠ€æœ¯
- ã€ŠPaLM: Scaling Language Modeling with Pathwaysã€‹- Google PaLMè®­ç»ƒç»éªŒ
- ã€ŠGPipe: Efficient Training of Giant Neural Networksã€‹- æµæ°´çº¿å¹¶è¡ŒæŠ€æœ¯

---

*"åŸºç¡€è®¾æ–½å†³å®šä¸Šå±‚å»ºç­‘ã€‚åœ¨è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­ï¼Œå¯é é«˜æ•ˆçš„åŸºç¡€è®¾æ–½æ˜¯å®ç°æŠ€æœ¯çªç ´çš„é‡è¦ä¿éšœã€‚"* ğŸ—ï¸