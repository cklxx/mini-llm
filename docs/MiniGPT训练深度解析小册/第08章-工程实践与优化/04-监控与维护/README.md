# 04 ç›‘æ§ä¸ç»´æŠ¤

> **ä»è¢«åŠ¨å“åº”åˆ°ä¸»åŠ¨é¢„é˜²ï¼šæ„å»ºæ™ºèƒ½åŒ–çš„ç³»ç»Ÿç›‘æ§ä¸ç»´æŠ¤ä½“ç³»**

## æ ¸å¿ƒæ€æƒ³

ç›‘æ§ä¸ç»´æŠ¤æ˜¯ä¿éšœè¯­è¨€æ¨¡å‹ç”Ÿäº§ç³»ç»Ÿé•¿æœŸç¨³å®šè¿è¡Œçš„å…³é”®ç¯èŠ‚ã€‚ä¸ä¼ ç»Ÿçš„è¢«åŠ¨ç›‘æ§ä¸åŒï¼Œç°ä»£æ™ºèƒ½ç›‘æ§ç³»ç»Ÿéœ€è¦å…·å¤‡é¢„æµ‹æ€§åˆ†æã€è‡ªåŠ¨åŒ–å“åº”å’ŒæŒç»­ä¼˜åŒ–çš„èƒ½åŠ›ã€‚é€šè¿‡å»ºç«‹å…¨æ–¹ä½çš„ç›‘æ§ä½“ç³»ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨é—®é¢˜å‘ç”Ÿå‰è¿›è¡Œé¢„è­¦ï¼Œåœ¨æ•…éšœå‘ç”Ÿæ—¶å¿«é€Ÿæ¢å¤ï¼Œåœ¨ç³»ç»Ÿè¿è¡Œä¸­æŒç»­ä¼˜åŒ–ã€‚

**å…³é”®æ´å¯Ÿ**ï¼š
- **å…¨æ ˆç›‘æ§**ï¼šä»åŸºç¡€è®¾æ–½åˆ°ä¸šåŠ¡æŒ‡æ ‡çš„ç«¯åˆ°ç«¯å¯è§‚æµ‹æ€§
- **æ™ºèƒ½å‘Šè­¦**ï¼šåŸºäºæœºå™¨å­¦ä¹ çš„å¼‚å¸¸æ£€æµ‹å’ŒåŠ¨æ€é˜ˆå€¼è°ƒæ•´
- **è‡ªæ„ˆèƒ½åŠ›**ï¼šè‡ªåŠ¨åŒ–çš„æ•…éšœæ£€æµ‹ã€è¯Šæ–­å’Œæ¢å¤æœºåˆ¶
- **æŒç»­ä¼˜åŒ–**ï¼šåŸºäºç›‘æ§æ•°æ®çš„ç³»ç»ŸæŒç»­æ”¹è¿›å’Œæ¨¡å‹æ¼”è¿›

ä»æ•°å­¦è§’åº¦çœ‹ï¼Œç›‘æ§ç³»ç»Ÿæ˜¯ä¸€ä¸ªå®æ—¶çŠ¶æ€ä¼°è®¡å’Œæ§åˆ¶é—®é¢˜ï¼Œéœ€è¦åœ¨ä¸ç¡®å®šæ€§ç¯å¢ƒä¸­ç»´æŒç³»ç»Ÿçš„æœ€ä¼˜è¿è¡ŒçŠ¶æ€ã€‚

## 4.1 ç³»ç»Ÿç›‘æ§çš„æ•°å­¦ç†è®º

### æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹çš„ç»Ÿè®¡æ¨¡å‹

**ARIMAæ¨¡å‹çš„çŠ¶æ€ç©ºé—´è¡¨ç¤º**ï¼š
å¯¹äºæ—¶é—´åºåˆ— $\{y_t\}$ï¼ŒARIMA(p,d,q)æ¨¡å‹å¯è¡¨ç¤ºä¸ºï¼š
$$(1-\phi_1B-...-\phi_pB^p)(1-B)^d y_t = (1+\theta_1B+...+\theta_qB^q)\epsilon_t$$

å…¶ä¸­ $B$ æ˜¯æ»åç®—å­ï¼Œ$\epsilon_t \sim N(0, \sigma^2)$ã€‚

**å¡å°”æ›¼æ»¤æ³¢çš„çŠ¶æ€ä¼°è®¡**ï¼š
çŠ¶æ€æ–¹ç¨‹ï¼š$\mathbf{x}_t = \mathbf{A}\mathbf{x}_{t-1} + \mathbf{w}_t$
è§‚æµ‹æ–¹ç¨‹ï¼š$\mathbf{y}_t = \mathbf{H}\mathbf{x}_t + \mathbf{v}_t$

é¢„æµ‹æ­¥éª¤ï¼š
$$\hat{\mathbf{x}}_{t|t-1} = \mathbf{A}\hat{\mathbf{x}}_{t-1|t-1}$$
$$\mathbf{P}_{t|t-1} = \mathbf{A}\mathbf{P}_{t-1|t-1}\mathbf{A}^T + \mathbf{Q}$$

æ›´æ–°æ­¥éª¤ï¼š
$$\mathbf{K}_t = \mathbf{P}_{t|t-1}\mathbf{H}^T(\mathbf{H}\mathbf{P}_{t|t-1}\mathbf{H}^T + \mathbf{R})^{-1}$$
$$\hat{\mathbf{x}}_{t|t} = \hat{\mathbf{x}}_{t|t-1} + \mathbf{K}_t(\mathbf{y}_t - \mathbf{H}\hat{\mathbf{x}}_{t|t-1})$$

**å¼‚å¸¸æ£€æµ‹çš„ç»Ÿè®¡é‡**ï¼š
åŸºäºé©¬å“ˆæ‹‰è¯ºæ¯”æ–¯è·ç¦»çš„å¼‚å¸¸å¾—åˆ†ï¼š
$$\text{Anomaly Score}_t = (\mathbf{y}_t - \hat{\mathbf{y}}_t)^T \mathbf{\Sigma}^{-1} (\mathbf{y}_t - \hat{\mathbf{y}}_t)$$

å…¶ä¸­ $\hat{\mathbf{y}}_t$ æ˜¯é¢„æµ‹å€¼ï¼Œ$\mathbf{\Sigma}$ æ˜¯åæ–¹å·®çŸ©é˜µã€‚

### SLAå¯ç”¨æ€§çš„æ¦‚ç‡æ¨¡å‹

**ç³»ç»Ÿå¯ç”¨æ€§è®¡ç®—**ï¼š
å¯¹äºä¸²è”ç³»ç»Ÿï¼Œæ€»å¯ç”¨æ€§ä¸ºï¼š
$$A_{total} = \prod_{i=1}^n A_i$$

å¯¹äºå¹¶è”ç³»ç»Ÿï¼Œæ€»å¯ç”¨æ€§ä¸ºï¼š
$$A_{total} = 1 - \prod_{i=1}^n (1 - A_i)$$

**MTTRå’ŒMTBFçš„å…³ç³»**ï¼š
ç³»ç»Ÿå¯ç”¨æ€§å¯è¡¨ç¤ºä¸ºï¼š
$$A = \frac{\text{MTBF}}{\text{MTBF} + \text{MTTR}}$$

å…¶ä¸­MTBFæ˜¯å¹³å‡æ— æ•…éšœæ—¶é—´ï¼ŒMTTRæ˜¯å¹³å‡æ¢å¤æ—¶é—´ã€‚

```python
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Union, Any, Callable
import matplotlib.pyplot as plt
import seaborn as sns
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict, deque
import time
import threading
import asyncio
import logging
from datetime import datetime, timedelta
import json
import warnings
warnings.filterwarnings('ignore')

# ç§‘å­¦è®¡ç®—å’Œæœºå™¨å­¦ä¹ 
from scipy import stats
from scipy.signal import find_peaks
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA
from sklearn.metrics import precision_recall_fscore_support

# ç›‘æ§å’ŒæŒ‡æ ‡åº“
from prometheus_client import Counter, Histogram, Gauge, Summary, generate_latest
import psutil
import GPUtil

# å¼‚å¸¸æ£€æµ‹åº“
try:
    from pyod.models.auto_encoder import AutoEncoder
    from pyod.models.iforest import IForest
    from pyod.models.lof import LOF
except ImportError:
    print("PyODåº“æœªå®‰è£…ï¼Œéƒ¨åˆ†å¼‚å¸¸æ£€æµ‹åŠŸèƒ½å°†ä¸å¯ç”¨")

class MetricType(Enum):
    """æŒ‡æ ‡ç±»å‹æšä¸¾"""
    COUNTER = "counter"
    GAUGE = "gauge"
    HISTOGRAM = "histogram"
    SUMMARY = "summary"

class AlertSeverity(Enum):
    """å‘Šè­¦ä¸¥é‡ç¨‹åº¦"""
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"

class AlertStatus(Enum):
    """å‘Šè­¦çŠ¶æ€"""
    FIRING = "firing"
    RESOLVED = "resolved"
    SUPPRESSED = "suppressed"

@dataclass
class MetricDefinition:
    """æŒ‡æ ‡å®šä¹‰"""
    name: str
    type: MetricType
    description: str
    labels: List[str] = field(default_factory=list)
    unit: str = ""
    aggregation_interval: int = 60  # èšåˆé—´éš”(ç§’)

@dataclass
class AlertRule:
    """å‘Šè­¦è§„åˆ™"""
    name: str
    metric_name: str
    condition: str  # ä¾‹å¦‚: "value > 0.8"
    severity: AlertSeverity
    duration: int = 300  # æŒç»­æ—¶é—´(ç§’)
    description: str = ""
    runbook_url: str = ""

@dataclass
class Alert:
    """å‘Šè­¦å®ä¾‹"""
    rule_name: str
    metric_name: str
    value: float
    threshold: float
    severity: AlertSeverity
    status: AlertStatus
    start_time: datetime
    end_time: Optional[datetime] = None
    labels: Dict[str, str] = field(default_factory=dict)
    annotations: Dict[str, str] = field(default_factory=dict)

class MetricCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self):
        self.metrics: Dict[str, Any] = {}
        self.metric_definitions: Dict[str, MetricDefinition] = {}
        self.data_points: Dict[str, deque] = {}
        self.lock = threading.Lock()
        
        # åˆå§‹åŒ–åŸºç¡€æŒ‡æ ‡
        self._initialize_basic_metrics()
    
    def _initialize_basic_metrics(self):
        """åˆå§‹åŒ–åŸºç¡€ç³»ç»ŸæŒ‡æ ‡"""
        
        basic_metrics = [
            MetricDefinition("cpu_usage_percent", MetricType.GAUGE, "CPUä½¿ç”¨ç‡", unit="%"),
            MetricDefinition("memory_usage_percent", MetricType.GAUGE, "å†…å­˜ä½¿ç”¨ç‡", unit="%"),
            MetricDefinition("disk_usage_percent", MetricType.GAUGE, "ç£ç›˜ä½¿ç”¨ç‡", unit="%"),
            MetricDefinition("network_bytes_sent", MetricType.COUNTER, "ç½‘ç»œå‘é€å­—èŠ‚æ•°", unit="bytes"),
            MetricDefinition("network_bytes_recv", MetricType.COUNTER, "ç½‘ç»œæ¥æ”¶å­—èŠ‚æ•°", unit="bytes"),
            MetricDefinition("gpu_usage_percent", MetricType.GAUGE, "GPUä½¿ç”¨ç‡", unit="%"),
            MetricDefinition("gpu_memory_usage_percent", MetricType.GAUGE, "GPUå†…å­˜ä½¿ç”¨ç‡", unit="%"),
        ]
        
        for metric_def in basic_metrics:
            self.register_metric(metric_def)
    
    def register_metric(self, metric_def: MetricDefinition):
        """æ³¨å†ŒæŒ‡æ ‡"""
        with self.lock:
            self.metric_definitions[metric_def.name] = metric_def
            self.data_points[metric_def.name] = deque(maxlen=10000)  # ä¿ç•™æœ€è¿‘10000ä¸ªæ•°æ®ç‚¹
            
            # åˆ›å»ºPrometheusæŒ‡æ ‡å¯¹è±¡
            if metric_def.type == MetricType.COUNTER:
                self.metrics[metric_def.name] = Counter(
                    metric_def.name, metric_def.description, metric_def.labels
                )
            elif metric_def.type == MetricType.GAUGE:
                self.metrics[metric_def.name] = Gauge(
                    metric_def.name, metric_def.description, metric_def.labels
                )
            elif metric_def.type == MetricType.HISTOGRAM:
                self.metrics[metric_def.name] = Histogram(
                    metric_def.name, metric_def.description, metric_def.labels
                )
            elif metric_def.type == MetricType.SUMMARY:
                self.metrics[metric_def.name] = Summary(
                    metric_def.name, metric_def.description, metric_def.labels
                )
    
    def record_metric(self, 
                     metric_name: str, 
                     value: float, 
                     labels: Optional[Dict[str, str]] = None,
                     timestamp: Optional[datetime] = None):
        """è®°å½•æŒ‡æ ‡å€¼"""
        
        if metric_name not in self.metrics:
            logging.warning(f"æœªæ³¨å†Œçš„æŒ‡æ ‡: {metric_name}")
            return
        
        timestamp = timestamp or datetime.now()
        
        with self.lock:
            # è®°å½•å†å²æ•°æ®ç‚¹
            data_point = {
                'timestamp': timestamp,
                'value': value,
                'labels': labels or {}
            }
            self.data_points[metric_name].append(data_point)
            
            # æ›´æ–°PrometheusæŒ‡æ ‡
            metric = self.metrics[metric_name]
            metric_def = self.metric_definitions[metric_name]
            
            if labels and metric_def.labels:
                if metric_def.type == MetricType.COUNTER:
                    metric.labels(**labels).inc(value)
                elif metric_def.type == MetricType.GAUGE:
                    metric.labels(**labels).set(value)
                elif metric_def.type == MetricType.HISTOGRAM:
                    metric.labels(**labels).observe(value)
                elif metric_def.type == MetricType.SUMMARY:
                    metric.labels(**labels).observe(value)
            else:
                if metric_def.type == MetricType.COUNTER:
                    metric.inc(value)
                elif metric_def.type == MetricType.GAUGE:
                    metric.set(value)
                elif metric_def.type == MetricType.HISTOGRAM:
                    metric.observe(value)
                elif metric_def.type == MetricType.SUMMARY:
                    metric.observe(value)
    
    def collect_system_metrics(self):
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        
        # CPUä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=1)
        self.record_metric("cpu_usage_percent", cpu_percent)
        
        # å†…å­˜ä½¿ç”¨ç‡
        memory = psutil.virtual_memory()
        self.record_metric("memory_usage_percent", memory.percent)
        
        # ç£ç›˜ä½¿ç”¨ç‡
        disk = psutil.disk_usage('/')
        disk_percent = (disk.used / disk.total) * 100
        self.record_metric("disk_usage_percent", disk_percent)
        
        # ç½‘ç»œç»Ÿè®¡
        network = psutil.net_io_counters()
        self.record_metric("network_bytes_sent", network.bytes_sent)
        self.record_metric("network_bytes_recv", network.bytes_recv)
        
        # GPUæŒ‡æ ‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            gpus = GPUtil.getGPUs()
            for i, gpu in enumerate(gpus):
                labels = {"gpu_id": str(i), "gpu_name": gpu.name}
                self.record_metric("gpu_usage_percent", gpu.load * 100, labels)
                self.record_metric("gpu_memory_usage_percent", gpu.memoryUtil * 100, labels)
        except Exception as e:
            logging.debug(f"GPUæŒ‡æ ‡æ”¶é›†å¤±è´¥: {e}")
    
    def get_metric_history(self, 
                          metric_name: str, 
                          start_time: Optional[datetime] = None,
                          end_time: Optional[datetime] = None) -> List[Dict]:
        """è·å–æŒ‡æ ‡å†å²æ•°æ®"""
        
        if metric_name not in self.data_points:
            return []
        
        with self.lock:
            data_points = list(self.data_points[metric_name])
        
        # æ—¶é—´è¿‡æ»¤
        if start_time or end_time:
            filtered_data = []
            for point in data_points:
                timestamp = point['timestamp']
                if start_time and timestamp < start_time:
                    continue
                if end_time and timestamp > end_time:
                    continue
                filtered_data.append(point)
            return filtered_data
        
        return data_points
    
    def get_prometheus_metrics(self) -> str:
        """è·å–Prometheusæ ¼å¼çš„æŒ‡æ ‡"""
        return generate_latest()

class AnomalyDetector:
    """å¼‚å¸¸æ£€æµ‹å™¨"""
    
    def __init__(self, window_size: int = 100, contamination: float = 0.1):
        self.window_size = window_size
        self.contamination = contamination
        self.models: Dict[str, Any] = {}
        self.scalers: Dict[str, StandardScaler] = {}
        self.training_data: Dict[str, deque] = {}
        self.lock = threading.Lock()
        
        # å¼‚å¸¸æ£€æµ‹ç»Ÿè®¡
        self.anomaly_stats = {
            'total_detections': 0,
            'true_positives': 0,
            'false_positives': 0,
            'false_negatives': 0
        }
    
    def train_detector(self, metric_name: str, data: List[float], method: str = "isolation_forest"):
        """è®­ç»ƒå¼‚å¸¸æ£€æµ‹æ¨¡å‹"""
        
        if len(data) < self.window_size:
            logging.warning(f"æ•°æ®ç‚¹ä¸è¶³ï¼Œæ— æ³•è®­ç»ƒæ£€æµ‹å™¨: {metric_name}")
            return
        
        # æ•°æ®é¢„å¤„ç†
        X = np.array(data).reshape(-1, 1)
        
        with self.lock:
            # æ ‡å‡†åŒ–
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            self.scalers[metric_name] = scaler
            
            # é€‰æ‹©æ£€æµ‹æ¨¡å‹
            if method == "isolation_forest":
                model = IsolationForest(contamination=self.contamination, random_state=42)
            elif method == "local_outlier_factor":
                from sklearn.neighbors import LocalOutlierFactor
                model = LocalOutlierFactor(contamination=self.contamination, novelty=True)
            elif method == "one_class_svm":
                from sklearn.svm import OneClassSVM
                model = OneClassSVM(gamma='auto', nu=self.contamination)
            else:
                model = IsolationForest(contamination=self.contamination, random_state=42)
            
            # è®­ç»ƒæ¨¡å‹
            if hasattr(model, 'fit_predict'):
                model.fit(X_scaled)
            else:
                model.fit(X_scaled)
            
            self.models[metric_name] = model
            
            # å­˜å‚¨è®­ç»ƒæ•°æ®ç”¨äºæŒç»­å­¦ä¹ 
            self.training_data[metric_name] = deque(data, maxlen=self.window_size * 10)
        
        logging.info(f"å¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒå®Œæˆ: {metric_name}")
    
    def detect_anomaly(self, metric_name: str, value: float) -> Dict:
        """æ£€æµ‹å¼‚å¸¸"""
        
        if metric_name not in self.models:
            return {
                'is_anomaly': False,
                'confidence': 0.0,
                'message': 'æœªè®­ç»ƒçš„æ£€æµ‹å™¨'
            }
        
        with self.lock:
            model = self.models[metric_name]
            scaler = self.scalers[metric_name]
        
        # æ•°æ®é¢„å¤„ç†
        X = np.array([[value]])
        X_scaled = scaler.transform(X)
        
        # å¼‚å¸¸æ£€æµ‹
        try:
            if hasattr(model, 'predict'):
                prediction = model.predict(X_scaled)[0]
                is_anomaly = prediction == -1
                
                # è®¡ç®—å¼‚å¸¸å¾—åˆ†
                if hasattr(model, 'decision_function'):
                    score = model.decision_function(X_scaled)[0]
                    confidence = abs(score)
                elif hasattr(model, 'score_samples'):
                    score = model.score_samples(X_scaled)[0]
                    confidence = abs(score)
                else:
                    confidence = 1.0 if is_anomaly else 0.0
            else:
                is_anomaly = False
                confidence = 0.0
        except Exception as e:
            logging.error(f"å¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")
            return {
                'is_anomaly': False,
                'confidence': 0.0,
                'message': f'æ£€æµ‹å¤±è´¥: {e}'
            }
        
        # æ›´æ–°ç»Ÿè®¡
        if is_anomaly:
            self.anomaly_stats['total_detections'] += 1
        
        # æŒç»­å­¦ä¹ ï¼šå°†æ–°æ•°æ®ç‚¹åŠ å…¥è®­ç»ƒé›†
        if metric_name in self.training_data:
            self.training_data[metric_name].append(value)
            
            # å®šæœŸé‡æ–°è®­ç»ƒ
            if len(self.training_data[metric_name]) % 100 == 0:
                self._retrain_model(metric_name)
        
        return {
            'is_anomaly': is_anomaly,
            'confidence': confidence,
            'value': value,
            'timestamp': datetime.now(),
            'model_type': type(model).__name__
        }
    
    def _retrain_model(self, metric_name: str):
        """é‡æ–°è®­ç»ƒæ¨¡å‹"""
        try:
            data = list(self.training_data[metric_name])
            self.train_detector(metric_name, data)
            logging.info(f"æ¨¡å‹é‡æ–°è®­ç»ƒå®Œæˆ: {metric_name}")
        except Exception as e:
            logging.error(f"æ¨¡å‹é‡æ–°è®­ç»ƒå¤±è´¥: {e}")
    
    def batch_detect_anomalies(self, 
                              metric_name: str, 
                              values: List[float]) -> List[Dict]:
        """æ‰¹é‡å¼‚å¸¸æ£€æµ‹"""
        
        results = []
        for value in values:
            result = self.detect_anomaly(metric_name, value)
            results.append(result)
        
        return results
    
    def get_anomaly_statistics(self) -> Dict:
        """è·å–å¼‚å¸¸æ£€æµ‹ç»Ÿè®¡"""
        
        total = self.anomaly_stats['total_detections']
        if total == 0:
            return {
                'total_detections': 0,
                'precision': 0.0,
                'recall': 0.0,
                'f1_score': 0.0
            }
        
        tp = self.anomaly_stats['true_positives']
        fp = self.anomaly_stats['false_positives']
        fn = self.anomaly_stats['false_negatives']
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
        
        return {
            'total_detections': total,
            'true_positives': tp,
            'false_positives': fp,
            'false_negatives': fn,
            'precision': precision,
            'recall': recall,
            'f1_score': f1
        }

class AlertManager:
    """å‘Šè­¦ç®¡ç†å™¨"""
    
    def __init__(self):
        self.alert_rules: Dict[str, AlertRule] = {}
        self.active_alerts: Dict[str, Alert] = {}
        self.alert_history: List[Alert] = []
        self.notification_channels: List[Callable] = []
        self.lock = threading.Lock()
        
        # å‘Šè­¦ç»Ÿè®¡
        self.alert_stats = {
            'total_alerts': 0,
            'resolved_alerts': 0,
            'average_resolution_time': 0.0
        }
    
    def add_alert_rule(self, rule: AlertRule):
        """æ·»åŠ å‘Šè­¦è§„åˆ™"""
        with self.lock:
            self.alert_rules[rule.name] = rule
        logging.info(f"å‘Šè­¦è§„åˆ™å·²æ·»åŠ : {rule.name}")
    
    def remove_alert_rule(self, rule_name: str):
        """ç§»é™¤å‘Šè­¦è§„åˆ™"""
        with self.lock:
            if rule_name in self.alert_rules:
                del self.alert_rules[rule_name]
                logging.info(f"å‘Šè­¦è§„åˆ™å·²ç§»é™¤: {rule_name}")
    
    def evaluate_rules(self, metric_name: str, value: float):
        """è¯„ä¼°å‘Šè­¦è§„åˆ™"""
        
        current_time = datetime.now()
        
        with self.lock:
            rules_to_evaluate = [
                rule for rule in self.alert_rules.values() 
                if rule.metric_name == metric_name
            ]
        
        for rule in rules_to_evaluate:
            try:
                # è¯„ä¼°æ¡ä»¶
                condition_met = self._evaluate_condition(rule.condition, value)
                alert_key = f"{rule.name}:{metric_name}"
                
                if condition_met:
                    if alert_key not in self.active_alerts:
                        # åˆ›å»ºæ–°å‘Šè­¦
                        alert = Alert(
                            rule_name=rule.name,
                            metric_name=metric_name,
                            value=value,
                            threshold=self._extract_threshold(rule.condition),
                            severity=rule.severity,
                            status=AlertStatus.FIRING,
                            start_time=current_time,
                            annotations={
                                'description': rule.description,
                                'runbook_url': rule.runbook_url
                            }
                        )
                        
                        with self.lock:
                            self.active_alerts[alert_key] = alert
                            self.alert_history.append(alert)
                            self.alert_stats['total_alerts'] += 1
                        
                        # å‘é€é€šçŸ¥
                        self._send_notification(alert)
                        
                        logging.warning(f"æ–°å‘Šè­¦: {rule.name} - {metric_name}={value}")
                    else:
                        # æ›´æ–°ç°æœ‰å‘Šè­¦
                        with self.lock:
                            self.active_alerts[alert_key].value = value
                
                else:
                    if alert_key in self.active_alerts:
                        # è§£å†³å‘Šè­¦
                        with self.lock:
                            alert = self.active_alerts[alert_key]
                            alert.status = AlertStatus.RESOLVED
                            alert.end_time = current_time
                            
                            # è®¡ç®—è§£å†³æ—¶é—´
                            resolution_time = (current_time - alert.start_time).total_seconds()
                            self._update_resolution_stats(resolution_time)
                            
                            del self.active_alerts[alert_key]
                        
                        # å‘é€è§£å†³é€šçŸ¥
                        self._send_resolution_notification(alert)
                        
                        logging.info(f"å‘Šè­¦å·²è§£å†³: {rule.name} - {metric_name}={value}")
                        
            except Exception as e:
                logging.error(f"å‘Šè­¦è§„åˆ™è¯„ä¼°å¤±è´¥: {rule.name}, é”™è¯¯: {e}")
    
    def _evaluate_condition(self, condition: str, value: float) -> bool:
        """è¯„ä¼°å‘Šè­¦æ¡ä»¶"""
        
        # ç®€å•çš„æ¡ä»¶è§£æå’Œè¯„ä¼°
        # æ”¯æŒçš„æ“ä½œç¬¦: >, <, >=, <=, ==, !=
        try:
            # æ›¿æ¢conditionä¸­çš„'value'ä¸ºå®é™…å€¼
            safe_condition = condition.replace('value', str(value))
            
            # å®‰å…¨è¯„ä¼°ï¼ˆä»…å…è®¸æ•°å­—å’ŒåŸºæœ¬æ“ä½œç¬¦ï¼‰
            allowed_chars = set('0123456789.><!=+-*/()')
            if not all(c in allowed_chars or c.isspace() for c in safe_condition):
                return False
            
            return eval(safe_condition)
        except:
            return False
    
    def _extract_threshold(self, condition: str) -> float:
        """ä»æ¡ä»¶ä¸­æå–é˜ˆå€¼"""
        import re
        # ç®€å•æå–æ•°å­—ä½œä¸ºé˜ˆå€¼
        numbers = re.findall(r'[-+]?\d*\.?\d+', condition)
        return float(numbers[0]) if numbers else 0.0
    
    def _send_notification(self, alert: Alert):
        """å‘é€å‘Šè­¦é€šçŸ¥"""
        for channel in self.notification_channels:
            try:
                channel(alert)
            except Exception as e:
                logging.error(f"é€šçŸ¥å‘é€å¤±è´¥: {e}")
    
    def _send_resolution_notification(self, alert: Alert):
        """å‘é€è§£å†³é€šçŸ¥"""
        for channel in self.notification_channels:
            try:
                if hasattr(channel, 'send_resolution'):
                    channel.send_resolution(alert)
                else:
                    # æ ‡è®°ä¸ºè§£å†³çŠ¶æ€åå‘é€
                    channel(alert)
            except Exception as e:
                logging.error(f"è§£å†³é€šçŸ¥å‘é€å¤±è´¥: {e}")
    
    def _update_resolution_stats(self, resolution_time: float):
        """æ›´æ–°è§£å†³æ—¶é—´ç»Ÿè®¡"""
        with self.lock:
            self.alert_stats['resolved_alerts'] += 1
            total_resolved = self.alert_stats['resolved_alerts']
            current_avg = self.alert_stats['average_resolution_time']
            
            # è®¡ç®—æ–°çš„å¹³å‡è§£å†³æ—¶é—´
            new_avg = ((current_avg * (total_resolved - 1)) + resolution_time) / total_resolved
            self.alert_stats['average_resolution_time'] = new_avg
    
    def add_notification_channel(self, channel: Callable):
        """æ·»åŠ é€šçŸ¥æ¸ é“"""
        self.notification_channels.append(channel)
    
    def get_active_alerts(self) -> List[Alert]:
        """è·å–æ´»è·ƒå‘Šè­¦"""
        with self.lock:
            return list(self.active_alerts.values())
    
    def get_alert_history(self, 
                         start_time: Optional[datetime] = None,
                         end_time: Optional[datetime] = None,
                         severity: Optional[AlertSeverity] = None) -> List[Alert]:
        """è·å–å‘Šè­¦å†å²"""
        
        with self.lock:
            alerts = list(self.alert_history)
        
        # è¿‡æ»¤æ¡ä»¶
        filtered_alerts = []
        for alert in alerts:
            if start_time and alert.start_time < start_time:
                continue
            if end_time and alert.start_time > end_time:
                continue
            if severity and alert.severity != severity:
                continue
            filtered_alerts.append(alert)
        
        return filtered_alerts
    
    def get_alert_statistics(self) -> Dict:
        """è·å–å‘Šè­¦ç»Ÿè®¡"""
        with self.lock:
            return dict(self.alert_stats)

class ModelDriftDetector:
    """æ¨¡å‹æ¼‚ç§»æ£€æµ‹å™¨"""
    
    def __init__(self, reference_window: int = 1000, detection_window: int = 100):
        self.reference_window = reference_window
        self.detection_window = detection_window
        self.reference_data: Dict[str, deque] = {}
        self.current_data: Dict[str, deque] = {}
        self.drift_history: List[Dict] = []
        self.lock = threading.Lock()
    
    def set_reference_data(self, feature_name: str, data: List[float]):
        """è®¾ç½®å‚è€ƒæ•°æ®"""
        with self.lock:
            self.reference_data[feature_name] = deque(data, maxlen=self.reference_window)
    
    def add_current_data(self, feature_name: str, value: float):
        """æ·»åŠ å½“å‰æ•°æ®"""
        with self.lock:
            if feature_name not in self.current_data:
                self.current_data[feature_name] = deque(maxlen=self.detection_window)
            self.current_data[feature_name].append(value)
    
    def detect_drift(self, feature_name: str) -> Dict:
        """æ£€æµ‹æ•°æ®æ¼‚ç§»"""
        
        if feature_name not in self.reference_data:
            return {'error': 'ç¼ºå°‘å‚è€ƒæ•°æ®'}
        
        if feature_name not in self.current_data:
            return {'error': 'ç¼ºå°‘å½“å‰æ•°æ®'}
        
        with self.lock:
            reference = list(self.reference_data[feature_name])
            current = list(self.current_data[feature_name])
        
        if len(current) < self.detection_window:
            return {'error': 'å½“å‰æ•°æ®ä¸è¶³'}
        
        # Kolmogorov-Smirnovæ£€éªŒ
        ks_statistic, ks_p_value = stats.ks_2samp(reference, current)
        
        # Jensen-Shannonæ•£åº¦
        js_divergence = self._jensen_shannon_divergence(reference, current)
        
        # Population Stability Index (PSI)
        psi = self._population_stability_index(reference, current)
        
        # ç»¼åˆæ¼‚ç§»å¾—åˆ†
        drift_score = (ks_statistic + js_divergence + psi) / 3.0
        
        # åˆ¤æ–­æ˜¯å¦å‘ç”Ÿæ¼‚ç§»
        is_drift = (ks_p_value < 0.05) or (psi > 0.2) or (js_divergence > 0.1)
        
        drift_result = {
            'feature_name': feature_name,
            'is_drift': is_drift,
            'drift_score': drift_score,
            'ks_statistic': ks_statistic,
            'ks_p_value': ks_p_value,
            'js_divergence': js_divergence,
            'psi': psi,
            'timestamp': datetime.now(),
            'reference_stats': {
                'mean': np.mean(reference),
                'std': np.std(reference),
                'count': len(reference)
            },
            'current_stats': {
                'mean': np.mean(current),
                'std': np.std(current),
                'count': len(current)
            }
        }
        
        # è®°å½•æ¼‚ç§»å†å²
        self.drift_history.append(drift_result)
        
        return drift_result
    
    def _jensen_shannon_divergence(self, X: List[float], Y: List[float], bins: int = 10) -> float:
        """è®¡ç®—Jensen-Shannonæ•£åº¦"""
        
        # åˆ›å»ºç›´æ–¹å›¾
        x_min, x_max = min(min(X), min(Y)), max(max(X), max(Y))
        x_bins = np.linspace(x_min, x_max, bins + 1)
        
        p, _ = np.histogram(X, bins=x_bins, density=True)
        q, _ = np.histogram(Y, bins=x_bins, density=True)
        
        # å½’ä¸€åŒ–
        p = p / np.sum(p) + 1e-10  # é¿å…é›¶å€¼
        q = q / np.sum(q) + 1e-10
        
        # è®¡ç®—JSæ•£åº¦
        m = (p + q) / 2
        js_div = 0.5 * stats.entropy(p, m) + 0.5 * stats.entropy(q, m)
        
        return js_div
    
    def _population_stability_index(self, reference: List[float], current: List[float], bins: int = 10) -> float:
        """è®¡ç®—Population Stability Index"""
        
        # åŸºäºå‚è€ƒæ•°æ®åˆ›å»ºåˆ†ç®±
        _, bin_edges = np.histogram(reference, bins=bins)
        
        # è®¡ç®—å„åˆ†ç®±çš„æ¯”ä¾‹
        ref_counts, _ = np.histogram(reference, bins=bin_edges)
        cur_counts, _ = np.histogram(current, bins=bin_edges)
        
        ref_percents = ref_counts / len(reference) + 1e-10
        cur_percents = cur_counts / len(current) + 1e-10
        
        # è®¡ç®—PSI
        psi = np.sum((cur_percents - ref_percents) * np.log(cur_percents / ref_percents))
        
        return psi
    
    def get_drift_history(self, feature_name: Optional[str] = None) -> List[Dict]:
        """è·å–æ¼‚ç§»å†å²"""
        
        if feature_name is None:
            return self.drift_history
        
        return [drift for drift in self.drift_history if drift['feature_name'] == feature_name]

class AutomatedResponseSystem:
    """è‡ªåŠ¨åŒ–å“åº”ç³»ç»Ÿ"""
    
    def __init__(self):
        self.response_rules: Dict[str, Callable] = {}
        self.response_history: List[Dict] = []
        self.lock = threading.Lock()
        
        # åˆå§‹åŒ–åŸºç¡€å“åº”ç­–ç•¥
        self._initialize_basic_responses()
    
    def _initialize_basic_responses(self):
        """åˆå§‹åŒ–åŸºç¡€å“åº”ç­–ç•¥"""
        
        # CPUä½¿ç”¨ç‡è¿‡é«˜çš„å“åº”
        self.register_response("high_cpu_usage", self._handle_high_cpu)
        
        # å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜çš„å“åº”
        self.register_response("high_memory_usage", self._handle_high_memory)
        
        # æœåŠ¡ä¸å¯ç”¨çš„å“åº”
        self.register_response("service_unavailable", self._handle_service_unavailable)
        
        # æ¨¡å‹æ¼‚ç§»çš„å“åº”
        self.register_response("model_drift", self._handle_model_drift)
    
    def register_response(self, trigger_name: str, response_func: Callable):
        """æ³¨å†Œå“åº”ç­–ç•¥"""
        with self.lock:
            self.response_rules[trigger_name] = response_func
        logging.info(f"å“åº”ç­–ç•¥å·²æ³¨å†Œ: {trigger_name}")
    
    def trigger_response(self, trigger_name: str, context: Dict) -> Dict:
        """è§¦å‘è‡ªåŠ¨åŒ–å“åº”"""
        
        if trigger_name not in self.response_rules:
            return {'error': f'æœªçŸ¥è§¦å‘å™¨: {trigger_name}'}
        
        try:
            response_func = self.response_rules[trigger_name]
            result = response_func(context)
            
            # è®°å½•å“åº”å†å²
            response_record = {
                'trigger_name': trigger_name,
                'context': context,
                'result': result,
                'timestamp': datetime.now()
            }
            
            with self.lock:
                self.response_history.append(response_record)
            
            logging.info(f"è‡ªåŠ¨åŒ–å“åº”æ‰§è¡Œ: {trigger_name}")
            return result
            
        except Exception as e:
            error_result = {'error': f'å“åº”æ‰§è¡Œå¤±è´¥: {e}'}
            logging.error(f"è‡ªåŠ¨åŒ–å“åº”å¤±è´¥: {trigger_name}, é”™è¯¯: {e}")
            return error_result
    
    def _handle_high_cpu(self, context: Dict) -> Dict:
        """å¤„ç†CPUä½¿ç”¨ç‡è¿‡é«˜"""
        
        cpu_usage = context.get('cpu_usage', 0)
        
        actions_taken = []
        
        # 1. è®°å½•è¯¦ç»†çš„ç³»ç»ŸçŠ¶æ€
        process_info = self._get_top_processes()
        actions_taken.append(f"è®°å½•äº†{len(process_info)}ä¸ªé«˜CPUè¿›ç¨‹")
        
        # 2. å¦‚æœCPUä½¿ç”¨ç‡æé«˜ï¼Œè€ƒè™‘é‡å¯é«˜æ¶ˆè€—è¿›ç¨‹
        if cpu_usage > 95:
            # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„è¿›ç¨‹ç®¡ç†é€»è¾‘
            actions_taken.append("CPUä½¿ç”¨ç‡>95%ï¼Œå·²æ ‡è®°ä¸ºéœ€è¦äººå·¥å¹²é¢„")
        
        # 3. å‘é€è¯¦ç»†å‘Šè­¦
        actions_taken.append("å·²å‘é€è¯¦ç»†CPUå‘Šè­¦")
        
        return {
            'success': True,
            'actions_taken': actions_taken,
            'process_info': process_info,
            'recommendation': 'å»ºè®®æ£€æŸ¥é«˜CPUè¿›ç¨‹å¹¶è€ƒè™‘ä¼˜åŒ–æˆ–é‡å¯'
        }
    
    def _handle_high_memory(self, context: Dict) -> Dict:
        """å¤„ç†å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"""
        
        memory_usage = context.get('memory_usage', 0)
        
        actions_taken = []
        
        # 1. å¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        gc.collect()
        actions_taken.append("æ‰§è¡Œäº†åƒåœ¾å›æ”¶")
        
        # 2. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        temp_cleaned = self._cleanup_temp_files()
        actions_taken.append(f"æ¸…ç†äº†{temp_cleaned}MBä¸´æ—¶æ–‡ä»¶")
        
        # 3. å¦‚æœå†…å­˜æé«˜ï¼Œå»ºè®®é‡å¯æœåŠ¡
        if memory_usage > 90:
            actions_taken.append("å†…å­˜ä½¿ç”¨ç‡>90%ï¼Œå»ºè®®é‡å¯æœåŠ¡")
        
        return {
            'success': True,
            'actions_taken': actions_taken,
            'temp_cleaned_mb': temp_cleaned,
            'recommendation': 'å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®æ£€æŸ¥å†…å­˜æ³„æ¼'
        }
    
    def _handle_service_unavailable(self, context: Dict) -> Dict:
        """å¤„ç†æœåŠ¡ä¸å¯ç”¨"""
        
        service_name = context.get('service_name', 'unknown')
        
        actions_taken = []
        
        # 1. å°è¯•å¥åº·æ£€æŸ¥
        health_status = self._perform_health_check(service_name)
        actions_taken.append(f"æ‰§è¡Œå¥åº·æ£€æŸ¥: {health_status}")
        
        # 2. å°è¯•é‡å¯æœåŠ¡
        restart_result = self._restart_service(service_name)
        actions_taken.append(f"å°è¯•é‡å¯æœåŠ¡: {restart_result}")
        
        # 3. æ£€æŸ¥ä¾èµ–æœåŠ¡
        dependency_status = self._check_dependencies(service_name)
        actions_taken.append(f"æ£€æŸ¥ä¾èµ–æœåŠ¡: {dependency_status}")
        
        return {
            'success': True,
            'actions_taken': actions_taken,
            'health_status': health_status,
            'restart_result': restart_result,
            'dependency_status': dependency_status
        }
    
    def _handle_model_drift(self, context: Dict) -> Dict:
        """å¤„ç†æ¨¡å‹æ¼‚ç§»"""
        
        feature_name = context.get('feature_name', 'unknown')
        drift_score = context.get('drift_score', 0)
        
        actions_taken = []
        
        # 1. è®°å½•æ¼‚ç§»è¯¦æƒ…
        actions_taken.append(f"è®°å½•äº†ç‰¹å¾{feature_name}çš„æ¼‚ç§»ï¼Œå¾—åˆ†: {drift_score}")
        
        # 2. å¦‚æœæ¼‚ç§»ä¸¥é‡ï¼Œè§¦å‘æ¨¡å‹é‡è®­ç»ƒæµç¨‹
        if drift_score > 0.5:
            retrain_triggered = self._trigger_model_retrain(feature_name)
            actions_taken.append(f"è§¦å‘æ¨¡å‹é‡è®­ç»ƒ: {retrain_triggered}")
        
        # 3. æ›´æ–°ç›‘æ§é˜ˆå€¼
        threshold_updated = self._update_drift_thresholds(feature_name, drift_score)
        actions_taken.append(f"æ›´æ–°æ¼‚ç§»é˜ˆå€¼: {threshold_updated}")
        
        return {
            'success': True,
            'actions_taken': actions_taken,
            'retrain_needed': drift_score > 0.5,
            'recommendation': 'æ£€æŸ¥æ•°æ®åˆ†å¸ƒå˜åŒ–å¹¶è€ƒè™‘æ¨¡å‹æ›´æ–°'
        }
    
    def _get_top_processes(self, top_n: int = 10) -> List[Dict]:
        """è·å–CPUä½¿ç”¨ç‡æœ€é«˜çš„è¿›ç¨‹"""
        processes = []
        try:
            for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_percent']):
                processes.append(proc.info)
            
            # æŒ‰CPUä½¿ç”¨ç‡é™åºæ’åº
            top_processes = sorted(processes, key=lambda x: x['cpu_percent'] or 0, reverse=True)[:top_n]
            return top_processes
        except Exception as e:
            logging.error(f"è·å–è¿›ç¨‹ä¿¡æ¯å¤±è´¥: {e}")
            return []
    
    def _cleanup_temp_files(self) -> float:
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        # æ¨¡æ‹Ÿæ¸…ç†ï¼Œå®é™…åº”è¯¥å®ç°çœŸæ­£çš„æ¸…ç†é€»è¾‘
        import random
        cleaned_mb = random.uniform(10, 100)
        return round(cleaned_mb, 2)
    
    def _perform_health_check(self, service_name: str) -> str:
        """æ‰§è¡Œå¥åº·æ£€æŸ¥"""
        # æ¨¡æ‹Ÿå¥åº·æ£€æŸ¥
        return "å¥åº·æ£€æŸ¥å®Œæˆ"
    
    def _restart_service(self, service_name: str) -> str:
        """é‡å¯æœåŠ¡"""
        # æ¨¡æ‹ŸæœåŠ¡é‡å¯
        return f"æœåŠ¡{service_name}é‡å¯å°è¯•å®Œæˆ"
    
    def _check_dependencies(self, service_name: str) -> str:
        """æ£€æŸ¥ä¾èµ–æœåŠ¡"""
        # æ¨¡æ‹Ÿä¾èµ–æ£€æŸ¥
        return "ä¾èµ–æœåŠ¡çŠ¶æ€æ­£å¸¸"
    
    def _trigger_model_retrain(self, feature_name: str) -> str:
        """è§¦å‘æ¨¡å‹é‡è®­ç»ƒ"""
        # æ¨¡æ‹Ÿé‡è®­ç»ƒè§¦å‘
        return f"ç‰¹å¾{feature_name}çš„é‡è®­ç»ƒä»»åŠ¡å·²åˆ›å»º"
    
    def _update_drift_thresholds(self, feature_name: str, drift_score: float) -> str:
        """æ›´æ–°æ¼‚ç§»é˜ˆå€¼"""
        # æ¨¡æ‹Ÿé˜ˆå€¼æ›´æ–°
        return f"ç‰¹å¾{feature_name}çš„æ¼‚ç§»é˜ˆå€¼å·²æ›´æ–°"
    
    def get_response_history(self, 
                           trigger_name: Optional[str] = None,
                           start_time: Optional[datetime] = None) -> List[Dict]:
        """è·å–å“åº”å†å²"""
        
        with self.lock:
            history = list(self.response_history)
        
        # è¿‡æ»¤æ¡ä»¶
        if trigger_name:
            history = [h for h in history if h['trigger_name'] == trigger_name]
        
        if start_time:
            history = [h for h in history if h['timestamp'] >= start_time]
        
        return history

class MonitoringSystem:
    """ç»¼åˆç›‘æ§ç³»ç»Ÿ"""
    
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or {}
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.metric_collector = MetricCollector()
        self.anomaly_detector = AnomalyDetector()
        self.alert_manager = AlertManager()
        self.drift_detector = ModelDriftDetector()
        self.response_system = AutomatedResponseSystem()
        
        # ç›‘æ§çº¿ç¨‹
        self.monitoring_thread = None
        self.is_running = False
        
        # è®¾ç½®åŸºç¡€å‘Šè­¦è§„åˆ™
        self._setup_basic_alert_rules()
        
        # è®¾ç½®é€šçŸ¥æ¸ é“
        self._setup_notification_channels()
        
        logging.info("ç›‘æ§ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def _setup_basic_alert_rules(self):
        """è®¾ç½®åŸºç¡€å‘Šè­¦è§„åˆ™"""
        
        basic_rules = [
            AlertRule(
                name="é«˜CPUä½¿ç”¨ç‡",
                metric_name="cpu_usage_percent",
                condition="value > 80",
                severity=AlertSeverity.WARNING,
                duration=300,
                description="CPUä½¿ç”¨ç‡è¶…è¿‡80%"
            ),
            AlertRule(
                name="æé«˜CPUä½¿ç”¨ç‡",
                metric_name="cpu_usage_percent",
                condition="value > 95",
                severity=AlertSeverity.CRITICAL,
                duration=60,
                description="CPUä½¿ç”¨ç‡è¶…è¿‡95%"
            ),
            AlertRule(
                name="é«˜å†…å­˜ä½¿ç”¨ç‡",
                metric_name="memory_usage_percent",
                condition="value > 85",
                severity=AlertSeverity.WARNING,
                duration=300,
                description="å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡85%"
            ),
            AlertRule(
                name="ç£ç›˜ç©ºé—´ä¸è¶³",
                metric_name="disk_usage_percent",
                condition="value > 90",
                severity=AlertSeverity.ERROR,
                duration=60,
                description="ç£ç›˜ä½¿ç”¨ç‡è¶…è¿‡90%"
            )
        ]
        
        for rule in basic_rules:
            self.alert_manager.add_alert_rule(rule)
    
    def _setup_notification_channels(self):
        """è®¾ç½®é€šçŸ¥æ¸ é“"""
        
        # æ·»åŠ æ—¥å¿—é€šçŸ¥æ¸ é“
        def log_notification(alert: Alert):
            logging.warning(f"å‘Šè­¦é€šçŸ¥: {alert.rule_name} - {alert.metric_name}={alert.value}")
        
        self.alert_manager.add_notification_channel(log_notification)
    
    def start_monitoring(self, interval: int = 30):
        """å¯åŠ¨ç›‘æ§"""
        
        if self.is_running:
            logging.warning("ç›‘æ§ç³»ç»Ÿå·²åœ¨è¿è¡Œ")
            return
        
        self.is_running = True
        
        def monitoring_loop():
            while self.is_running:
                try:
                    # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
                    self.metric_collector.collect_system_metrics()
                    
                    # è·å–æœ€æ–°æŒ‡æ ‡å¹¶è¿›è¡Œå¼‚å¸¸æ£€æµ‹å’Œå‘Šè­¦è¯„ä¼°
                    for metric_name in ['cpu_usage_percent', 'memory_usage_percent', 'disk_usage_percent']:
                        history = self.metric_collector.get_metric_history(metric_name)
                        if history:
                            latest_value = history[-1]['value']
                            
                            # å¼‚å¸¸æ£€æµ‹
                            if len(history) > 100:  # æœ‰è¶³å¤Ÿæ•°æ®æ—¶æ‰è¿›è¡Œå¼‚å¸¸æ£€æµ‹
                                values = [point['value'] for point in history[-100:]]
                                
                                # è®­ç»ƒå¼‚å¸¸æ£€æµ‹å™¨ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
                                if metric_name not in self.anomaly_detector.models:
                                    self.anomaly_detector.train_detector(metric_name, values)
                                
                                # æ£€æµ‹å¼‚å¸¸
                                anomaly_result = self.anomaly_detector.detect_anomaly(metric_name, latest_value)
                                if anomaly_result['is_anomaly']:
                                    logging.warning(f"æ£€æµ‹åˆ°å¼‚å¸¸: {metric_name}={latest_value}, ç½®ä¿¡åº¦={anomaly_result['confidence']}")
                            
                            # å‘Šè­¦è¯„ä¼°
                            self.alert_manager.evaluate_rules(metric_name, latest_value)
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å‘Šè­¦éœ€è¦è‡ªåŠ¨åŒ–å“åº”
                    active_alerts = self.alert_manager.get_active_alerts()
                    for alert in active_alerts:
                        self._handle_alert_response(alert)
                    
                    time.sleep(interval)
                    
                except Exception as e:
                    logging.error(f"ç›‘æ§å¾ªç¯é”™è¯¯: {e}")
                    time.sleep(interval)
        
        self.monitoring_thread = threading.Thread(target=monitoring_loop, daemon=True)
        self.monitoring_thread.start()
        
        logging.info(f"ç›‘æ§ç³»ç»Ÿå·²å¯åŠ¨ï¼Œç›‘æ§é—´éš”: {interval}ç§’")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        
        self.is_running = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=5)
        
        logging.info("ç›‘æ§ç³»ç»Ÿå·²åœæ­¢")
    
    def _handle_alert_response(self, alert: Alert):
        """å¤„ç†å‘Šè­¦å“åº”"""
        
        # æ ¹æ®å‘Šè­¦ç±»å‹è§¦å‘è‡ªåŠ¨åŒ–å“åº”
        if alert.metric_name == 'cpu_usage_percent' and alert.severity in [AlertSeverity.ERROR, AlertSeverity.CRITICAL]:
            context = {
                'cpu_usage': alert.value,
                'alert': alert
            }
            self.response_system.trigger_response('high_cpu_usage', context)
        
        elif alert.metric_name == 'memory_usage_percent' and alert.severity in [AlertSeverity.ERROR, AlertSeverity.CRITICAL]:
            context = {
                'memory_usage': alert.value,
                'alert': alert
            }
            self.response_system.trigger_response('high_memory_usage', context)
    
    def add_custom_metric(self, metric_def: MetricDefinition):
        """æ·»åŠ è‡ªå®šä¹‰æŒ‡æ ‡"""
        self.metric_collector.register_metric(metric_def)
    
    def record_metric(self, metric_name: str, value: float, labels: Optional[Dict] = None):
        """è®°å½•æŒ‡æ ‡å€¼"""
        self.metric_collector.record_metric(metric_name, value, labels)
    
    def add_alert_rule(self, rule: AlertRule):
        """æ·»åŠ å‘Šè­¦è§„åˆ™"""
        self.alert_manager.add_alert_rule(rule)
    
    def get_system_status(self) -> Dict:
        """è·å–ç³»ç»ŸçŠ¶æ€æ¦‚è§ˆ"""
        
        # è·å–æœ€æ–°ç³»ç»ŸæŒ‡æ ‡
        metrics_status = {}
        for metric_name in ['cpu_usage_percent', 'memory_usage_percent', 'disk_usage_percent']:
            history = self.metric_collector.get_metric_history(metric_name)
            if history:
                latest = history[-1]
                metrics_status[metric_name] = {
                    'value': latest['value'],
                    'timestamp': latest['timestamp']
                }
        
        # è·å–æ´»è·ƒå‘Šè­¦
        active_alerts = self.alert_manager.get_active_alerts()
        
        # è·å–å¼‚å¸¸æ£€æµ‹ç»Ÿè®¡
        anomaly_stats = self.anomaly_detector.get_anomaly_statistics()
        
        # è·å–å‘Šè­¦ç»Ÿè®¡
        alert_stats = self.alert_manager.get_alert_statistics()
        
        return {
            'monitoring_status': 'running' if self.is_running else 'stopped',
            'metrics': metrics_status,
            'active_alerts_count': len(active_alerts),
            'active_alerts': [
                {
                    'rule_name': alert.rule_name,
                    'severity': alert.severity.value,
                    'value': alert.value,
                    'start_time': alert.start_time
                } for alert in active_alerts
            ],
            'anomaly_detection': anomaly_stats,
            'alert_statistics': alert_stats,
            'timestamp': datetime.now()
        }
    
    def get_dashboard_data(self) -> Dict:
        """è·å–ä»ªè¡¨æ¿æ•°æ®"""
        
        dashboard_data = {}
        
        # è·å–å„ç§æŒ‡æ ‡çš„æ—¶é—´åºåˆ—æ•°æ®
        for metric_name in ['cpu_usage_percent', 'memory_usage_percent', 'disk_usage_percent']:
            history = self.metric_collector.get_metric_history(
                metric_name, 
                start_time=datetime.now() - timedelta(hours=24)
            )
            
            dashboard_data[metric_name] = {
                'timestamps': [point['timestamp'] for point in history],
                'values': [point['value'] for point in history],
                'latest_value': history[-1]['value'] if history else 0
            }
        
        # è·å–å‘Šè­¦è¶‹åŠ¿
        alert_history = self.alert_manager.get_alert_history(
            start_time=datetime.now() - timedelta(hours=24)
        )
        
        # æŒ‰å°æ—¶ç»Ÿè®¡å‘Šè­¦æ•°é‡
        hourly_alerts = defaultdict(int)
        for alert in alert_history:
            hour_key = alert.start_time.strftime('%Y-%m-%d %H:00')
            hourly_alerts[hour_key] += 1
        
        dashboard_data['alert_trend'] = {
            'hours': list(hourly_alerts.keys()),
            'counts': list(hourly_alerts.values())
        }
        
        return dashboard_data

## åº”ç”¨æŒ‡å¯¼

### ç›‘æ§æœ€ä½³å®è·µ

1. **åˆ†å±‚ç›‘æ§ç­–ç•¥**ï¼š
   - åŸºç¡€è®¾æ–½å±‚ï¼šCPUã€å†…å­˜ã€ç£ç›˜ã€ç½‘ç»œ
   - åº”ç”¨å±‚ï¼šè¯·æ±‚å»¶è¿Ÿã€é”™è¯¯ç‡ã€ååé‡
   - ä¸šåŠ¡å±‚ï¼šç”¨æˆ·ä½“éªŒã€ä¸šåŠ¡æŒ‡æ ‡

2. **å‘Šè­¦è§„åˆ™è®¾è®¡**ï¼š
   - åŸºäºå†å²æ•°æ®è®¾å®šåŠ¨æ€é˜ˆå€¼
   - å®æ–½å‘Šè­¦åˆ†çº§å’Œå‡çº§æœºåˆ¶
   - é¿å…å‘Šè­¦é£æš´å’Œç–²åŠ³

3. **å¼‚å¸¸æ£€æµ‹ä¼˜åŒ–**ï¼š
   - é€‰æ‹©åˆé€‚çš„æ£€æµ‹ç®—æ³•
   - å®šæœŸé‡è®­ç»ƒæ£€æµ‹æ¨¡å‹
   - ç»“åˆä¸šåŠ¡çŸ¥è¯†è°ƒæ•´æ£€æµ‹ç­–ç•¥

4. **è‡ªåŠ¨åŒ–å“åº”**ï¼š
   - å»ºç«‹å“åº”ç­–ç•¥åº“
   - å®æ–½æ¸è¿›å¼è‡ªåŠ¨åŒ–
   - ä¿æŒäººå·¥å¹²é¢„èƒ½åŠ›

## æ€§èƒ½ä¸æ‰©å±•æ€§

### ç›‘æ§ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–

- **æ•°æ®é‡‡æ ·**ï¼šå¯¹é«˜é¢‘æŒ‡æ ‡ä½¿ç”¨é‡‡æ ·ç­–ç•¥
- **å­˜å‚¨ä¼˜åŒ–**ï¼šä½¿ç”¨æ—¶é—´åºåˆ—æ•°æ®åº“
- **è®¡ç®—åˆ†å¸ƒ**ï¼šå°†å¤æ‚è®¡ç®—åˆ†å¸ƒåˆ°å¤šä¸ªèŠ‚ç‚¹
- **ç¼“å­˜ç­–ç•¥**ï¼šç¼“å­˜è®¡ç®—ç»“æœå’ŒæŸ¥è¯¢ç»“æœ

### å¯æ‰©å±•æ€§è®¾è®¡

1. **æ°´å¹³æ‰©å±•**ï¼š
   - ç›‘æ§ç»„ä»¶çš„åˆ†å¸ƒå¼éƒ¨ç½²
   - è´Ÿè½½å‡è¡¡å’Œæ•…éšœè½¬ç§»
   - æ•°æ®åˆ†ç‰‡å’Œå¤åˆ¶

2. **æ’ä»¶åŒ–æ¶æ„**ï¼š
   - å¯æ’æ‹”çš„ç›‘æ§æº
   - å¯æ‰©å±•çš„å‘Šè­¦æ¸ é“
   - è‡ªå®šä¹‰å“åº”ç­–ç•¥

## æ‰©å±•é˜…è¯»

- ã€ŠSite Reliability Engineeringã€‹- è°·æ­ŒSREå®è·µ
- ã€ŠMonitoring and Observabilityã€‹- ç›‘æ§ä¸å¯è§‚æµ‹æ€§
- ã€ŠThe Art of Monitoringã€‹- ç›‘æ§çš„è‰ºæœ¯
- ã€ŠChaos Engineeringã€‹- æ··æ²Œå·¥ç¨‹å®è·µ

---

*"ç›‘æ§ä¸æ˜¯ç›®çš„ï¼Œè€Œæ˜¯ä¿éšœæœåŠ¡å¯é æ€§çš„æ‰‹æ®µã€‚å¥½çš„ç›‘æ§ç³»ç»Ÿåº”è¯¥æ˜¯é€æ˜çš„ã€æ™ºèƒ½çš„ã€å¯æ“ä½œçš„ã€‚"* ğŸ“Š