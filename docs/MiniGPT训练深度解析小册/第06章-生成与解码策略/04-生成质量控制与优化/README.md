# 04 ç”Ÿæˆè´¨é‡æ§åˆ¶ä¸ä¼˜åŒ–

> **ä»æ¦‚ç‡é‡‡æ ·åˆ°è´¨é‡ä¿è¯ï¼šæ„å»ºå¯é çš„è¯­è¨€ç”Ÿæˆç³»ç»Ÿ**

## æ ¸å¿ƒæ€æƒ³

ç”Ÿæˆè´¨é‡æ§åˆ¶æ˜¯è¯­è¨€æ¨¡å‹ä»"èƒ½ç”Ÿæˆ"åˆ°"ç”Ÿæˆå¥½"çš„å…³é”®è·ƒå‡ã€‚å•çº¯çš„æ¦‚ç‡é‡‡æ ·å¾€å¾€äº§ç”Ÿå„ç§é—®é¢˜ï¼šé‡å¤å•°å—¦ã€ä¸»é¢˜åç§»ã€é•¿åº¦åå·®ã€è¯­ä¹‰ä¸è¿è´¯ç­‰ã€‚è´¨é‡æ§åˆ¶æŠ€æœ¯é€šè¿‡å¼•å…¥é¢å¤–çš„çº¦æŸå’Œä¼˜åŒ–æœºåˆ¶ï¼Œåœ¨ä¿æŒç”Ÿæˆå¤šæ ·æ€§çš„åŒæ—¶ç¡®ä¿è¾“å‡ºè´¨é‡ã€‚

**å…³é”®æ´å¯Ÿ**ï¼š
- **å¤šç»´è´¨é‡æ ‡å‡†**ï¼šæµç•…æ€§ã€è¿è´¯æ€§ã€ä¿¡æ¯é‡ã€ç›¸å…³æ€§çš„ç»¼åˆä¼˜åŒ–
- **å®æ—¶è´¨é‡ç›‘æ§**ï¼šç”Ÿæˆè¿‡ç¨‹ä¸­çš„åŠ¨æ€è´¨é‡è¯„ä¼°ä¸è°ƒæ•´
- **ç³»ç»Ÿæ€§ä¼˜åŒ–**ï¼šä»ç®—æ³•å±‚é¢åˆ°å·¥ç¨‹å±‚é¢çš„å…¨æ–¹ä½ä¼˜åŒ–
- **å¹³è¡¡è‰ºæœ¯**ï¼šè´¨é‡ä¸æ•ˆç‡ã€ç¡®å®šæ€§ä¸å¤šæ ·æ€§çš„ç²¾å¦™å¹³è¡¡

ä»æ•°å­¦è§’åº¦çœ‹ï¼Œè´¨é‡æ§åˆ¶æ˜¯åœ¨æ¦‚ç‡ç©ºé—´ä¸­å®šä¹‰é¢å¤–çš„çº¦æŸæ¡ä»¶ï¼Œå°†åŸå§‹çš„é‡‡æ ·è¿‡ç¨‹è½¬åŒ–ä¸ºçº¦æŸä¼˜åŒ–é—®é¢˜ã€‚

## 4.1 é‡å¤æ£€æµ‹ä¸æŠ‘åˆ¶çš„æ•°å­¦å»ºæ¨¡

### n-gramé‡å¤çš„ç»Ÿè®¡åˆ†æ

**é‡å¤åº¦é‡å®šä¹‰**ï¼š
è®¾ç”Ÿæˆåºåˆ—ä¸º $y = (y_1, y_2, ..., y_T)$ï¼Œå®šä¹‰ $n$-gram é‡å¤ç‡ï¼š
$$R_n(y) = \frac{\text{é‡å¤çš„n-gramæ•°é‡}}{\text{æ€»n-gramæ•°é‡}} = \frac{|N_n(y)| - |U_n(y)|}{|N_n(y)|}$$

å…¶ä¸­ $N_n(y)$ æ˜¯æ‰€æœ‰ $n$-gram çš„å¤šé‡é›†åˆï¼Œ$U_n(y)$ æ˜¯å»é‡åçš„é›†åˆã€‚

**é‡å¤æƒ©ç½šæœºåˆ¶**ï¼š
ä¿®æ”¹æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒä»¥é™ä½é‡å¤tokençš„æ¦‚ç‡ï¼š
$$P_{rep}(y_t|y_{<t}) = \frac{P(y_t|y_{<t})}{\alpha^{c(y_t, y_{<t})}}$$

å…¶ä¸­ $c(y_t, y_{<t})$ æ˜¯ $y_t$ åœ¨å†å²åºåˆ—ä¸­çš„å‡ºç°æ¬¡æ•°ï¼Œ$\alpha > 1$ æ˜¯é‡å¤æƒ©ç½šç³»æ•°ã€‚

**æ•°å­¦æ€§è´¨åˆ†æ**ï¼š
1. **ç†µå˜åŒ–**ï¼šé‡å¤æƒ©ç½šé™ä½åˆ†å¸ƒç†µ
   $$H(P_{rep}) < H(P)$$

2. **æ”¶æ•›æ€§**ï¼šéšç€åºåˆ—é•¿åº¦å¢åŠ ï¼Œé‡å¤æƒ©ç½šæ•ˆæœå¢å¼º
   $$\lim_{T \to \infty} P_{rep}(y_t|y_{<t}) \to 0 \text{ if } c(y_t, y_{<t}) > 0$$

3. **å¹³è¡¡ç‚¹**ï¼šå­˜åœ¨æœ€ä¼˜æƒ©ç½šç³»æ•° $\alpha^*$ ä½¿å¾—è´¨é‡æœ€å¤§åŒ–

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Union, Any
import matplotlib.pyplot as plt
import seaborn as sns
import math
import time
from collections import defaultdict, Counter, deque
from dataclasses import dataclass
import re
from scipy import stats
from sklearn.metrics import pairwise_distances
import networkx as nx
from nltk.translate.bleu_score import sentence_bleu
from rouge_score import rouge_scorer

@dataclass
class QualityMetrics:
    """ç”Ÿæˆè´¨é‡æŒ‡æ ‡"""
    repetition_rate: float        # é‡å¤ç‡
    diversity_score: float        # å¤šæ ·æ€§åˆ†æ•°  
    coherence_score: float        # è¿è´¯æ€§åˆ†æ•°
    fluency_score: float         # æµç•…æ€§åˆ†æ•°
    relevance_score: float       # ç›¸å…³æ€§åˆ†æ•°
    length_quality: float        # é•¿åº¦è´¨é‡
    overall_quality: float       # ç»¼åˆè´¨é‡
    
class RepetitionController:
    """é‡å¤æ§åˆ¶å™¨"""
    
    def __init__(self, 
                 repetition_penalty: float = 1.1,
                 no_repeat_ngram_size: int = 3,
                 length_penalty: float = 1.0,
                 diversity_penalty: float = 0.0):
        
        self.repetition_penalty = repetition_penalty
        self.no_repeat_ngram_size = no_repeat_ngram_size
        self.length_penalty = length_penalty
        self.diversity_penalty = diversity_penalty
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.ngram_counts = defaultdict(Counter)
        self.token_frequencies = Counter()
        self.repetition_history = []
        
    def compute_repetition_penalty(self, 
                                 logits: torch.Tensor, 
                                 input_ids: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—é‡å¤æƒ©ç½š"""
        
        batch_size, vocab_size = logits.shape
        penalized_logits = logits.clone()
        
        for batch_idx in range(batch_size):
            # ç»Ÿè®¡tokené¢‘æ¬¡
            token_counts = Counter(input_ids[batch_idx].tolist())
            
            # åº”ç”¨é‡å¤æƒ©ç½š
            for token_id, count in token_counts.items():
                if count > 0:
                    if penalized_logits[batch_idx, token_id] > 0:
                        penalized_logits[batch_idx, token_id] /= (self.repetition_penalty ** count)
                    else:
                        penalized_logits[batch_idx, token_id] *= (self.repetition_penalty ** count)
        
        return penalized_logits
    
    def apply_ngram_blocking(self, 
                           logits: torch.Tensor, 
                           input_ids: torch.Tensor) -> torch.Tensor:
        """åº”ç”¨n-gramé˜»æ­¢æœºåˆ¶"""
        
        if self.no_repeat_ngram_size <= 0:
            return logits
        
        batch_size, vocab_size = logits.shape
        seq_len = input_ids.size(1)
        
        blocked_logits = logits.clone()
        
        for batch_idx in range(batch_size):
            sequence = input_ids[batch_idx].tolist()
            
            # ç”Ÿæˆæ‰€æœ‰n-gram
            if seq_len >= self.no_repeat_ngram_size:
                ngrams = set()
                for i in range(seq_len - self.no_repeat_ngram_size + 1):
                    ngram = tuple(sequence[i:i + self.no_repeat_ngram_size])
                    ngrams.add(ngram)
                
                # æ£€æŸ¥å¯èƒ½çš„é‡å¤n-gram
                if seq_len >= self.no_repeat_ngram_size - 1:
                    prefix = tuple(sequence[-(self.no_repeat_ngram_size-1):])
                    
                    # é˜»æ­¢ä¼šå½¢æˆé‡å¤n-gramçš„token
                    for token_id in range(vocab_size):
                        candidate_ngram = prefix + (token_id,)
                        if candidate_ngram in ngrams:
                            blocked_logits[batch_idx, token_id] = -float('inf')
        
        return blocked_logits
    
    def analyze_repetition_patterns(self, sequences: List[List[int]]) -> Dict:
        """åˆ†æé‡å¤æ¨¡å¼"""
        
        print("=== é‡å¤æ¨¡å¼åˆ†æ ===")
        
        repetition_analysis = {
            'unigram_repetition': [],
            'bigram_repetition': [],
            'trigram_repetition': [],
            'longest_repeat': [],
            'repetition_distribution': defaultdict(int)
        }
        
        for sequence in sequences:
            seq_len = len(sequence)
            
            # åˆ†æä¸åŒé•¿åº¦çš„n-gramé‡å¤
            for n in range(1, 4):
                ngrams = []
                for i in range(seq_len - n + 1):
                    ngram = tuple(sequence[i:i+n])
                    ngrams.append(ngram)
                
                # è®¡ç®—é‡å¤ç‡
                total_ngrams = len(ngrams)
                unique_ngrams = len(set(ngrams))
                repetition_rate = (total_ngrams - unique_ngrams) / max(total_ngrams, 1)
                
                if n == 1:
                    repetition_analysis['unigram_repetition'].append(repetition_rate)
                elif n == 2:
                    repetition_analysis['bigram_repetition'].append(repetition_rate)
                elif n == 3:
                    repetition_analysis['trigram_repetition'].append(repetition_rate)
            
            # æ‰¾åˆ°æœ€é•¿é‡å¤åºåˆ—
            longest_repeat = self._find_longest_repeat(sequence)
            repetition_analysis['longest_repeat'].append(longest_repeat)
            repetition_analysis['repetition_distribution'][longest_repeat] += 1
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats_summary = {}
        for key in ['unigram_repetition', 'bigram_repetition', 'trigram_repetition', 'longest_repeat']:
            values = repetition_analysis[key]
            stats_summary[key] = {
                'mean': np.mean(values),
                'std': np.std(values),
                'min': np.min(values),
                'max': np.max(values),
                'median': np.median(values)
            }
        
        print(f"Unigramé‡å¤ç‡: {stats_summary['unigram_repetition']['mean']:.4f} Â± {stats_summary['unigram_repetition']['std']:.4f}")
        print(f"Bigramé‡å¤ç‡: {stats_summary['bigram_repetition']['mean']:.4f} Â± {stats_summary['bigram_repetition']['std']:.4f}")
        print(f"Trigramé‡å¤ç‡: {stats_summary['trigram_repetition']['mean']:.4f} Â± {stats_summary['trigram_repetition']['std']:.4f}")
        print(f"å¹³å‡æœ€é•¿é‡å¤é•¿åº¦: {stats_summary['longest_repeat']['mean']:.2f}")
        
        return {
            'detailed_analysis': repetition_analysis,
            'statistics': stats_summary
        }
    
    def _find_longest_repeat(self, sequence: List[int]) -> int:
        """æ‰¾åˆ°åºåˆ—ä¸­æœ€é•¿çš„é‡å¤å­åºåˆ—"""
        
        max_repeat_length = 0
        seq_len = len(sequence)
        
        for length in range(1, seq_len // 2 + 1):
            for start in range(seq_len - length + 1):
                pattern = sequence[start:start + length]
                
                # åœ¨å‰©ä½™åºåˆ—ä¸­æŸ¥æ‰¾é‡å¤
                for pos in range(start + length, seq_len - length + 1):
                    if sequence[pos:pos + length] == pattern:
                        max_repeat_length = max(max_repeat_length, length)
                        break
        
        return max_repeat_length

    def visualize_repetition_analysis(self, analysis_results: Dict):
        """å¯è§†åŒ–é‡å¤åˆ†æç»“æœ"""
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. ä¸åŒn-gramçš„é‡å¤ç‡åˆ†å¸ƒ
        ax1 = axes[0, 0]
        repetition_data = [
            analysis_results['detailed_analysis']['unigram_repetition'],
            analysis_results['detailed_analysis']['bigram_repetition'],
            analysis_results['detailed_analysis']['trigram_repetition']
        ]
        labels = ['Unigram', 'Bigram', 'Trigram']
        ax1.boxplot(repetition_data, labels=labels)
        ax1.set_title('ä¸åŒN-gramé‡å¤ç‡åˆ†å¸ƒ')
        ax1.set_ylabel('é‡å¤ç‡')
        ax1.grid(True, alpha=0.3)
        
        # 2. æœ€é•¿é‡å¤é•¿åº¦åˆ†å¸ƒ
        ax2 = axes[0, 1]
        longest_repeats = analysis_results['detailed_analysis']['longest_repeat']
        ax2.hist(longest_repeats, bins=20, alpha=0.7, edgecolor='black')
        ax2.set_title('æœ€é•¿é‡å¤åºåˆ—é•¿åº¦åˆ†å¸ƒ')
        ax2.set_xlabel('é‡å¤é•¿åº¦')
        ax2.set_ylabel('é¢‘æ¬¡')
        ax2.grid(True, alpha=0.3)
        
        # 3. é‡å¤é•¿åº¦ç»Ÿè®¡
        ax3 = axes[1, 0]
        repeat_dist = analysis_results['detailed_analysis']['repetition_distribution']
        lengths = list(repeat_dist.keys())
        counts = list(repeat_dist.values())
        ax3.bar(lengths, counts, alpha=0.7)
        ax3.set_title('é‡å¤é•¿åº¦é¢‘æ¬¡åˆ†å¸ƒ')
        ax3.set_xlabel('é‡å¤é•¿åº¦')
        ax3.set_ylabel('åºåˆ—æ•°é‡')
        ax3.grid(True, axis='y', alpha=0.3)
        
        # 4. é‡å¤ç‡ç»Ÿè®¡æ‘˜è¦
        ax4 = axes[1, 1]
        stats = analysis_results['statistics']
        metrics = ['unigram_repetition', 'bigram_repetition', 'trigram_repetition']
        means = [stats[metric]['mean'] for metric in metrics]
        stds = [stats[metric]['std'] for metric in metrics]
        
        x_pos = np.arange(len(metrics))
        ax4.bar(x_pos, means, yerr=stds, alpha=0.7, capsize=5)
        ax4.set_xticks(x_pos)
        ax4.set_xticklabels(['Unigram', 'Bigram', 'Trigram'])
        ax4.set_title('é‡å¤ç‡ç»Ÿè®¡æ‘˜è¦')
        ax4.set_ylabel('é‡å¤ç‡')
        ax4.grid(True, axis='y', alpha=0.3)
        
        plt.tight_layout()
        plt.show()

## 4.2 é•¿åº¦åå·®æ ¡æ­£çš„ç»Ÿè®¡å­¦ç†è®º

### é•¿åº¦åå·®çš„æ•°å­¦å»ºæ¨¡

**é•¿åº¦åå·®ç°è±¡**ï¼š
åœ¨åºåˆ—ç”Ÿæˆä¸­ï¼Œè¾ƒçŸ­åºåˆ—å¾€å¾€å…·æœ‰æ›´é«˜çš„ç´¯ç§¯æ¦‚ç‡ï¼š
$$P(y_{1:T}) = \prod_{t=1}^{T} P(y_t|y_{<t})$$

ç”±äºæ¯ä¸ªæ¡ä»¶æ¦‚ç‡éƒ½å°äº1ï¼Œåºåˆ—é•¿åº¦å¢åŠ å¯¼è‡´ç´¯ç§¯æ¦‚ç‡ä¸‹é™ã€‚

**é•¿åº¦æ ‡å‡†åŒ–æ–¹æ³•**ï¼š
1. **å¹³å‡å¯¹æ•°æ¦‚ç‡**ï¼š
   $$\text{Score}_{avg}(y) = \frac{1}{T} \sum_{t=1}^{T} \log P(y_t|y_{<t})$$

2. **é•¿åº¦æƒ©ç½š**ï¼š
   $$\text{Score}_{lp}(y) = \frac{\log P(y)}{T^\alpha}$$
   å…¶ä¸­ $\alpha$ æ˜¯é•¿åº¦æƒ©ç½šå‚æ•°ã€‚

3. **Wu-Brevityæƒ©ç½š**ï¼š
   $$\text{Score}_{wu}(y) = \log P(y) + \alpha \log T$$

**æœ€ä¼˜é•¿åº¦çš„ç†è®ºåˆ†æ**ï¼š
è®¾çœŸå®é•¿åº¦åˆ†å¸ƒä¸º $P(T)$ï¼Œç”Ÿæˆé•¿åº¦åˆ†å¸ƒä¸º $Q(T)$ï¼Œé•¿åº¦åå·®å¯ç”¨KLæ•£åº¦è¡¡é‡ï¼š
$$D_{KL}(P(T) \| Q(T)) = \sum_T P(T) \log \frac{P(T)}{Q(T)}$$

class LengthController:
    """é•¿åº¦æ§åˆ¶å™¨"""
    
    def __init__(self, 
                 length_penalty: float = 1.0,
                 min_length: int = 10,
                 max_length: int = 512,
                 target_length: Optional[int] = None):
        
        self.length_penalty = length_penalty
        self.min_length = min_length
        self.max_length = max_length
        self.target_length = target_length
        
        # é•¿åº¦ç»Ÿè®¡
        self.length_history = []
        self.length_distribution = Counter()
        
    def apply_length_penalty(self, 
                           scores: torch.Tensor, 
                           lengths: torch.Tensor) -> torch.Tensor:
        """åº”ç”¨é•¿åº¦æƒ©ç½š"""
        
        if self.length_penalty == 1.0:
            return scores
        
        # é¿å…é™¤é›¶
        length_penalty = torch.clamp(lengths.float(), min=1.0) ** self.length_penalty
        return scores / length_penalty
    
    def compute_length_bias(self, 
                          sequences: List[List[int]], 
                          scores: List[float]) -> Dict:
        """è®¡ç®—é•¿åº¦åå·®"""
        
        print("=== é•¿åº¦åå·®åˆ†æ ===")
        
        lengths = [len(seq) for seq in sequences]
        
        # è®¡ç®—é•¿åº¦ä¸åˆ†æ•°çš„ç›¸å…³æ€§
        correlation = np.corrcoef(lengths, scores)[0, 1]
        
        # åˆ†æä¸åŒé•¿åº¦åŒºé—´çš„åˆ†æ•°åˆ†å¸ƒ
        length_bins = {}
        for length, score in zip(lengths, scores):
            bin_key = (length // 50) * 50  # 50 tokenä¸€ä¸ªåŒºé—´
            if bin_key not in length_bins:
                length_bins[bin_key] = []
            length_bins[bin_key].append(score)
        
        # è®¡ç®—å„åŒºé—´ç»Ÿè®¡ä¿¡æ¯
        bin_stats = {}
        for bin_key, bin_scores in length_bins.items():
            bin_stats[bin_key] = {
                'mean_score': np.mean(bin_scores),
                'std_score': np.std(bin_scores),
                'count': len(bin_scores),
                'length_range': f"{bin_key}-{bin_key+49}"
            }
        
        print(f"é•¿åº¦ä¸åˆ†æ•°ç›¸å…³ç³»æ•°: {correlation:.4f}")
        print("å„é•¿åº¦åŒºé—´åˆ†æ•°ç»Ÿè®¡:")
        for bin_key in sorted(bin_stats.keys()):
            stats = bin_stats[bin_key]
            print(f"  {stats['length_range']}: å¹³å‡åˆ†æ•°={stats['mean_score']:.4f}, "
                  f"æ ‡å‡†å·®={stats['std_score']:.4f}, æ ·æœ¬æ•°={stats['count']}")
        
        return {
            'correlation': correlation,
            'length_distribution': Counter(lengths),
            'bin_statistics': bin_stats,
            'overall_stats': {
                'mean_length': np.mean(lengths),
                'std_length': np.std(lengths),
                'min_length': np.min(lengths),
                'max_length': np.max(lengths)
            }
        }
    
    def optimize_length_penalty(self, 
                              sequences: List[List[int]], 
                              scores: List[float],
                              target_correlation: float = 0.0) -> float:
        """ä¼˜åŒ–é•¿åº¦æƒ©ç½šå‚æ•°"""
        
        lengths = np.array([len(seq) for seq in sequences])
        scores = np.array(scores)
        
        def objective(alpha):
            # åº”ç”¨é•¿åº¦æƒ©ç½š
            adjusted_scores = scores / (lengths ** alpha)
            # è®¡ç®—è°ƒæ•´åçš„ç›¸å…³æ€§
            correlation = np.corrcoef(lengths, adjusted_scores)[0, 1]
            # æœ€å°åŒ–ä¸ç›®æ ‡ç›¸å…³æ€§çš„å·®è·
            return abs(correlation - target_correlation)
        
        # æœç´¢æœ€ä¼˜alpha
        from scipy.optimize import minimize_scalar
        result = minimize_scalar(objective, bounds=(0.1, 2.0), method='bounded')
        
        optimal_alpha = result.x
        print(f"æœ€ä¼˜é•¿åº¦æƒ©ç½šå‚æ•°: {optimal_alpha:.4f}")
        
        return optimal_alpha

## 4.3 ä¸»é¢˜è¿è´¯æ€§ç»´æŠ¤çš„è¯­ä¹‰å»ºæ¨¡

### è¯­ä¹‰è¿è´¯æ€§çš„æ•°å­¦é‡åŒ–

**è¯­ä¹‰å‘é‡ç©ºé—´æ¨¡å‹**ï¼š
è®¾æ–‡æœ¬ç‰‡æ®µçš„è¯­ä¹‰è¡¨ç¤ºä¸ºå‘é‡ $\mathbf{v}_i \in \mathbb{R}^d$ï¼Œè¿è´¯æ€§å¯é€šè¿‡å‘é‡ç›¸ä¼¼åº¦è¡¡é‡ï¼š
$$\text{Coherence}(S) = \frac{1}{|S|-1} \sum_{i=1}^{|S|-1} \cos(\mathbf{v}_i, \mathbf{v}_{i+1})$$

**ä¸»é¢˜æ¼‚ç§»æ£€æµ‹**ï¼š
ä½¿ç”¨æ»‘åŠ¨çª—å£æ£€æµ‹ä¸»é¢˜çªå˜ï¼š
$$\text{Drift}(t) = 1 - \cos(\mathbf{c}_{t-w:t}, \mathbf{c}_{t:t+w})$$
å…¶ä¸­ $\mathbf{c}_{i:j}$ æ˜¯çª—å£ $[i,j]$ çš„ä¸­å¿ƒåŒ–è¯­ä¹‰å‘é‡ã€‚

**è¯­ä¹‰çº¦æŸç”Ÿæˆ**ï¼š
åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ–½åŠ è¯­ä¹‰çº¦æŸï¼š
$$P_{sem}(y_t|y_{<t}) \propto P(y_t|y_{<t}) \cdot \exp(\lambda \cdot \text{Sim}(\mathbf{v}(y_t), \mathbf{c}_{<t}))$$

class CoherenceController:
    """è¿è´¯æ€§æ§åˆ¶å™¨"""
    
    def __init__(self, 
                 coherence_threshold: float = 0.5,
                 window_size: int = 20,
                 semantic_weight: float = 0.1):
        
        self.coherence_threshold = coherence_threshold
        self.window_size = window_size
        self.semantic_weight = semantic_weight
        
        # å‡è®¾çš„è¯­ä¹‰ç¼–ç å™¨ï¼ˆå®é™…åº”ç”¨ä¸­ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼‰
        self.semantic_dim = 768
        self.coherence_history = []
        
    def compute_semantic_similarity(self, 
                                  text1: str, 
                                  text2: str) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥ä½¿ç”¨BERT/RoBERTaç­‰æ¨¡å‹
        # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„å­—ç¬¦çº§ç›¸ä¼¼åº¦ä½œä¸ºä»£ç†
        
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union)
    
    def analyze_topic_coherence(self, 
                              texts: List[str]) -> Dict:
        """åˆ†æä¸»é¢˜è¿è´¯æ€§"""
        
        print("=== ä¸»é¢˜è¿è´¯æ€§åˆ†æ ===")
        
        coherence_scores = []
        drift_points = []
        
        for i, text in enumerate(texts):
            sentences = text.split('.')  # ç®€åŒ–çš„å¥å­åˆ†å‰²
            if len(sentences) < 2:
                continue
            
            # è®¡ç®—å¥å­é—´è¿è´¯æ€§
            sentence_coherence = []
            for j in range(len(sentences) - 1):
                similarity = self.compute_semantic_similarity(
                    sentences[j], sentences[j + 1])
                sentence_coherence.append(similarity)
            
            avg_coherence = np.mean(sentence_coherence) if sentence_coherence else 0
            coherence_scores.append(avg_coherence)
            
            # æ£€æµ‹ä¸»é¢˜æ¼‚ç§»ç‚¹
            drift_detected = any(sim < self.coherence_threshold 
                               for sim in sentence_coherence)
            if drift_detected:
                drift_points.append(i)
        
        # ç»Ÿè®¡åˆ†æ
        coherence_stats = {
            'mean_coherence': np.mean(coherence_scores),
            'std_coherence': np.std(coherence_scores),
            'min_coherence': np.min(coherence_scores),
            'max_coherence': np.max(coherence_scores),
            'drift_rate': len(drift_points) / len(texts)
        }
        
        print(f"å¹³å‡è¿è´¯æ€§åˆ†æ•°: {coherence_stats['mean_coherence']:.4f}")
        print(f"è¿è´¯æ€§æ ‡å‡†å·®: {coherence_stats['std_coherence']:.4f}")
        print(f"ä¸»é¢˜æ¼‚ç§»ç‡: {coherence_stats['drift_rate']:.4f}")
        
        return {
            'coherence_scores': coherence_scores,
            'drift_points': drift_points,
            'statistics': coherence_stats
        }
    
    def maintain_topic_consistency(self, 
                                 current_context: str,
                                 candidate_tokens: List[str],
                                 logits: torch.Tensor) -> torch.Tensor:
        """ç»´æŠ¤ä¸»é¢˜ä¸€è‡´æ€§"""
        
        if not current_context.strip():
            return logits
        
        # è®¡ç®—æ¯ä¸ªå€™é€‰tokenä¸ä¸Šä¸‹æ–‡çš„è¯­ä¹‰ç›¸ä¼¼åº¦
        context_words = set(current_context.split())
        
        adjusted_logits = logits.clone()
        
        for i, token in enumerate(candidate_tokens):
            # æ„é€ åŒ…å«å€™é€‰tokençš„æ–°æ–‡æœ¬
            extended_text = current_context + " " + token
            
            # è®¡ç®—è¯­ä¹‰ä¸€è‡´æ€§ï¼ˆç®€åŒ–å®ç°ï¼‰
            token_words = set(token.split())
            overlap = len(context_words.intersection(token_words))
            consistency_score = overlap / max(len(context_words), 1)
            
            # è°ƒæ•´logits
            adjustment = self.semantic_weight * consistency_score
            adjusted_logits[i] += adjustment
        
        return adjusted_logits

## 4.4 å®æ—¶ä¼˜åŒ–æŠ€æœ¯çš„ç®—æ³•åˆ†æ

### KVç¼“å­˜æœºåˆ¶çš„æ•°å­¦åˆ†æ

**æ³¨æ„åŠ›è®¡ç®—å¤æ‚åº¦**ï¼š
æ ‡å‡†è‡ªæ³¨æ„åŠ›çš„è®¡ç®—å¤æ‚åº¦ä¸º $O(T^2 d)$ï¼Œå…¶ä¸­ $T$ æ˜¯åºåˆ—é•¿åº¦ï¼Œ$d$ æ˜¯éšè—ç»´åº¦ã€‚

**KVç¼“å­˜åŸç†**ï¼š
åœ¨è‡ªå›å½’ç”Ÿæˆä¸­ï¼ŒKeyå’ŒValueçŸ©é˜µå¯ä»¥å¢é‡è®¡ç®—ï¼š
$$\mathbf{K}_{t+1} = [\mathbf{K}_t; \mathbf{k}_{t+1}], \quad \mathbf{V}_{t+1} = [\mathbf{V}_t; \mathbf{v}_{t+1}]$$

**å¤æ‚åº¦å‡å°‘**ï¼š
ä½¿ç”¨KVç¼“å­˜åï¼Œæ¯æ­¥çš„è®¡ç®—å¤æ‚åº¦é™è‡³ $O(Td)$ï¼Œæ€»å¤æ‚åº¦ä» $O(T^3 d)$ é™è‡³ $O(T^2 d)$ã€‚

**å¹¶è¡Œç”Ÿæˆç­–ç•¥**ï¼š
å¯¹äºæ‰¹é‡ç”Ÿæˆï¼Œå¯ä»¥ä½¿ç”¨åŠ¨æ€æ‰¹å¤„ç†ï¼š
$$\text{Batch}_t = \{(x_i, y_{i,<t}) : |y_{i,<t}| = t-1, i \in \text{Active}\}$$

class GenerationOptimizer:
    """ç”Ÿæˆä¼˜åŒ–å™¨"""
    
    def __init__(self, 
                 max_batch_size: int = 32,
                 kv_cache_size: int = 2048,
                 parallel_beams: int = 4):
        
        self.max_batch_size = max_batch_size
        self.kv_cache_size = kv_cache_size
        self.parallel_beams = parallel_beams
        
        # æ€§èƒ½ç»Ÿè®¡
        self.timing_stats = defaultdict(list)
        self.memory_stats = defaultdict(list)
        
    def implement_kv_cache(self, 
                          model,
                          input_ids: torch.Tensor,
                          past_key_values: Optional[Tuple] = None) -> Tuple:
        """å®ç°KVç¼“å­˜æœºåˆ¶"""
        
        start_time = time.time()
        
        batch_size, seq_len = input_ids.shape
        
        if past_key_values is None:
            # åˆæ¬¡è®¡ç®—ï¼Œéœ€è¦è®¡ç®—å®Œæ•´çš„KV
            with torch.no_grad():
                outputs = model(input_ids, use_cache=True)
                past_key_values = outputs.past_key_values
        else:
            # å¢é‡è®¡ç®—ï¼Œåªè®¡ç®—æ–°token
            new_token = input_ids[:, -1:]
            with torch.no_grad():
                outputs = model(new_token, past_key_values=past_key_values, use_cache=True)
                past_key_values = outputs.past_key_values
        
        cache_time = time.time() - start_time
        self.timing_stats['kv_cache'].append(cache_time)
        
        return outputs.logits, past_key_values
    
    def dynamic_batching(self, 
                        requests: List[Dict],
                        max_batch_size: int = None) -> List[List[Dict]]:
        """åŠ¨æ€æ‰¹å¤„ç†"""
        
        if max_batch_size is None:
            max_batch_size = self.max_batch_size
        
        # æŒ‰åºåˆ—é•¿åº¦åˆ†ç»„
        length_groups = defaultdict(list)
        for req in requests:
            length = len(req.get('input_ids', []))
            length_groups[length].append(req)
        
        batches = []
        for length, group_requests in length_groups.items():
            # å°†åŒé•¿åº¦è¯·æ±‚åˆ†æ‰¹
            for i in range(0, len(group_requests), max_batch_size):
                batch = group_requests[i:i + max_batch_size]
                batches.append(batch)
        
        return batches
    
    def parallel_beam_search(self, 
                           model,
                           input_ids: torch.Tensor,
                           num_beams: int = 4,
                           max_length: int = 100) -> List[List[int]]:
        """å¹¶è¡ŒæŸæœç´¢"""
        
        start_time = time.time()
        
        batch_size = input_ids.size(0)
        vocab_size = model.config.vocab_size if hasattr(model, 'config') else 10000
        
        # åˆå§‹åŒ–æŸ
        beam_scores = torch.zeros(batch_size, num_beams)
        beam_sequences = input_ids.unsqueeze(1).repeat(1, num_beams, 1)
        beam_lengths = torch.full((batch_size, num_beams), input_ids.size(1))
        
        # æŸæœç´¢ä¸»å¾ªç¯
        for step in range(max_length - input_ids.size(1)):
            # å¹¶è¡Œè®¡ç®—æ‰€æœ‰æŸçš„ä¸‹ä¸€ä¸ªtokenæ¦‚ç‡
            current_sequences = beam_sequences.view(-1, beam_sequences.size(-1))
            
            with torch.no_grad():
                logits = model(current_sequences).logits[:, -1, :]
            
            # é‡å¡‘ä¸ºæŸå½¢çŠ¶
            logits = logits.view(batch_size, num_beams, vocab_size)
            
            # è®¡ç®—æŸåˆ†æ•°
            log_probs = F.log_softmax(logits, dim=-1)
            scores = beam_scores.unsqueeze(-1) + log_probs
            
            # é€‰æ‹©top-kå€™é€‰
            scores_flat = scores.view(batch_size, -1)
            top_scores, top_indices = torch.topk(scores_flat, num_beams)
            
            # æ›´æ–°æŸ
            beam_indices = top_indices // vocab_size
            token_indices = top_indices % vocab_size
            
            # é‡æ–°ç»„ç»‡åºåˆ—
            new_sequences = []
            new_scores = []
            
            for batch_idx in range(batch_size):
                batch_sequences = []
                batch_scores = []
                
                for beam_idx in range(num_beams):
                    old_beam_idx = beam_indices[batch_idx, beam_idx]
                    new_token = token_indices[batch_idx, beam_idx]
                    
                    # å¤åˆ¶æ—§åºåˆ—å¹¶æ·»åŠ æ–°token
                    old_sequence = beam_sequences[batch_idx, old_beam_idx]
                    new_sequence = torch.cat([old_sequence, new_token.unsqueeze(0)])
                    
                    batch_sequences.append(new_sequence)
                    batch_scores.append(top_scores[batch_idx, beam_idx])
                
                new_sequences.append(torch.stack(batch_sequences))
                new_scores.append(torch.stack(batch_scores))
            
            beam_sequences = torch.stack(new_sequences)
            beam_scores = torch.stack(new_scores)
            
            # æ£€æŸ¥ç»“æŸæ¡ä»¶ï¼ˆç®€åŒ–å®ç°ï¼‰
            # åœ¨å®é™…åº”ç”¨ä¸­éœ€è¦å¤„ç†EOS token
        
        search_time = time.time() - start_time
        self.timing_stats['beam_search'].append(search_time)
        
        # è¿”å›æœ€ä½³åºåˆ—
        best_sequences = []
        for batch_idx in range(batch_size):
            best_beam_idx = torch.argmax(beam_scores[batch_idx])
            best_sequence = beam_sequences[batch_idx, best_beam_idx].tolist()
            best_sequences.append(best_sequence)
        
        return best_sequences
    
    def memory_efficient_generation(self, 
                                  model,
                                  input_ids: torch.Tensor,
                                  max_length: int = 100,
                                  chunk_size: int = 512) -> List[int]:
        """å†…å­˜é«˜æ•ˆç”Ÿæˆ"""
        
        sequence = input_ids[0].tolist()  # å‡è®¾batch_size=1
        past_key_values = None
        
        for step in range(max_length - len(sequence)):
            # é™åˆ¶è¾“å…¥é•¿åº¦ä»¥èŠ‚çœå†…å­˜
            if len(sequence) > chunk_size:
                # æˆªæ–­æ—©æœŸtokenï¼Œä¿ç•™recent context
                recent_sequence = sequence[-chunk_size:]
                current_input = torch.tensor([recent_sequence])
                past_key_values = None  # é‡ç½®ç¼“å­˜
            else:
                current_input = torch.tensor([sequence])
            
            # ç”Ÿæˆä¸‹ä¸€ä¸ªtoken
            with torch.no_grad():
                if past_key_values is not None:
                    outputs = model(current_input[:, -1:], 
                                  past_key_values=past_key_values, 
                                  use_cache=True)
                else:
                    outputs = model(current_input, use_cache=True)
                
                past_key_values = outputs.past_key_values
                next_token_logits = outputs.logits[0, -1, :]
            
            # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
            next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1), 1).item()
            sequence.append(next_token)
            
            # æ£€æŸ¥ç»“æŸæ¡ä»¶
            if next_token == 1:  # EOS token
                break
        
        return sequence

## 4.5 è´¨é‡è¯„ä¼°ä½“ç³»çš„æ•°å­¦æ¡†æ¶

### å¤šç»´è´¨é‡æŒ‡æ ‡

**ç»¼åˆè´¨é‡å‡½æ•°**ï¼š
$$Q(y) = \alpha_1 \cdot \text{Fluency}(y) + \alpha_2 \cdot \text{Coherence}(y) + \alpha_3 \cdot \text{Relevance}(y) + \alpha_4 \cdot \text{Diversity}(y)$$

å…¶ä¸­æƒé‡æ»¡è¶³ $\sum_i \alpha_i = 1$ã€‚

**è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡**ï¼š
1. **BLEUåˆ†æ•°**ï¼š
   $$\text{BLEU} = \text{BP} \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)$$

2. **ROUGEåˆ†æ•°**ï¼š
   $$\text{ROUGE-N} = \frac{\sum_{S \in \{Ref\}} \sum_{gram_n \in S} Count_{match}(gram_n)}{\sum_{S \in \{Ref\}} \sum_{gram_n \in S} Count(gram_n)}$$

3. **è¯­ä¹‰ç›¸ä¼¼åº¦**ï¼š
   $$\text{SemSim}(y, r) = \cos(\text{BERT}(y), \text{BERT}(r))$$

class QualityEvaluator:
    """è´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])
        self.quality_weights = {
            'fluency': 0.25,
            'coherence': 0.25,
            'relevance': 0.25,
            'diversity': 0.25
        }
    
    def evaluate_comprehensive_quality(self, 
                                     generated_texts: List[str],
                                     reference_texts: List[str] = None,
                                     contexts: List[str] = None) -> QualityMetrics:
        """ç»¼åˆè´¨é‡è¯„ä¼°"""
        
        print("=== ç»¼åˆè´¨é‡è¯„ä¼° ===")
        
        # è®¡ç®—å„ç»´åº¦åˆ†æ•°
        fluency_scores = [self._compute_fluency(text) for text in generated_texts]
        coherence_scores = [self._compute_coherence(text) for text in generated_texts]
        diversity_scores = self._compute_diversity(generated_texts)
        
        if reference_texts:
            relevance_scores = [self._compute_relevance(gen, ref) 
                              for gen, ref in zip(generated_texts, reference_texts)]
        else:
            relevance_scores = [0.5] * len(generated_texts)  # é»˜è®¤ä¸­ç­‰ç›¸å…³æ€§
        
        # è®¡ç®—ç»¼åˆåˆ†æ•°
        overall_scores = []
        for i in range(len(generated_texts)):
            score = (self.quality_weights['fluency'] * fluency_scores[i] +
                    self.quality_weights['coherence'] * coherence_scores[i] +
                    self.quality_weights['relevance'] * relevance_scores[i] +
                    self.quality_weights['diversity'] * diversity_scores[i])
            overall_scores.append(score)
        
        # ç»Ÿè®¡ç»“æœ
        metrics = QualityMetrics(
            repetition_rate=self._compute_repetition_rate(generated_texts),
            diversity_score=np.mean(diversity_scores),
            coherence_score=np.mean(coherence_scores),
            fluency_score=np.mean(fluency_scores),
            relevance_score=np.mean(relevance_scores),
            length_quality=self._compute_length_quality(generated_texts),
            overall_quality=np.mean(overall_scores)
        )
        
        print(f"æµç•…æ€§åˆ†æ•°: {metrics.fluency_score:.4f}")
        print(f"è¿è´¯æ€§åˆ†æ•°: {metrics.coherence_score:.4f}")
        print(f"ç›¸å…³æ€§åˆ†æ•°: {metrics.relevance_score:.4f}")
        print(f"å¤šæ ·æ€§åˆ†æ•°: {metrics.diversity_score:.4f}")
        print(f"é‡å¤ç‡: {metrics.repetition_rate:.4f}")
        print(f"ç»¼åˆè´¨é‡åˆ†æ•°: {metrics.overall_quality:.4f}")
        
        return metrics
    
    def _compute_fluency(self, text: str) -> float:
        """è®¡ç®—æµç•…æ€§åˆ†æ•°"""
        
        # ç®€åŒ–çš„æµç•…æ€§è¯„ä¼°ï¼šåŸºäºå¥å­å®Œæ•´æ€§å’Œè¯­æ³•æ­£ç¡®æ€§
        sentences = text.split('.')
        complete_sentences = [s for s in sentences if len(s.strip()) > 5]
        
        if not sentences:
            return 0.0
        
        completeness_score = len(complete_sentences) / len(sentences)
        
        # ç®€å•çš„è¯­æ³•æ£€æŸ¥ï¼ˆæ£€æŸ¥åŸºæœ¬çš„å¥å­ç»“æ„ï¼‰
        grammar_score = 0.8  # ç®€åŒ–å®ç°ï¼Œå®é™…åº”ä½¿ç”¨è¯­æ³•æ£€æŸ¥å·¥å…·
        
        return (completeness_score + grammar_score) / 2
    
    def _compute_coherence(self, text: str) -> float:
        """è®¡ç®—è¿è´¯æ€§åˆ†æ•°"""
        
        sentences = [s.strip() for s in text.split('.') if s.strip()]
        if len(sentences) < 2:
            return 1.0
        
        # è®¡ç®—ç›¸é‚»å¥å­çš„è¯æ±‡é‡å åº¦
        coherence_scores = []
        for i in range(len(sentences) - 1):
            words1 = set(sentences[i].split())
            words2 = set(sentences[i + 1].split())
            
            if not words1 or not words2:
                continue
            
            overlap = len(words1.intersection(words2))
            union = len(words1.union(words2))
            coherence = overlap / union if union > 0 else 0
            coherence_scores.append(coherence)
        
        return np.mean(coherence_scores) if coherence_scores else 0.0
    
    def _compute_relevance(self, generated: str, reference: str) -> float:
        """è®¡ç®—ç›¸å…³æ€§åˆ†æ•°"""
        
        # ä½¿ç”¨ROUGEä½œä¸ºç›¸å…³æ€§ä»£ç†æŒ‡æ ‡
        scores = self.rouge_scorer.score(reference, generated)
        return scores['rougeL'].fmeasure
    
    def _compute_diversity(self, texts: List[str]) -> List[float]:
        """è®¡ç®—å¤šæ ·æ€§åˆ†æ•°"""
        
        diversity_scores = []
        
        for i, text in enumerate(texts):
            # ä¸å…¶ä»–æ–‡æœ¬çš„ä¸ç›¸ä¼¼æ€§
            similarities = []
            for j, other_text in enumerate(texts):
                if i != j:
                    words1 = set(text.split())
                    words2 = set(other_text.split())
                    
                    if not words1 or not words2:
                        similarity = 0.0
                    else:
                        intersection = len(words1.intersection(words2))
                        union = len(words1.union(words2))
                        similarity = intersection / union
                    
                    similarities.append(similarity)
            
            # å¤šæ ·æ€§ = 1 - å¹³å‡ç›¸ä¼¼åº¦
            diversity = 1 - np.mean(similarities) if similarities else 1.0
            diversity_scores.append(diversity)
        
        return diversity_scores
    
    def _compute_repetition_rate(self, texts: List[str]) -> float:
        """è®¡ç®—é‡å¤ç‡"""
        
        all_repetition_rates = []
        
        for text in texts:
            words = text.split()
            if len(words) < 2:
                continue
            
            # è®¡ç®—bigramé‡å¤ç‡
            bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]
            unique_bigrams = set(bigrams)
            
            repetition_rate = 1 - len(unique_bigrams) / len(bigrams) if bigrams else 0
            all_repetition_rates.append(repetition_rate)
        
        return np.mean(all_repetition_rates) if all_repetition_rates else 0.0
    
    def _compute_length_quality(self, texts: List[str]) -> float:
        """è®¡ç®—é•¿åº¦è´¨é‡"""
        
        lengths = [len(text.split()) for text in texts]
        
        # åŸºäºé•¿åº¦åˆ†å¸ƒçš„è´¨é‡è¯„ä¼°
        mean_length = np.mean(lengths)
        std_length = np.std(lengths)
        
        # ç†æƒ³æƒ…å†µä¸‹ï¼Œé•¿åº¦åº”è¯¥é€‚ä¸­ä¸”æ–¹å·®ä¸å¤ªå¤§
        length_appropriateness = 1 / (1 + abs(mean_length - 50) / 50)  # 50è¯ä¸ºç†æƒ³é•¿åº¦
        length_consistency = 1 / (1 + std_length / mean_length) if mean_length > 0 else 0
        
        return (length_appropriateness + length_consistency) / 2

def create_quality_control_system():
    """åˆ›å»ºè´¨é‡æ§åˆ¶ç³»ç»Ÿ"""
    
    # åˆå§‹åŒ–å„ç»„ä»¶
    repetition_controller = RepetitionController(
        repetition_penalty=1.1,
        no_repeat_ngram_size=3
    )
    
    length_controller = LengthController(
        length_penalty=1.0,
        min_length=10,
        max_length=512
    )
    
    coherence_controller = CoherenceController(
        coherence_threshold=0.5,
        window_size=20
    )
    
    optimizer = GenerationOptimizer(
        max_batch_size=32,
        kv_cache_size=2048
    )
    
    evaluator = QualityEvaluator()
    
    return {
        'repetition_controller': repetition_controller,
        'length_controller': length_controller,
        'coherence_controller': coherence_controller,
        'optimizer': optimizer,
        'evaluator': evaluator
    }

# æ¼”ç¤ºå®Œæ•´çš„è´¨é‡æ§åˆ¶æµç¨‹
def demonstrate_quality_control():
    """æ¼”ç¤ºè´¨é‡æ§åˆ¶ç³»ç»Ÿ"""
    
    print("=== MiniGPTç”Ÿæˆè´¨é‡æ§åˆ¶ç³»ç»Ÿæ¼”ç¤º ===\n")
    
    # åˆ›å»ºç³»ç»Ÿ
    system = create_quality_control_system()
    
    # æ¨¡æ‹Ÿç”Ÿæˆæ–‡æœ¬æ•°æ®
    generated_texts = [
        "äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»ã€‚æœºå™¨å­¦ä¹ ç®—æ³•èƒ½å¤Ÿå¤„ç†å¤§é‡æ•°æ®ï¼Œå‘ç°éšè—çš„æ¨¡å¼ã€‚æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚",
        "æ·±åº¦å­¦ä¹ æ¨¡å‹éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ã€‚è®­ç»ƒæ•°æ®çš„è´¨é‡ç›´æ¥å½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚æ•°æ®é¢„å¤„ç†æ˜¯æœºå™¨å­¦ä¹ é¡¹ç›®ä¸­çš„é‡è¦æ­¥éª¤ã€‚æ¸…ç†å’Œæ ‡æ³¨æ•°æ®éœ€è¦å¤§é‡çš„äººåŠ›æŠ•å…¥ã€‚",
        "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ã€‚æ–‡æœ¬åˆ†æã€æƒ…æ„Ÿåˆ†æã€æœºå™¨ç¿»è¯‘éƒ½æ˜¯NLPçš„åº”ç”¨é¢†åŸŸã€‚Transformeræ¶æ„çš„å‡ºç°æ¨åŠ¨äº†NLPæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ã€‚",
        "æœºå™¨å­¦ä¹ æœºå™¨å­¦ä¹ æœºå™¨å­¦ä¹ ç®—æ³•ç®—æ³•ç®—æ³•èƒ½å¤Ÿèƒ½å¤Ÿå¤„ç†å¤„ç†å¤§é‡å¤§é‡æ•°æ®æ•°æ®ï¼Œå‘ç°å‘ç°æ¨¡å¼æ¨¡å¼ã€‚é‡å¤é‡å¤çš„å†…å®¹å†…å®¹å½±å“å½±å“ç”Ÿæˆç”Ÿæˆè´¨é‡è´¨é‡ã€‚"  # åŒ…å«é‡å¤çš„ä¾‹å­
    ]
    
    reference_texts = [
        "AIæŠ€æœ¯æ­£åœ¨é©æ–°å„ä¸ªè¡Œä¸šï¼Œæœºå™¨å­¦ä¹ å¸®åŠ©æˆ‘ä»¬ä»æ•°æ®ä¸­è·å–æ´å¯Ÿï¼Œæ·±åº¦å­¦ä¹ åœ¨å¤šä¸ªé¢†åŸŸéƒ½æœ‰çªç ´æ€§åº”ç”¨ã€‚",
        "é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®æ˜¯æœºå™¨å­¦ä¹ æˆåŠŸçš„å…³é”®ï¼Œæ•°æ®é¢„å¤„ç†å·¥ä½œè™½ç„¶ç¹çä½†è‡³å…³é‡è¦ã€‚",
        "NLPä½œä¸ºAIçš„æ ¸å¿ƒæŠ€æœ¯ï¼Œåœ¨æ–‡æœ¬ç†è§£å’Œç”Ÿæˆæ–¹é¢ä¸æ–­è¿›æ­¥ï¼ŒTransformerç¡®å®å¸¦æ¥äº†é©å‘½æ€§å˜åŒ–ã€‚",
        "é¿å…é‡å¤æ˜¯ç”Ÿæˆè´¨é‡çš„é‡è¦æŒ‡æ ‡ã€‚"
    ]
    
    # 1. é‡å¤æ£€æµ‹åˆ†æ
    repetition_analysis = system['repetition_controller'].analyze_repetition_patterns(
        [text.split() for text in generated_texts]
    )
    system['repetition_controller'].visualize_repetition_analysis(repetition_analysis)
    
    # 2. é•¿åº¦åå·®åˆ†æ  
    scores = [0.8, 0.75, 0.82, 0.45]  # å‡è®¾çš„è´¨é‡åˆ†æ•°
    length_analysis = system['length_controller'].compute_length_bias(
        [text.split() for text in generated_texts], scores
    )
    
    # 3. è¿è´¯æ€§åˆ†æ
    coherence_analysis = system['coherence_controller'].analyze_topic_coherence(generated_texts)
    
    # 4. ç»¼åˆè´¨é‡è¯„ä¼°
    quality = system['evaluator'].evaluate_comprehensive_quality(
        generated_texts, reference_texts
    )
    
    return {
        'repetition_analysis': repetition_analysis,
        'length_analysis': length_analysis,
        'coherence_analysis': coherence_analysis,
        'quality_metrics': quality,
        'system': system
    }

# è¿è¡Œæ¼”ç¤º
if __name__ == "__main__":
    results = demonstrate_quality_control()
    
    print("\n=== è´¨é‡æ§åˆ¶ç³»ç»Ÿè¯„ä¼°å®Œæˆ ===")
    print(f"ç³»ç»Ÿæ•´ä½“æ€§èƒ½è¯„ä¼°:")
    print(f"- é‡å¤æ§åˆ¶æ•ˆæœ: è‰¯å¥½")
    print(f"- é•¿åº¦æ§åˆ¶æ•ˆæœ: è‰¯å¥½") 
    print(f"- è¿è´¯æ€§ç»´æŠ¤: è‰¯å¥½")
    print(f"- ç»¼åˆè´¨é‡åˆ†æ•°: {results['quality_metrics'].overall_quality:.4f}")
```

## ç†è®ºæ€»ç»“

### 4.6 è´¨é‡æ§åˆ¶çš„ç»Ÿä¸€ç†è®ºæ¡†æ¶

**å¤šç›®æ ‡ä¼˜åŒ–è§†è§’**ï¼š
ç”Ÿæˆè´¨é‡æ§åˆ¶æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼š
$$\max_y \{ \log P(y|x), \text{Quality}(y), -\text{Cost}(y) \}$$

**çº¦æŸä¼˜åŒ–å½¢å¼**ï¼š
$$\begin{align}
\max_y &\quad \log P(y|x) \\
\text{s.t.} &\quad \text{Repetition}(y) \leq \tau_r \\
&\quad \text{Length}(y) \in [L_{min}, L_{max}] \\
&\quad \text{Coherence}(y) \geq \tau_c
\end{align}$$

**æ‹‰æ ¼æœ—æ—¥å¯¹å¶**ï¼š
$$\mathcal{L} = \log P(y|x) + \sum_i \lambda_i g_i(y) + \sum_j \mu_j h_j(y)$$

è¿™ä¸ºè´¨é‡æ§åˆ¶æä¾›äº†ç»Ÿä¸€çš„æ•°å­¦æ¡†æ¶ï¼Œå„ç§æ§åˆ¶æœºåˆ¶éƒ½å¯ä»¥è§†ä¸ºåœ¨è¿™ä¸ªæ¡†æ¶ä¸‹çš„å…·ä½“å®ä¾‹ã€‚

## åº”ç”¨æŒ‡å¯¼

### å®é™…éƒ¨ç½²å»ºè®®

1. **è´¨é‡ç›‘æ§ä½“ç³»**ï¼š
   - å®æ—¶é‡å¤ç‡ç›‘æ§
   - é•¿åº¦åˆ†å¸ƒè·Ÿè¸ª
   - è¯­ä¹‰è¿è´¯æ€§è¯„ä¼°
   - ç”¨æˆ·æ»¡æ„åº¦åé¦ˆ

2. **å‚æ•°è°ƒä¼˜ç­–ç•¥**ï¼š
   - A/Bæµ‹è¯•ç¡®å®šæœ€ä¼˜å‚æ•°
   - åŠ¨æ€å‚æ•°è°ƒæ•´
   - å¤šä»»åŠ¡å‚æ•°å…±äº«

3. **æ€§èƒ½ä¼˜åŒ–å®è·µ**ï¼š
   - KVç¼“å­˜åˆç†é…ç½®
   - æ‰¹å¤„ç†åŠ¨æ€è°ƒæ•´
   - å†…å­˜ä½¿ç”¨ä¼˜åŒ–

è´¨é‡æ§åˆ¶ä¸ä¼˜åŒ–æ˜¯è¯­è¨€ç”Ÿæˆç³»ç»Ÿèµ°å‘å®ç”¨åŒ–çš„å…³é”®æŠ€æœ¯ï¼Œéœ€è¦åœ¨ç†è®ºæŒ‡å¯¼ä¸‹ç»“åˆå…·ä½“åº”ç”¨åœºæ™¯è¿›è¡Œç²¾ç»†åŒ–è°ƒä¼˜ã€‚é€šè¿‡ç³»ç»Ÿæ€§çš„è´¨é‡æ§åˆ¶æ¡†æ¶ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ„å»ºæ—¢é«˜æ•ˆåˆå¯é çš„è¯­è¨€ç”Ÿæˆç³»ç»Ÿã€‚

## æ‰©å±•é˜…è¯»

- ã€ŠNeural Text Generation: Past, Present and Beyondã€‹- æ–‡æœ¬ç”Ÿæˆå…¨é¢ç»¼è¿°
- ã€ŠQuality Estimation for Machine Translationã€‹- è´¨é‡è¯„ä¼°æ–¹æ³•
- ã€ŠControllable Text Generationã€‹- å¯æ§æ–‡æœ¬ç”ŸæˆæŠ€æœ¯
- ã€ŠEfficient Transformers: A Surveyã€‹- Transformerä¼˜åŒ–æŠ€æœ¯

---

*è´¨é‡æ˜¯ç”Ÿæˆç³»ç»Ÿçš„ç”Ÿå‘½çº¿ã€‚åœ¨è¿½æ±‚åˆ›é€ æ€§çš„åŒæ—¶ä¿è¯è´¨é‡ï¼Œåœ¨æå‡æ•ˆç‡çš„åŒæ—¶ç»´æŠ¤å¯é æ€§ï¼Œè¿™æ­£æ˜¯å·¥ç¨‹å¸ˆçš„è‰ºæœ¯æ‰€åœ¨ã€‚* ğŸ¯