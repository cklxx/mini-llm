# 01 è‡ªå›å½’ç”Ÿæˆæ•°å­¦åŸç†

> **ä»æ¡ä»¶æ¦‚ç‡åˆ°åºåˆ—åˆ›é€ ï¼šè§£æè¯­è¨€ç”Ÿæˆçš„æ•°å­¦æœ¬è´¨**

## æ ¸å¿ƒæ€æƒ³

è‡ªå›å½’ç”Ÿæˆæ˜¯ç°ä»£è¯­è¨€æ¨¡å‹çš„åŸºçŸ³ï¼Œå®ƒå°†å¤æ‚çš„åºåˆ—ç”Ÿæˆé—®é¢˜åˆ†è§£ä¸ºä¸€ç³»åˆ—æ¡ä»¶æ¦‚ç‡é¢„æµ‹ã€‚è¿™ç§åˆ†è§£ä¸ä»…ä½¿é—®é¢˜åœ¨è®¡ç®—ä¸Šå˜å¾—å¯è¡Œï¼Œæ›´é‡è¦çš„æ˜¯å®ƒç¬¦åˆäººç±»è¯­è¨€äº§ç”Ÿçš„è®¤çŸ¥è¿‡ç¨‹â€”â€”æˆ‘ä»¬è¯´è¯æ—¶ä¹Ÿæ˜¯ä¸€ä¸ªè¯ä¸€ä¸ªè¯åœ°ç»„ç»‡ï¼Œæ¯ä¸ªè¯çš„é€‰æ‹©éƒ½ä¾èµ–äºå‰é¢å·²ç»è¯´å‡ºçš„å†…å®¹ã€‚

**å…³é”®æ´å¯Ÿ**ï¼š
- **å› æœåˆ†è§£**ï¼šå¤æ‚è”åˆåˆ†å¸ƒçš„é“¾å¼æ³•åˆ™åˆ†è§£
- **æ¡ä»¶ç‹¬ç«‹å‡è®¾**ï¼šç®€åŒ–å»ºæ¨¡çš„æ•°å­¦è¿‘ä¼¼ä¸åˆç†æ€§åˆ†æ
- **æœç´¢ç©ºé—´å‡ ä½•**ï¼šæŒ‡æ•°çº§åºåˆ—ç©ºé—´çš„ç»“æ„ç‰¹æ€§
- **è¯¯å·®ä¼ æ’­æœºåˆ¶**ï¼šç”Ÿæˆè¿‡ç¨‹ä¸­ä¸ç¡®å®šæ€§çš„ç´¯ç§¯æ•ˆåº”

ä»æ•°å­¦è§’åº¦çœ‹ï¼Œè‡ªå›å½’ç”Ÿæˆæ˜¯åœ¨é«˜ç»´ç¦»æ•£åºåˆ—ç©ºé—´ä¸­è¿›è¡Œçš„æ¡ä»¶é‡‡æ ·è¿‡ç¨‹ï¼Œæ¯ä¸€æ­¥éƒ½åœ¨å½“å‰ä¸Šä¸‹æ–‡çº¦æŸä¸‹è¿›è¡Œå±€éƒ¨å†³ç­–ï¼Œä½†è¿™äº›å±€éƒ¨å†³ç­–çš„ç»„åˆå´èƒ½äº§ç”Ÿå…¨å±€è¿è´¯çš„è¯­è¨€åºåˆ—ã€‚

## 1.1 æ¡ä»¶æ¦‚ç‡åˆ†è§£çš„æ•°å­¦åŸºç¡€

### é“¾å¼æ³•åˆ™ä¸å› æœåˆ†è§£

**è”åˆæ¦‚ç‡çš„é“¾å¼åˆ†è§£**ï¼š
å¯¹äºé•¿åº¦ä¸º$T$çš„åºåˆ—$y = (y_1, y_2, ..., y_T)$ï¼Œå…¶è”åˆæ¦‚ç‡å¯ä»¥é€šè¿‡é“¾å¼æ³•åˆ™å®Œå…¨åˆ†è§£ï¼š

$$P(y_{1:T}) = P(y_1) \cdot P(y_2|y_1) \cdot P(y_3|y_{1:2}) \cdots P(y_T|y_{1:T-1}) = \prod_{t=1}^{T} P(y_t|y_{<t})$$

è¿™ä¸ªåˆ†è§£æ˜¯æ•°å­¦ä¸Šç²¾ç¡®çš„ï¼Œæ²¡æœ‰ä»»ä½•è¿‘ä¼¼ã€‚å…³é”®åœ¨äºå¦‚ä½•å»ºæ¨¡æ¯ä¸ªæ¡ä»¶æ¦‚ç‡$P(y_t|y_{<t})$ã€‚

**å› æœæ€§çº¦æŸ**ï¼š
è‡ªå›å½’æ¨¡å‹æ–½åŠ äº†é‡è¦çš„å› æœæ€§çº¦æŸï¼š$P(y_t|y_{<t}) = P(y_t|y_{1:t-1})$ï¼Œå³æœªæ¥ä¸èƒ½å½±å“è¿‡å»ã€‚è¿™ç§çº¦æŸåœ¨æ•°å­¦ä¸Šè¡¨ç°ä¸ºï¼š

$$\frac{\partial P(y_t|y_{<t})}{\partial y_s} = 0, \quad \forall s > t$$

**æ¡ä»¶ç‹¬ç«‹å‡è®¾çš„è¯¯å·®åˆ†æ**ï¼š
åœ¨å®é™…å»ºæ¨¡ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸å‡è®¾åœ¨ç»™å®šè¶³å¤Ÿé•¿çš„ä¸Šä¸‹æ–‡çª—å£$w$ä¸‹ï¼Œè¿œè·ç¦»ä¾èµ–å¯ä»¥å¿½ç•¥ï¼š

$$P(y_t|y_{<t}) \approx P(y_t|y_{\max(1, t-w):t-1})$$

è¿™ç§è¿‘ä¼¼çš„è¯¯å·®å¯ä»¥é€šè¿‡æ¡ä»¶äº’ä¿¡æ¯æ¥é‡åŒ–ï¼š
$$\text{Error} = I(y_t; y_{<t-w} | y_{t-w:t-1})$$

å…¶ä¸­$I(\cdot; \cdot | \cdot)$æ˜¯æ¡ä»¶äº’ä¿¡æ¯ã€‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Union, Any
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import math
from collections import defaultdict
from dataclasses import dataclass

@dataclass
class SequenceState:
    """åºåˆ—ç”ŸæˆçŠ¶æ€"""
    tokens: List[int]           # å½“å‰tokenåºåˆ—
    logits: torch.Tensor       # ä¸‹ä¸€ä¸ªtokençš„logits
    probabilities: torch.Tensor # ä¸‹ä¸€ä¸ªtokençš„æ¦‚ç‡åˆ†å¸ƒ
    entropy: float             # å½“å‰åˆ†å¸ƒçš„ç†µ
    perplexity: float          # å½“å‰å›°æƒ‘åº¦
    step: int                  # ç”Ÿæˆæ­¥æ•°
    
class AutoregressiveAnalyzer:
    """è‡ªå›å½’ç”Ÿæˆæ•°å­¦åˆ†æå™¨"""
    
    def __init__(self, vocab_size: int = 1000, context_window: int = 512):
        self.vocab_size = vocab_size
        self.context_window = context_window
        self.generation_history = []
        
    def analyze_conditional_decomposition(self, sequences: List[List[int]]) -> Dict:
        """åˆ†ææ¡ä»¶æ¦‚ç‡åˆ†è§£çš„æ•°å­¦ç‰¹æ€§"""
        
        print("=== æ¡ä»¶æ¦‚ç‡åˆ†è§£åˆ†æ ===")
        
        # ç»Ÿè®¡æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ
        conditional_stats = defaultdict(lambda: defaultdict(int))
        context_entropy = []
        
        for sequence in sequences:
            for t in range(1, len(sequence)):
                # æå–ä¸Šä¸‹æ–‡å’Œç›®æ ‡token
                context = tuple(sequence[max(0, t-self.context_window):t])
                target = sequence[t]
                
                # ç»Ÿè®¡æ¡ä»¶é¢‘æ¬¡
                conditional_stats[context][target] += 1
        
        # è®¡ç®—æ¡ä»¶ç†µ
        total_conditional_entropy = 0
        context_count = 0
        
        for context, target_counts in conditional_stats.items():
            if len(target_counts) < 2:  # è·³è¿‡åªæœ‰ä¸€ä¸ªç›®æ ‡çš„ä¸Šä¸‹æ–‡
                continue
                
            total_count = sum(target_counts.values())
            context_entropy_val = 0
            
            for target, count in target_counts.items():
                prob = count / total_count
                context_entropy_val -= prob * math.log2(prob)
            
            context_entropy.append(context_entropy_val)
            total_conditional_entropy += context_entropy_val * total_count
            context_count += total_count
        
        avg_conditional_entropy = total_conditional_entropy / context_count if context_count > 0 else 0
        
        # åˆ†æä¸Šä¸‹æ–‡é•¿åº¦å¯¹æ¡ä»¶ç†µçš„å½±å“
        context_length_analysis = self._analyze_context_length_effect(sequences)
        
        results = {
            'average_conditional_entropy': avg_conditional_entropy,
            'entropy_distribution': context_entropy,
            'unique_contexts': len(conditional_stats),
            'context_length_effect': context_length_analysis,
            'decomposition_quality': self._evaluate_decomposition_quality(sequences)
        }
        
        # å¯è§†åŒ–ç»“æœ
        self._visualize_conditional_analysis(results)
        
        return results
    
    def _analyze_context_length_effect(self, sequences: List[List[int]]) -> Dict:
        """åˆ†æä¸Šä¸‹æ–‡é•¿åº¦å¯¹é¢„æµ‹è´¨é‡çš„å½±å“"""
        
        context_lengths = [1, 2, 4, 8, 16, 32]
        length_effects = {}
        
        for ctx_len in context_lengths:
            conditional_entropies = []
            
            for seq in sequences:
                for t in range(ctx_len, len(seq)):
                    # ä½¿ç”¨ä¸åŒé•¿åº¦çš„ä¸Šä¸‹æ–‡
                    context = tuple(seq[t-ctx_len:t])
                    
                    # è®¡ç®—åœ¨è¯¥ä¸Šä¸‹æ–‡ä¸‹çš„æ¡ä»¶åˆ†å¸ƒç†µï¼ˆç®€åŒ–è®¡ç®—ï¼‰
                    # å®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„ç»Ÿè®¡
                    estimated_entropy = self._estimate_conditional_entropy(context, seq[t])
                    conditional_entropies.append(estimated_entropy)
            
            length_effects[ctx_len] = {
                'mean_entropy': np.mean(conditional_entropies) if conditional_entropies else 0,
                'std_entropy': np.std(conditional_entropies) if conditional_entropies else 0
            }
        
        return length_effects
    
    def _estimate_conditional_entropy(self, context: Tuple[int, ...], target: int) -> float:
        """ä¼°è®¡ç»™å®šä¸Šä¸‹æ–‡çš„æ¡ä»¶ç†µï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # åŸºäºä¸Šä¸‹æ–‡å¤æ‚åº¦çš„å¯å‘å¼ä¼°è®¡
        context_complexity = len(set(context)) / len(context) if context else 1
        base_entropy = 2.0  # åŸºç¡€ç†µå€¼
        
        # ä¸Šä¸‹æ–‡è¶Šå¤æ‚ï¼Œæ¡ä»¶ç†µè¶Šå°ï¼ˆæ›´ç¡®å®šï¼‰
        estimated_entropy = base_entropy * (1 - 0.5 * context_complexity)
        
        return max(0.1, estimated_entropy)
    
    def _evaluate_decomposition_quality(self, sequences: List[List[int]]) -> Dict:
        """è¯„ä¼°æ¡ä»¶åˆ†è§£çš„è´¨é‡"""
        
        # è®¡ç®—ä¸åŒåˆ†è§£æ–¹å¼çš„KLæ•£åº¦
        forward_entropy = self._compute_forward_entropy(sequences)
        
        # ç†è®ºæœ€ä¼˜ç†µï¼ˆåŸºäºè¯é¢‘åˆ†å¸ƒï¼‰
        optimal_entropy = self._compute_optimal_entropy(sequences)
        
        # åˆ†è§£è´¨é‡ = 1 - (å®é™…ç†µ - æœ€ä¼˜ç†µ) / æœ€å¤§ç†µ
        max_entropy = math.log2(self.vocab_size)
        quality = 1 - (forward_entropy - optimal_entropy) / (max_entropy - optimal_entropy)
        
        return {
            'forward_entropy': forward_entropy,
            'optimal_entropy': optimal_entropy,
            'decomposition_quality': max(0, quality),
            'entropy_gap': forward_entropy - optimal_entropy
        }
    
    def _compute_forward_entropy(self, sequences: List[List[int]]) -> float:
        """è®¡ç®—å‰å‘æ¡ä»¶ç†µ"""
        total_entropy = 0
        total_positions = 0
        
        for seq in sequences:
            for t in range(1, len(seq)):
                # è®¡ç®—ä½ç½®tçš„æ¡ä»¶ç†µï¼ˆç®€åŒ–è®¡ç®—ï¼‰
                pos_entropy = 2.0 - 0.1 * min(t, 10)  # éšä½ç½®å‡å°‘çš„å¯å‘å¼ç†µ
                total_entropy += pos_entropy
                total_positions += 1
        
        return total_entropy / total_positions if total_positions > 0 else 0
    
    def _compute_optimal_entropy(self, sequences: List[List[int]]) -> float:
        """è®¡ç®—ç†è®ºæœ€ä¼˜ç†µ"""
        # åŸºäºæ‰€æœ‰tokençš„é¢‘ç‡åˆ†å¸ƒè®¡ç®—ç†è®ºä¸‹ç•Œ
        all_tokens = []
        for seq in sequences:
            all_tokens.extend(seq)
        
        if not all_tokens:
            return 0
        
        # è®¡ç®—tokené¢‘ç‡åˆ†å¸ƒ
        token_counts = defaultdict(int)
        for token in all_tokens:
            token_counts[token] += 1
        
        total_tokens = len(all_tokens)
        entropy = 0
        
        for count in token_counts.values():
            prob = count / total_tokens
            entropy -= prob * math.log2(prob)
        
        return entropy
    
    def _visualize_conditional_analysis(self, results: Dict):
        """å¯è§†åŒ–æ¡ä»¶åˆ†æç»“æœ"""
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # æ¡ä»¶ç†µåˆ†å¸ƒ
        if results['entropy_distribution']:
            axes[0, 0].hist(results['entropy_distribution'], bins=20, alpha=0.7, edgecolor='black')
            axes[0, 0].set_title('æ¡ä»¶ç†µåˆ†å¸ƒ')
            axes[0, 0].set_xlabel('æ¡ä»¶ç†µ (bits)')
            axes[0, 0].set_ylabel('é¢‘æ¬¡')
            axes[0, 0].grid(True, alpha=0.3)
        
        # ä¸Šä¸‹æ–‡é•¿åº¦æ•ˆåº”
        length_data = results['context_length_effect']
        if length_data:
            lengths = list(length_data.keys())
            entropies = [data['mean_entropy'] for data in length_data.values()]
            errors = [data['std_entropy'] for data in length_data.values()]
            
            axes[0, 1].errorbar(lengths, entropies, yerr=errors, marker='o', capsize=5)
            axes[0, 1].set_title('ä¸Šä¸‹æ–‡é•¿åº¦å¯¹æ¡ä»¶ç†µçš„å½±å“')
            axes[0, 1].set_xlabel('ä¸Šä¸‹æ–‡é•¿åº¦')
            axes[0, 1].set_ylabel('å¹³å‡æ¡ä»¶ç†µ')
            axes[0, 1].set_xscale('log')
            axes[0, 1].grid(True, alpha=0.3)
        
        # åˆ†è§£è´¨é‡æŒ‡æ ‡
        quality_data = results['decomposition_quality']
        metrics = ['å‰å‘ç†µ', 'æœ€ä¼˜ç†µ', 'ç†µå·®', 'åˆ†è§£è´¨é‡']
        values = [
            quality_data['forward_entropy'],
            quality_data['optimal_entropy'],
            quality_data['entropy_gap'],
            quality_data['decomposition_quality']
        ]
        
        axes[1, 0].bar(metrics, values, alpha=0.7)
        axes[1, 0].set_title('æ¡ä»¶åˆ†è§£è´¨é‡æŒ‡æ ‡')
        axes[1, 0].set_ylabel('æ•°å€¼')
        axes[1, 0].tick_params(axis='x', rotation=45)
        axes[1, 0].grid(True, alpha=0.3)
        
        # ç»Ÿè®¡æ±‡æ€»
        stats_text = f"""
        å¹³å‡æ¡ä»¶ç†µ: {results['average_conditional_entropy']:.3f} bits
        å”¯ä¸€ä¸Šä¸‹æ–‡æ•°: {results['unique_contexts']}
        åˆ†è§£è´¨é‡: {quality_data['decomposition_quality']:.3f}
        ç†µä¼˜åŒ–ç©ºé—´: {quality_data['entropy_gap']:.3f} bits
        """
        
        axes[1, 1].text(0.1, 0.5, stats_text, transform=axes[1, 1].transAxes,
                        fontsize=12, verticalalignment='center',
                        bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        axes[1, 1].set_title('ç»Ÿè®¡æ±‡æ€»')
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        plt.show()

    def analyze_search_space_complexity(self, max_length: int = 20, 
                                      branching_factors: List[int] = None) -> Dict:
        """åˆ†æåºåˆ—ç”Ÿæˆæœç´¢ç©ºé—´çš„å¤æ‚æ€§"""
        
        print("=== æœç´¢ç©ºé—´å¤æ‚æ€§åˆ†æ ===")
        
        if branching_factors is None:
            branching_factors = [10, 50, 100, 500, 1000]  # ä¸åŒçš„æœ‰æ•ˆè¯æ±‡é‡
        
        complexity_analysis = {}
        
        for branching_factor in branching_factors:
            # è®¡ç®—ä¸åŒé•¿åº¦ä¸‹çš„æœç´¢ç©ºé—´å¤§å°
            space_sizes = []
            cumulative_sizes = []
            
            for length in range(1, max_length + 1):
                # å›ºå®šé•¿åº¦åºåˆ—çš„æœç´¢ç©ºé—´
                space_size = branching_factor ** length
                space_sizes.append(space_size)
                
                # æ‰€æœ‰é•¿åº¦â‰¤lengthçš„åºåˆ—æ€»æ•°
                cumulative_size = sum(branching_factor ** l for l in range(1, length + 1))
                cumulative_sizes.append(cumulative_size)
            
            complexity_analysis[branching_factor] = {
                'lengths': list(range(1, max_length + 1)),
                'space_sizes': space_sizes,
                'cumulative_sizes': cumulative_sizes,
                'growth_rate': self._analyze_growth_rate(space_sizes),
                'effective_search_ratio': self._compute_effective_search_ratio(space_sizes)
            }
        
        # å¯è§†åŒ–æœç´¢ç©ºé—´å¤æ‚æ€§
        self._visualize_search_complexity(complexity_analysis)
        
        return complexity_analysis
    
    def _analyze_growth_rate(self, space_sizes: List[int]) -> Dict:
        """åˆ†ææœç´¢ç©ºé—´å¢é•¿ç‡"""
        
        if len(space_sizes) < 2:
            return {'exponential_base': 1, 'growth_consistency': 0}
        
        # è®¡ç®—è¿ç»­æ¯”ç‡
        ratios = []
        for i in range(1, len(space_sizes)):
            if space_sizes[i-1] > 0:
                ratio = space_sizes[i] / space_sizes[i-1]
                ratios.append(ratio)
        
        if not ratios:
            return {'exponential_base': 1, 'growth_consistency': 0}
        
        # ä¼°è®¡æŒ‡æ•°åº•æ•°
        avg_ratio = np.mean(ratios)
        ratio_std = np.std(ratios)
        
        # å¢é•¿ä¸€è‡´æ€§ï¼ˆæ ‡å‡†å·®çš„å€’æ•°ï¼‰
        consistency = 1 / (1 + ratio_std / avg_ratio) if avg_ratio > 0 else 0
        
        return {
            'exponential_base': avg_ratio,
            'growth_consistency': consistency,
            'ratio_variance': ratio_std
        }
    
    def _compute_effective_search_ratio(self, space_sizes: List[int]) -> List[float]:
        """è®¡ç®—æœ‰æ•ˆæœç´¢æ¯”ä¾‹"""
        
        # å‡è®¾å®é™…å¯è¡Œçš„åºåˆ—æ•°é‡è¿œå°äºç†è®ºæœç´¢ç©ºé—´
        effective_ratios = []
        
        for i, size in enumerate(space_sizes):
            # æœ‰æ•ˆåºåˆ—æ¯”ä¾‹éšé•¿åº¦æŒ‡æ•°è¡°å‡ï¼ˆå¯å‘å¼æ¨¡å‹ï¼‰
            length = i + 1
            effective_ratio = 1.0 / (size ** 0.1) if size > 0 else 0
            effective_ratios.append(effective_ratio)
        
        return effective_ratios
    
    def _visualize_search_complexity(self, complexity_data: Dict):
        """å¯è§†åŒ–æœç´¢ç©ºé—´å¤æ‚æ€§"""
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # æœç´¢ç©ºé—´å¤§å°éšé•¿åº¦å˜åŒ–
        for branching_factor, data in complexity_data.items():
            lengths = data['lengths']
            space_sizes = data['space_sizes']
            
            axes[0, 0].semilogy(lengths, space_sizes, marker='o', 
                               label=f'è¯æ±‡é‡={branching_factor}')
        
        axes[0, 0].set_title('æœç´¢ç©ºé—´å¤§å° vs åºåˆ—é•¿åº¦')
        axes[0, 0].set_xlabel('åºåˆ—é•¿åº¦')
        axes[0, 0].set_ylabel('æœç´¢ç©ºé—´å¤§å° (log scale)')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # ç´¯ç§¯æœç´¢ç©ºé—´
        for branching_factor, data in complexity_data.items():
            lengths = data['lengths']
            cumulative_sizes = data['cumulative_sizes']
            
            axes[0, 1].semilogy(lengths, cumulative_sizes, marker='s',
                               label=f'è¯æ±‡é‡={branching_factor}')
        
        axes[0, 1].set_title('ç´¯ç§¯æœç´¢ç©ºé—´')
        axes[0, 1].set_xlabel('æœ€å¤§åºåˆ—é•¿åº¦')
        axes[0, 1].set_ylabel('ç´¯ç§¯ç©ºé—´å¤§å° (log scale)')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # å¢é•¿ç‡åˆ†æ
        branching_factors = list(complexity_data.keys())
        growth_rates = [data['growth_rate']['exponential_base'] 
                       for data in complexity_data.values()]
        consistency = [data['growth_rate']['growth_consistency']
                      for data in complexity_data.values()]
        
        axes[1, 0].scatter(branching_factors, growth_rates, 
                          s=[c*100 for c in consistency], alpha=0.6)
        axes[1, 0].set_title('æœç´¢ç©ºé—´å¢é•¿ç‡ (æ°”æ³¡å¤§å°=ä¸€è‡´æ€§)')
        axes[1, 0].set_xlabel('åˆ†æ”¯å› å­ (æœ‰æ•ˆè¯æ±‡é‡)')
        axes[1, 0].set_ylabel('å¹³å‡å¢é•¿ç‡')
        axes[1, 0].set_xscale('log')
        axes[1, 0].grid(True, alpha=0.3)
        
        # æœ‰æ•ˆæœç´¢æ¯”ä¾‹
        sample_data = list(complexity_data.values())[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ•°æ®é›†ä½œä¸ºç¤ºä¾‹
        lengths = sample_data['lengths']
        effective_ratios = sample_data['effective_search_ratio']
        
        axes[1, 1].semilogy(lengths, effective_ratios, 'r-o')
        axes[1, 1].set_title('æœ‰æ•ˆæœç´¢ç©ºé—´æ¯”ä¾‹')
        axes[1, 1].set_xlabel('åºåˆ—é•¿åº¦')
        axes[1, 1].set_ylabel('æœ‰æ•ˆæ¯”ä¾‹ (log scale)')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

class ExposureBiasAnalyzer:
    """æ›å…‰åå·®åˆ†æå™¨"""
    
    def __init__(self):
        self.error_propagation_history = []
        
    def analyze_exposure_bias(self, true_sequences: List[List[int]], 
                            model_predictions: List[List[Tuple[int, float]]]) -> Dict:
        """åˆ†ææ›å…‰åå·®çš„æ•°å­¦ç‰¹æ€§"""
        
        print("=== æ›å…‰åå·®åˆ†æ ===")
        
        if len(true_sequences) != len(model_predictions):
            raise ValueError("çœŸå®åºåˆ—å’Œé¢„æµ‹åºåˆ—æ•°é‡ä¸åŒ¹é…")
        
        bias_metrics = {
            'teacher_forcing_accuracy': [],
            'free_running_accuracy': [],
            'error_propagation': [],
            'distribution_shift': []
        }
        
        for true_seq, pred_seq in zip(true_sequences, model_predictions):
            # åˆ†æå•ä¸ªåºåˆ—çš„æ›å…‰åå·®
            seq_analysis = self._analyze_single_sequence_bias(true_seq, pred_seq)
            
            for key in bias_metrics:
                if key in seq_analysis:
                    bias_metrics[key].append(seq_analysis[key])
        
        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        summary_stats = {}
        for key, values in bias_metrics.items():
            if values:
                summary_stats[key] = {
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'min': np.min(values),
                    'max': np.max(values)
                }
        
        # åˆ†æè¯¯å·®ä¼ æ’­æ¨¡å¼
        error_propagation_analysis = self._analyze_error_propagation_patterns(bias_metrics)
        
        results = {
            'bias_metrics': bias_metrics,
            'summary_statistics': summary_stats,
            'error_propagation': error_propagation_analysis,
            'bias_severity': self._compute_bias_severity(summary_stats)
        }
        
        # å¯è§†åŒ–æ›å…‰åå·®
        self._visualize_exposure_bias(results)
        
        return results
    
    def _analyze_single_sequence_bias(self, true_seq: List[int], 
                                    pred_seq: List[Tuple[int, float]]) -> Dict:
        """åˆ†æå•ä¸ªåºåˆ—çš„æ›å…‰åå·®"""
        
        seq_len = min(len(true_seq), len(pred_seq))
        
        # Teacher forcingå‡†ç¡®ç‡ï¼ˆä½¿ç”¨çœŸå®å†å²ï¼‰
        tf_correct = 0
        for i in range(seq_len):
            if i == 0 or true_seq[i] == pred_seq[i][0]:
                tf_correct += 1
        tf_accuracy = tf_correct / seq_len if seq_len > 0 else 0
        
        # Free runningå‡†ç¡®ç‡ï¼ˆä½¿ç”¨é¢„æµ‹å†å²ï¼‰
        fr_correct = 0
        predicted_history = []
        
        for i in range(seq_len):
            if i == 0:
                # ç¬¬ä¸€ä¸ªtokenç›´æ¥æ¯”è¾ƒ
                if true_seq[i] == pred_seq[i][0]:
                    fr_correct += 1
                    predicted_history.append(pred_seq[i][0])
                else:
                    predicted_history.append(pred_seq[i][0])
            else:
                # ä½¿ç”¨é¢„æµ‹å†å²è¿›è¡Œæ¯”è¾ƒï¼ˆç®€åŒ–å¤„ç†ï¼‰
                if true_seq[i] == pred_seq[i][0]:
                    fr_correct += 1
                predicted_history.append(pred_seq[i][0])
        
        fr_accuracy = fr_correct / seq_len if seq_len > 0 else 0
        
        # è¯¯å·®ä¼ æ’­åˆ†æ
        error_positions = []
        for i in range(seq_len):
            if true_seq[i] != pred_seq[i][0]:
                error_positions.append(i)
        
        error_propagation = len(error_positions) / seq_len if seq_len > 0 else 0
        
        # åˆ†å¸ƒåç§»ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
        distribution_shift = abs(tf_accuracy - fr_accuracy)
        
        return {
            'teacher_forcing_accuracy': tf_accuracy,
            'free_running_accuracy': fr_accuracy,
            'error_propagation': error_propagation,
            'distribution_shift': distribution_shift,
            'error_positions': error_positions
        }
    
    def _analyze_error_propagation_patterns(self, bias_metrics: Dict) -> Dict:
        """åˆ†æè¯¯å·®ä¼ æ’­æ¨¡å¼"""
        
        error_props = bias_metrics.get('error_propagation', [])
        
        if not error_props:
            return {'pattern': 'insufficient_data'}
        
        # è®¡ç®—è¯¯å·®ä¼ æ’­çš„ç»Ÿè®¡ç‰¹æ€§
        mean_propagation = np.mean(error_props)
        propagation_variance = np.var(error_props)
        
        # åˆ†ç±»è¯¯å·®ä¼ æ’­æ¨¡å¼
        if mean_propagation < 0.1:
            pattern = 'low_propagation'
        elif mean_propagation < 0.3:
            pattern = 'moderate_propagation'
        else:
            pattern = 'high_propagation'
        
        # åˆ†æä¼ æ’­çš„ä¸€è‡´æ€§
        consistency = 1 / (1 + propagation_variance) if propagation_variance >= 0 else 1
        
        return {
            'pattern': pattern,
            'mean_propagation': mean_propagation,
            'propagation_variance': propagation_variance,
            'consistency': consistency
        }
    
    def _compute_bias_severity(self, summary_stats: Dict) -> Dict:
        """è®¡ç®—æ›å…‰åå·®ä¸¥é‡ç¨‹åº¦"""
        
        if 'distribution_shift' not in summary_stats:
            return {'severity': 'unknown', 'score': 0}
        
        # åŸºäºåˆ†å¸ƒåç§»è®¡ç®—åå·®ä¸¥é‡ç¨‹åº¦
        shift_mean = summary_stats['distribution_shift']['mean']
        
        if shift_mean < 0.05:
            severity = 'mild'
            score = shift_mean * 20  # 0-1 scale
        elif shift_mean < 0.15:
            severity = 'moderate'
            score = 0.2 + (shift_mean - 0.05) * 8  # 0.2-1.0 scale
        else:
            severity = 'severe'
            score = min(1.0, 0.8 + (shift_mean - 0.15) * 2)
        
        return {
            'severity': severity,
            'score': score,
            'shift_magnitude': shift_mean
        }
    
    def _visualize_exposure_bias(self, results: Dict):
        """å¯è§†åŒ–æ›å…‰åå·®åˆ†æ"""
        
        bias_metrics = results['bias_metrics']
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # Teacher forcing vs Free running å‡†ç¡®ç‡å¯¹æ¯”
        if 'teacher_forcing_accuracy' in bias_metrics and 'free_running_accuracy' in bias_metrics:
            tf_acc = bias_metrics['teacher_forcing_accuracy']
            fr_acc = bias_metrics['free_running_accuracy']
            
            axes[0, 0].scatter(tf_acc, fr_acc, alpha=0.6)
            axes[0, 0].plot([0, 1], [0, 1], 'r--', alpha=0.8, label='å®Œç¾å¯¹åº”')
            axes[0, 0].set_xlabel('Teacher Forcing å‡†ç¡®ç‡')
            axes[0, 0].set_ylabel('Free Running å‡†ç¡®ç‡')
            axes[0, 0].set_title('å‡†ç¡®ç‡å¯¹æ¯”')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
        
        # åˆ†å¸ƒåç§»åˆ†å¸ƒ
        if 'distribution_shift' in bias_metrics:
            shifts = bias_metrics['distribution_shift']
            axes[0, 1].hist(shifts, bins=20, alpha=0.7, edgecolor='black')
            axes[0, 1].set_xlabel('åˆ†å¸ƒåç§»å¹…åº¦')
            axes[0, 1].set_ylabel('é¢‘æ¬¡')
            axes[0, 1].set_title('åˆ†å¸ƒåç§»åˆ†å¸ƒ')
            axes[0, 1].grid(True, alpha=0.3)
        
        # è¯¯å·®ä¼ æ’­åˆ†æ
        if 'error_propagation' in bias_metrics:
            error_props = bias_metrics['error_propagation']
            axes[1, 0].hist(error_props, bins=20, alpha=0.7, edgecolor='black', color='orange')
            axes[1, 0].set_xlabel('è¯¯å·®ä¼ æ’­æ¯”ä¾‹')
            axes[1, 0].set_ylabel('é¢‘æ¬¡')
            axes[1, 0].set_title('è¯¯å·®ä¼ æ’­åˆ†å¸ƒ')
            axes[1, 0].grid(True, alpha=0.3)
        
        # åå·®ä¸¥é‡ç¨‹åº¦æ±‡æ€»
        severity_info = results.get('bias_severity', {})
        summary_text = f"""
        åå·®ä¸¥é‡ç¨‹åº¦: {severity_info.get('severity', 'unknown')}
        åå·®åˆ†æ•°: {severity_info.get('score', 0):.3f}
        å¹³å‡åˆ†å¸ƒåç§»: {severity_info.get('shift_magnitude', 0):.3f}
        
        è¯¯å·®ä¼ æ’­æ¨¡å¼: {results.get('error_propagation', {}).get('pattern', 'unknown')}
        ä¼ æ’­ä¸€è‡´æ€§: {results.get('error_propagation', {}).get('consistency', 0):.3f}
        """
        
        axes[1, 1].text(0.1, 0.5, summary_text, transform=axes[1, 1].transAxes,
                        fontsize=12, verticalalignment='center',
                        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
        axes[1, 1].set_title('æ›å…‰åå·®æ±‡æ€»')
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        plt.show()

# ç»¼åˆæ¼”ç¤ºï¼šè‡ªå›å½’ç”Ÿæˆæ•°å­¦åŸç†
def demonstrate_autoregressive_principles():
    """æ¼”ç¤ºè‡ªå›å½’ç”Ÿæˆçš„æ•°å­¦åŸç†"""
    
    print("="*60)
    print("è‡ªå›å½’ç”Ÿæˆæ•°å­¦åŸç† - ç»¼åˆæ¼”ç¤º")
    print("="*60)
    
    # 1. åˆ›å»ºåˆ†æå™¨
    analyzer = AutoregressiveAnalyzer(vocab_size=1000, context_window=16)
    exposure_analyzer = ExposureBiasAnalyzer()
    
    # 2. ç”Ÿæˆæ¨¡æ‹Ÿåºåˆ—æ•°æ®
    print("\n1. ç”Ÿæˆæ¨¡æ‹Ÿåºåˆ—æ•°æ®")
    
    # æ¨¡æ‹Ÿä¸€äº›æœ‰ç»“æ„çš„åºåˆ—ï¼ˆå¦‚ç®€å•çš„è¯­æ³•æ¨¡å¼ï¼‰
    sequences = []
    for i in range(50):
        # ç”Ÿæˆé•¿åº¦ä¸º10-30çš„åºåˆ—
        seq_length = np.random.randint(10, 31)
        
        # ä½¿ç”¨é©¬å°”å¯å¤«é“¾ç”Ÿæˆæœ‰ä¾èµ–å…³ç³»çš„åºåˆ—
        sequence = [2]  # BOS token
        for j in range(seq_length - 1):
            # åŸºäºå‰é¢çš„tokené€‰æ‹©ä¸‹ä¸€ä¸ªtoken
            if len(sequence) == 1:
                next_token = np.random.randint(3, 100)
            else:
                # ç®€å•çš„ä¾èµ–æ¨¡å¼ï¼šä¸‹ä¸€ä¸ªtokenä¸å‰é¢tokenæœ‰å…³
                prev_token = sequence[-1]
                if prev_token < 50:
                    next_token = np.random.choice([prev_token + 1, prev_token + 2, prev_token - 1], 
                                                p=[0.5, 0.3, 0.2])
                else:
                    next_token = np.random.randint(3, 50)
                
                next_token = max(3, min(999, next_token))  # ä¿æŒåœ¨æœ‰æ•ˆèŒƒå›´å†…
                
            sequence.append(next_token)
        
        sequence.append(1)  # EOS token
        sequences.append(sequence)
    
    print(f"ç”Ÿæˆäº† {len(sequences)} ä¸ªåºåˆ—ï¼Œå¹³å‡é•¿åº¦ {np.mean([len(s) for s in sequences]):.1f}")
    
    # 3. æ¡ä»¶æ¦‚ç‡åˆ†è§£åˆ†æ
    print("\n2. æ¡ä»¶æ¦‚ç‡åˆ†è§£åˆ†æ")
    decomposition_results = analyzer.analyze_conditional_decomposition(sequences)
    
    print(f"å¹³å‡æ¡ä»¶ç†µ: {decomposition_results['average_conditional_entropy']:.3f} bits")
    print(f"å”¯ä¸€ä¸Šä¸‹æ–‡æ•°: {decomposition_results['unique_contexts']}")
    print(f"åˆ†è§£è´¨é‡: {decomposition_results['decomposition_quality']['decomposition_quality']:.3f}")
    
    # 4. æœç´¢ç©ºé—´å¤æ‚æ€§åˆ†æ
    print("\n3. æœç´¢ç©ºé—´å¤æ‚æ€§åˆ†æ")
    complexity_results = analyzer.analyze_search_space_complexity(
        max_length=25, 
        branching_factors=[10, 50, 100, 500]
    )
    
    # åˆ†æç»“æœ
    for bf, data in complexity_results.items():
        growth_rate = data['growth_rate']['exponential_base']
        print(f"è¯æ±‡é‡ {bf}: å¹³å‡å¢é•¿ç‡ {growth_rate:.1f}x")
    
    # 5. æ›å…‰åå·®åˆ†æ
    print("\n4. æ›å…‰åå·®åˆ†æ")
    
    # æ¨¡æ‹Ÿteacher forcingå’Œfree runningçš„é¢„æµ‹ç»“æœ
    model_predictions = []
    for seq in sequences[:20]:  # ä½¿ç”¨å‰20ä¸ªåºåˆ—è¿›è¡Œåˆ†æ
        pred_seq = []
        for i, true_token in enumerate(seq):
            # æ¨¡æ‹Ÿæ¨¡å‹é¢„æµ‹ï¼šå¤§éƒ¨åˆ†æ­£ç¡®ï¼Œä½†æœ‰ä¸€å®šé”™è¯¯ç‡
            if np.random.random() < 0.8:  # 80%å‡†ç¡®ç‡
                pred_token = true_token
            else:
                pred_token = np.random.randint(3, 100)
            
            # æ¨¡æ‹Ÿé¢„æµ‹æ¦‚ç‡
            confidence = np.random.uniform(0.7, 0.95)
            pred_seq.append((pred_token, confidence))
        
        model_predictions.append(pred_seq)
    
    exposure_results = exposure_analyzer.analyze_exposure_bias(
        sequences[:20], model_predictions
    )
    
    bias_severity = exposure_results['bias_severity']
    print(f"æ›å…‰åå·®ä¸¥é‡ç¨‹åº¦: {bias_severity['severity']}")
    print(f"åå·®åˆ†æ•°: {bias_severity['score']:.3f}")
    print(f"å¹³å‡åˆ†å¸ƒåç§»: {bias_severity['shift_magnitude']:.3f}")
    
    # 6. ç†è®ºåˆ†ææ€»ç»“
    print(f"\n=== è‡ªå›å½’ç”Ÿæˆæ•°å­¦åŸç†æ€»ç»“ ===")
    
    theoretical_insights = [
        "ğŸ” æ¡ä»¶æ¦‚ç‡åˆ†è§£æ˜¯æ•°å­¦ä¸Šç²¾ç¡®çš„ï¼Œä½†å®é™…å»ºæ¨¡å­˜åœ¨è¿‘ä¼¼",
        "ğŸ“ˆ æœç´¢ç©ºé—´éšåºåˆ—é•¿åº¦æŒ‡æ•°å¢é•¿ï¼Œéœ€è¦é«˜æ•ˆçš„æœç´¢ç­–ç•¥",
        "âš ï¸ æ›å…‰åå·®æ˜¯è®­ç»ƒä¸æ¨ç†åˆ†å¸ƒä¸åŒ¹é…å¯¼è‡´çš„å›ºæœ‰é—®é¢˜",
        "ğŸ¯ ç”Ÿæˆè´¨é‡ä¾èµ–äºæ¡ä»¶æ¦‚ç‡ä¼°è®¡çš„å‡†ç¡®æ€§",
        "âš–ï¸ æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡æ˜¯ç”Ÿæˆç­–ç•¥è®¾è®¡çš„æ ¸å¿ƒ"
    ]
    
    for insight in theoretical_insights:
        print(f"  {insight}")
    
    # 7. æ•°å­¦å…¬å¼éªŒè¯
    print(f"\n5. å…³é”®æ•°å­¦å…¬å¼éªŒè¯")
    
    # éªŒè¯é“¾å¼æ³•åˆ™
    sample_seq = sequences[0][:5]  # å–å‰5ä¸ªtoken
    print(f"æ ·æœ¬åºåˆ—: {sample_seq}")
    
    # è®¡ç®—è”åˆæ¦‚ç‡çš„é“¾å¼åˆ†è§£ï¼ˆç®€åŒ–æ¼”ç¤ºï¼‰
    joint_prob_log = 0
    for i in range(1, len(sample_seq)):
        # æ¨¡æ‹Ÿæ¡ä»¶æ¦‚ç‡ï¼ˆåŸºäºç®€å•çš„é¢‘ç‡ç»Ÿè®¡ï¼‰
        conditional_prob = 0.1  # ç®€åŒ–ä¸ºå›ºå®šå€¼
        joint_prob_log += math.log(conditional_prob)
    
    print(f"é“¾å¼åˆ†è§£å¯¹æ•°æ¦‚ç‡: {joint_prob_log:.3f}")
    
    # éªŒè¯ç†µè®¡ç®—
    vocab_subset = list(range(10, 20))
    uniform_probs = [1/len(vocab_subset)] * len(vocab_subset)
    entropy = -sum(p * math.log2(p) for p in uniform_probs)
    print(f"å‡åŒ€åˆ†å¸ƒç†µ (10ä¸ªtoken): {entropy:.3f} bits")
    print(f"ç†è®ºæœ€å¤§ç†µ (log2(10)): {math.log2(10):.3f} bits")
    
    print(f"\nè‡ªå›å½’ç”Ÿæˆçš„æ•°å­¦åŸç†åˆ†æå®Œæˆ!")
    print(f"è¿™äº›åŸç†ä¸ºç†è§£å’Œæ”¹è¿›è¯­è¨€æ¨¡å‹ç”Ÿæˆæä¾›äº†ç†è®ºåŸºç¡€")

# è¿è¡Œç»¼åˆæ¼”ç¤º
demonstrate_autoregressive_principles()
```

ç»§ç»­å®Œæˆç¬¬01èŠ‚çš„å‰©ä½™å†…å®¹ï¼Œæ·±å…¥åˆ†æç”Ÿæˆè¿‡ç¨‹çš„åŠ¨åŠ›å­¦ç‰¹æ€§ï¼š

## 1.2 åºåˆ—ç”Ÿæˆçš„ä¿¡æ¯è®ºåˆ†æ

### ç”Ÿæˆè¿‡ç¨‹çš„ç†µåˆ†æ

**æ¡ä»¶ç†µçš„å®šä¹‰ä¸è®¡ç®—**ï¼š
ç»™å®šä¸Šä¸‹æ–‡$x$ï¼Œç›®æ ‡åºåˆ—$y$çš„æ¡ä»¶ç†µä¸ºï¼š
$$H(Y|X) = -\sum_{y} P(y|x) \log P(y|x)$$

è¿™ä¸ªç†µå€¼åæ˜ äº†åœ¨ç»™å®šä¸Šä¸‹æ–‡ä¸‹ç”Ÿæˆçš„ä¸ç¡®å®šæ€§ã€‚æ›´é«˜çš„ç†µæ„å‘³ç€æ›´å¤šçš„ç”Ÿæˆå¯èƒ½æ€§ï¼Œä½†ä¹Ÿå¯èƒ½å¯¼è‡´ç”Ÿæˆè´¨é‡çš„ä¸‹é™ã€‚

**äº’ä¿¡æ¯ä¸ä¸Šä¸‹æ–‡æ•ˆç”¨**ï¼š
ä¸Šä¸‹æ–‡$x$å¯¹é¢„æµ‹$y_t$çš„ä¿¡æ¯è´¡çŒ®å¯ä»¥é€šè¿‡äº’ä¿¡æ¯é‡åŒ–ï¼š
$$I(Y_t; X) = H(Y_t) - H(Y_t|X) = \sum_{y_t, x} P(y_t, x) \log \frac{P(y_t, x)}{P(y_t)P(x)}$$

**ç”Ÿæˆå¤šæ ·æ€§ä¸ä¸€è‡´æ€§çš„æƒè¡¡**ï¼š
è¿™æ˜¯ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ ¸å¿ƒçŸ›ç›¾ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹ç›®æ ‡å‡½æ•°æ¥å½¢å¼åŒ–è¿™ä¸ªæƒè¡¡ï¼š
$$\mathcal{L} = -\mathbb{E}_{y \sim P(\cdot|x)}[\log P(y|x)] - \lambda H(P(\cdot|x)) + \mu \text{Consistency}(y, x)$$

å…¶ä¸­ç¬¬ä¸€é¡¹æ˜¯ä¼¼ç„¶æŸå¤±ï¼Œç¬¬äºŒé¡¹é¼“åŠ±å¤šæ ·æ€§ï¼Œç¬¬ä¸‰é¡¹ä¿è¯ä¸€è‡´æ€§ã€‚

```python
class InformationTheoreticAnalyzer:
    """ä¿¡æ¯è®ºåˆ†æå™¨"""
    
    def __init__(self):
        self.entropy_history = []
        self.mutual_info_cache = {}
    
    def analyze_generation_entropy(self, model, contexts: List[str], 
                                  num_samples: int = 100) -> Dict:
        """åˆ†æç”Ÿæˆè¿‡ç¨‹çš„ç†µç‰¹æ€§"""
        
        print("=== ç”Ÿæˆç†µåˆ†æ ===")
        
        entropy_results = {
            'conditional_entropies': [],
            'generation_diversity': [],
            'context_information': [],
            'entropy_dynamics': []
        }
        
        for context in contexts:
            # ä¸ºæ¯ä¸ªä¸Šä¸‹æ–‡ç”Ÿæˆå¤šä¸ªæ ·æœ¬
            samples = self._generate_samples(model, context, num_samples)
            
            # è®¡ç®—æ¡ä»¶ç†µ
            conditional_entropy = self._compute_conditional_entropy(samples)
            entropy_results['conditional_entropies'].append(conditional_entropy)
            
            # è®¡ç®—ç”Ÿæˆå¤šæ ·æ€§
            diversity = self._compute_generation_diversity(samples)
            entropy_results['generation_diversity'].append(diversity)
            
            # è®¡ç®—ä¸Šä¸‹æ–‡ä¿¡æ¯é‡
            context_info = self._compute_context_information(context, samples)
            entropy_results['context_information'].append(context_info)
            
            # åˆ†æç†µçš„åŠ¨æ€å˜åŒ–
            entropy_dynamics = self._analyze_entropy_dynamics(samples)
            entropy_results['entropy_dynamics'].append(entropy_dynamics)
        
        # å¯è§†åŒ–ç†µåˆ†æç»“æœ
        self._visualize_entropy_analysis(entropy_results)
        
        return entropy_results
    
    def _generate_samples(self, model, context: str, num_samples: int) -> List[str]:
        """ç”Ÿæˆæ ·æœ¬ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥ä½¿ç”¨çœŸå®çš„æ¨¡å‹ç”Ÿæˆ
        samples = []
        base_responses = [
            "This is a sample response",
            "Another possible response here",
            "A different kind of answer",
            "Yet another response option",
            "Final sample response text"
        ]
        
        for i in range(num_samples):
            # æ¨¡æ‹Ÿç”Ÿæˆè¿‡ç¨‹çš„éšæœºæ€§
            base_idx = i % len(base_responses)
            variation = f" variation_{i // len(base_responses)}"
            sample = base_responses[base_idx] + variation
            samples.append(sample)
        
        return samples
    
    def _compute_conditional_entropy(self, samples: List[str]) -> float:
        """è®¡ç®—æ¡ä»¶ç†µ"""
        
        if not samples:
            return 0.0
        
        # åŸºäºtokençº§åˆ«çš„ç»Ÿè®¡
        all_tokens = []
        for sample in samples:
            tokens = sample.split()
            all_tokens.extend(tokens)
        
        if not all_tokens:
            return 0.0
        
        # è®¡ç®—tokené¢‘ç‡åˆ†å¸ƒ
        token_counts = Counter(all_tokens)
        total_tokens = len(all_tokens)
        
        # è®¡ç®—ç†µ
        entropy = 0.0
        for count in token_counts.values():
            prob = count / total_tokens
            entropy -= prob * math.log2(prob)
        
        return entropy
    
    def _compute_generation_diversity(self, samples: List[str]) -> float:
        """è®¡ç®—ç”Ÿæˆå¤šæ ·æ€§"""
        
        if len(samples) < 2:
            return 0.0
        
        # ä½¿ç”¨ç¼–è¾‘è·ç¦»è®¡ç®—å¤šæ ·æ€§
        total_distance = 0
        comparisons = 0
        
        for i in range(len(samples)):
            for j in range(i+1, len(samples)):
                distance = self._edit_distance(samples[i].split(), samples[j].split())
                total_distance += distance
                comparisons += 1
        
        avg_distance = total_distance / comparisons if comparisons > 0 else 0
        
        # æ ‡å‡†åŒ–å¤šæ ·æ€§åˆ†æ•°
        max_length = max(len(s.split()) for s in samples) if samples else 1
        diversity = avg_distance / max_length
        
        return min(1.0, diversity)
    
    def _edit_distance(self, seq1: List[str], seq2: List[str]) -> int:
        """è®¡ç®—ç¼–è¾‘è·ç¦»"""
        
        m, n = len(seq1), len(seq2)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        
        # åˆå§‹åŒ–
        for i in range(m + 1):
            dp[i][0] = i
        for j in range(n + 1):
            dp[0][j] = j
        
        # å¡«è¡¨
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if seq1[i-1] == seq2[j-1]:
                    dp[i][j] = dp[i-1][j-1]
                else:
                    dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])
        
        return dp[m][n]
    
    def _compute_context_information(self, context: str, samples: List[str]) -> float:
        """è®¡ç®—ä¸Šä¸‹æ–‡ä¿¡æ¯é‡"""
        
        context_tokens = set(context.lower().split())
        
        if not context_tokens:
            return 0.0
        
        total_overlap = 0
        for sample in samples:
            sample_tokens = set(sample.lower().split())
            overlap = len(context_tokens.intersection(sample_tokens))
            total_overlap += overlap
        
        avg_overlap = total_overlap / (len(samples) * len(context_tokens)) if samples and context_tokens else 0
        
        return avg_overlap
    
    def _analyze_entropy_dynamics(self, samples: List[str]) -> Dict:
        """åˆ†æç†µçš„åŠ¨æ€å˜åŒ–"""
        
        if not samples:
            return {'position_entropies': [], 'entropy_trend': 0}
        
        # æŒ‰ä½ç½®åˆ†æç†µå˜åŒ–
        max_length = max(len(s.split()) for s in samples)
        position_entropies = []
        
        for pos in range(max_length):
            pos_tokens = []
            for sample in samples:
                tokens = sample.split()
                if pos < len(tokens):
                    pos_tokens.append(tokens[pos])
            
            if pos_tokens:
                pos_entropy = self._compute_token_entropy(pos_tokens)
                position_entropies.append(pos_entropy)
            else:
                position_entropies.append(0.0)
        
        # è®¡ç®—ç†µè¶‹åŠ¿
        if len(position_entropies) > 1:
            entropy_trend = np.polyfit(range(len(position_entropies)), position_entropies, 1)[0]
        else:
            entropy_trend = 0.0
        
        return {
            'position_entropies': position_entropies,
            'entropy_trend': entropy_trend,
            'max_entropy_position': np.argmax(position_entropies) if position_entropies else 0
        }
    
    def _compute_token_entropy(self, tokens: List[str]) -> float:
        """è®¡ç®—tokenåˆ—è¡¨çš„ç†µ"""
        
        if not tokens:
            return 0.0
        
        token_counts = Counter(tokens)
        total_tokens = len(tokens)
        
        entropy = 0.0
        for count in token_counts.values():
            prob = count / total_tokens
            entropy -= prob * math.log2(prob)
        
        return entropy
    
    def _visualize_entropy_analysis(self, results: Dict):
        """å¯è§†åŒ–ç†µåˆ†æç»“æœ"""
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # æ¡ä»¶ç†µåˆ†å¸ƒ
        if results['conditional_entropies']:
            axes[0, 0].hist(results['conditional_entropies'], bins=15, alpha=0.7, edgecolor='black')
            axes[0, 0].set_title('æ¡ä»¶ç†µåˆ†å¸ƒ')
            axes[0, 0].set_xlabel('æ¡ä»¶ç†µ (bits)')
            axes[0, 0].set_ylabel('é¢‘æ¬¡')
            axes[0, 0].grid(True, alpha=0.3)
        
        # ç”Ÿæˆå¤šæ ·æ€§ vs æ¡ä»¶ç†µ
        if results['conditional_entropies'] and results['generation_diversity']:
            axes[0, 1].scatter(results['conditional_entropies'], results['generation_diversity'], alpha=0.6)
            axes[0, 1].set_xlabel('æ¡ä»¶ç†µ')
            axes[0, 1].set_ylabel('ç”Ÿæˆå¤šæ ·æ€§')
            axes[0, 1].set_title('ç†µ-å¤šæ ·æ€§å…³ç³»')
            axes[0, 1].grid(True, alpha=0.3)
        
        # ä¸Šä¸‹æ–‡ä¿¡æ¯åˆ©ç”¨
        if results['context_information']:
            axes[1, 0].hist(results['context_information'], bins=15, alpha=0.7, 
                           edgecolor='black', color='green')
            axes[1, 0].set_title('ä¸Šä¸‹æ–‡ä¿¡æ¯åˆ©ç”¨')
            axes[1, 0].set_xlabel('ä¿¡æ¯åˆ©ç”¨ç‡')
            axes[1, 0].set_ylabel('é¢‘æ¬¡')
            axes[1, 0].grid(True, alpha=0.3)
        
        # ç†µåŠ¨æ€å˜åŒ–ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªæ ·æœ¬ä½œä¸ºç¤ºä¾‹ï¼‰
        if results['entropy_dynamics'] and results['entropy_dynamics'][0]['position_entropies']:
            sample_dynamics = results['entropy_dynamics'][0]
            positions = range(len(sample_dynamics['position_entropies']))
            entropies = sample_dynamics['position_entropies']
            
            axes[1, 1].plot(positions, entropies, 'b-o', linewidth=2)
            axes[1, 1].set_title(f'ä½ç½®ç†µå˜åŒ– (è¶‹åŠ¿: {sample_dynamics["entropy_trend"]:.3f})')
            axes[1, 1].set_xlabel('ä½ç½®')
            axes[1, 1].set_ylabel('ç†µå€¼')
            axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

# é«˜çº§ä¿¡æ¯è®ºåˆ†ææ¼”ç¤º
def demonstrate_information_theoretic_analysis():
    """æ¼”ç¤ºä¿¡æ¯è®ºåˆ†æ"""
    
    print("="*60)
    print("ç”Ÿæˆè¿‡ç¨‹ä¿¡æ¯è®ºåˆ†æ")
    print("="*60)
    
    # åˆ›å»ºåˆ†æå™¨
    info_analyzer = InformationTheoreticAnalyzer()
    
    # å‡†å¤‡æµ‹è¯•ä¸Šä¸‹æ–‡
    test_contexts = [
        "What is machine learning?",
        "Explain the concept of entropy.",
        "How do neural networks work?",
        "Describe the process of photosynthesis.",
        "What are the benefits of renewable energy?"
    ]
    
    print(f"\nåˆ†æ {len(test_contexts)} ä¸ªä¸Šä¸‹æ–‡çš„ç”Ÿæˆç‰¹æ€§")
    
    # æ‰§è¡Œä¿¡æ¯è®ºåˆ†æ
    entropy_results = info_analyzer.analyze_generation_entropy(
        model=None,  # ä½¿ç”¨ç®€åŒ–æ¨¡å‹
        contexts=test_contexts,
        num_samples=20
    )
    
    # åˆ†æç»“æœ
    avg_entropy = np.mean(entropy_results['conditional_entropies'])
    avg_diversity = np.mean(entropy_results['generation_diversity'])
    avg_context_info = np.mean(entropy_results['context_information'])
    
    print(f"\n=== ä¿¡æ¯è®ºåˆ†æç»“æœ ===")
    print(f"å¹³å‡æ¡ä»¶ç†µ: {avg_entropy:.3f} bits")
    print(f"å¹³å‡ç”Ÿæˆå¤šæ ·æ€§: {avg_diversity:.3f}")
    print(f"å¹³å‡ä¸Šä¸‹æ–‡ä¿¡æ¯åˆ©ç”¨: {avg_context_info:.3f}")
    
    # åˆ†æç†µåŠ¨æ€
    avg_entropy_trend = np.mean([d['entropy_trend'] for d in entropy_results['entropy_dynamics']])
    print(f"å¹³å‡ç†µå˜åŒ–è¶‹åŠ¿: {avg_entropy_trend:.4f} bits/position")
    
    if avg_entropy_trend > 0:
        print("â†’ ç”Ÿæˆè¿‡ç¨‹ä¸­ä¸ç¡®å®šæ€§é€’å¢ï¼ˆå‘æ•£è¶‹åŠ¿ï¼‰")
    elif avg_entropy_trend < -0.01:
        print("â†’ ç”Ÿæˆè¿‡ç¨‹ä¸­ä¸ç¡®å®šæ€§é€’å‡ï¼ˆæ”¶æ•›è¶‹åŠ¿ï¼‰")
    else:
        print("â†’ ç”Ÿæˆè¿‡ç¨‹ä¸­ä¸ç¡®å®šæ€§ç›¸å¯¹ç¨³å®š")

# è¿è¡Œä¿¡æ¯è®ºåˆ†ææ¼”ç¤º
demonstrate_information_theoretic_analysis()
```

## 1.3 é•¿åºåˆ—å»ºæ¨¡çš„æ•°å­¦æŒ‘æˆ˜

### è¯¯å·®ä¼ æ’­çš„æ•°å­¦å»ºæ¨¡

åœ¨é•¿åºåˆ—ç”Ÿæˆä¸­ï¼Œæ—©æœŸçš„é¢„æµ‹é”™è¯¯ä¼šé€šè¿‡è‡ªå›å½’æœºåˆ¶ä¼ æ’­å¹¶æ”¾å¤§ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æ•°å­¦æ¡†æ¶æ¥åˆ†æè¿™ä¸ªç°è±¡ï¼š

**è¯¯å·®ä¼ æ’­æ¨¡å‹**ï¼š
è®¾$\epsilon_t$ä¸ºæ—¶é—´æ­¥$t$çš„é¢„æµ‹è¯¯å·®ï¼Œåˆ™è¯¯å·®åœ¨åºåˆ—ä¸­çš„ä¼ æ’­å¯ä»¥å»ºæ¨¡ä¸ºï¼š
$$\epsilon_{t+1} = \alpha \epsilon_t + \beta \xi_t + \gamma f(y_{<t})$$

å…¶ä¸­ï¼š
- $\alpha$æ˜¯è¯¯å·®çš„è‡ªç›¸å…³ç³»æ•°
- $\beta$æ˜¯æ–°è¯¯å·®çš„å¼•å…¥ç³»æ•°  
- $\xi_t$æ˜¯ç‹¬ç«‹çš„å™ªå£°é¡¹
- $f(y_{<t})$æ˜¯å†å²ä¾èµ–é¡¹

**ç´¯ç§¯è¯¯å·®çš„æœŸæœ›**ï¼š
$$\mathbb{E}[\epsilon_T] = \alpha^T \epsilon_0 + \sum_{t=1}^{T} \alpha^{T-t} \mathbb{E}[\beta \xi_t + \gamma f(y_{<t})]$$

å½“$|\alpha| < 1$æ—¶ï¼Œè¯¯å·®ä¼šé€æ¸è¡°å‡ï¼›å½“$|\alpha| \geq 1$æ—¶ï¼Œè¯¯å·®ä¼šæŒ‡æ•°å¢é•¿ã€‚

```python
class LongSequenceAnalyzer:
    """é•¿åºåˆ—å»ºæ¨¡åˆ†æå™¨"""
    
    def __init__(self):
        self.error_propagation_models = {}
        
    def analyze_error_propagation(self, sequence_lengths: List[int], 
                                error_rates: List[float]) -> Dict:
        """åˆ†æè¯¯å·®ä¼ æ’­ç‰¹æ€§"""
        
        print("=== é•¿åºåˆ—è¯¯å·®ä¼ æ’­åˆ†æ ===")
        
        propagation_results = {
            'error_growth': [],
            'stability_analysis': [],
            'critical_lengths': [],
            'mitigation_strategies': []
        }
        
        for seq_len in sequence_lengths:
            for error_rate in error_rates:
                # æ¨¡æ‹Ÿè¯¯å·®ä¼ æ’­è¿‡ç¨‹
                error_trajectory = self._simulate_error_propagation(seq_len, error_rate)
                
                # åˆ†æè¯¯å·®å¢é•¿
                growth_analysis = self._analyze_error_growth(error_trajectory)
                propagation_results['error_growth'].append(growth_analysis)
                
                # ç¨³å®šæ€§åˆ†æ
                stability = self._analyze_stability(error_trajectory)
                propagation_results['stability_analysis'].append(stability)
        
        # æ‰¾å‡ºä¸´ç•Œé•¿åº¦
        critical_lengths = self._find_critical_lengths(propagation_results)
        propagation_results['critical_lengths'] = critical_lengths
        
        # å¯è§†åŒ–è¯¯å·®ä¼ æ’­
        self._visualize_error_propagation(propagation_results, sequence_lengths, error_rates)
        
        return propagation_results
    
    def _simulate_error_propagation(self, seq_length: int, base_error_rate: float, 
                                  alpha: float = 0.8, beta: float = 0.1) -> List[float]:
        """æ¨¡æ‹Ÿè¯¯å·®ä¼ æ’­è¿‡ç¨‹"""
        
        errors = [base_error_rate]  # åˆå§‹è¯¯å·®
        
        for t in range(1, seq_length):
            # è¯¯å·®ä¼ æ’­æ¨¡å‹ï¼šÎµ_{t+1} = Î±*Îµ_t + Î²*Î¾_t
            propagated_error = alpha * errors[-1]
            new_error = beta * np.random.normal(0, base_error_rate)
            
            # è€ƒè™‘ä¸Šä¸‹æ–‡ä¸¢å¤±çš„å½±å“
            context_decay = 1.0 - (t / seq_length) * 0.2  # è½»å¾®çš„ä¸Šä¸‹æ–‡è¡°å‡
            
            total_error = (propagated_error + new_error) / context_decay
            
            # è¯¯å·®ä¸èƒ½è¶…è¿‡1
            total_error = min(1.0, max(0.0, total_error))
            errors.append(total_error)
        
        return errors
    
    def _analyze_error_growth(self, error_trajectory: List[float]) -> Dict:
        """åˆ†æè¯¯å·®å¢é•¿æ¨¡å¼"""
        
        if len(error_trajectory) < 2:
            return {'growth_rate': 0, 'growth_type': 'stable'}
        
        # è®¡ç®—å¢é•¿ç‡
        initial_error = error_trajectory[0]
        final_error = error_trajectory[-1]
        
        if initial_error > 0:
            growth_factor = final_error / initial_error
            growth_rate = (growth_factor - 1) / len(error_trajectory)
        else:
            growth_rate = 0
        
        # åˆ†ç±»å¢é•¿ç±»å‹
        if growth_rate > 0.01:
            growth_type = 'exponential'
        elif growth_rate > 0.001:
            growth_type = 'linear'
        elif growth_rate > -0.001:
            growth_type = 'stable'
        else:
            growth_type = 'decay'
        
        # è®¡ç®—è¯¯å·®æ–¹å·®
        error_variance = np.var(error_trajectory)
        
        return {
            'growth_rate': growth_rate,
            'growth_type': growth_type,
            'final_error': final_error,
            'error_variance': error_variance,
            'trajectory': error_trajectory
        }
    
    def _analyze_stability(self, error_trajectory: List[float]) -> Dict:
        """åˆ†æè¯¯å·®ç¨³å®šæ€§"""
        
        if len(error_trajectory) < 10:
            return {'stability_score': 0, 'is_stable': False}
        
        # è®¡ç®—ååŠæ®µçš„æ ‡å‡†å·®
        mid_point = len(error_trajectory) // 2
        late_errors = error_trajectory[mid_point:]
        
        stability_score = 1.0 / (1.0 + np.std(late_errors))
        is_stable = np.std(late_errors) < 0.1
        
        # è¶‹åŠ¿åˆ†æ
        if len(late_errors) > 1:
            trend = np.polyfit(range(len(late_errors)), late_errors, 1)[0]
        else:
            trend = 0
        
        return {
            'stability_score': stability_score,
            'is_stable': is_stable,
            'late_error_std': np.std(late_errors),
            'trend': trend
        }
    
    def _find_critical_lengths(self, propagation_results: Dict) -> List[int]:
        """æ‰¾å‡ºä¸´ç•Œåºåˆ—é•¿åº¦"""
        
        critical_lengths = []
        
        for growth_data in propagation_results['error_growth']:
            trajectory = growth_data.get('trajectory', [])
            
            # æ‰¾å‡ºè¯¯å·®å¼€å§‹å¿«é€Ÿå¢é•¿çš„ä½ç½®
            for i in range(1, len(trajectory)):
                if i > 5:  # è‡³å°‘5æ­¥åæ‰è€ƒè™‘
                    error_increase = trajectory[i] - trajectory[i-1]
                    if error_increase > 0.05:  # 5%çš„è¯¯å·®å¢é•¿é˜ˆå€¼
                        critical_lengths.append(i)
                        break
        
        return critical_lengths
    
    def _visualize_error_propagation(self, results: Dict, seq_lengths: List[int], 
                                   error_rates: List[float]):
        """å¯è§†åŒ–è¯¯å·®ä¼ æ’­åˆ†æ"""
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # é€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§çš„è¯¯å·®è½¨è¿¹è¿›è¡Œå¯è§†åŒ–
        sample_trajectories = [growth['trajectory'] for growth in results['error_growth'][:5]]
        
        # è¯¯å·®ä¼ æ’­è½¨è¿¹
        for i, trajectory in enumerate(sample_trajectories):
            axes[0, 0].plot(trajectory, label=f'è½¨è¿¹ {i+1}', alpha=0.7)
        
        axes[0, 0].set_title('è¯¯å·®ä¼ æ’­è½¨è¿¹')
        axes[0, 0].set_xlabel('åºåˆ—ä½ç½®')
        axes[0, 0].set_ylabel('ç´¯ç§¯è¯¯å·®')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # å¢é•¿ç‡åˆ†å¸ƒ
        growth_rates = [g['growth_rate'] for g in results['error_growth']]
        axes[0, 1].hist(growth_rates, bins=15, alpha=0.7, edgecolor='black')
        axes[0, 1].set_title('è¯¯å·®å¢é•¿ç‡åˆ†å¸ƒ')
        axes[0, 1].set_xlabel('å¢é•¿ç‡')
        axes[0, 1].set_ylabel('é¢‘æ¬¡')
        axes[0, 1].grid(True, alpha=0.3)
        
        # ç¨³å®šæ€§åˆ†æ
        stability_scores = [s['stability_score'] for s in results['stability_analysis']]
        is_stable = [s['is_stable'] for s in results['stability_analysis']]
        
        stable_count = sum(is_stable)
        unstable_count = len(is_stable) - stable_count
        
        axes[1, 0].bar(['ç¨³å®š', 'ä¸ç¨³å®š'], [stable_count, unstable_count], alpha=0.7)
        axes[1, 0].set_title('ç¨³å®šæ€§ç»Ÿè®¡')
        axes[1, 0].set_ylabel('æ•°é‡')
        axes[1, 0].grid(True, alpha=0.3)
        
        # ä¸´ç•Œé•¿åº¦åˆ†æ
        if results['critical_lengths']:
            axes[1, 1].hist(results['critical_lengths'], bins=15, alpha=0.7, 
                           edgecolor='black', color='red')
            axes[1, 1].set_title('ä¸´ç•Œé•¿åº¦åˆ†å¸ƒ')
            axes[1, 1].set_xlabel('ä¸´ç•Œåºåˆ—é•¿åº¦')
            axes[1, 1].set_ylabel('é¢‘æ¬¡')
            axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

def comprehensive_mathematical_analysis():
    """è‡ªå›å½’ç”Ÿæˆæ•°å­¦åŸç†ç»¼åˆåˆ†æ"""
    
    print("="*60)
    print("è‡ªå›å½’ç”Ÿæˆæ•°å­¦åŸç† - ç»¼åˆåˆ†æ")
    print("="*60)
    
    # 1. é•¿åºåˆ—åˆ†æ
    print("\n1. é•¿åºåˆ—è¯¯å·®ä¼ æ’­åˆ†æ")
    
    long_seq_analyzer = LongSequenceAnalyzer()
    
    # æµ‹è¯•ä¸åŒçš„åºåˆ—é•¿åº¦å’Œè¯¯å·®ç‡
    test_lengths = [10, 20, 50, 100, 200]
    test_error_rates = [0.01, 0.05, 0.1, 0.2]
    
    propagation_results = long_seq_analyzer.analyze_error_propagation(
        test_lengths, test_error_rates
    )
    
    # åˆ†æç»“æœæ±‡æ€»
    avg_growth_rate = np.mean([g['growth_rate'] for g in propagation_results['error_growth']])
    stable_ratio = np.mean([s['is_stable'] for s in propagation_results['stability_analysis']])
    
    print(f"å¹³å‡è¯¯å·®å¢é•¿ç‡: {avg_growth_rate:.4f}")
    print(f"ç¨³å®šåºåˆ—æ¯”ä¾‹: {stable_ratio:.3f}")
    
    if propagation_results['critical_lengths']:
        avg_critical_length = np.mean(propagation_results['critical_lengths'])
        print(f"å¹³å‡ä¸´ç•Œé•¿åº¦: {avg_critical_length:.1f}")
    
    # 2. æ•°å­¦å…¬å¼æ€»ç»“
    print(f"\n2. å…³é”®æ•°å­¦å…¬å¼æ€»ç»“")
    
    formulas = {
        "é“¾å¼åˆ†è§£": "P(yâ‚:T) = âˆáµ¢ P(yáµ¢|y<áµ¢)",
        "æ¡ä»¶ç†µ": "H(Y|X) = -âˆ‘ P(y|x) log P(y|x)",
        "äº’ä¿¡æ¯": "I(X;Y) = H(Y) - H(Y|X)",
        "è¯¯å·®ä¼ æ’­": "Îµâ‚œâ‚Šâ‚ = Î±Â·Îµâ‚œ + Î²Â·Î¾â‚œ",
        "æœç´¢å¤æ‚åº¦": "O(|V|^T) for vocabulary V, length T"
    }
    
    for name, formula in formulas.items():
        print(f"  {name}: {formula}")
    
    # 3. å®è·µæŒ‡å¯¼
    print(f"\n3. å®è·µæŒ‡å¯¼åŸåˆ™")
    
    principles = [
        "ğŸ¯ é€‰æ‹©åˆé€‚çš„ä¸Šä¸‹æ–‡çª—å£å¤§å°å¹³è¡¡æ•ˆç‡ä¸å‡†ç¡®æ€§",
        "ğŸ“Š ç›‘æ§æ¡ä»¶ç†µä»¥è¯„ä¼°ç”Ÿæˆçš„ä¸ç¡®å®šæ€§",
        "ğŸ” ä½¿ç”¨äº’ä¿¡æ¯åˆ†æä¸Šä¸‹æ–‡çš„æœ‰æ•ˆæ€§",
        "âš ï¸ åœ¨é•¿åºåˆ—ç”Ÿæˆä¸­æ³¨æ„è¯¯å·®ç´¯ç§¯é—®é¢˜", 
        "âš–ï¸ åœ¨å¤šæ ·æ€§ä¸ä¸€è‡´æ€§ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ç‚¹",
        "ğŸ› ï¸ æ ¹æ®ä»»åŠ¡ç‰¹æ€§è°ƒæ•´ç”Ÿæˆç­–ç•¥å‚æ•°"
    ]
    
    for principle in principles:
        print(f"  {principle}")
    
    print(f"\n=== è‡ªå›å½’ç”Ÿæˆæ•°å­¦åŸç†åˆ†æå®Œæˆ ===")
    print(f"è¿™äº›æ•°å­¦åŸºç¡€ä¸ºç†è§£å’Œä¼˜åŒ–è¯­è¨€æ¨¡å‹ç”Ÿæˆæä¾›äº†ç†è®ºæ”¯æ’‘")

# è¿è¡Œç»¼åˆåˆ†æ
comprehensive_mathematical_analysis()
```

è¿™æ ·æˆ‘å°±å®Œæˆäº†ç¬¬06ç« ç¬¬01èŠ‚"è‡ªå›å½’ç”Ÿæˆæ•°å­¦åŸç†"çš„å®Œæ•´å†…å®¹ã€‚è¿™ä¸€èŠ‚æ·±å…¥åˆ†æäº†ï¼š

1. **æ¡ä»¶æ¦‚ç‡åˆ†è§£çš„æ•°å­¦åŸºç¡€**ï¼šé“¾å¼æ³•åˆ™ã€å› æœæ€§çº¦æŸã€æ¡ä»¶ç‹¬ç«‹å‡è®¾çš„è¯¯å·®åˆ†æ
2. **æœç´¢ç©ºé—´å¤æ‚æ€§åˆ†æ**ï¼šæŒ‡æ•°å¢é•¿çš„æ•°å­¦å»ºæ¨¡ã€æœ‰æ•ˆæœç´¢æ¯”ä¾‹ã€å¢é•¿ç‡åˆ†æ
3. **æ›å…‰åå·®çš„æ•°å­¦å»ºæ¨¡**ï¼šTeacher forcing vs Free runningã€è¯¯å·®ä¼ æ’­æœºåˆ¶ã€åˆ†å¸ƒåç§»åˆ†æ
4. **ä¿¡æ¯è®ºåˆ†æ**ï¼šç”Ÿæˆè¿‡ç¨‹çš„ç†µåˆ†æã€å¤šæ ·æ€§ä¸ä¸€è‡´æ€§æƒè¡¡ã€ä¸Šä¸‹æ–‡ä¿¡æ¯åˆ©ç”¨
5. **é•¿åºåˆ—å»ºæ¨¡æŒ‘æˆ˜**ï¼šè¯¯å·®ä¼ æ’­çš„æ•°å­¦æ¨¡å‹ã€ç¨³å®šæ€§åˆ†æã€ä¸´ç•Œé•¿åº¦è¯†åˆ«

æ¯ä¸ªæ¦‚å¿µéƒ½æœ‰ä¸¥æ ¼çš„æ•°å­¦æ¨å¯¼ã€å®Œæ•´çš„ä»£ç å®ç°å’Œå¯è§†åŒ–åˆ†æï¼Œä¸ºè¯»è€…æä¾›äº†ultra-deepçš„ç†è®ºç†è§£å’Œå®è·µæŒ‡å¯¼ã€‚

<function_calls>
<invoke name="TodoWrite">
<parameter name="todos">[{"content": "Research best practices for technical tutorial writing", "status": "completed", "priority": "high", "id": "1"}, {"content": "Design comprehensive folder structure for MiniGPT handbook", "status": "completed", "priority": "high", "id": "2"}, {"content": "Create detailed outline with ultra-deep analysis approach", "status": "completed", "priority": "high", "id": "3"}, {"content": "Write handbook introduction and navigation guide", "status": "completed", "priority": "high", "id": "4"}, {"content": "Implement chapter-by-chapter content with mathematical rigor", "status": "in_progress", "priority": "high", "id": "5"}, {"content": "Write Chapter 1: Mathematical Foundations", "status": "completed", "priority": "high", "id": "6"}, {"content": "Write Chapter 2: Transformer Architecture", "status": "completed", "priority": "high", "id": "7"}, {"content": "Write Chapter 3: Pre-training Theory and Implementation", "status": "completed", "priority": "high", "id": "8"}, {"content": "Write Chapter 4: Supervised Fine-tuning Deep Dive", "status": "completed", "priority": "high", "id": "14"}, {"content": "Write Chapter 5: Reinforcement Learning from Human Feedback", "status": "completed", "priority": "high", "id": "19"}, {"content": "Write Chapter 6: Generation and Decoding Strategies", "status": "in_progress", "priority": "high", "id": "25"}, {"content": "Create Chapter 6 folder structure and README", "status": "completed", "priority": "medium", "id": "28"}, {"content": "Write Chapter 6 Section 1: Autoregressive Generation Mathematical Principles", "status": "completed", "priority": "high", "id": "29"}, {"content": "Write Chapter 6 Section 2: Classical Decoding Algorithms Deep Analysis", "status": "in_progress", "priority": "high", "id": "30"}, {"content": "Write Chapter 6 Section 3: Advanced Sampling Strategies and Control", "status": "pending", "priority": "high", "id": "31"}, {"content": "Write Chapter 6 Section 4: Generation Quality Control and Optimization", "status": "pending", "priority": "high", "id": "32"}]