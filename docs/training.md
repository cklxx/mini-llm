# ğŸ‹ï¸ è®­ç»ƒä¸ä¼˜åŒ–

`src/training` æ¨¡å—å°è£…äº†ä»æ•°æ®é›†åˆ°ä¼˜åŒ–å™¨çš„å®Œæ•´è®­ç»ƒé€»è¾‘ã€‚æœ¬ç¯‡æ–‡æ¡£ä»‹ç»å…³é”®ç±»ã€é»˜è®¤ç­–ç•¥ä»¥åŠå¦‚ä½•æ‰©å±•ã€‚

## æ•°æ®é›†å°è£…
- **LanguageModelingDataset**ï¼šæ¥æ”¶çº¯æ–‡æœ¬åˆ—è¡¨ï¼Œè‡ªåŠ¨ç¼–ç ã€æˆªæ–­/å¡«å……åˆ° `max_length`ï¼Œå¹¶å°†æ ‡ç­¾å¯¹é½è‡³ä¸‹ä¸€ä¸ª tokenã€‚
- **ConversationDataset**ï¼šé¢å‘ç›‘ç£å¾®è°ƒ (SFT)ï¼Œè¾“å…¥/è¾“å‡ºåˆ†åˆ«ç¼–ç å¹¶æ‹¼æ¥æˆ `input_ids` ä¸ `labels`ï¼Œåªå¯¹åŠ©æ‰‹å›å¤éƒ¨åˆ†è®¡ç®—æŸå¤±ã€‚

## è®­ç»ƒå™¨
### PreTrainer
- ä¼˜åŒ–å™¨ï¼š`AdamW(lr=1e-4, weight_decay=0.01)`
- å­¦ä¹ ç‡è°ƒåº¦ï¼š`CosineAnnealingLR`ï¼ˆ`T_max=1000`ï¼‰
- è®­ç»ƒå¾ªç¯ï¼šå‰å‘ â†’ è®¡ç®—äº¤å‰ç†µæŸå¤±ï¼ˆå¿½ç•¥ PADï¼‰â†’ åå‘ â†’ æ¢¯åº¦è£å‰ªï¼ˆ`max_norm=1.0`ï¼‰â†’ `optimizer.step()` â†’ `scheduler.step()`
- è¿›åº¦ï¼šä½¿ç”¨ `tqdm` æ‰“å° batch çº§è¿›åº¦ä¸ç»Ÿè®¡ä¿¡æ¯

### SFTTrainer
- ç»§æ‰¿è‡ª `PreTrainer`
- è°ƒæ•´å­¦ä¹ ç‡ä¸º `5e-5`
- é‡å†™ `compute_loss`ï¼Œé€šè¿‡æ©ç ç¡®ä¿åªå¯¹æ ‡ç­¾ä¸­é PAD çš„ä½ç½®ç´¯ç§¯æŸå¤±ï¼Œé€‚åˆå¯¹è¯ç±»ä»»åŠ¡

### DPOTrainer
- åŒæ—¶ç»´æŠ¤ç­–ç•¥æ¨¡å‹ä¸å‚è€ƒæ¨¡å‹ï¼Œå†»ç»“å‚è€ƒæ¨¡å‹å‚æ•°
- `compute_dpo_loss` è®¡ç®—é€‰æ‹©/æ‹’ç»æ ·æœ¬ç›¸å¯¹æ¦‚ç‡å·®ï¼Œæ”¯æŒå¯è°ƒçš„ `beta`
- ä¸ `create_trainer('dpo', ...)` å·¥å‚å‡½æ•°é…åˆä½¿ç”¨

## å†…å­˜ä¸æ€§èƒ½ä¼˜åŒ–
`memory_optimizer.py` æä¾›ä¸€ç³»åˆ—å·¥å…·ï¼š

- **MemoryConfig**ï¼šé›†ä¸­ç®¡ç† AMPã€æ¢¯åº¦ç´¯ç§¯ã€åŠ¨æ€ batchã€å†…å­˜ç›‘æ§ç­‰å¼€å…³
- **MemoryMonitor**ï¼šæ”¶é›† CUDA/MPS/CPU å†…å­˜å ç”¨ï¼Œå¹¶æ”¯æŒå‘¨æœŸæ€§æ¸…ç†ç¼“å­˜
- **MixedPrecisionManager**ï¼šå°è£… `torch.cuda.amp` çš„ `autocast` ä¸ `GradScaler`
- **GradientAccumulator**ï¼šå®ç°æ¢¯åº¦ç´¯ç§¯ä¸è‡ªåŠ¨æ¢¯åº¦è£å‰ª
- **DynamicBatchSizer**ï¼šæ ¹æ® OOM æƒ…å†µè‡ªåŠ¨ç¼©å‡ batch å¤§å°

åœ¨å®é™…è®­ç»ƒä¸­ï¼Œå¯ä»¥ç»“åˆè¿™äº›ç»„ä»¶ï¼š
```python
import torch
from src.training.memory_optimizer import MemoryConfig, MemoryOptimizer

# å‡è®¾ dataloaderã€tokenizerã€trainerã€device å·²å°±ç»ª
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
memory_opt = MemoryOptimizer(model, MemoryConfig(enable_amp=True, gradient_accumulation_steps=4), device)

for step, batch in enumerate(dataloader):
    if isinstance(batch, dict):
        input_ids = batch["input_ids"].to(device)
        labels = batch["labels"].to(device)
    else:
        input_ids = batch.to(device)
        labels = torch.cat(
            [input_ids[:, 1:], torch.full((input_ids.size(0), 1), tokenizer.pad_id, device=device)],
            dim=1,
        )

    with memory_opt.optimize_step_context(optimizer) as ctx:
        logits = model(input_ids)
        loss = trainer.compute_loss(logits, labels)
        scaled_loss = memory_opt.compute_loss(loss)
        memory_opt.backward(scaled_loss)

        if ctx["should_update"]:
            # å‚æ•°æ›´æ–°ç”±ä¸Šä¸‹æ–‡è‡ªåŠ¨å®Œæˆ
            pass
```

## å·¥å‚å‡½æ•°
`create_trainer(training_type, ...)` æ ¹æ®å­—ç¬¦ä¸²åˆ›å»ºé¢„è®­ç»ƒã€SFT æˆ– DPO è®­ç»ƒå™¨ï¼Œä¾¿äºåœ¨è„šæœ¬ä¸­åˆ‡æ¢ä¸åŒé˜¶æ®µçš„è®­ç»ƒé€»è¾‘ã€‚

## æ‰©å±•å»ºè®®
1. æ–°çš„è®­ç»ƒé˜¶æ®µï¼šç»§æ‰¿ `PreTrainer` å¹¶å®ç°è‡ªå®šä¹‰çš„ `compute_loss`
2. è‡ªå®šä¹‰ä¼˜åŒ–å™¨/è°ƒåº¦å™¨ï¼šåœ¨å­ç±» `__init__` ä¸­æ›¿æ¢ä¼˜åŒ–å™¨æˆ–è°ƒåº¦å™¨
3. æ–°çš„å†…å­˜ç­–ç•¥ï¼šæ‰©å±• `MemoryConfig` å­—æ®µå¹¶åœ¨ `TrainingContext` ä¸­è¯»å–

é€šè¿‡ä»¥ä¸Šæ¨¡å—ï¼ŒMini-LLM å¯ä»¥è¦†ç›–ä»åŸºç¡€è¯­è¨€æ¨¡å‹è®­ç»ƒåˆ°åå¥½ä¼˜åŒ–çš„å¸¸è§éœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒä»£ç ç»“æ„æ¸…æ™°æ˜“äºä¿®æ”¹ã€‚
