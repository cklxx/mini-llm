# ğŸ—£ï¸ æ¨ç†ä¸æ–‡æœ¬ç”Ÿæˆ

Mini-LLM çš„æ¨ç†ç»„ä»¶ä½äº `src/inference/generator.py`ã€‚`TextGenerator` åœ¨ä¿æŒ API ç®€æ´çš„åŒæ—¶ï¼Œæä¾›å¤šç§ä¸»æµçš„è§£ç ç­–ç•¥ï¼Œå¹¶åœ¨åˆå§‹åŒ–æ—¶å°†æ¨¡å‹ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡ã€åˆ‡æ¢åˆ° `eval()` æ¨¡å¼ï¼Œé¿å…è®­ç»ƒæ€å‰¯ä½œç”¨ã€‚ã€F:src/inference/generator.pyâ€ L27-L44ã€‘

## GenerationConfig å­—æ®µè¯¦è§£

`GenerationConfig` ä¸ºæ‰€æœ‰ç”Ÿæˆç­–ç•¥æä¾›ç»Ÿä¸€å…¥å£ï¼Œè¡¨æ ¼åˆ—å‡ºäº†å¸¸ç”¨å­—æ®µåŠå…¶å½±å“ã€‚ã€F:src/inference/generator.pyâ€ L12-L25ã€‘

| å­—æ®µ | ä½œç”¨ | è°ƒå‚å»ºè®® |
| ---- | ---- | -------- |
| `max_length` | æ§åˆ¶è¿½åŠ  token çš„æœ€å¤§æ­¥æ•°ï¼›åˆ°è¾¾ä¸Šé™å³åœæ­¢ | è®­ç»ƒé˜¶æ®µè‹¥ä¸Šä¸‹æ–‡å¾ˆé•¿ï¼Œå¯ä½¿ç”¨è¾ƒå°çš„ `max_length` ä¿æŒå“åº”ç®€çŸ­ |
| `temperature` | å¯¹ logits ç¼©æ”¾ï¼Œæ¸©åº¦è¶Šé«˜è¶Šéšæœº | æŒ‡ä»¤å¾®è°ƒæ¨¡å‹å¸¸ç”¨ `0.7~0.9`ï¼Œè‹¥è¾“å‡ºè¿‡äºå‘æ•£å¯ä¸‹é™ |
| `top_k` / `top_p` | é™åˆ¶å€™é€‰ token æ•°é‡æˆ–ç´¯è®¡æ¦‚ç‡ï¼Œæå‡å¤šæ ·æ€§ | ä¸ `temperature` è”åˆä½¿ç”¨ï¼›ç”Ÿæˆäº‹å®ç±»å›ç­”æ—¶å¯è°ƒä½ |
| `repetition_penalty` | å¯¹å·²ç”Ÿæˆ token çš„ logits è¿›è¡Œæƒ©ç½šï¼Œé˜²æ­¢å¤è¯» | å¤§äº 1 ä¼šé™ä½é‡å¤æ¦‚ç‡ï¼›è¿‡é«˜å¯èƒ½å¯¼è‡´è¯­å¥ä¸è¿è´¯ã€F:src/inference/generator.pyâ€ L45-L78ã€‘ |
| `num_beams` / `early_stopping` | æŸæœç´¢å®½åº¦ä¸åœæ­¢æ¡ä»¶ | `num_beams>1` æ—¶è‡ªåŠ¨èµ° `beam_search` åˆ†æ”¯ï¼Œå¹¶åœ¨æ‰€æœ‰ beam ç”Ÿæˆ EOS æ—¶æå‰ç»“æŸã€F:src/inference/generator.pyâ€ L167-L225ã€‘ |
| `do_sample` | æ˜¯å¦å¯ç”¨é‡‡æ ·æ¨¡å¼ | False æ—¶åœ¨ `sample_generate` å†…ä¼šé€€åŒ–ä¸º argmaxï¼Œç›¸å½“äºè´ªå¿ƒè§£ç ã€F:src/inference/generator.pyâ€ L125-L166ã€‘ |

## è§£ç æµç¨‹æ‹†è§£

- **è´ªå¿ƒæœç´¢**ï¼šå¾ªç¯å‰å‘æ¨ç†å¹¶é€‰æ‹©æœ€å¤§æ¦‚ç‡ tokenï¼Œé€‚åˆéªŒè¯æˆ–ç¡®å®šæ€§å›å¤ã€‚ã€F:src/inference/generator.pyâ€ L103-L124ã€‘
- **éšæœºé‡‡æ ·**ï¼šå…ˆæŒ‰æ¸©åº¦ç¼©æ”¾ logitsï¼Œå†ä¾æ¬¡åº”ç”¨é‡å¤æƒ©ç½šã€Top-k/Top-p è¿‡æ»¤ï¼Œå¹¶è°ƒç”¨ `torch.multinomial` é‡‡æ ·ï¼›è‹¥ `do_sample=False` åˆ™ä¼šæ”¹ä¸ºå–æœ€å¤§æ¦‚ç‡ï¼Œå…¼å®¹é‡‡æ ·å’Œç¡®å®šæ€§åœºæ™¯ã€‚ã€F:src/inference/generator.pyâ€ L125-L166ã€‘
- **æŸæœç´¢**ï¼šç»´æŠ¤å¤šä¸ªå€™é€‰ beamï¼Œè®¡ç®—ç´¯è®¡å¯¹æ•°æ¦‚ç‡å¹¶å®æ—¶æ›´æ–°æœ€ä½³åºåˆ—ï¼›å½“ `early_stopping=True` ä¸”æ‰€æœ‰ beam éƒ½ç”Ÿæˆ EOS æ—¶æå‰è¿”å›ã€‚ã€F:src/inference/generator.pyâ€ L167-L225ã€‘
- **è¾…åŠ©å·¥å…·**ï¼š`apply_repetition_penalty` å¯¹å·²å‡ºç° token è°ƒæ•´ logitsï¼Œ`top_k_filtering`/`top_p_filtering` è´Ÿè´£è£å‰ªä¸ç¬¦åˆæ¡ä»¶çš„å€™é€‰ã€‚ã€F:src/inference/generator.pyâ€ L45-L102ã€‘

è°ƒç”¨ `TextGenerator.generate` æ—¶ï¼Œä¼šæ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©ä¸Šè¿°ç­–ç•¥å¹¶åœ¨ç»“æŸåè§£ç ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å…é‡å¤æ‰‹åŠ¨åˆ†æ”¯åˆ¤æ–­ã€‚ã€F:src/inference/generator.pyâ€ L227-L245ã€‘

### æ‰¹é‡ä¸å¢é‡æ¨ç†

- `sample_generate` æ”¯æŒè¾“å…¥ `[batch, seq_len]` çš„å¼ é‡ï¼Œå¯ä¸€æ¬¡æ€§å¯¹å¤šæ¡ prompt ç”Ÿæˆå›å¤ï¼›åœ¨æ•™å­¦åœºæ™¯ä¸­å¯ç›´æ¥ç”¨ `torch.stack` åˆå¹¶æ‰¹é‡ä»¥æ¼”ç¤ºæ‰¹å¤„ç†æ•ˆæœã€‚ã€F:src/inference/generator.pyâ€ L125-L166ã€‘
- è‹¥å¸Œæœ›åœ¨å¾ªç¯å¤–é€ token æ¨ç†ï¼Œå¯ä»¥å‚è€ƒ `TextGenerator.stream_generate` ç­‰å¢é‡ç”Ÿæˆæ¥å£ï¼Œåœ¨æ¯æ­¥é€‰æ‹© argmax å¹¶å°†æ–° token æ‹¼æ¥åˆ°è¾“å…¥ä¸­ï¼Œæ¼”ç¤ºäº†â€œå¢é‡ decodeâ€ çš„å†™æ³•ã€‚ã€F:src/inference/generator.pyâ€ L200-L262ã€‘
- ç»“åˆè®­ç»ƒé˜¶æ®µä¿å­˜çš„ `GenerationConfig` é»˜è®¤å€¼ï¼Œå¯é€šè¿‡ `MiniGPT.generate` å¿«é€Ÿå¯¹æ¯”ä¸¤ç§å®ç°çš„è¾“å‡ºå·®å¼‚ï¼Œå¸®åŠ©å­¦ç”Ÿç†è§£å°è£…å±‚çš„ä»·å€¼ã€‚ã€F:src/model/transformer.pyâ€ L451-L500ã€‘

## ä½¿ç”¨ç¤ºä¾‹
```python
import torch
from src.inference.generator import TextGenerator, GenerationConfig

text_generator = TextGenerator(model, tokenizer, device="cpu")
prompt_ids = tokenizer.encode("Mini-LLM", add_special_tokens=True)

config = GenerationConfig(
    max_length=60,
    temperature=0.8,
    top_k=40,
    top_p=0.95,
    repetition_penalty=1.1,
)
output_ids = text_generator.sample_generate(
    input_ids=torch.tensor([prompt_ids]),
    config=config
)
print(tokenizer.decode(output_ids[0].tolist()))
```

## ä¸æ¨¡å‹é›†æˆçš„å»ºè®®

1. `TextGenerator` åˆå§‹åŒ–é˜¶æ®µå·²ç»è°ƒç”¨ `model.eval()`ï¼Œè‹¥åœ¨å¤–éƒ¨åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼è¯·åœ¨æ¨ç†å‰å†æ¬¡è®¾ä¸º `eval()`ï¼Œå¦åˆ™ Dropout ä¼šå¸¦æ¥éšæœºå™ªå£°ã€‚ã€F:src/inference/generator.pyâ€ L38-L44ã€‘
2. æŸæœç´¢é»˜è®¤ä¸ä¼šåº”ç”¨é•¿åº¦æƒ©ç½šï¼Œå¯æ ¹æ®éœ€æ±‚åœ¨ `GenerationConfig` ä¸­æ·»åŠ  `length_penalty` å­—æ®µå¹¶åœ¨è‡ªå®šä¹‰åˆ†æ”¯å†…ä½¿ç”¨ï¼Œé¿å… beam åå‘çŸ­å¥ã€‚ã€F:src/inference/generator.pyâ€ L167-L225ã€‘
3. é•¿æ–‡æœ¬ç”Ÿæˆæ—¶å»ºè®®æ­é… `MemoryOptimizer.optimize_model_for_inference()`ï¼Œé‡Šæ”¾æ¢¯åº¦ç¼“å­˜å‡å°‘æ˜¾å­˜å ç”¨ã€‚ã€F:src/training/memory_optimizer.pyâ€ L21-L176ã€‘
4. å¦‚éœ€é›†æˆæ–°çš„æ§åˆ¶ç­–ç•¥ï¼ˆå¤šæ ·æ€§æƒ©ç½šã€åŠ¨æ€æ¸©åº¦ç­‰ï¼‰ï¼Œå¯ä»¥ç»§æ‰¿ `TextGenerator` å¹¶é‡å†™ `sample_generate`ï¼Œå¤ç”¨ç°æœ‰çš„é‡å¤æƒ©ç½šä¸è¿‡æ»¤é€»è¾‘ã€‚

## æœåŠ¡åŒ–ä¸è°ƒè¯•æç¤º

- `TextGenerator.chat` ç®€åŒ–äº†å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡çš„æ„é€ ï¼Œé€‚åˆä½œä¸º HTTP/CLI demo çš„å¿«é€Ÿå…¥å£ï¼›çœŸå®æœåŠ¡ä¸­å¯ä»¥æ›¿æ¢ä¸ºç»“æ„åŒ–çš„å¯¹è¯å†å²å¹¶é…åˆ `TokenizerManager` ç»Ÿä¸€ç‰¹æ®Š tokenã€‚ã€F:src/inference/generator.pyâ€ L227-L318ã€‘ã€F:src/training/pipeline/tokenizer_manager.pyâ€ L14-L118ã€‘
- æ¨ç†è„šæœ¬è‹¥éœ€è¦ä¸è®­ç»ƒé˜¶æ®µä¿æŒä¸€è‡´çš„ç›‘æ§æŒ‡æ ‡ï¼Œå¯å¤ç”¨ `TrainingMonitor.get_gradient_norm` ç­‰å·¥å…·åœ¨æ¨ç†åæ£€æŸ¥æƒé‡å˜åŒ–æ˜¯å¦å¼‚å¸¸ã€‚ã€F:src/training/training_monitor.pyâ€ L120-L170ã€‘
- ä¸ºä¿è¯å†ç°æ€§ï¼Œå»ºè®®åœ¨æ¨ç†å‰è½½å…¥è®­ç»ƒç›®å½•ä¸‹çš„ `training_config_snapshot.json`ï¼Œå°†å…¶ä¸­çš„ `max_generate_length`ã€`temperature` ç­‰å­—æ®µåŒæ­¥åˆ° `GenerationConfig`ï¼Œé¿å…è®­ç»ƒ/æ¨ç†å‚æ•°ä¸ä¸€è‡´å¯¼è‡´è¾“å‡ºåå·®ã€‚ã€F:src/training/pipeline/pipeline.pyâ€ L41-L79ã€‘ã€F:src/model/config.pyâ€ L55-L115ã€‘

`TextGenerator` åªä¾èµ– PyTorch ä¸åˆ†è¯å™¨æ¥å£ï¼Œå¯è½»æ¾ç§»æ¤åˆ°å…¶ä»–é¡¹ç›®æˆ–è„šæœ¬ä¸­ï¼›åœ¨æœåŠ¡åŒ–åœºæ™¯ä¸­å¯ç»“åˆæ‰¹é‡å‰å‘å’Œ KV Cache è¿›ä¸€æ­¥æå‡ååã€‚
