# ğŸš€ å¿«é€Ÿä¸Šæ‰‹ Mini-LLM

æœ¬æŒ‡å—å¸®åŠ©ä½ åœ¨æœ¬åœ°ç¯å¢ƒä¸­å®Œæˆå®‰è£…ã€å‡†å¤‡æœ€å°æ•°æ®é›†ï¼Œå¹¶è·‘é€šä¸€æ¬¡è®­ç»ƒä¸æ¨ç†æµç¨‹ã€‚

## ç¯å¢ƒè¦æ±‚
- Python 3.10+ï¼ˆæ¨è 3.11ï¼Œä¸ `pyproject.toml` ä¿æŒä¸€è‡´ï¼‰
- PyTorch 2.1 åŠä»¥ä¸Šç‰ˆæœ¬ï¼ˆåŒ…å« CUDA/MPS æ”¯æŒæ—¶å¯è‡ªåŠ¨æ£€æµ‹ï¼‰
- `pip` æˆ– `uv` ç”¨äºå®‰è£…ä¾èµ–

> æƒ³ç¡®è®¤è®¾å¤‡æ˜¯å¦è¢«æ­£ç¡®è¯†åˆ«ï¼Œå¯åœ¨å…‹éš†ä»“åº“åæ‰§è¡Œï¼š
> ```bash
> python -c "from src.training.pipeline.environment import TrainingEnvironment;\
> from config.training_config import get_config;\
> env=TrainingEnvironment(get_config('tiny'),'sft');print(env.device)"
> ```
> è¿™æ®µä»£ç ä¼šè°ƒç”¨ `_setup_device`ï¼Œè¾“å‡º `cuda`/`mps`/`cpu`ï¼ŒåŒæ—¶åœ¨å·¥ä½œç›®å½•ç”Ÿæˆé…ç½®å¿«ç…§ï¼ŒéªŒè¯ç¯å¢ƒå†™æƒé™ã€‚ã€F:src/training/pipeline/environment.pyâ€ L12-L63ã€‘

## å®‰è£…æ­¥éª¤
```bash
git clone https://github.com/your-org/mini-llm.git
cd mini-llm
pip install -e .
```
> å¦‚æœä½ ä½¿ç”¨ `uv`ï¼Œå¯ä»¥åœ¨ä»“åº“æ ¹ç›®å½•æ‰§è¡Œ `uv sync`ã€‚

## è¿è¡Œæ ‡å‡†è®­ç»ƒæµæ°´çº¿
æ¨èé€šè¿‡è„šæœ¬å…¥å£å¤ç°å®éªŒï¼Œå®ƒä¼šè‡ªåŠ¨è§£æé…ç½®ã€å‡†å¤‡åˆ†è¯å™¨ã€é‡‡æ ·æ•°æ®å¹¶ä¿å­˜æ£€æŸ¥ç‚¹ã€‚

```bash
uv run python scripts/train.py --mode sft --config medium --auto-resume
```

å¸¸ç”¨å‚æ•°ï¼š

- `--mode {pretrain,sft,dpo,rlhf}`ï¼šåˆ‡æ¢è®­ç»ƒé˜¶æ®µï¼›
- `--retrain-tokenizer`ï¼šå¼ºåˆ¶é‡æ–°è®­ç»ƒå¹¶è¦†ç›–ç°æœ‰åˆ†è¯å™¨ï¼›
- `--resume` / `--auto-resume`ï¼šä»æŒ‡å®šæˆ–æœ€æ–°æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒï¼›
- `--learning-rate`ã€`--batch-size`ã€`--warmup-steps`ï¼šå‘½ä»¤è¡Œè¦†ç›–é…ç½®æ–‡ä»¶æ•°å€¼ã€‚ã€F:src/training/pipeline/cli.pyâ€ L12-L151ã€‘

è„šæœ¬å†…éƒ¨ä¼šæ„å»º `MiniGPTTrainer`ï¼Œå®ƒå°† `TrainingEnvironment`ã€`DatasetPreparer`ã€`TrainingLoopRunner` ç­‰æ¨¡å—ä¸²è”èµ·æ¥ï¼Œå¹¶åœ¨è¾“å‡ºç›®å½•ä¸­æŒä¹…åŒ–é…ç½®å¿«ç…§ä¸æ•°æ®é›†ç»Ÿè®¡ï¼Œæ–¹ä¾¿è¿½è¸ªå®éªŒã€‚ã€F:src/training/pipeline/app.pyâ€ L25-L204ã€‘ã€F:src/training/pipeline/data_manager.pyâ€ L24-L199ã€‘

è¿è¡Œç»“æŸåï¼Œæ£€æŸ¥ `checkpoints/<mode>_<config>/`ï¼š

- `training_config_snapshot.json`ï¼šè®°å½•å½“æ¬¡è¿è¡Œçš„æ‰€æœ‰è¶…å‚ï¼Œä¾¿äºå¤ç°ã€‚ã€F:src/training/pipeline/environment.pyâ€ L32-L63ã€‘
- `dataset_stats.json`ï¼šåˆ—å‡ºæ¯ä¸ªæ•°æ®æ–‡ä»¶çš„åŸå§‹/é‡‡æ ·æ•°é‡ï¼Œä»¥åŠéªŒè¯é›†å æ¯”ï¼Œå¸®åŠ©æ’æŸ¥æ ·æœ¬ä¸è¶³ã€‚ã€F:src/training/pipeline/environment.pyâ€ L56-L63ã€‘ã€F:src/training/pipeline/data_manager.pyâ€ L146-L209ã€‘
- `regression/`ï¼šè‹¥å¯ç”¨äº†å›å½’è¯„ä¼°ï¼Œä¼šä¿å­˜å›ºå®šæç¤ºçš„é€šè¿‡ç‡ï¼Œå¯ç”¨äºå¯¹é½å›å½’æ£€æŸ¥ã€‚ã€F:src/training/pipeline/regression_suite.pyâ€ L22-L147ã€‘

## æœ€å°åŒ–æ•™å­¦ç¤ºä¾‹
è‹¥ä½ æƒ³å¿«é€Ÿç†è§£åº•å±‚è®­ç»ƒå¾ªç¯ï¼Œå¯ä»¥ä½¿ç”¨ä¸‹åˆ—å°‘é‡ä»£ç å¤ç°ä¸€ä¸ªè¯­è¨€æ¨¡å‹ batch çš„è®­ç»ƒï¼š

```python
import torch
from torch.utils.data import DataLoader

from src.model.config import get_tiny_config
from src.model.transformer import MiniGPT
from src.tokenizer.bpe_tokenizer import BPETokenizer
from src.training.datasets import LanguageModelingDataset
from src.training.trainer import PreTrainer

texts = [
    "ä½ å¥½ï¼ŒMini-LLM!",
    "Transformer æ¶æ„æ¼”ç¤º",
    "å°æ¨¡å‹ä¹Ÿèƒ½è®­ç»ƒ",
]

tokenizer = BPETokenizer(vocab_size=256)
tokenizer.train(texts)

dataset = LanguageModelingDataset(texts, tokenizer, max_length=64)
dataloader = DataLoader(dataset, batch_size=2)

config = get_tiny_config()
model = MiniGPT(config)
trainer = PreTrainer(model, tokenizer, device="cpu")

loss = trainer.train_epoch(dataloader)
print(f"epoch loss: {loss:.4f}")
```

ç¤ºä¾‹ä¸­å„è¡Œä»£ç ä¸æ ¸å¿ƒæ¨¡å—çš„å¯¹åº”å…³ç³»ï¼š

- `get_tiny_config()` è¿”å›çº¦ 1M å‚æ•°çš„é»˜è®¤é…ç½®ï¼Œè‡ªåŠ¨éªŒè¯æ³¨æ„åŠ›å¤´ä¸éšè—ç»´åº¦çš„æ•´é™¤å…³ç³»ï¼Œé€‚åˆ CPU å¿«é€Ÿå®éªŒã€‚ã€F:src/model/config.pyâ€ L159-L176ã€‘
- `MiniGPT(config)` ä¼šæ ¹æ®é…ç½®é€‰æ‹©æ˜¯å¦å¯ç”¨ RoPEã€GQA ä»¥åŠ MoEï¼Œå¹¶æ„å»ºå®Œæ•´çš„ Transformer Decoderã€‚ã€F:src/model/transformer.pyâ€ L314-L443ã€‘
- `BPETokenizer` åœ¨ `train(texts)` æ—¶å®Œæˆä¸­æ–‡å‹å¥½é¢„å¤„ç†ä¸ BPE åˆå¹¶ï¼Œé»˜è®¤æ³¨å†Œ `<PAD>/<UNK>/<BOS>/<EOS>` å››ä¸ªç‰¹æ®Šç¬¦å·ï¼Œå¯¹åº”çš„ ID ä¼šåœ¨æ•°æ®é›†å’Œæ¨¡å‹ä¸­å¤ç”¨ã€‚ã€F:src/tokenizer/bpe_tokenizer.pyâ€ L77-L199ã€‘
- `LanguageModelingDataset` ä¼šåœ¨æˆªæ–­/å¡«å……åè¿”å› `(input, target, loss_mask)`ï¼Œå®Œå…¨å¤ç”¨ MiniMind çš„é¢„è®­ç»ƒæ ·æœ¬ç»“æ„ï¼Œloss mask è‡ªåŠ¨å¿½ç•¥ PAD åŒºåŸŸã€‚ã€F:src/training/datasets/language_modeling.pyâ€ L11-L115ã€‘
- `PreTrainer` åˆå§‹åŒ–æ—¶ç»‘å®š `AdamW` ä¸ä½™å¼¦é€€ç«è°ƒåº¦å™¨ï¼Œ`train_epoch` ä¼šåœ¨æ¯ä¸ª batch ä¸Šæ‰§è¡Œå‰å‘ã€åå‘ä¸æ¢¯åº¦è£å‰ªï¼Œæ¼”ç¤ºæœ€å°åŒ–è®­ç»ƒé—­ç¯ã€‚ã€F:src/training/trainer.pyâ€ L13-L204ã€‘

## æ¨ç†ç¤ºä¾‹
è®­ç»ƒç»“æŸåå¯ä»¥ç›´æ¥å¤ç”¨æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼š

```python
import torch
from src.inference.generator import TextGenerator, GenerationConfig

prompt = "Mini-LLM"
prompt_ids = tokenizer.encode(prompt, add_special_tokens=True)

text_generator = TextGenerator(model, tokenizer, device="cpu")
output_ids = text_generator.sample_generate(
    input_ids=torch.tensor([prompt_ids]),
    config=GenerationConfig(max_length=40, top_p=0.9, temperature=0.8)
)
print(tokenizer.decode(output_ids[0].tolist()))
```

`TextGenerator.sample_generate` ä¼šåœ¨é‡‡æ ·å‰åº”ç”¨æ¸©åº¦ç¼©æ”¾ã€é‡å¤æƒ©ç½šã€Top-k/Top-p è¿‡æ»¤ï¼Œå¹¶æ ¹æ® `do_sample` è‡ªåŠ¨é€‰æ‹©é‡‡æ ·æˆ–è´ªå¿ƒç­–ç•¥ï¼›å¦‚éœ€æ‰¹é‡æ¨ç†å¯ç›´æ¥ä¼ å…¥å¤šä¸ª `prompt_ids`ã€‚ã€F:src/inference/generator.pyâ€ L125-L166ã€‘

## å¸¸è§é—®é¢˜æ’æŸ¥

- **æ•°æ®æœªè¢«åŠ è½½**ï¼šç¡®è®¤æ•°æ®æ–‡ä»¶ä½äº `config.data_dir` æˆ– `MINIGPT_DATA_DIR` æ‰€æŒ‡å‘çš„ç›®å½•ï¼Œ`DataResolver` ä¼šæŒ‰æ¨¡å¼æŸ¥æ‰¾å¹¶æ‰“å°ç¼ºå¤±è­¦å‘Šã€‚ã€F:src/training/pipeline/data_manager.pyâ€ L28-L124ã€‘
- **æ˜¾å­˜ä¸è¶³**ï¼šåœ¨å‘½ä»¤è¡ŒåŠ å…¥ `--batch-size` æˆ–è°ƒå°é…ç½®ä¸­çš„ `gradient_accumulation_steps`ï¼›åŒæ—¶ç¡®ä¿ `MINIGPT_MEMORY_THRESHOLD` è®¾ç½®åˆç†ä»¥è§¦å‘è‡ªåŠ¨æ¸…ç†ã€‚ã€F:config/training_config.pyâ€ L124-L205ã€‘ã€F:src/training/pipeline/memory_hooks.pyâ€ L1-L78ã€‘
- **è®­ç»ƒè¢«ä¸­æ–­**ï¼šè‹¥ç»ˆç«¯æç¤ºâ€œæ”¶åˆ°ä¸­æ–­ä¿¡å·â€ï¼Œè®­ç»ƒå™¨ä¼šä¿å­˜æœ€æ–° checkpointï¼Œå¯ç”¨ `--auto-resume` æ— ç¼ç»§ç»­ã€‚ã€F:src/training/pipeline/app.pyâ€ L118-L186ã€‘
- **è¾“å‡ºé€€åŒ–**ï¼šå¯ç”¨å›å½’è¯„ä¼°æˆ–æ£€æŸ¥ `TrainingMonitor` çš„æ¢¯åº¦å¼‚å¸¸æç¤ºï¼Œå¿…è¦æ—¶é™ä½å­¦ä¹ ç‡æˆ–å»¶é•¿ warmupã€‚ã€F:src/training/training_monitor.pyâ€ L120-L222ã€‘ã€F:src/training/pipeline/regression_suite.pyâ€ L22-L147ã€‘

## ä¸‹ä¸€æ­¥
- é˜…è¯» [model.md](model.md) ç†è§£ `MiniGPTConfig` å¯é…ç½®é¡¹
- é˜…è¯» [data.md](data.md) äº†è§£çœŸå®æ•°æ®é›†éœ€è¦éµå¾ªçš„å­—æ®µæ ¼å¼
- é˜…è¯» [training.md](training.md) æŒæ¡æ··åˆç²¾åº¦ã€æ¢¯åº¦ç´¯ç§¯ã€æ£€æŸ¥ç‚¹ç­‰é«˜çº§ç‰¹æ€§

è‡³æ­¤ï¼Œä½ å·²ç»å¯ä»¥åœ¨æœ¬åœ°å¤ç°å®éªŒå¹¶å¯¹æ¡†æ¶è¿›è¡ŒäºŒæ¬¡å¼€å‘ã€‚
