# ğŸš€ å¿«é€Ÿä¸Šæ‰‹ Mini-LLM

æœ¬æŒ‡å—å¸®åŠ©ä½ åœ¨æœ¬åœ°ç¯å¢ƒä¸­å®Œæˆå®‰è£…ã€å‡†å¤‡æœ€å°æ•°æ®é›†ï¼Œå¹¶è·‘é€šä¸€æ¬¡è®­ç»ƒä¸æ¨ç†æµç¨‹ã€‚

## ç¯å¢ƒè¦æ±‚
- Python 3.10+ï¼ˆæ¨è 3.11ï¼Œä¸ `pyproject.toml` ä¿æŒä¸€è‡´ï¼‰
- PyTorch 2.1 åŠä»¥ä¸Šç‰ˆæœ¬ï¼ˆåŒ…å« CUDA/MPS æ”¯æŒæ—¶å¯è‡ªåŠ¨æ£€æµ‹ï¼‰
- `pip` æˆ– `uv` ç”¨äºå®‰è£…ä¾èµ–

> æƒ³ç¡®è®¤è®¾å¤‡æ˜¯å¦è¢«æ­£ç¡®è¯†åˆ«ï¼Œå¯åœ¨å…‹éš†ä»“åº“åæ‰§è¡Œï¼š
> ```bash
> python -c "from config.training_config import get_config;\
> from src.training.pipeline.pipeline import TrainingPipeline;\
> pipeline=TrainingPipeline(get_config('tiny'),'sft');print(pipeline.device)"
> ```
> è¿™æ®µä»£ç ä¼šè¾“å‡º `cuda`/`mps`/`cpu` å¹¶ç”Ÿæˆé…ç½®å¿«ç…§ï¼ŒéªŒè¯ç¯å¢ƒå†™æƒé™ã€‚ã€F:src/training/pipeline/pipeline.pyâ€ L23-L79ã€‘

## å®‰è£…æ­¥éª¤
```bash
git clone https://github.com/your-org/mini-llm.git
cd mini-llm
pip install -e .
```
> å¦‚æœä½ ä½¿ç”¨ `uv`ï¼Œå¯ä»¥åœ¨ä»“åº“æ ¹ç›®å½•æ‰§è¡Œ `uv sync`ã€‚

## è¿è¡Œæ ‡å‡†è®­ç»ƒæµæ°´çº¿
æ¨èé€šè¿‡è„šæœ¬å…¥å£å¤ç°å®éªŒï¼Œå®ƒä¼šè‡ªåŠ¨è§£æé…ç½®ã€å‡†å¤‡åˆ†è¯å™¨ã€é‡‡æ ·æ•°æ®å¹¶ä¿å­˜æ£€æŸ¥ç‚¹ã€‚

```bash
uv run python scripts/train.py --mode sft --config medium --auto-resume
```

å¸¸ç”¨å‚æ•°ï¼š

- `--mode {pretrain,sft,dpo,rlhf}`ï¼šåˆ‡æ¢è®­ç»ƒé˜¶æ®µï¼›
- `--retrain-tokenizer`ï¼šå¼ºåˆ¶é‡æ–°è®­ç»ƒå¹¶è¦†ç›–ç°æœ‰åˆ†è¯å™¨ï¼›
- `--resume` / `--auto-resume`ï¼šä»æŒ‡å®šæˆ–æœ€æ–°æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒï¼›
- `--learning-rate`ã€`--batch-size`ã€`--warmup-steps`ï¼šå‘½ä»¤è¡Œè¦†ç›–é…ç½®æ–‡ä»¶æ•°å€¼ã€‚ã€F:src/training/pipeline/cli.pyâ€ L12-L151ã€‘

è„šæœ¬å†…éƒ¨ä¼šæ„å»º `TrainingPipeline`ï¼Œå®ƒå°† `DatasetPreparer`ã€`TrainingLoopRunner` ç­‰æ¨¡å—ä¸²è”èµ·æ¥ï¼Œå¹¶åœ¨è¾“å‡ºç›®å½•ä¸­æŒä¹…åŒ–é…ç½®å¿«ç…§ä¸æ•°æ®é›†ç»Ÿè®¡ï¼Œæ–¹ä¾¿è¿½è¸ªå®éªŒã€‚ã€F:src/training/pipeline/pipeline.pyâ€ L23-L213ã€‘ã€F:src/training/pipeline/data_manager.pyâ€ L24-L199ã€‘

è¿è¡Œç»“æŸåï¼Œæ£€æŸ¥ `checkpoints/<mode>_<config>/`ï¼š

- `training_config_snapshot.json`ï¼šè®°å½•å½“æ¬¡è¿è¡Œçš„æ‰€æœ‰è¶…å‚ï¼Œä¾¿äºå¤ç°ã€‚ã€F:src/training/pipeline/pipeline.pyâ€ L41-L79ã€‘
- `dataset_stats.json`ï¼šåˆ—å‡ºæ¯ä¸ªæ•°æ®æ–‡ä»¶çš„åŸå§‹/é‡‡æ ·æ•°é‡ï¼Œä»¥åŠéªŒè¯é›†å æ¯”ï¼Œå¸®åŠ©æ’æŸ¥æ ·æœ¬ä¸è¶³ã€‚ã€F:src/training/pipeline/pipeline.pyâ€ L81-L125ã€‘ã€F:src/training/pipeline/data_manager.pyâ€ L146-L209ã€‘

## æœ€å°åŒ–æ•™å­¦ç¤ºä¾‹
è‹¥ä½ æƒ³å¿«é€Ÿè§‚å¯Ÿåº•å±‚è®­ç»ƒå¾ªç¯çš„æ¯ä¸ªé˜¶æ®µï¼Œæ¨èç›´æ¥è¿è¡Œå†…ç½®çš„å…¨æ ˆè°ƒè¯•è„šæœ¬ï¼š

```bash
uv run python scripts/debug_fullstack.py --mode pretrain --model-size tiny --prompt "ä½ å¥½ï¼ŒMiniGPTï¼"
```

è„šæœ¬ä¼šè‡ªåŠ¨ï¼š

- è½½å…¥ `TrainingPipeline` å¹¶ä¿å­˜é…ç½®å¿«ç…§ï¼Œç¡®ä¿ä»»ä½•è°ƒè¯•éƒ½æœ‰å®Œæ•´ä¸Šä¸‹æ–‡å¯è¿½æº¯ã€‚ã€F:scripts/debug_fullstack.pyâ€ L1-L214ã€‘ã€F:src/training/pipeline/pipeline.pyâ€ L23-L213ã€‘
- æ‰“å°é¦–ä¸ª batch çš„åŸå§‹æ–‡æœ¬ã€token IDã€loss mask ç­‰å…³é”®å¼ é‡å½¢çŠ¶ï¼Œä¸ºæ•°æ®æ£€æŸ¥å’Œå¼‚å¸¸å®šä½æä¾›ä¾æ®ã€‚ã€F:scripts/debug_fullstack.pyâ€ L86-L140ã€‘ã€F:src/training/datasets/language_modeling.pyâ€ L11-L115ã€‘
- å¤ç”¨æ­£å¼è®­ç»ƒå¾ªç¯çš„æŸå¤±ä¸æ¢¯åº¦é€»è¾‘æ‰§è¡Œä¸€æ¬¡ä¼˜åŒ–æ­¥éª¤ï¼Œå¹¶è¾“å‡ºæ¢¯åº¦èŒƒæ•°ï¼›éšåä»¥ç»™å®š `--prompt` åšæ¨ç†ï¼Œå½¢æˆé—­ç¯ã€‚ã€F:scripts/debug_fullstack.pyâ€ L146-L189ã€‘ã€F:src/training/pipeline/training_loop.pyâ€ L18-L620ã€‘

è‹¥éœ€åœ¨ Notebook ä¸­æ‰‹å†™ä»£ç ï¼Œä¹Ÿå¯ä»¿ç…§è„šæœ¬ä¸­å¯¹ `TrainingPipeline` çš„ä½¿ç”¨æ–¹å¼ï¼Œæ‰‹åŠ¨è·å– `DatasetPreparer`ã€`TrainingLoopRunner` ç­‰ç»„ä»¶æ‰§è¡Œè‡ªå®šä¹‰å®éªŒã€‚

## æ¨ç†ç¤ºä¾‹
è®­ç»ƒç»“æŸåå¯ä»¥ç›´æ¥å¤ç”¨æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼š

```python
import torch
from src.inference.generator import TextGenerator, GenerationConfig

prompt = "Mini-LLM"
prompt_ids = tokenizer.encode(prompt, add_special_tokens=True)

text_generator = TextGenerator(model, tokenizer, device="cpu")
output_ids = text_generator.sample_generate(
    input_ids=torch.tensor([prompt_ids]),
    config=GenerationConfig(max_length=40, top_p=0.9, temperature=0.8)
)
print(tokenizer.decode(output_ids[0].tolist()))
```

`TextGenerator.sample_generate` ä¼šåœ¨é‡‡æ ·å‰åº”ç”¨æ¸©åº¦ç¼©æ”¾ã€é‡å¤æƒ©ç½šã€Top-k/Top-p è¿‡æ»¤ï¼Œå¹¶æ ¹æ® `do_sample` è‡ªåŠ¨é€‰æ‹©é‡‡æ ·æˆ–è´ªå¿ƒç­–ç•¥ï¼›å¦‚éœ€æ‰¹é‡æ¨ç†å¯ç›´æ¥ä¼ å…¥å¤šä¸ª `prompt_ids`ã€‚ã€F:src/inference/generator.pyâ€ L125-L166ã€‘

## å¸¸è§é—®é¢˜æ’æŸ¥

- **æ•°æ®æœªè¢«åŠ è½½**ï¼šç¡®è®¤æ•°æ®æ–‡ä»¶ä½äº `config.data_dir` æˆ– `MINIGPT_DATA_DIR` æ‰€æŒ‡å‘çš„ç›®å½•ï¼Œ`DataResolver` ä¼šæŒ‰æ¨¡å¼æŸ¥æ‰¾å¹¶æ‰“å°ç¼ºå¤±è­¦å‘Šã€‚ã€F:src/training/pipeline/data_manager.pyâ€ L28-L124ã€‘
- **æ˜¾å­˜ä¸è¶³**ï¼šåœ¨å‘½ä»¤è¡ŒåŠ å…¥ `--batch-size` æˆ–è°ƒå°é…ç½®ä¸­çš„ `gradient_accumulation_steps`ï¼›å¿…è¦æ—¶ç¼©çŸ­ `max_steps` ä»¥å¿«é€ŸéªŒè¯æµç¨‹ã€‚ã€F:config/training_config.pyâ€ L124-L205ã€‘
- **è®­ç»ƒè¢«ä¸­æ–­**ï¼šè‹¥ç»ˆç«¯æç¤ºâ€œæ”¶åˆ°ä¸­æ–­ä¿¡å·â€ï¼Œè®­ç»ƒå™¨ä¼šä¿å­˜æœ€æ–° checkpointï¼Œå¯ç”¨ `--auto-resume` æ— ç¼ç»§ç»­ã€‚ã€F:src/training/pipeline/pipeline.pyâ€ L214-L230ã€‘
- **è¾“å‡ºæ³¢åŠ¨**ï¼šæ£€æŸ¥ `TrainingMonitor` çš„æ¢¯åº¦å¼‚å¸¸æç¤ºå¹¶é€‚å½“é™ä½å­¦ä¹ ç‡æˆ–å»¶é•¿ warmupã€‚ã€F:src/training/training_monitor.pyâ€ L120-L470ã€‘

## ä¸‹ä¸€æ­¥
- é˜…è¯» [model.md](model.md) ç†è§£ `MiniGPTConfig` å¯é…ç½®é¡¹
- é˜…è¯» [data.md](data.md) äº†è§£çœŸå®æ•°æ®é›†éœ€è¦éµå¾ªçš„å­—æ®µæ ¼å¼
- é˜…è¯» [training.md](training.md) æŒæ¡æ··åˆç²¾åº¦ã€æ¢¯åº¦ç´¯ç§¯ã€æ£€æŸ¥ç‚¹ç­‰é«˜çº§ç‰¹æ€§

è‡³æ­¤ï¼Œä½ å·²ç»å¯ä»¥åœ¨æœ¬åœ°å¤ç°å®éªŒå¹¶å¯¹æ¡†æ¶è¿›è¡ŒäºŒæ¬¡å¼€å‘ã€‚
