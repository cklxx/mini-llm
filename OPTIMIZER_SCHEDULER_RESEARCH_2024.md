# 2024-2025 LLMä¼˜åŒ–å™¨ä¸å­¦ä¹ ç‡è°ƒåº¦å™¨ç ”ç©¶æŠ¥å‘Š

## ğŸ“Š ç ”ç©¶æ‘˜è¦

åŸºäº2024-2025æœ€æ–°è®ºæ–‡å’Œå®è·µ,æœ¬æŠ¥å‘Šè°ƒç ”äº†é€‚åˆTransformer/LLMè®­ç»ƒçš„æœ€ä¼˜é…ç½®ã€‚

**æ ¸å¿ƒå‘ç°**:
- **Muonä¼˜åŒ–å™¨**: ç›¸æ¯”AdamWèŠ‚çœ48%è®¡ç®—é‡,è¾¾åˆ°ç›¸åŒæ•ˆæœ
- **Warmup-Stable-Decayè°ƒåº¦å™¨**: æ— éœ€é¢„è®¾æ€»æ­¥æ•°,æ”¯æŒæŒç»­è®­ç»ƒ
- **æ··åˆä¼˜åŒ–ç­–ç•¥**: Muon(2Då‚æ•°) + AdamW(1Då‚æ•°) = æœ€ä¼˜é…ç½®

---

## ğŸ”¬ ä¼˜åŒ–å™¨å¯¹æ¯” (2024)

### 1. **Muon** (â­ æ¨èç”¨äºå¤§æ¨¡å‹)

**è®ºæ–‡**: https://arxiv.org/abs/2502.16982 (2024)

**æ ¸å¿ƒç‰¹ç‚¹**:
- Momentum Orthogonalized by Newton-Schulz
- å¯¹æƒé‡çŸ©é˜µä½¿ç”¨Newton-Schulzæ­£äº¤åŒ–
- ç›¸æ¯”AdamWèŠ‚çœ~48%è®¡ç®—é‡(FLOPs)
- **Kimi-2 (1Tå‚æ•°)**ä½¿ç”¨æ­¤ä¼˜åŒ–å™¨

**æ€§èƒ½æ•°æ®**:
```
è®­ç»ƒæ•ˆç‡: Muonè¾¾åˆ°ç›®æ ‡lossåªéœ€AdamWçš„52%è®­ç»ƒæ­¥æ•°
å†…å­˜å ç”¨: ä¸SGD-momentumç›¸å½“ (è¿œä½äºAdamW)
è®­ç»ƒç¨³å®šæ€§: lossæ›²çº¿æ›´å¹³æ»‘
```

**æ¨èé…ç½®**:
```python
# æ··åˆæ¨¡å¼ (æœ€ä¼˜)
opts = get_hybrid_optimizer(
    model,
    muon_lr=0.02,      # Muonå­¦ä¹ ç‡æ˜¯AdamWçš„20å€
    adamw_lr=1e-3,
    weight_decay=0.01
)

# ä½¿ç”¨
loss.backward()
opts['muon'].step()
opts['adamw'].step()
scheduler.step()  # è°ƒåº¦å™¨
opts['muon'].zero_grad()
opts['adamw'].zero_grad()
```

**é€‚ç”¨åœºæ™¯**:
- âœ… å¤§æ¨¡å‹é¢„è®­ç»ƒ (>1Bå‚æ•°)
- âœ… è®¡ç®—èµ„æºå—é™
- âœ… éœ€è¦æ›´å¿«æ”¶æ•›
- âŒ æå°æ¨¡å‹ (<100M) - å¢ç›Šä¸æ˜æ˜¾

---

### 2. **AdamW** (â­ ä»æ˜¯é»„é‡‘æ ‡å‡†)

**è®ºæ–‡**: Decoupled Weight Decay Regularization (2019)

**ä¸ºä»€ä¹ˆAdamWä»ç„¶é‡è¦**:
- GPT-3, Llama, Chinchilla, BLOOMéƒ½ä½¿ç”¨AdamW
- ç»è¿‡å……åˆ†éªŒè¯,ç¨³å®šå¯é 
- å¹¿æ³›æ”¯æŒ,æ˜“äºè°ƒè¯•
- ä¸å„ç§è°ƒåº¦å™¨å…¼å®¹æ€§æœ€å¥½

**æ¨èé…ç½®** (åŸºäºGPT-3):
```python
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=3e-4,
    betas=(0.9, 0.95),  # GPT-3é…ç½® (é¢„è®­ç»ƒ)
    # betas=(0.9, 0.999),  # BERTé…ç½® (å¾®è°ƒ)
    eps=1e-8,
    weight_decay=0.1
)
```

**å‚æ•°è¯´æ˜**:
- `Î²1=0.9`: ä¸€é˜¶çŸ©(åŠ¨é‡)
- `Î²2=0.95`: é¢„è®­ç»ƒç”¨ | `Î²2=0.999`: å¾®è°ƒç”¨
- `weight_decay=0.1`: è¾ƒå¤§å€¼æœ‰åŠ©äºé˜²æ­¢è¿‡æ‹Ÿåˆ

---

### 3. **Sophia** (äºŒé˜¶ä¼˜åŒ–å™¨)

**è®ºæ–‡**: https://arxiv.org/abs/2305.14342 (2023)

**æ ¸å¿ƒç‰¹ç‚¹**:
- ä½¿ç”¨å¯¹è§’Hessianä¼°è®¡ (äºŒé˜¶ä¿¡æ¯)
- åœ¨ç›¸åŒæ­¥æ•°ä¸‹è¾¾åˆ°æ›´ä½loss
- é€‚åˆå¤§æ¨¡å‹é¢„è®­ç»ƒ

**æ€§èƒ½æ•°æ®**:
```
540Mæ¨¡å‹ + Sophia = 770Mæ¨¡å‹ + AdamW (ç›¸åŒæ­¥æ•°)
æ”¶æ•›é€Ÿåº¦: æ¯”AdamWå¿«~30%
```

**æ¨èé…ç½®**:
```python
optimizer = Sophia(
    model.parameters(),
    lr=1e-4,
    betas=(0.965, 0.99),
    rho=0.04,  # Hessianè£å‰ªå‚æ•°
    weight_decay=0.1
)
```

**é€‚ç”¨åœºæ™¯**:
- âœ… å¤§æ¨¡å‹é¢„è®­ç»ƒ (125M-13B)
- âœ… æœ‰å……è¶³è®¡ç®—èµ„æº
- âŒ å¾®è°ƒä»»åŠ¡ - ä¸å¦‚AdamW
- âŒ æå°batch - æ•ˆæœä¸ä½³

---

### 4. **Lion** (å†…å­˜é«˜æ•ˆ)

**è®ºæ–‡**: https://arxiv.org/abs/2302.06675 (2023)

**æ ¸å¿ƒç‰¹ç‚¹**:
- åŸºäºç¬¦å·(sign)çš„æ›´æ–°
- å†…å­˜å ç”¨ä»…ä¸ºAdamWçš„ä¸€åŠ
- åœ¨æŸäº›NLPä»»åŠ¡ä¸Šä¼˜äºAdamW

**æ€§èƒ½æ•°æ®**:
```
å†…å­˜å ç”¨: 1xå‚æ•°é‡ (AdamWä¸º2x)
æ”¶æ•›é€Ÿåº¦: ä¸AdamWç›¸å½“æˆ–æ›´å¿«
```

**æ¨èé…ç½®**:
```python
optimizer = Lion(
    model.parameters(),
    lr=1e-4,  # Lionçš„lré€šå¸¸æ¯”AdamWä½10å€
    betas=(0.9, 0.99),
    weight_decay=0.01
)
```

**é€‚ç”¨åœºæ™¯**:
- âœ… å†…å­˜å—é™ç¯å¢ƒ
- âœ… å¤§batchè®­ç»ƒ
- âŒ æŸäº›ä»»åŠ¡è¡¨ç°ä¸ç¨³å®š

---

## ğŸ“ˆ å­¦ä¹ ç‡è°ƒåº¦å™¨å¯¹æ¯” (2024)

### 1. **Warmup + Cosine Decay** (â­ æ¨è)

**ä½¿ç”¨è€…**: GPT-3, Llama, Chinchilla, BLOOM, Pythia

**ç‰¹ç‚¹**:
- ä¸šç•Œæ ‡å‡†é…ç½®
- warmupé˜²æ­¢åˆæœŸä¸ç¨³å®š
- cosineå¹³æ»‘ä¸‹é™åˆ°10% peak lr

**ä»£ç **:
```python
from src.model.optimizers import get_warmup_cosine_schedule

scheduler = get_warmup_cosine_schedule(
    optimizer,
    num_warmup_steps=4000,    # æ€»æ­¥æ•°çš„5-10%
    num_training_steps=100000,
    num_cycles=0.5,            # åŠå‘¨æœŸ
    min_lr_ratio=0.1           # æœ€ä½é™åˆ°10%
)
```

**å­¦ä¹ ç‡æ›²çº¿**:
```
Step 0-4000:     0 â†’ peak_lr      (çº¿æ€§warmup)
Step 4000-100k:  peak_lr â†’ 0.1*peak_lr  (cosine decay)
```

**æ¨èå‚æ•°**:
- Warmup: æ€»æ­¥æ•°çš„5-10% (å¤ªé•¿æµªè´¹,å¤ªçŸ­ä¸ç¨³å®š)
- Min LR: 10% peak_lr (GPT-3é…ç½®)
- Cycles: 0.5 (åŠå‘¨æœŸ,å•è°ƒä¸‹é™)

---

### 2. **Inverse Sqrt** (TransformeråŸå§‹)

**ä½¿ç”¨è€…**: åŸå§‹Transformerè®ºæ–‡ "Attention is All You Need"

**ç‰¹ç‚¹**:
- ç»å…¸é…ç½®,ç®€å•æœ‰æ•ˆ
- warmupåæŒ‰ `1/âˆšstep` è¡°å‡
- æ°¸ä¸é™åˆ°0,é€‚åˆæŒç»­è®­ç»ƒ

**ä»£ç **:
```python
from src.model.optimizers import get_inverse_sqrt_schedule

scheduler = get_inverse_sqrt_schedule(
    optimizer,
    num_warmup_steps=4000
)
```

**å­¦ä¹ ç‡æ›²çº¿**:
```
Step 0-4000:   çº¿æ€§warmup
Step 4000+:    lr âˆ 1/âˆšstep
```

**é€‚ç”¨åœºæ™¯**:
- âœ… ç»å…¸Transformeræ¶æ„
- âœ… ä¸ç¡®å®šæ€»è®­ç»ƒæ­¥æ•°
- âŒ ç°ä»£LLM - cosineæ›´ä¼˜

---

### 3. **Warmup-Stable-Decay (WSD)** (â­ 2024æœ€æ–°)

**è®ºæ–‡**: https://arxiv.org/abs/2410.05192 (2024)

**æ ¸å¿ƒåˆ›æ–°**:
- **æ— éœ€é¢„è®¾æ€»è®­ç»ƒæ­¥æ•°**
- æ”¯æŒæŒç»­è®­ç»ƒå’Œä¸­é€”checkpoint
- 3é˜¶æ®µ: Warmup â†’ Stable â†’ Decay

**ä»£ç **:
```python
from src.model.optimizers import get_wsd_schedule

scheduler = get_wsd_schedule(
    optimizer,
    num_warmup_steps=4000,
    num_stable_steps=80000,  # é•¿æ—¶é—´ç¨³å®šè®­ç»ƒ
    num_decay_steps=16000,   # æœ€åè¡°å‡
    min_lr_ratio=0.1
)
```

**å­¦ä¹ ç‡æ›²çº¿**:
```
Step 0-4k:      0 â†’ peak_lr           (warmup)
Step 4k-84k:    peak_lr (å¸¸æ•°)         (stable)
Step 84k-100k:  peak_lr â†’ 0.1*peak_lr (decay)
```

**ä¼˜åŠ¿**:
- å¯ä»¥éšæ—¶åˆ†å‰å‡ºcheckpointå¹¶å¿«é€Ÿè¡°å‡
- é€‚åˆä¸ç¡®å®šè®­ç»ƒæ—¶é•¿çš„åœºæ™¯
- ä¸»åˆ†æ”¯å¯ä»¥æ— é™æœŸstableè®­ç»ƒ

**é€‚ç”¨åœºæ™¯**:
- âœ… æŒç»­é¢„è®­ç»ƒ
- âœ… æ¢ç´¢æ€§è®­ç»ƒ
- âœ… éœ€è¦çµæ´»æ€§
- âŒ æ˜ç¡®è®­ç»ƒé¢„ç®— - cosineæ›´ç›´æ¥

---

## ğŸ¯ æ¨èé…ç½®æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: å°æ¨¡å‹ (<500Må‚æ•°) - æ ‡å‡†é…ç½®

```python
# ä¼˜åŒ–å™¨
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=3e-4,
    betas=(0.9, 0.95),
    weight_decay=0.1
)

# è°ƒåº¦å™¨
scheduler = get_warmup_cosine_schedule(
    optimizer,
    num_warmup_steps=4000,
    num_training_steps=100000,
    min_lr_ratio=0.1
)
```

**ç†ç”±**: AdamWç»è¿‡å……åˆ†éªŒè¯,ç¨³å®šå¯é 

---

### æ–¹æ¡ˆ2: ä¸­å‹æ¨¡å‹ (500M-5Bå‚æ•°) - å¹³è¡¡é…ç½®

```python
# ä¼˜åŒ–å™¨: Muon + AdamWæ··åˆ
opts = get_hybrid_optimizer(
    model,
    muon_lr=0.02,
    adamw_lr=1e-3,
    weight_decay=0.01
)

# è°ƒåº¦å™¨: Warmup + Cosine
scheduler_muon = get_warmup_cosine_schedule(
    opts['muon'], 4000, 100000
)
scheduler_adamw = get_warmup_cosine_schedule(
    opts['adamw'], 4000, 100000
)

# è®­ç»ƒå¾ªç¯
loss.backward()
opts['muon'].step()
opts['adamw'].step()
scheduler_muon.step()
scheduler_adamw.step()
opts['muon'].zero_grad()
opts['adamw'].zero_grad()
```

**ç†ç”±**: Muonæå‡æ•ˆç‡,æ··åˆä½¿ç”¨å…¼é¡¾ç¨³å®šæ€§

---

### æ–¹æ¡ˆ3: å¤§æ¨¡å‹ (>5Bå‚æ•°) - æ¿€è¿›ä¼˜åŒ–

```python
# ä¼˜åŒ–å™¨: Muonä¸»å¯¼
opts = get_hybrid_optimizer(
    model,
    muon_lr=0.03,  # æ›´é«˜å­¦ä¹ ç‡
    adamw_lr=1.5e-3,
    weight_decay=0.01
)

# è°ƒåº¦å™¨: WSD (çµæ´»æ€§)
scheduler_muon = get_wsd_schedule(
    opts['muon'],
    num_warmup_steps=8000,
    num_stable_steps=200000,
    num_decay_steps=50000
)
scheduler_adamw = get_wsd_schedule(
    opts['adamw'],
    num_warmup_steps=8000,
    num_stable_steps=200000,
    num_decay_steps=50000
)
```

**ç†ç”±**:
- MuonèŠ‚çœ48%è®¡ç®—é‡
- WSDæ”¯æŒæŒç»­è®­ç»ƒ
- é€‚åˆæ¢ç´¢æ€§é¢„è®­ç»ƒ

---

### æ–¹æ¡ˆ4: å¾®è°ƒä»»åŠ¡ - ä¿å®ˆé…ç½®

```python
# ä¼˜åŒ–å™¨
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=5e-5,  # å¾®è°ƒç”¨æ›´å°lr
    betas=(0.9, 0.999),  # Î²2=0.999ç”¨äºå¾®è°ƒ
    weight_decay=0.01
)

# è°ƒåº¦å™¨: çº¿æ€§warmup + çº¿æ€§decay
def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):
    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))
        return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=500,
    num_training_steps=10000
)
```

**ç†ç”±**: å¾®è°ƒéœ€è¦ä¿å®ˆ,é˜²æ­¢ç¾éš¾æ€§é—å¿˜

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”è¡¨

| ä¼˜åŒ–å™¨ | å†…å­˜å ç”¨ | æ”¶æ•›é€Ÿåº¦ | ç¨³å®šæ€§ | æ¨èåœºæ™¯ |
|--------|---------|---------|--------|---------|
| **Muon** | 1x | â­â­â­â­â­ | â­â­â­â­ | å¤§æ¨¡å‹é¢„è®­ç»ƒ |
| **AdamW** | 2x | â­â­â­â­ | â­â­â­â­â­ | é€šç”¨(é»„é‡‘æ ‡å‡†) |
| **Sophia** | 2x | â­â­â­â­â­ | â­â­â­ | å¤§æ¨¡å‹é¢„è®­ç»ƒ |
| **Lion** | 1x | â­â­â­â­ | â­â­â­ | å†…å­˜å—é™ |

| è°ƒåº¦å™¨ | å¤æ‚åº¦ | æ•ˆæœ | çµæ´»æ€§ | æ¨èåœºæ™¯ |
|--------|-------|------|--------|---------|
| **Warmup+Cosine** | ç®€å• | â­â­â­â­â­ | â­â­â­ | æ ‡å‡†é¢„è®­ç»ƒ |
| **Inverse Sqrt** | ç®€å• | â­â­â­â­ | â­â­â­â­ | ç»å…¸Transformer |
| **WSD** | ä¸­ç­‰ | â­â­â­â­ | â­â­â­â­â­ | æŒç»­è®­ç»ƒ |

---

## ğŸ”¬ å®éªŒå»ºè®®

### å¯¹æ¯”å®éªŒè®¾ç½®

```python
# å®éªŒ1: AdamWåŸºçº¿
exp1_optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95))
exp1_scheduler = get_warmup_cosine_schedule(exp1_optimizer, 4000, 100000)

# å®éªŒ2: Muonæ··åˆ
exp2_opts = get_hybrid_optimizer(model, muon_lr=0.02, adamw_lr=1e-3)
exp2_scheduler_muon = get_warmup_cosine_schedule(exp2_opts['muon'], 4000, 100000)
exp2_scheduler_adamw = get_warmup_cosine_schedule(exp2_opts['adamw'], 4000, 100000)

# å®éªŒ3: WSDè°ƒåº¦å™¨
exp3_optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95))
exp3_scheduler = get_wsd_schedule(exp3_optimizer, 4000, 80000, 16000)
```

### è¯„ä¼°æŒ‡æ ‡

```python
# è®°å½•
- Training Lossæ›²çº¿
- éªŒè¯é›†Perplexity
- GPUå†…å­˜å³°å€¼
- è®­ç»ƒé€Ÿåº¦ (tokens/sec)
- æ”¶æ•›æ­¥æ•°

# TensorBoardå¯è§†åŒ–
writer.add_scalars('Loss', {
    'AdamW': loss_adamw,
    'Muon': loss_muon,
    'WSD': loss_wsd
}, step)
```

---

## âœ… å¿«é€Ÿå¼€å§‹

### å½“å‰é¡¹ç›®ä½¿ç”¨

**å·²å®ç°** (scripts/train.py):
```python
# å½“å‰é…ç½®
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=3e-4,
    weight_decay=0.01,
    betas=(0.9, 0.95),
    eps=1e-8
)

scheduler = get_lr_scheduler(
    optimizer,
    warmup_steps=4000,
    max_steps=100000
)
# âœ… Warmup + Cosine Decayå·²å®ç°
```

### å‡çº§åˆ°Muon (å¯é€‰)

```python
# ä¿®æ”¹scripts/train.py
# æ›¿æ¢optimizeréƒ¨åˆ†ä¸º:

from src.model.optimizers import get_hybrid_optimizer, get_warmup_cosine_schedule

# åˆ›å»ºæ··åˆä¼˜åŒ–å™¨
opts = get_hybrid_optimizer(
    model,
    muon_lr=0.02,
    adamw_lr=1e-3,
    weight_decay=0.01
)

# åˆ›å»ºè°ƒåº¦å™¨
scheduler_muon = get_warmup_cosine_schedule(
    opts['muon'],
    num_warmup_steps=4000,
    num_training_steps=100000
)
scheduler_adamw = get_warmup_cosine_schedule(
    opts['adamw'],
    num_warmup_steps=4000,
    num_training_steps=100000
)

# è®­ç»ƒå¾ªç¯ä¸­
loss.backward()
opts['muon'].step()
opts['adamw'].step()
scheduler_muon.step()
scheduler_adamw.step()
opts['muon'].zero_grad()
opts['adamw'].zero_grad()
```

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **Muon** (2024)
   - è®ºæ–‡: https://arxiv.org/abs/2502.16982
   - ä½œè€…: Kimi AIå›¢é˜Ÿ

2. **Sophia** (2023)
   - è®ºæ–‡: https://arxiv.org/abs/2305.14342
   - å®éªŒ: 125M-13Bå‚æ•°æ¨¡å‹

3. **Lion** (2023)
   - è®ºæ–‡: https://arxiv.org/abs/2302.06675
   - Google Research

4. **WSDè°ƒåº¦å™¨** (2024)
   - è®ºæ–‡: https://arxiv.org/abs/2410.05192
   - ç‰¹ç‚¹: æ— éœ€é¢„è®¾æ€»æ­¥æ•°

5. **GPT-3é…ç½®** (2020)
   - AdamW with Î²2=0.95
   - Warmup + Cosine Decay to 10%

6. **Llama/Chinchilla** (2022-2023)
   - AdamW with Î²2=0.95
   - Cosine Decay to 10%

---

**æœ€åæ›´æ–°**: 2025-10-08
**ä½œè€…**: MiniGPT Team
**åŸºäº**: 2024-2025æœ€æ–°ç ”ç©¶
