#!/usr/bin/env python3
"""Orchestrate progressive MiniGPT training runs aligned with the scaling plan."""

from __future__ import annotations

import argparse
import json
import sys
from collections import OrderedDict
from collections.abc import Iterable, Iterator
from contextlib import contextmanager
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any

PROJECT_ROOT = Path(__file__).resolve().parents[1]


@contextmanager
def _project_import_context() -> Iterator[None]:
    """Temporarily prepend the repo root and src directory to ``sys.path``."""

    added: list[str] = []
    for candidate in (PROJECT_ROOT, PROJECT_ROOT / "src"):
        candidate_str = str(candidate)
        if candidate_str not in sys.path:
            sys.path.insert(0, candidate_str)
            added.append(candidate_str)
    try:
        yield
    finally:
        for candidate_str in added:
            try:
                sys.path.remove(candidate_str)
            except ValueError:  # pragma: no cover - defensive cleanup
                pass


@dataclass
class PhaseDefinition:
    """Describe a single training pass for a stage."""

    name: str
    mode: str
    description: str
    overrides: dict[str, int | float | bool] = field(default_factory=dict)
    resume_from_previous: bool = False
    auto_resume: bool = False
    retrain_tokenizer: bool = False
    config: str | None = None


@dataclass
class StageDefinition:
    """Describe a progressive stage composed of one or more phases."""

    id: str
    title: str
    target_params: str
    base_config: str
    focus: str
    dataset_scope: str
    checkpoints: str
    iteration_guidance: str
    phases: list[PhaseDefinition]


PROGRESSIVE_STAGES: OrderedDict[str, StageDefinition] = OrderedDict(
    (
        (
            "stage0",
            StageDefinition(
                id="stage0",
                title="Tokenizer ä¸æ•°æ®ç®¡çº¿å†’çƒŸéªŒè¯",
                target_params="â‰¤20M",
                base_config="small",
                focus="éªŒè¯ Tokenizer/æ•°æ®æ¸…æ´—/æ—¥å¿—æ˜¯å¦å·¥ä½œï¼Œå¿«é€Ÿè§‚å¯ŸæŸå¤±æ›²çº¿",
                dataset_scope="æŠ½æ · 1â€“2B tokenï¼Œä¸»è¯­è¨€ Python/TypeScript",
                checkpoints="é¦–æ¬¡å…¨é‡è®­ç»ƒï¼Œæ— éœ€ç»§æ‰¿ checkpoint",
                iteration_guidance="åœ¨å®Œæˆå†’çƒŸéªŒè¯åå†æ¬¡è¿­ä»£ 1â€“2 æ¬¡ä»¥ç¡®è®¤ä¿®å¤åçš„æ•°æ®/é…ç½®ç¨³å®š",
                phases=[
                    PhaseDefinition(
                        name="é˜¶æ®µ0 é¢„è®­ç»ƒ",
                        mode="pretrain",
                        description="ä»¥å°æ‰¹é‡è®­ç»ƒ 2â€“3k æ­¥ï¼Œç¡®è®¤ååä¸æŒ‡æ ‡é‡‡é›†",
                        overrides={
                            "max_steps": 3000,
                            "warmup_steps": 120,
                            "eval_steps": 200,
                            "save_steps": 600,
                        },
                    ),
                ],
            ),
        ),
        (
            "stage1",
            StageDefinition(
                id="stage1",
                title="è½»é‡å¯å¤ç°åŸºçº¿",
                target_params="60â€“80M",
                base_config="medium",
                focus="å»ºç«‹ç¨³å®šçš„å¤šè¯­è¨€è¡¥å…¨åŸºçº¿ï¼Œå¹¶éªŒè¯çŸ­ SFT æ¥å£",
                dataset_scope="4â€“6B token é¢„è®­ç»ƒ + çº¦ 20K æ¡æŒ‡ä»¤å¾®è°ƒæ ·æœ¬",
                checkpoints="ä»é˜¶æ®µ 0 äº§å‡ºçš„æƒé‡å¼€å§‹å¯åŠ é€Ÿæ”¶æ•›",
                iteration_guidance="å»ºè®®è‡³å°‘å¾ªç¯ä¸¤è½®ï¼šç¬¬ä¸€è½®è·å¾—åŸºçº¿ï¼Œç¬¬äºŒè½®é’ˆå¯¹è¯„æµ‹åé¦ˆè°ƒæ•´å­¦ä¹ ç‡/é‡‡æ ·",
                phases=[
                    PhaseDefinition(
                        name="é˜¶æ®µ1 é¢„è®­ç»ƒ",
                        mode="pretrain",
                        description="è·‘æ»¡ 40k æ­¥å·¦å³ï¼Œè§‚å¯Ÿ HumanEval/MBPP åˆå§‹åˆ†æ•°",
                        overrides={
                            "max_steps": 40000,
                            "warmup_steps": 800,
                            "eval_steps": 1000,
                            "save_steps": 4000,
                        },
                        resume_from_previous=True,
                        auto_resume=True,
                    ),
                    PhaseDefinition(
                        name="é˜¶æ®µ1 æŒ‡ä»¤å¾®è°ƒ",
                        mode="sft",
                        description="åŠ è½½é˜¶æ®µ1é¢„è®­ç»ƒæƒé‡ï¼Œè·‘ 2â€“3k æ­¥çŸ­ç¨‹æŒ‡ä»¤å¾®è°ƒ",
                        overrides={
                            "max_steps": 3000,
                            "warmup_steps": 60,
                            "eval_steps": 200,
                            "save_steps": 600,
                        },
                        resume_from_previous=True,
                        auto_resume=True,
                    ),
                ],
            ),
        ),
        (
            "stage2",
            StageDefinition(
                id="stage2",
                title="200M çº§æ€§èƒ½å†²åˆº",
                target_params="180â€“220M",
                base_config="foundation",
                focus="æ‰©å±•ä¸Šä¸‹æ–‡åˆ° 4Kï¼Œè¦†ç›– 15â€“25B token å¹¶æ‰©å¤§æŒ‡ä»¤é›†",
                dataset_scope="ä¸»è¯­è¨€é«˜è´¨é‡ä»“åº“ + 80Kâ€“120K æŒ‡ä»¤/æ‰§è¡Œåé¦ˆ",
                checkpoints="ç»§æ‰¿é˜¶æ®µ1æƒé‡ï¼Œä¿ç•™è¯„æµ‹æŒ‚é’©ä¸è’¸é¦æ¥å£",
                iteration_guidance="åœ¨ä¸åŒæ•°æ®æ··åˆæˆ–è’¸é¦ç­–ç•¥ä¹‹é—´åšå¤šæ¬¡ A/Bï¼Œå¯¹é½å…³é”®æŒ‡æ ‡åå†æ™‹çº§ä¸‹ä¸€é˜¶æ®µ",
                phases=[
                    PhaseDefinition(
                        name="é˜¶æ®µ2 é¢„è®­ç»ƒ",
                        mode="pretrain",
                        description="1â€“1.5 å¤©éå†æ•°æ®ï¼Œä¸»è¦å…³æ³¨æŸå¤±ç¨³å®šæ€§ä¸åå",
                        overrides={
                            "max_steps": 120000,
                            "warmup_steps": 3600,
                            "eval_steps": 3000,
                            "save_steps": 6000,
                        },
                        resume_from_previous=True,
                        auto_resume=True,
                    ),
                    PhaseDefinition(
                        name="é˜¶æ®µ2 æŒ‡ä»¤å¾®è°ƒ",
                        mode="sft",
                        description="ç»§ç»­æ‰©å……é«˜è´¨é‡æŒ‡ä»¤æ ·æœ¬ï¼Œç¡®ä¿ IDE ä½“éªŒ",
                        overrides={
                            "max_steps": 6000,
                            "warmup_steps": 120,
                            "eval_steps": 400,
                            "save_steps": 1200,
                        },
                        resume_from_previous=True,
                        auto_resume=True,
                    ),
                ],
            ),
        ),
        (
            "stage3",
            StageDefinition(
                id="stage3",
                title="å‘ 350M+ æ‰©å±•è¯•ç‚¹",
                target_params=">=350M",
                base_config="large",
                focus="åœ¨ç¡®è®¤ 200M æ»¡è¶³åº”ç”¨éœ€æ±‚åæ‰©å¤§æ·±åº¦ä¸å®½åº¦",
                dataset_scope="æ²¿ç”¨é˜¶æ®µ2æ¸…æ´—æˆæœï¼Œè¿½åŠ å¤§æ¨¡å‹ç‰¹åŒ–ä»»åŠ¡",
                checkpoints="æ‰¿æ¥é˜¶æ®µ2 æŒ‡ä»¤å¾®è°ƒåçš„æœ€æ–° checkpoint",
                iteration_guidance="é€šè¿‡å¤šè½®ç¼©çŸ­ç‰ˆè®­ç»ƒ (å¦‚ 20K æ­¥) å¿«é€ŸéªŒè¯æ˜¾å­˜/ååï¼Œå†è¿›å…¥æ­£å¼å…¨é‡è®­ç»ƒ",
                phases=[
                    PhaseDefinition(
                        name="é˜¶æ®µ3 é¢„è®­ç»ƒ",
                        mode="pretrain",
                        description="ä»¥ç»è¿‡éªŒè¯çš„è¶…å‚å¯åŠ¨ï¼Œé‡ç‚¹ç›‘æ§æ˜¾å­˜ä¸åå",
                        overrides={
                            "max_steps": 160000,
                            "warmup_steps": 4800,
                            "eval_steps": 4000,
                            "save_steps": 8000,
                        },
                        resume_from_previous=True,
                        auto_resume=True,
                    ),
                ],
            ),
        ),
    )
)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="æ ¹æ®æ¸è¿›å¼é…ç½®æ‰§è¡Œ MiniGPT è®­ç»ƒé˜¶æ®µ",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    parser.add_argument(
        "--stages",
        nargs="+",
        default=["stage0", "stage1", "stage2", "stage3"],
        choices=list(PROGRESSIVE_STAGES.keys()),
        help="é€‰æ‹©éœ€è¦è¿è¡Œçš„é˜¶æ®µï¼Œé»˜è®¤å…¨é‡",
    )
    parser.add_argument(
        "--execute",
        action="store_true",
        help="æ‰§è¡Œå®é™…è®­ç»ƒã€‚é»˜è®¤ä»…æ‰“å°è®¡åˆ’ (dry-run)",
    )
    parser.add_argument(
        "--resume-from",
        type=str,
        default=None,
        help="é¦–ä¸ªé˜¶æ®µ/é˜¶æ®µå†…é¦–ä¸ªå­é˜¶æ®µèµ·ç‚¹çš„ checkpoint è·¯å¾„",
    )
    parser.add_argument(
        "--learning-rate",
        type=float,
        default=None,
        help="è¦†ç›–æ‰€æœ‰é˜¶æ®µçš„å­¦ä¹ ç‡",
    )
    parser.add_argument(
        "--max-steps",
        type=int,
        default=None,
        help="è¦†ç›–æ‰€æœ‰é˜¶æ®µçš„æœ€å¤§è®­ç»ƒæ­¥æ•°",
    )
    parser.add_argument(
        "--warmup-steps",
        type=int,
        default=None,
        help="è¦†ç›–æ‰€æœ‰é˜¶æ®µçš„ warmup æ­¥æ•°",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=None,
        help="è¦†ç›–æ‰€æœ‰é˜¶æ®µçš„ batch size",
    )
    parser.add_argument(
        "--gradient-accumulation-steps",
        type=int,
        default=None,
        help="è¦†ç›–æ‰€æœ‰é˜¶æ®µçš„æ¢¯åº¦ç´¯ç§¯æ­¥æ•°",
    )
    parser.add_argument(
        "--auto-resume",
        action="store_true",
        help="å¼ºåˆ¶æ‰€æœ‰å­é˜¶æ®µå¯ç”¨ auto resume",
    )
    parser.add_argument(
        "--retrain-tokenizer",
        action="store_true",
        help="å¼ºåˆ¶æ‰€æœ‰å­é˜¶æ®µé‡æ–°è®­ç»ƒ tokenizer",
    )
    parser.add_argument(
        "--plan-output",
        type=Path,
        default=None,
        help="å°†è®­ç»ƒé˜¶æ®µè®¡åˆ’å¯¼å‡ºä¸º JSON æ–‡ä»¶",
    )
    return parser.parse_args()


def apply_overrides(config, overrides: dict[str, int | float | bool]) -> None:
    for key, value in overrides.items():
        if value is None:
            continue
        if not hasattr(config, key):
            print(f"âš ï¸  é…ç½® {config.model_size} æœªåŒ…å«å±æ€§ {key}ï¼Œå·²è·³è¿‡è¦†ç›–")
            continue
        setattr(config, key, value)


def plan_to_json(stage_ids: Iterable[str]) -> list[dict[str, Any]]:
    plan = []
    for stage_id in stage_ids:
        stage = PROGRESSIVE_STAGES[stage_id]
        plan.append(
            {
                "id": stage.id,
                "title": stage.title,
                "target_params": stage.target_params,
                "base_config": stage.base_config,
                "focus": stage.focus,
                "dataset_scope": stage.dataset_scope,
                "checkpoints": stage.checkpoints,
                "iteration_guidance": stage.iteration_guidance,
                "phases": [
                    {
                        "name": phase.name,
                        "mode": phase.mode,
                        "description": phase.description,
                        "overrides": phase.overrides,
                        "resume_from_previous": phase.resume_from_previous,
                        "auto_resume": phase.auto_resume,
                        "retrain_tokenizer": phase.retrain_tokenizer,
                        "config": phase.config or stage.base_config,
                    }
                    for phase in stage.phases
                ],
            }
        )
    return plan


def render_plan(stage_ids: Iterable[str]) -> None:
    plan = plan_to_json(stage_ids)
    print("\n=== æ¸è¿›å¼è®­ç»ƒè·¯çº¿å›¾ ===")
    for stage in plan:
        print(
            f"\n[{stage['id']}] {stage['title']}\n"
            f"  ç›®æ ‡å‚æ•°é‡: {stage['target_params']}\n"
            f"  è®­ç»ƒé…ç½®: {stage['base_config']}\n"
            f"  æ ¸å¿ƒç›®æ ‡: {stage['focus']}\n"
            f"  æ•°æ®èŒƒå›´: {stage['dataset_scope']}\n"
            f"  checkpoint ç­–ç•¥: {stage['checkpoints']}\n"
            f"  è¿­ä»£å»ºè®®: {stage['iteration_guidance']}"
        )
        for phase in stage["phases"]:
            print(
                f"    - {phase['name']} ({phase['mode']})\n"
                f"      æè¿°: {phase['description']}\n"
                f"      è¦†ç›–é¡¹: {phase['overrides'] or 'æ— '}\n"
                f"      ç»§æ‰¿ä¸Šä¸€é˜¶æ®µ: {'æ˜¯' if phase['resume_from_previous'] else 'å¦'}"
            )


def load_training_dependencies() -> tuple[object, type]:
    """Import heavy training dependencies lazily."""

    try:
        with _project_import_context():
            from config.training_config import get_config as get_training_config  # type: ignore
            from training.pipeline.app import MiniGPTTrainer  # type: ignore
    except ModuleNotFoundError as exc:  # pragma: no cover - import error messaging
        if exc.name == "torch":
            raise SystemExit(
                "æœªæ£€æµ‹åˆ° PyTorch (torch) åº“ã€‚è¯·å…ˆå®‰è£… GPU/CPU ç‰ˆ PyTorch åå†ä½¿ç”¨ --execute æ‰§è¡Œè®­ç»ƒã€‚"
            ) from exc
        raise

    return get_training_config, MiniGPTTrainer


def execute_plan(args: argparse.Namespace, stage_ids: Iterable[str]) -> None:
    get_training_config, MiniGPTTrainer = load_training_dependencies()
    last_checkpoint: str | None = args.resume_from

    for stage_id in stage_ids:
        stage = PROGRESSIVE_STAGES[stage_id]
        print(f"\n===== æ‰§è¡Œ {stage.id.upper()} : {stage.title} =====")
        for phase in stage.phases:
            config_name = phase.config or stage.base_config
            config = get_training_config(config_name)

            combined_overrides: dict[str, int | float | bool] = {}
            combined_overrides.update(phase.overrides)

            # Apply CLI overrides last so they always win
            if args.max_steps is not None:
                combined_overrides["max_steps"] = args.max_steps
            if args.warmup_steps is not None:
                combined_overrides["warmup_steps"] = args.warmup_steps
            if args.batch_size is not None:
                combined_overrides["batch_size"] = args.batch_size
            if args.gradient_accumulation_steps is not None:
                combined_overrides["gradient_accumulation_steps"] = args.gradient_accumulation_steps
            if args.learning_rate is not None:
                combined_overrides["learning_rate"] = args.learning_rate

            print(
                f"\nâ†’ {phase.name} [{phase.mode}] | é…ç½®: {config_name}\n"
                f"   æè¿°: {phase.description}"
            )
            apply_overrides(config, combined_overrides)

            for key in sorted(combined_overrides):
                value = getattr(config, key, None)
                print(f"   - {key}: {value}")

            resume_from: str | None = None
            if phase.resume_from_previous:
                resume_from = last_checkpoint
            if resume_from:
                print(f"   å°†ä» {resume_from} æ¢å¤")

            auto_resume = args.auto_resume or phase.auto_resume
            retrain_tokenizer = args.retrain_tokenizer or phase.retrain_tokenizer

            trainer = MiniGPTTrainer(config, mode=phase.mode)
            final_path = trainer.train(
                resume_from=resume_from,
                auto_resume=auto_resume,
                retrain_tokenizer=retrain_tokenizer,
            )
            last_checkpoint = final_path
            print(f"âœ… {phase.name} å®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜åˆ° {final_path}")


def main() -> None:
    args = parse_args()
    stage_ids = args.stages
    plan_json = plan_to_json(stage_ids)

    if args.plan_output is not None:
        args.plan_output.parent.mkdir(parents=True, exist_ok=True)
        args.plan_output.write_text(json.dumps(plan_json, ensure_ascii=False, indent=2) + "\n", encoding="utf-8")
        print(f"ğŸ“„ æ¸è¿›å¼è®¡åˆ’å·²ä¿å­˜è‡³ {args.plan_output}")

    if not args.execute:
        render_plan(stage_ids)
        print("\nğŸ’¡ ä½¿ç”¨ --execute ä»¥å®é™…è¿è¡Œè®­ç»ƒï¼›æ”¯æŒ --resume-from æŒ‡å®šåˆå§‹ checkpointã€‚")
        return

    execute_plan(args, stage_ids)


if __name__ == "__main__":
    main()
