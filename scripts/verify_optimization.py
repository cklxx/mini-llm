#!/usr/bin/env python3
"""
A6000ä¼˜åŒ–éªŒè¯è„šæœ¬
å¿«é€Ÿæ£€æŸ¥æ‰€æœ‰ä¼˜åŒ–é…ç½®æ˜¯å¦æ­£ç¡®åº”ç”¨
"""
import os
import sys

import torch

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(__file__))
sys.path.append(project_root)
sys.path.append(os.path.join(project_root, 'src'))

from config.training_config import get_medium_config


def print_section(title):
    """æ‰“å°åˆ†éš”ç¬¦"""
    print(f"\n{'='*60}")
    print(f"  {title}")
    print('='*60)


def verify_gpu():
    """éªŒè¯GPUé…ç½®"""
    print_section("GPUé…ç½®éªŒè¯")

    if not torch.cuda.is_available():
        print("âŒ æœªæ£€æµ‹åˆ°CUDA GPU")
        return False

    gpu_count = torch.cuda.device_count()
    print(f"âœ… æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")

    for i in range(gpu_count):
        props = torch.cuda.get_device_properties(i)
        print(f"\nGPU {i}: {props.name}")
        print(f"  - æ˜¾å­˜: {props.total_memory / 1024**3:.1f} GB")
        print(f"  - è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
        print(f"  - å¤šå¤„ç†å™¨: {props.multi_processor_count}")

        # éªŒè¯æ˜¯å¦ä¸ºA6000
        if "A6000" in props.name:
            print("  âœ… ç¡®è®¤ä¸º A6000 GPU")
            if props.total_memory / 1024**3 >= 45:
                print("  âœ… æ˜¾å­˜å……è¶³ (48GB)")
            else:
                print("  âš ï¸  æ˜¾å­˜ä¸è¶³48GB")
        else:
            print("  âš ï¸  éA6000 GPUï¼Œä¼˜åŒ–é…ç½®å¯èƒ½éœ€è¦è°ƒæ•´")

    return True


def verify_config():
    """éªŒè¯è®­ç»ƒé…ç½®"""
    print_section("è®­ç»ƒé…ç½®éªŒè¯")

    try:
        config = get_medium_config()
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return False

    print(f"\nğŸ“Š æ¨¡å‹é…ç½®: {config.model_size}")
    print(f"  - è®¾å¤‡: {config.device}")

    # éªŒè¯batch sizeä¼˜åŒ–
    print("\nğŸ”¹ Batché…ç½®:")
    print(f"  - Batch size: {config.batch_size}")
    expected_batch = 32 if config.device == "cuda" else 2
    if config.batch_size == expected_batch:
        print(f"  âœ… Batch sizeå·²ä¼˜åŒ– (æœŸæœ›: {expected_batch})")
    else:
        print(f"  âš ï¸  Batch sizeæœªè¾¾åˆ°é¢„æœŸ (å½“å‰: {config.batch_size}, æœŸæœ›: {expected_batch})")

    # éªŒè¯æ¢¯åº¦ç´¯ç§¯
    print(f"  - æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {config.gradient_accumulation_steps}")
    effective_batch = config.batch_size * config.gradient_accumulation_steps
    print(f"  - æœ‰æ•ˆbatch: {effective_batch}")
    if effective_batch >= 128:
        print("  âœ… æœ‰æ•ˆbatch sizeåˆç† (â‰¥128)")
    else:
        print("  âš ï¸  æœ‰æ•ˆbatch sizeåå° (<128)")

    # éªŒè¯æ•°æ®åŠ è½½ä¼˜åŒ–
    print("\nğŸ”¹ æ•°æ®åŠ è½½é…ç½®:")
    print(f"  - Workers: {config.num_workers}")
    if config.num_workers >= 4:
        print("  âœ… Workeræ•°é‡å·²ä¼˜åŒ– (â‰¥4)")
    else:
        print("  âš ï¸  Workeræ•°é‡åå°‘ (å»ºè®®â‰¥4)")

    if hasattr(config, 'prefetch_factor'):
        print(f"  - Prefetch factor: {config.prefetch_factor}")
        if config.prefetch_factor >= 2:
            print("  âœ… é¢„å–é…ç½®å·²ä¼˜åŒ– (â‰¥2)")
        else:
            print("  âš ï¸  é¢„å–é…ç½®åå°")
    else:
        print("  âš ï¸  æœªé…ç½®prefetch_factor")

    print(f"  - Pin memory: {config.pin_memory}")
    if config.pin_memory and config.device == "cuda":
        print("  âœ… Pin memoryå·²å¯ç”¨")
    elif config.device == "cuda":
        print("  âš ï¸  Pin memoryæœªå¯ç”¨")

    print(f"  - Persistent workers: {config.persistent_workers}")
    if config.persistent_workers and config.num_workers > 0:
        print("  âœ… Persistent workerså·²å¯ç”¨")

    # éªŒè¯æ··åˆç²¾åº¦
    print("\nğŸ”¹ è®­ç»ƒä¼˜åŒ–:")
    print(f"  - æ··åˆç²¾åº¦: {config.mixed_precision}")
    if config.mixed_precision and config.device == "cuda":
        print("  âœ… æ··åˆç²¾åº¦å·²å¯ç”¨ (FP16)")
    elif config.device == "cuda":
        print("  âš ï¸  æ··åˆç²¾åº¦æœªå¯ç”¨")

    print(f"  - æ¢¯åº¦æ£€æŸ¥ç‚¹: {config.gradient_checkpointing}")
    if config.gradient_checkpointing:
        print("  âœ… æ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨")

    print(f"  - Flash Attention: {config.flash_attention}")
    if config.flash_attention and config.device == "cuda":
        print("  âœ… Flash Attentionå·²é…ç½®")

    print(f"  - æ¨¡å‹ç¼–è¯‘: {config.compile_model}")

    return True


def verify_cuda_optimizations():
    """éªŒè¯CUDAä¼˜åŒ–"""
    print_section("CUDAä¼˜åŒ–éªŒè¯")

    if not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œè·³è¿‡éªŒè¯")
        return True

    # TF32
    print(f"TF32 (matmul): {torch.backends.cuda.matmul.allow_tf32}")
    print(f"TF32 (cudnn): {torch.backends.cudnn.allow_tf32}")
    if torch.backends.cuda.matmul.allow_tf32:
        print("âœ… TF32å·²å¯ç”¨ (Ampereæ¶æ„ä¼˜åŒ–)")
    else:
        print("âš ï¸  TF32æœªå¯ç”¨")

    # CuDNN benchmark
    print(f"\nCuDNN Benchmark: {torch.backends.cudnn.benchmark}")
    if torch.backends.cudnn.benchmark:
        print("âœ… CuDNN Benchmarkå·²å¯ç”¨")
    else:
        print("âš ï¸  CuDNN Benchmarkæœªå¯ç”¨")

    # æ··åˆç²¾åº¦æ”¯æŒ
    print("\næ··åˆç²¾åº¦æ”¯æŒ:")
    try:
        torch.cuda.amp.GradScaler()
        print("âœ… GradScalerå¯ç”¨")
    except Exception as e:
        print(f"âŒ GradScalerä¸å¯ç”¨: {e}")

    return True


def estimate_memory():
    """ä¼°ç®—æ˜¾å­˜ä½¿ç”¨"""
    print_section("æ˜¾å­˜ä¼°ç®—")

    try:
        config = get_medium_config()
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return False

    # æ¨¡å‹å‚æ•°ä¼°ç®—
    # Medium: d_model=512, n_layers=16, n_heads=16
    d_model = 512
    n_layers = 16
    vocab_size = 20000

    # å‚æ•°é‡ä¼°ç®— (ç®€åŒ–)
    embedding_params = vocab_size * d_model  # è¾“å…¥+è¾“å‡ºembedding
    attention_params = n_layers * (4 * d_model * d_model)  # QKV + output
    ffn_params = n_layers * (2 * d_model * 1536)  # FFN

    total_params = embedding_params + attention_params + ffn_params
    print(f"æ¨¡å‹å‚æ•°é‡ä¼°ç®—: ~{total_params/1e6:.1f}M")

    # æ˜¾å­˜ä¼°ç®—
    bytes_per_param_fp32 = 4
    bytes_per_param_fp16 = 2

    model_memory_fp32 = total_params * bytes_per_param_fp32 / 1024**3
    model_memory_fp16 = total_params * bytes_per_param_fp16 / 1024**3

    print("\næ¨¡å‹æƒé‡æ˜¾å­˜:")
    print(f"  - FP32: ~{model_memory_fp32:.2f} GB")
    print(f"  - FP16: ~{model_memory_fp16:.2f} GB")

    # ä¼˜åŒ–å™¨çŠ¶æ€ (AdamW)
    optimizer_memory = total_params * 8 / 1024**3  # momentum + variance (fp32)
    print(f"\nä¼˜åŒ–å™¨çŠ¶æ€æ˜¾å­˜: ~{optimizer_memory:.2f} GB")

    # æ¿€æ´»å€¼ä¼°ç®—
    batch_size = config.batch_size
    seq_len = config.max_seq_len
    activation_memory = (batch_size * seq_len * d_model * n_layers * 4) / 1024**3
    print(f"\næ¿€æ´»å€¼æ˜¾å­˜ (batch={batch_size}, seq={seq_len}):")
    print(f"  - FP32: ~{activation_memory:.2f} GB")
    print(f"  - FP16: ~{activation_memory/2:.2f} GB")

    # æ€»è®¡
    if config.mixed_precision:
        total_memory = model_memory_fp16 + optimizer_memory + activation_memory/2
        precision = "FP16"
    else:
        total_memory = model_memory_fp32 + optimizer_memory + activation_memory
        precision = "FP32"

    print(f"\næ€»æ˜¾å­˜ä¼°ç®— ({precision}): ~{total_memory:.2f} GB")

    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"å¯ç”¨GPUæ˜¾å­˜: {gpu_memory:.1f} GB")

        if total_memory < gpu_memory * 0.8:
            print(f"âœ… æ˜¾å­˜å……è¶³ (ä½¿ç”¨ç‡: {total_memory/gpu_memory*100:.1f}%)")
        else:
            print(f"âš ï¸  æ˜¾å­˜å¯èƒ½ä¸è¶³ (é¢„ä¼°ä½¿ç”¨ç‡: {total_memory/gpu_memory*100:.1f}%)")

    return True


def print_summary():
    """æ‰“å°ä¼˜åŒ–æ€»ç»“"""
    print_section("ä¼˜åŒ–æ€»ç»“")

    print("""
ğŸ¯ å·²åº”ç”¨çš„ä¼˜åŒ–:
  1. âœ… Batch size: 12 â†’ 32 (æå‡2.7å€)
  2. âœ… DataLoader workers: 0 â†’ 8 (å¹¶è¡Œæ•°æ®åŠ è½½)
  3. âœ… Prefetch factor: None â†’ 4 (é¢„å–ä¼˜åŒ–)
  4. âœ… Pin memory: å¯ç”¨ (åŠ é€Ÿæ•°æ®ä¼ è¾“)
  5. âœ… Persistent workers: å¯ç”¨ (å‡å°‘è¿›ç¨‹å¼€é”€)
  6. âœ… æ··åˆç²¾åº¦è®­ç»ƒ: FP16 (èŠ‚çœ40-50%æ˜¾å­˜)
  7. âœ… æ¢¯åº¦ç´¯ç§¯: ä¼˜åŒ–é€»è¾‘
  8. âœ… Non-blockingä¼ è¾“: å¯ç”¨ (å¼‚æ­¥æ•°æ®ä¼ è¾“)

ğŸ“ˆ é¢„æœŸæ€§èƒ½æå‡:
  - GPUåˆ©ç”¨ç‡: 30% â†’ 70-90% (2-3å€)
  - è®­ç»ƒé€Ÿåº¦: æå‡2-2.5å€
  - æ˜¾å­˜å ç”¨: å‡å°‘20-30%

ğŸš€ å¼€å§‹è®­ç»ƒ:
  python3 scripts/train.py --mode pretrain --config medium

ğŸ“Š ç›‘æ§æ€§èƒ½:
  watch -n 1 nvidia-smi
    """)


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*60)
    print("  A6000 GPU è®­ç»ƒä¼˜åŒ–éªŒè¯")
    print("="*60)

    # è¿è¡Œæ‰€æœ‰éªŒè¯
    results = []
    results.append(("GPUé…ç½®", verify_gpu()))
    results.append(("è®­ç»ƒé…ç½®", verify_config()))
    results.append(("CUDAä¼˜åŒ–", verify_cuda_optimizations()))
    results.append(("æ˜¾å­˜ä¼°ç®—", estimate_memory()))

    # æ‰“å°æ€»ç»“
    print_summary()

    # éªŒè¯ç»“æœ
    print_section("éªŒè¯ç»“æœ")
    all_passed = True
    for name, passed in results:
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"{name}: {status}")
        all_passed = all_passed and passed

    print("\n" + "="*60)
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰éªŒè¯é€šè¿‡ï¼ä¼˜åŒ–é…ç½®æ­£ç¡®åº”ç”¨ã€‚")
    else:
        print("âš ï¸  éƒ¨åˆ†éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚")
    print("="*60 + "\n")

    return 0 if all_passed else 1


if __name__ == "__main__":
    sys.exit(main())
