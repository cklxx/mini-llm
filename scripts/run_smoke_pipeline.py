#!/usr/bin/env python3
"""Run a minimal end-to-end training + inference smoke test.

The script prepares tiny synthetic datasets, runs the four training
modes (pretrain â†’ sft â†’ dpo â†’ rlhf) with extremely small budgets, and
finally performs a single inference call on the RLHF checkpoint.  The
goal is to guarantee that every stage of the pipeline can execute
successfully on CPU-only environments within a few minutes.
"""
from __future__ import annotations

import json
import os
import shutil
import subprocess
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable

PROJECT_ROOT = Path(__file__).resolve().parents[1]
DATA_DIR = PROJECT_ROOT / "data"
CHECKPOINT_DIR = PROJECT_ROOT / "checkpoints"


DATASETS: dict[Path, list[dict[str, object]]] = {
    DATA_DIR / "pretrain_hq.jsonl": [
        {
            "text": "Mini-LLM focuses on compact transformer experiments for quick iteration.",
        },
        {
            "text": "Progressive training allows staged scaling without large compute.",
        },
        {
            "text": "Curriculum style pretraining can warm up tokenizers and pipelines.",
        },
    ],
    DATA_DIR / "sft_mini_512.jsonl": [
        {
            "conversations": [
                {"role": "user", "content": "è¯·ç”¨ä¸€å¥è¯æè¿° Mini-LLM æ¡†æ¶ã€‚"},
                {
                    "role": "assistant",
                    "content": "Mini-LLM æ˜¯ä¸€ä¸ªæ•™å­¦å‹å¥½çš„å°å‹è¯­è¨€æ¨¡å‹è®­ç»ƒä¸æ¨ç†æ¼”ç¤ºæ¡†æ¶ã€‚",
                },
            ]
        },
        {
            "conversations": [
                {"role": "user", "content": "å¦‚ä½•å¿«é€ŸéªŒè¯è®­ç»ƒæµç¨‹ï¼Ÿ"},
                {
                    "role": "assistant",
                    "content": "å¯ä»¥è¿è¡Œå†…ç½®çš„çƒŸé›¾æµ‹è¯•è„šæœ¬ï¼Œä¾æ¬¡æ‰§è¡Œé¢„è®­ç»ƒã€SFTã€DPO å’Œ RLHFã€‚",
                },
            ]
        },
    ],
    DATA_DIR / "alex_identity.jsonl": [
        {
            "conversations": [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯åŠ©æ•™ Alexï¼Œè´Ÿè´£è§£é‡Š Mini-LLM çš„è®¾è®¡ã€‚",
                },
                {"role": "user", "content": "è¯·åšä¸ªè‡ªæˆ‘ä»‹ç»ã€‚"},
                {
                    "role": "assistant",
                    "content": "å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯åŠ©æ•™ Alexï¼Œä¸“æ³¨äºè®²è§£ Mini-LLM çš„è®­ç»ƒä¸æ¨ç†æµç¨‹ã€‚",
                },
            ]
        },
        {
            "conversations": [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯åŠ©æ•™ Alexï¼Œè´Ÿè´£è§£é‡Š Mini-LLM çš„è®¾è®¡ã€‚",
                },
                {"role": "user", "content": "ä½ èƒ½æä¾›å“ªäº›å¸®åŠ©ï¼Ÿ"},
                {
                    "role": "assistant",
                    "content": "æˆ‘å¯ä»¥å¸¦ä½ è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµæ°´çº¿ï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µæ¼”ç¤ºæ–‡æœ¬ç”Ÿæˆã€‚",
                },
            ]
        },
    ],
    DATA_DIR / "ultra_think.jsonl": [
        {
            "conversations": [
                {
                    "role": "user",
                    "content": "è¯·æ·±å…¥åˆ†æä¸ºä»€ä¹ˆè¦è¿›è¡Œæ¸è¿›å¼è®­ç»ƒã€‚",
                },
                {
                    "role": "assistant",
                    "content": (
                        "<ultra_think>æ¸è¿›å¼è®­ç»ƒå…è®¸åœ¨ä¿è¯ç¨³å®šæ€§çš„å‰æä¸‹é€æ­¥æ‰©å±•æ¨¡å‹ï¼š"
                        "å…ˆç”¨å°‘é‡æ•°æ®éªŒè¯æµç¨‹ï¼Œå†é€æ­¥æ”¾å¤§è§„æ¨¡ï¼Œä»¥ä¾¿åŠæ—¶å‘ç°é—®é¢˜å¹¶æ§åˆ¶èµ„æºã€‚</ultra_think>"
                    ),
                },
            ]
        }
    ],
    DATA_DIR / "dpo.jsonl": [
        {
            "prompt": "ç”¨æˆ·: å¸®æˆ‘æ€»ç»“ Mini-LLM çš„ç›®æ ‡ã€‚",
            "chosen": "åŠ©æ‰‹: Mini-LLM æ—¨åœ¨ç”¨æœ€å°çš„å·¥ç¨‹æˆæœ¬å±•ç¤ºå®Œæ•´çš„è®­ç»ƒä¸æ¨ç†æµç¨‹ã€‚",
            "rejected": "åŠ©æ‰‹: æˆ‘ä¸çŸ¥é“ã€‚",
        },
        {
            "prompt": "ç”¨æˆ·: ä¸ºä»€ä¹ˆè¦è·‘ä¸€éå…¨éƒ¨æµç¨‹ï¼Ÿ",
            "chosen": (
                "åŠ©æ‰‹: é€šè¿‡è¿è¡Œé¢„è®­ç»ƒã€SFTã€DPO å’Œ RLHFï¼Œå¯ä»¥ç¡®è®¤æ•°æ®ç®¡çº¿ã€æ£€æŸ¥ç‚¹ä¸æ¨ç†æ¥å£ååŒæ­£å¸¸ã€‚"
            ),
            "rejected": "åŠ©æ‰‹: ä¸éœ€è¦ã€‚",
        },
    ],
}


@dataclass
class Stage:
    mode: str
    max_steps: int
    warmup_steps: int = 0
    extra_args: tuple[str, ...] = ()


STAGES: tuple[Stage, ...] = (
    Stage(mode="pretrain", max_steps=4),
    Stage(mode="sft", max_steps=3, extra_args=("--auto-resume",)),
    Stage(mode="dpo", max_steps=3, extra_args=("--auto-resume",)),
    Stage(mode="rlhf", max_steps=3, extra_args=("--auto-resume",)),
)


def ensure_jsonl(path: Path, records: Iterable[dict[str, object]]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as handle:
        for record in records:
            handle.write(json.dumps(record, ensure_ascii=False) + "\n")


def prepare_datasets() -> None:
    print("ğŸ“¦ Preparing synthetic datasets...")
    for path, records in DATASETS.items():
        ensure_jsonl(path, records)
        print(f"  â€¢ {path.relative_to(PROJECT_ROOT)} ({len(records)} samples)")


def clean_previous_runs() -> None:
    for stage in STAGES:
        ckpt_dir = CHECKPOINT_DIR / f"{stage.mode}_tiny"
        if ckpt_dir.exists():
            print(f"ğŸ§¹ Removing stale checkpoints: {ckpt_dir.relative_to(PROJECT_ROOT)}")
            shutil.rmtree(ckpt_dir)


def run_stage(stage: Stage, env: dict[str, str]) -> None:
    cmd = [
        sys.executable,
        str(PROJECT_ROOT / "scripts" / "train.py"),
        "--mode",
        stage.mode,
        "--config",
        "tiny",
        "--max-steps",
        str(stage.max_steps),
        "--batch-size",
        "2",
        "--warmup-steps",
        str(stage.warmup_steps),
        *stage.extra_args,
    ]
    print(f"\nğŸš€ Running {stage.mode.upper()} stage (max_steps={stage.max_steps})")
    subprocess.run(cmd, check=True, cwd=PROJECT_ROOT, env=env)


def run_inference(model_path: Path, env: dict[str, str]) -> None:
    prompt = "è¯·ç”¨ä¸¤å¥è¯æ€»ç»“åˆšåˆšè®­ç»ƒå®Œæˆçš„æ¨¡å‹æµç¨‹ã€‚"
    cmd = [
        sys.executable,
        str(PROJECT_ROOT / "scripts" / "generate.py"),
        "--model-path",
        str(model_path),
        "--mode",
        "single",
        "--prompt",
        prompt,
        "--max-new-tokens",
        "32",
        "--device",
        "cpu",
    ]
    print("\nğŸ§ª Running inference sanity check...")
    subprocess.run(cmd, check=True, cwd=PROJECT_ROOT, env=env)


def main() -> None:
    prepare_datasets()
    clean_previous_runs()

    env = os.environ.copy()
    env.update(
        {
            "MINIGPT_VAL_SPLIT": "0.0",
            "MINIGPT_PRETRAIN_HQ_RATIO": "1.0",
            "MINIGPT_PRETRAIN_WIKI_RATIO": "0.0",
            "MINIGPT_PRETRAIN_CHINA_RATIO": "0.0",
            "MINIGPT_PRETRAIN_PJ_RATIO": "0.0",
            "MINIGPT_SFT_MAIN_RATIO": "1.0",
            "MINIGPT_SFT_MAIN_MAX": "16",
            "MINIGPT_MEMORY_MONITOR": "0",
            "MINIGPT_DISABLE_MANIFEST": "1",
            "PYTHONUNBUFFERED": "1",
        }
    )

    for stage in STAGES:
        run_stage(stage, env)

    final_model = CHECKPOINT_DIR / f"{STAGES[-1].mode}_tiny" / "final_model.pt"
    if not final_model.exists():
        raise SystemExit(f"âŒ Expected model checkpoint not found: {final_model}")

    run_inference(final_model, env)
    print("\nâœ… Smoke test finished successfully!")


if __name__ == "__main__":
    main()
