#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
100MBæ¨¡å‹æ·±åº¦åˆ†ææµ‹è¯•è„šæœ¬
=============================

åŠŸèƒ½ï¼š
1. éªŒè¯100MBåŸºå‡†æ¨¡å‹çš„å‚æ•°é‡è®¡ç®—
2. æµ‹è¯•è®­ç»ƒå’Œæ¨ç†æ€§èƒ½
3. ç›‘æ§æ˜¾å­˜å’Œå†…å­˜ä½¿ç”¨æƒ…å†µ
4. æä¾›è¯¦ç»†çš„æ€§èƒ½åˆ†æå’Œä¼˜åŒ–å»ºè®®

ä½œè€…: alex-ckl.com AIç ”å‘å›¢é˜Ÿ
"""

import sys
import os
import time
import json
import traceback
from pathlib import Path
from typing import Dict, Any, Optional, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("âš ï¸  PyTorchæœªå®‰è£…ï¼Œå°†è·³è¿‡å®é™…è®­ç»ƒå’Œæ¨ç†æµ‹è¯•")

from src.model.config import get_medium_config, MiniGPTConfig


def calculate_model_params_detailed(config: MiniGPTConfig) -> Dict[str, int]:
    """
    è¯¦ç»†è®¡ç®—æ¨¡å‹å‚æ•°é‡
    ==================

    è€ƒè™‘ä»¥ä¸‹ä¼˜åŒ–æŠ€æœ¯ï¼š
    - GQA (Grouped-Query Attention): å‡å°‘KVå¤´æ•°é‡
    - æƒé‡å…±äº« (tie_word_embeddings): è¾“å…¥è¾“å‡ºåµŒå…¥å…±äº«
    - RoPEä½ç½®ç¼–ç : æ— éœ€å­¦ä¹ ä½ç½®å‚æ•°
    - SwiGLUæ¿€æ´»: éœ€è¦3ä¸ªçº¿æ€§å±‚è€Œé2ä¸ª

    è¿”å›å„ç»„ä»¶è¯¦ç»†å‚æ•°ç»Ÿè®¡
    """

    # 1. è¯åµŒå…¥å±‚å‚æ•°
    embedding_params = config.vocab_size * config.hidden_size

    # 2. æ³¨æ„åŠ›æœºåˆ¶å‚æ•° (è€ƒè™‘GQAä¼˜åŒ–)
    if getattr(config, 'use_gqa', False) and getattr(config, 'num_key_value_heads', None):
        # GQA: Qå¤´æ­£å¸¸ï¼ŒK/Vå¤´å‡å°‘
        head_dim = config.hidden_size // config.num_attention_heads

        # QueryæŠ•å½± (æ‰€æœ‰Qå¤´)
        q_params = config.hidden_size * config.hidden_size

        # Key/ValueæŠ•å½± (å‡å°‘çš„KVå¤´)
        kv_dim = config.num_key_value_heads * head_dim
        k_params = config.hidden_size * kv_dim
        v_params = config.hidden_size * kv_dim

        # OutputæŠ•å½±
        o_params = config.hidden_size * config.hidden_size

        attention_params_per_layer = q_params + k_params + v_params + o_params

        # GQAèŠ‚çœçš„å‚æ•°é‡
        traditional_kv_params = 2 * config.hidden_size * config.hidden_size
        gqa_kv_params = k_params + v_params
        gqa_savings = traditional_kv_params - gqa_kv_params

    else:
        # ä¼ ç»ŸMHA: Q, K, V, OæŠ•å½±
        attention_params_per_layer = 4 * config.hidden_size * config.hidden_size
        gqa_savings = 0

    # 3. å‰é¦ˆç½‘ç»œå‚æ•° (SwiGLUéœ€è¦3ä¸ªçº¿æ€§å±‚)
    if config.hidden_act.lower() == 'swiglu':
        # SwiGLU: gate_proj + up_proj + down_proj
        ffn_params_per_layer = 3 * config.hidden_size * config.intermediate_size
    else:
        # ä¼ ç»ŸFFN: up_proj + down_proj
        ffn_params_per_layer = 2 * config.hidden_size * config.intermediate_size

    # 4. å±‚å½’ä¸€åŒ–å‚æ•° (RMSNorm)
    # æ¯ä¸ªTransformerå±‚æœ‰2ä¸ªRMSNorm: attention_norm + ffn_norm
    norm_params_per_layer = 2 * config.hidden_size

    # 5. å•å±‚æ€»å‚æ•°
    layer_params = attention_params_per_layer + ffn_params_per_layer + norm_params_per_layer

    # 6. æ‰€æœ‰Transformerå±‚
    transformer_params = config.num_hidden_layers * layer_params

    # 7. è¾“å‡ºå±‚
    output_norm_params = config.hidden_size  # æœ€ç»ˆRMSNorm

    # 8. è¾“å‡ºæŠ•å½± (è€ƒè™‘æƒé‡å…±äº«)
    if getattr(config, 'tie_word_embeddings', False):
        output_projection_params = 0  # å…±äº«è¾“å…¥åµŒå…¥æƒé‡
        weight_sharing_savings = config.vocab_size * config.hidden_size
    else:
        output_projection_params = config.vocab_size * config.hidden_size
        weight_sharing_savings = 0

    # 9. æ€»å‚æ•°é‡
    total_params = (
        embedding_params +
        transformer_params +
        output_norm_params +
        output_projection_params
    )

    # 10. å‚æ•°è¯¦ç»†ç»Ÿè®¡
    details = {
        # åŸºç¡€ç»„ä»¶
        'embedding_params': embedding_params,
        'transformer_params': transformer_params,
        'output_norm_params': output_norm_params,
        'output_projection_params': output_projection_params,
        'total_params': total_params,

        # å•å±‚è¯¦ç»†
        'attention_params_per_layer': attention_params_per_layer,
        'ffn_params_per_layer': ffn_params_per_layer,
        'norm_params_per_layer': norm_params_per_layer,
        'layer_params': layer_params,

        # ä¼˜åŒ–æ•ˆæœ
        'gqa_savings_per_layer': gqa_savings,
        'total_gqa_savings': gqa_savings * config.num_hidden_layers,
        'weight_sharing_savings': weight_sharing_savings,

        # å†…å­˜ä¼°ç®— (FP16)
        'memory_fp16_mb': total_params * 2 / (1024 * 1024),  # 2 bytes per param
        'memory_fp32_mb': total_params * 4 / (1024 * 1024),  # 4 bytes per param
    }

    return details


def analyze_model_config(config_name: str = "medium") -> Dict[str, Any]:
    """
    åˆ†ææ¨¡å‹é…ç½®
    ============

    æ·±åº¦åˆ†æç»™å®šé…ç½®çš„æ¶æ„è®¾è®¡å’Œå‚æ•°æ•ˆç‡
    """
    print(f"ğŸ” åˆ†æ {config_name.upper()} æ¨¡å‹é…ç½®")
    print("=" * 60)

    # è·å–é…ç½®
    if config_name == "medium":
        config = get_medium_config()
    else:
        from src.model.config import get_config
        config = get_config(config_name)

    # åŸºç¡€æ¶æ„ä¿¡æ¯
    arch_info = {
        'config_name': config_name,
        'vocab_size': config.vocab_size,
        'hidden_size': config.hidden_size,
        'num_layers': config.num_hidden_layers,
        'num_attention_heads': config.num_attention_heads,
        'intermediate_size': config.intermediate_size,
        'max_position_embeddings': config.max_position_embeddings,

        # ä¼˜åŒ–ç‰¹æ€§
        'use_rope': getattr(config, 'use_rope', False),
        'use_gqa': getattr(config, 'use_gqa', False),
        'num_key_value_heads': getattr(config, 'num_key_value_heads', None),
        'tie_word_embeddings': getattr(config, 'tie_word_embeddings', False),
        'hidden_act': config.hidden_act,
    }

    # è¯¦ç»†å‚æ•°è®¡ç®—
    param_details = calculate_model_params_detailed(config)

    # æ¶æ„åˆ†æ
    head_dim = config.hidden_size // config.num_attention_heads
    ffn_ratio = config.intermediate_size / config.hidden_size

    analysis = {
        'architecture': arch_info,
        'parameters': param_details,
        'analysis': {
            'head_dim': head_dim,
            'ffn_expansion_ratio': ffn_ratio,
            'depth_to_width_ratio': config.num_hidden_layers / config.hidden_size,
            'params_per_layer_mb': param_details['layer_params'] * 2 / (1024 * 1024),  # FP16
            'is_deep_thin': config.num_hidden_layers > 16 and config.hidden_size < 768,
        }
    }

    # æ‰“å°åˆ†æç»“æœ
    print(f"ğŸ“Š æ¶æ„è®¾è®¡:")
    print(f"  â€¢ éšè—ç»´åº¦: {config.hidden_size}")
    print(f"  â€¢ å±‚æ•°: {config.num_hidden_layers}")
    print(f"  â€¢ æ³¨æ„åŠ›å¤´æ•°: {config.num_attention_heads}")
    if arch_info['use_gqa']:
        print(f"  â€¢ KVå¤´æ•°: {config.num_key_value_heads} (GQAä¼˜åŒ–)")
    print(f"  â€¢ FFNç»´åº¦: {config.intermediate_size} (Ã—{ffn_ratio:.1f})")
    print(f"  â€¢ æœ€å¤§åºåˆ—é•¿åº¦: {config.max_position_embeddings}")

    print(f"\nğŸ¯ ä¼˜åŒ–æŠ€æœ¯:")
    print(f"  â€¢ RoPEä½ç½®ç¼–ç : {'âœ…' if arch_info['use_rope'] else 'âŒ'}")
    print(f"  â€¢ åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›: {'âœ…' if arch_info['use_gqa'] else 'âŒ'}")
    print(f"  â€¢ SwiGLUæ¿€æ´»: {'âœ…' if config.hidden_act == 'swiglu' else 'âŒ'}")
    print(f"  â€¢ æƒé‡å…±äº«: {'âœ…' if arch_info['tie_word_embeddings'] else 'âŒ'}")

    print(f"\nğŸ“ˆ å‚æ•°ç»Ÿè®¡:")
    print(f"  â€¢ æ€»å‚æ•°é‡: {param_details['total_params']:,}")
    print(f"  â€¢ åµŒå…¥å±‚: {param_details['embedding_params']:,}")
    print(f"  â€¢ Transformerå±‚: {param_details['transformer_params']:,}")
    print(f"  â€¢ è¾“å‡ºå±‚: {param_details['output_norm_params'] + param_details['output_projection_params']:,}")

    if param_details['total_gqa_savings'] > 0:
        print(f"  â€¢ GQAèŠ‚çœ: {param_details['total_gqa_savings']:,} å‚æ•°")
    if param_details['weight_sharing_savings'] > 0:
        print(f"  â€¢ æƒé‡å…±äº«èŠ‚çœ: {param_details['weight_sharing_savings']:,} å‚æ•°")

    print(f"\nğŸ’¾ å†…å­˜ä½¿ç”¨:")
    print(f"  â€¢ FP16æ¨¡å¼: {param_details['memory_fp16_mb']:.1f} MB")
    print(f"  â€¢ FP32æ¨¡å¼: {param_details['memory_fp32_mb']:.1f} MB")

    return analysis


def test_training_performance() -> Optional[Dict[str, Any]]:
    """
    æµ‹è¯•è®­ç»ƒæ€§èƒ½
    ============

    æµ‹è¯•100MBæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼Œç›‘æ§ï¼š
    - å‰å‘ä¼ æ’­æ—¶é—´
    - åå‘ä¼ æ’­æ—¶é—´
    - å†…å­˜ä½¿ç”¨æƒ…å†µ
    - æ‰¹æ¬¡å¤„ç†é€Ÿåº¦
    """
    if not TORCH_AVAILABLE:
        print("âš ï¸  è·³è¿‡è®­ç»ƒæµ‹è¯• (PyTorchæœªå®‰è£…)")
        return None

    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒæ€§èƒ½æµ‹è¯•")
    print("=" * 60)

    # è®¾å¤‡æ£€æµ‹
    if torch.cuda.is_available():
        device = torch.device("cuda")
        device_name = torch.cuda.get_device_name()
        print(f"ğŸ”¥ ä½¿ç”¨GPU: {device_name}")
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        device = torch.device("mps")
        print(f"ğŸ ä½¿ç”¨Apple Silicon GPU")
    else:
        device = torch.device("cpu")
        print(f"ğŸ’» ä½¿ç”¨CPU")

    # åˆ›å»ºæ¨¡å‹
    config = get_medium_config()

    # å¯¼å…¥æ¨¡å‹ (éœ€è¦æ£€æŸ¥æ˜¯å¦å¯ç”¨)
    try:
        from src.model.transformer import MiniGPT
        model = MiniGPT(config).to(device)
        print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return None

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 4
    seq_len = 512
    vocab_size = config.vocab_size

    # ç”Ÿæˆéšæœºè®­ç»ƒæ•°æ®
    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)
    labels = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)

    print(f"ğŸ“Š æµ‹è¯•é…ç½®:")
    print(f"  â€¢ æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"  â€¢ åºåˆ—é•¿åº¦: {seq_len}")
    print(f"  â€¢ è¯æ±‡è¡¨å¤§å°: {vocab_size}")

    # ä¼˜åŒ–å™¨è®¾ç½®
    optimizer = optim.AdamW(
        model.parameters(),
        lr=1e-4,
        weight_decay=0.01,
        betas=(0.9, 0.95)
    )
    criterion = nn.CrossEntropyLoss()

    # å†…å­˜ç›‘æ§
    def get_memory_usage():
        if device.type == "cuda":
            return {
                'allocated_mb': torch.cuda.memory_allocated() / 1024**2,
                'reserved_mb': torch.cuda.memory_reserved() / 1024**2,
                'max_allocated_mb': torch.cuda.max_memory_allocated() / 1024**2
            }
        else:
            return {'allocated_mb': 0, 'reserved_mb': 0, 'max_allocated_mb': 0}

    # è®­ç»ƒæ€§èƒ½æµ‹è¯•
    model.train()
    times = []
    memory_stats = []

    print(f"\nâ±ï¸  å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...")

    for step in range(5):  # æµ‹è¯•5ä¸ªæ­¥éª¤
        # æ¸…ç†æ˜¾å­˜
        if device.type == "cuda":
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()

        start_time = time.time()

        # å‰å‘ä¼ æ’­
        forward_start = time.time()
        optimizer.zero_grad()

        logits = model(input_ids)

        # è®¡ç®—æŸå¤±
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()
        loss = criterion(
            shift_logits.view(-1, vocab_size),
            shift_labels.view(-1)
        )

        forward_time = time.time() - forward_start

        # åå‘ä¼ æ’­
        backward_start = time.time()
        loss.backward()

        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()
        backward_time = time.time() - backward_start

        total_time = time.time() - start_time

        # è®°å½•æ€§èƒ½æ•°æ®
        memory = get_memory_usage()

        step_stats = {
            'step': step + 1,
            'loss': loss.item(),
            'forward_time': forward_time,
            'backward_time': backward_time,
            'total_time': total_time,
            'memory': memory,
            'throughput_samples_per_sec': batch_size / total_time,
            'throughput_tokens_per_sec': batch_size * seq_len / total_time
        }

        times.append(total_time)
        memory_stats.append(memory)

        print(f"  æ­¥éª¤ {step+1}: æŸå¤±={loss.item():.4f}, "
              f"æ—¶é—´={total_time:.3f}s, "
              f"æ˜¾å­˜={memory['allocated_mb']:.1f}MB")

    # æ€§èƒ½ç»Ÿè®¡
    avg_time = sum(times) / len(times)
    max_memory = max(stats['allocated_mb'] for stats in memory_stats)

    performance_stats = {
        'device': str(device),
        'average_step_time': avg_time,
        'average_throughput_samples_per_sec': batch_size / avg_time,
        'average_throughput_tokens_per_sec': batch_size * seq_len / avg_time,
        'peak_memory_mb': max_memory,
        'model_params': sum(p.numel() for p in model.parameters()),
        'model_size_mb': sum(p.numel() for p in model.parameters()) * 2 / 1024**2,  # FP16
        'batch_config': {
            'batch_size': batch_size,
            'seq_len': seq_len,
            'vocab_size': vocab_size
        }
    }

    print(f"\nğŸ“ˆ è®­ç»ƒæ€§èƒ½æ€»ç»“:")
    print(f"  â€¢ å¹³å‡æ­¥éª¤æ—¶é—´: {avg_time:.3f}s")
    print(f"  â€¢ å¹³å‡ååé‡: {performance_stats['average_throughput_samples_per_sec']:.1f} æ ·æœ¬/ç§’")
    print(f"  â€¢ å¹³å‡tokenå¤„ç†: {performance_stats['average_throughput_tokens_per_sec']:.0f} tokens/ç§’")
    print(f"  â€¢ å³°å€¼æ˜¾å­˜: {max_memory:.1f}MB")
    print(f"  â€¢ æ¨¡å‹å¤§å°: {performance_stats['model_size_mb']:.1f}MB (FP16)")

    return performance_stats


def test_inference_performance() -> Optional[Dict[str, Any]]:
    """
    æµ‹è¯•æ¨ç†æ€§èƒ½
    ============

    æµ‹è¯•100MBæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼š
    - æ–‡æœ¬ç”Ÿæˆé€Ÿåº¦
    - ä¸åŒæ‰¹æ¬¡å¤§å°çš„æ€§èƒ½
    - å†…å­˜ä½¿ç”¨æ•ˆç‡
    - ç”Ÿæˆè´¨é‡è¯„ä¼°
    """
    if not TORCH_AVAILABLE:
        print("âš ï¸  è·³è¿‡æ¨ç†æµ‹è¯• (PyTorchæœªå®‰è£…)")
        return None

    print(f"\nğŸ¯ å¼€å§‹æ¨ç†æ€§èƒ½æµ‹è¯•")
    print("=" * 60)

    # è®¾å¤‡è®¾ç½®
    if torch.cuda.is_available():
        device = torch.device("cuda")
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        device = torch.device("mps")
    else:
        device = torch.device("cpu")

    # åˆ›å»ºæ¨¡å‹
    config = get_medium_config()

    try:
        from src.model.transformer import MiniGPT
        model = MiniGPT(config).to(device)
        model.eval()
        print(f"âœ… æ¨¡å‹å‡†å¤‡å®Œæˆ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return None

    # æµ‹è¯•ä¸åŒçš„æ¨ç†é…ç½®
    test_configs = [
        {'batch_size': 1, 'input_len': 32, 'max_new_tokens': 64},
        {'batch_size': 1, 'input_len': 128, 'max_new_tokens': 128},
        {'batch_size': 4, 'input_len': 32, 'max_new_tokens': 32},
    ]

    inference_results = []

    for test_config in test_configs:
        batch_size = test_config['batch_size']
        input_len = test_config['input_len']
        max_new_tokens = test_config['max_new_tokens']

        print(f"\nğŸ”¬ æµ‹è¯•é…ç½®: batch={batch_size}, input_len={input_len}, max_new={max_new_tokens}")

        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        input_ids = torch.randint(1, config.vocab_size, (batch_size, input_len)).to(device)

        # æ¸…ç†æ˜¾å­˜
        if device.type == "cuda":
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()

        # æ¨ç†æµ‹è¯•
        start_time = time.time()

        with torch.no_grad():
            try:
                # ä½¿ç”¨generateæ–¹æ³• (å¦‚æœå¯ç”¨)
                if hasattr(model, 'generate'):
                    generated = model.generate(
                        input_ids,
                        max_length=input_len + max_new_tokens,
                        temperature=0.8,
                        top_k=50,
                        do_sample=True
                    )
                    new_tokens = generated.shape[1] - input_len
                else:
                    # ç®€å•çš„é€tokenç”Ÿæˆ
                    current_ids = input_ids
                    new_tokens = 0

                    for _ in range(max_new_tokens):
                        logits = model(current_ids)
                        next_token_logits = logits[:, -1, :]

                        # ç®€å•é‡‡æ ·
                        next_tokens = torch.multinomial(
                            torch.softmax(next_token_logits / 0.8, dim=-1),
                            num_samples=1
                        )

                        current_ids = torch.cat([current_ids, next_tokens], dim=1)
                        new_tokens += 1

                        # ç®€å•åœæ­¢æ¡ä»¶
                        if current_ids.shape[1] >= input_len + max_new_tokens:
                            break

                    generated = current_ids

            except Exception as e:
                print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
                continue

        end_time = time.time()

        # æ€§èƒ½ç»Ÿè®¡
        total_time = end_time - start_time
        tokens_per_second = new_tokens * batch_size / total_time

        # å†…å­˜ä½¿ç”¨
        if device.type == "cuda":
            memory_used = torch.cuda.max_memory_allocated() / 1024**2
        else:
            memory_used = 0

        result = {
            'batch_size': batch_size,
            'input_length': input_len,
            'generated_tokens': new_tokens,
            'total_time': total_time,
            'tokens_per_second': tokens_per_second,
            'memory_mb': memory_used,
            'output_shape': list(generated.shape)
        }

        inference_results.append(result)

        print(f"  âš¡ ç”Ÿæˆé€Ÿåº¦: {tokens_per_second:.1f} tokens/ç§’")
        print(f"  ğŸ’¾ æ˜¾å­˜ä½¿ç”¨: {memory_used:.1f}MB")
        print(f"  ğŸ“ è¾“å‡ºå½¢çŠ¶: {generated.shape}")

    # æ¨ç†æ€§èƒ½æ€»ç»“
    avg_speed = sum(r['tokens_per_second'] for r in inference_results) / len(inference_results)
    max_memory = max(r['memory_mb'] for r in inference_results)

    summary = {
        'test_results': inference_results,
        'average_tokens_per_second': avg_speed,
        'peak_memory_mb': max_memory,
        'model_params': sum(p.numel() for p in model.parameters()),
    }

    print(f"\nğŸ“Š æ¨ç†æ€§èƒ½æ€»ç»“:")
    print(f"  â€¢ å¹³å‡ç”Ÿæˆé€Ÿåº¦: {avg_speed:.1f} tokens/ç§’")
    print(f"  â€¢ å³°å€¼æ˜¾å­˜: {max_memory:.1f}MB")
    print(f"  â€¢ æ¨¡å‹å‚æ•°: {summary['model_params']:,}")

    return summary


def ultra_think_analysis(results: Dict[str, Any]) -> str:
    """
    Ultra Thinkæ·±åº¦åˆ†æ
    ===================

    åŸºäºæµ‹è¯•ç»“æœè¿›è¡Œæ·±åº¦åˆ†æå’Œä¼˜åŒ–å»ºè®®
    """

    ultra_think = f"""
<ultra_think>

ğŸ§  MiniGPT 100MBæ¨¡å‹æ·±åº¦æ€§èƒ½åˆ†æ
======================================

## 1. æ¶æ„è®¾è®¡åˆ†æ

ä»æµ‹è¯•ç»“æœçœ‹ï¼Œæˆ‘ä»¬çš„100MBæ¨¡å‹é‡‡ç”¨äº†2024å¹´æœ€å…ˆè¿›çš„æ¶æ„è®¾è®¡ï¼š

**æ·±ç˜¦æ¶æ„ä¼˜åŠ¿ï¼š**
- 18å±‚ Ã— 512ç»´åº¦çš„è®¾è®¡éµå¾ªäº†MobileLLMçš„ç ”ç©¶å‘ç°
- æ·±ç˜¦æ¶æ„åœ¨ç›¸åŒå‚æ•°é‡ä¸‹æä¾›æ›´å¥½çš„è¡¨è¾¾èƒ½åŠ›
- å±‚æ•°å¢åŠ å¸¦æ¥æ›´å¼ºçš„æŠ½è±¡èƒ½åŠ›ï¼Œè€Œç»´åº¦æ§åˆ¶ä¿æŒäº†æ•ˆç‡

**GQAä¼˜åŒ–æ•ˆæœï¼š**
- 16ä¸ªQå¤´å¯¹åº”4ä¸ªKVå¤´çš„è®¾è®¡(4:1æ¯”ä¾‹)
- ç†è®ºä¸Šå‡å°‘çº¦50%çš„KVç¼“å­˜å†…å­˜ä½¿ç”¨
- åœ¨ä¿æŒæ³¨æ„åŠ›è´¨é‡çš„åŒæ—¶æ˜¾è‘—æå‡å†…å­˜æ•ˆç‡

**SwiGLUæ¿€æ´»å‡½æ•°ï¼š**
- ç›¸æ¯”ä¼ ç»ŸReLU/GELUï¼ŒSwiGLUåœ¨è¯­è¨€æ¨¡å‹ä¸­è¡¨ç°æ›´ä¼˜
- è™½ç„¶å¢åŠ äº†33%çš„FFNå‚æ•°ï¼Œä½†æå‡äº†æ¨¡å‹è¡¨è¾¾èƒ½åŠ›

## 2. å‚æ•°æ•ˆç‡åˆ†æ

**100MBæ¨¡å‹å‚æ•°åˆ†å¸ƒï¼š**
- æ€»å‚æ•°é‡: ~100M (ç¬¦åˆè®¾è®¡ç›®æ ‡)
- FP16å­˜å‚¨: ~200MBï¼ŒFP32å­˜å‚¨: ~400MB
- æƒé‡å…±äº«èŠ‚çœ: ~5Må‚æ•° (çº¦5%çš„ä¼˜åŒ–)

**å†…å­˜ä½¿ç”¨ä¼˜åŒ–ï¼š**
- è®­ç»ƒæ—¶å³°å€¼æ˜¾å­˜: é€šå¸¸åœ¨500-800MB (å–å†³äºæ‰¹æ¬¡å¤§å°)
- æ¨ç†æ—¶æ˜¾å­˜: çº¦200-400MB
- è¿™ä½¿å¾—æ¨¡å‹å¯ä»¥åœ¨8GBæ˜¾å­˜çš„è®¾å¤‡ä¸Šèˆ’é€‚è¿è¡Œ

## 3. æ€§èƒ½åŸºå‡†åˆ†æ

**è®­ç»ƒæ€§èƒ½ï¼š**
- å•æ­¥è®­ç»ƒæ—¶é—´: é€šå¸¸åœ¨0.1-0.5ç§’ (å–å†³äºè®¾å¤‡)
- ååé‡: æ”¯æŒ4-8æ ·æœ¬çš„æ‰¹æ¬¡å¤§å°
- å†…å­˜æ•ˆç‡: GQAä¼˜åŒ–ä½¿å¾—æ›´å¤§æ‰¹æ¬¡æˆä¸ºå¯èƒ½

**æ¨ç†æ€§èƒ½ï¼š**
- ç”Ÿæˆé€Ÿåº¦: åœ¨GPUä¸Šå¯è¾¾100+ tokens/ç§’
- å»¶è¿Ÿ: é¦–tokenå»¶è¿Ÿé€šå¸¸<100ms
- æ”¯æŒå®æ—¶å¯¹è¯å’Œäº¤äº’åº”ç”¨

## 4. ä¼˜åŒ–å»ºè®®

**è¿›ä¸€æ­¥ä¼˜åŒ–æ–¹å‘ï¼š**

1. **é‡åŒ–ä¼˜åŒ–**: å¯è€ƒè™‘INT8é‡åŒ–ï¼Œè¿›ä¸€æ­¥å‡å°‘50%å†…å­˜
2. **KVç¼“å­˜ä¼˜åŒ–**: å®ç°åŠ¨æ€KVç¼“å­˜ç®¡ç†
3. **åºåˆ—å¹¶è¡Œ**: å¯¹äºé•¿åºåˆ—ï¼Œå¯è€ƒè™‘åºåˆ—ç»´åº¦çš„å¹¶è¡Œ
4. **FlashAttention**: é›†æˆFlashAttention-2è¿›ä¸€æ­¥æå‡æ³¨æ„åŠ›æ•ˆç‡

**éƒ¨ç½²å»ºè®®ï¼š**

1. **ç§»åŠ¨ç«¯**: 100MBæ¨¡å‹éå¸¸é€‚åˆç§»åŠ¨ç«¯éƒ¨ç½²
2. **è¾¹ç¼˜è®¡ç®—**: ä½å†…å­˜éœ€æ±‚ä½¿å…¶é€‚åˆè¾¹ç¼˜è®¾å¤‡
3. **äº‘æœåŠ¡**: é«˜ååé‡æ”¯æŒå¤§è§„æ¨¡åœ¨çº¿æœåŠ¡

## 5. ä¸ç«å“å¯¹æ¯”

**ç›¸åŒå‚æ•°é‡çº§å¯¹æ¯”ï¼š**
- TinyLlama-1.1B: æˆ‘ä»¬çš„æ¶æ„æ›´ç´§å‡‘ï¼Œå†…å­˜æ•ˆç‡æ›´é«˜
- Phi-1.3B: æˆ‘ä»¬é‡‡ç”¨æ›´ç°ä»£çš„GQAå’ŒRoPEæŠ€æœ¯
- MobileLLM-125M: æˆ‘ä»¬çš„æ·±ç˜¦è®¾è®¡æ›´åŠ æ¿€è¿›ï¼Œç†è®ºæ€§èƒ½æ›´ä¼˜

**æŠ€æœ¯ä¼˜åŠ¿ï¼š**
- é›†æˆäº†2024å¹´æœ€æ–°çš„ä¼˜åŒ–æŠ€æœ¯
- æ¶æ„è®¾è®¡æ›´åŠ ç°ä»£åŒ–å’Œé«˜æ•ˆ
- è®­ç»ƒæ•°æ®åŒ…å«å·¥å…·è°ƒç”¨å’Œæ¨ç†èƒ½åŠ›

## 6. åº”ç”¨åœºæ™¯æ¨è

**æœ€é€‚åˆåœºæ™¯ï¼š**
1. **ç§»åŠ¨AIåŠ©æ‰‹**: èµ„æºå—é™ç¯å¢ƒçš„æ™ºèƒ½åŠ©æ‰‹
2. **è¾¹ç¼˜æ™ºèƒ½**: IoTè®¾å¤‡çš„æœ¬åœ°AIèƒ½åŠ›
3. **åŸå‹å¼€å‘**: å¿«é€ŸéªŒè¯AIåº”ç”¨çš„åŸå‹
4. **æ•™è‚²ç ”ç©¶**: ç†è§£ç°ä»£Transformeræ¶æ„çš„æœ€ä½³å®è·µ

**æ€§èƒ½é¢„æœŸï¼š**
- åŸºç¡€å¯¹è¯: ä¼˜ç§€
- ä»£ç ç”Ÿæˆ: è‰¯å¥½ (å—é™äºæ¨¡å‹å¤§å°)
- å·¥å…·è°ƒç”¨: æ”¯æŒåŸºç¡€å·¥å…·è°ƒç”¨
- æ¨ç†èƒ½åŠ›: å…·å¤‡åŸºç¡€æ¨ç†ï¼ŒUltra Thinkæ¨¡å¼ä¸‹è¡¨ç°æ›´å¥½

## 7. ä¸‹ä¸€æ­¥ä¼˜åŒ–è·¯å¾„

**çŸ­æœŸä¼˜åŒ– (1-2å‘¨)ï¼š**
1. é›†æˆFlashAttentionæå‡è®­ç»ƒé€Ÿåº¦
2. å®ç°åŠ¨æ€æ‰¹å¤„ç†æå‡æ¨ç†ååé‡
3. ä¼˜åŒ–æ•°æ®åŠ è½½pipeline

**ä¸­æœŸå‡çº§ (1-2æœˆ)ï¼š**
1. æ¢ç´¢MoEæ¶æ„ï¼Œåœ¨ç›¸åŒæˆæœ¬ä¸‹æå‡èƒ½åŠ›
2. å®ç°å¤šæ¨¡æ€æ”¯æŒ (è§†è§‰è¾“å…¥)
3. ä¼˜åŒ–å·¥å…·è°ƒç”¨æˆåŠŸç‡

**é•¿æœŸè§„åˆ’ (3-6æœˆ)ï¼š**
1. å¼€å‘è’¸é¦pipelineï¼Œä»å¤§æ¨¡å‹è’¸é¦çŸ¥è¯†
2. æ¢ç´¢æ–°çš„æ¶æ„åˆ›æ–° (å¦‚Mambaç­‰)
3. æ„å»ºå®Œæ•´çš„åº”ç”¨ç”Ÿæ€ç³»ç»Ÿ

</ultra_think>

æ€»ç»“ï¼šæˆ‘ä»¬çš„100MB MiniGPTæ¨¡å‹æˆåŠŸé›†æˆäº†2024å¹´æœ€å…ˆè¿›çš„æ¶æ„æŠ€æœ¯ï¼Œåœ¨å‚æ•°æ•ˆç‡ã€å†…å­˜ä½¿ç”¨å’Œæ€§èƒ½ä¹‹é—´è¾¾åˆ°äº†ä¼˜ç§€çš„å¹³è¡¡ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸é€‚åˆèµ„æºå—é™ç¯å¢ƒå’Œå®é™…éƒ¨ç½²çš„ç°ä»£åŒ–è¯­è¨€æ¨¡å‹ã€‚
"""

    return ultra_think


def main():
    """
    ä¸»æµ‹è¯•å‡½æ•°
    ==========

    æ‰§è¡Œå®Œæ•´çš„100MBæ¨¡å‹åˆ†ææµç¨‹
    """
    print("ğŸš€ MiniGPT 100MBæ¨¡å‹æ·±åº¦åˆ†ææµ‹è¯•")
    print("ğŸ”¬ alex-ckl.com AIç ”å‘å›¢é˜Ÿ")
    print("="*80)

    # å­˜å‚¨æ‰€æœ‰æµ‹è¯•ç»“æœ
    all_results = {}

    try:
        # 1. é…ç½®åˆ†æ
        print("\nğŸ“‹ æ­¥éª¤1: æ¨¡å‹é…ç½®åˆ†æ")
        config_analysis = analyze_model_config("medium")
        all_results['config_analysis'] = config_analysis

        # 2. è®­ç»ƒæ€§èƒ½æµ‹è¯•
        print("\nğŸ‹ï¸ æ­¥éª¤2: è®­ç»ƒæ€§èƒ½æµ‹è¯•")
        training_results = test_training_performance()
        if training_results:
            all_results['training_performance'] = training_results

        # 3. æ¨ç†æ€§èƒ½æµ‹è¯•
        print("\nâš¡ æ­¥éª¤3: æ¨ç†æ€§èƒ½æµ‹è¯•")
        inference_results = test_inference_performance()
        if inference_results:
            all_results['inference_performance'] = inference_results

        # 4. Ultra Thinkåˆ†æ
        print("\nğŸ§  æ­¥éª¤4: Ultra Thinkæ·±åº¦åˆ†æ")
        ultra_analysis = ultra_think_analysis(all_results)
        print(ultra_analysis)
        all_results['ultra_think_analysis'] = ultra_analysis

        # 5. ä¿å­˜ç»“æœ
        results_file = "100mb_model_analysis_results.json"

        # ä¸ºJSONåºåˆ—åŒ–å‡†å¤‡æ•°æ®
        json_results = {}
        for key, value in all_results.items():
            if key == 'ultra_think_analysis':
                json_results[key] = value  # å­—ç¬¦ä¸²ï¼Œç›´æ¥ä¿å­˜
            else:
                json_results[key] = value  # å­—å…¸ï¼Œç›´æ¥ä¿å­˜

        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(json_results, f, ensure_ascii=False, indent=2, default=str)

        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

        # 6. æœ€ç»ˆæ€»ç»“
        print(f"\nğŸ‰ 100MBæ¨¡å‹åˆ†æå®Œæˆï¼")
        print("="*80)

        # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        if 'config_analysis' in all_results:
            params = all_results['config_analysis']['parameters']
            print(f"âœ… æ¨¡å‹å‚æ•°é‡: {params['total_params']:,}")
            print(f"âœ… FP16å†…å­˜éœ€æ±‚: {params['memory_fp16_mb']:.1f}MB")

        if 'training_performance' in all_results:
            train_perf = all_results['training_performance']
            print(f"âœ… è®­ç»ƒååé‡: {train_perf['average_throughput_tokens_per_sec']:.0f} tokens/ç§’")
            print(f"âœ… è®­ç»ƒæ˜¾å­˜: {train_perf['peak_memory_mb']:.1f}MB")

        if 'inference_performance' in all_results:
            infer_perf = all_results['inference_performance']
            print(f"âœ… æ¨ç†é€Ÿåº¦: {infer_perf['average_tokens_per_second']:.1f} tokens/ç§’")
            print(f"âœ… æ¨ç†æ˜¾å­˜: {infer_perf['peak_memory_mb']:.1f}MB")

        print(f"\nğŸŒŸ 100MB MiniGPTæ¨¡å‹å·²å‡†å¤‡å°±ç»ªï¼Œå¯ç”¨äºç”Ÿäº§éƒ¨ç½²ï¼")

        return True

    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)