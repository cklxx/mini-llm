#!/usr/bin/env python3
"""
æ¶æ„å‡çº§æµ‹è¯•è„šæœ¬
éªŒè¯æ‰€æœ‰æ–°æ¶æ„ç»„ä»¶çš„æ­£ç¡®æ€§ï¼šRoPEã€GQAã€æ·±åº¦ä¼˜åŒ–ã€æƒé‡å…±äº«ç­‰
"""

import os
import sys

sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

try:  # pragma: no cover - optional dependency guard
    import pytest
except ModuleNotFoundError:  # pragma: no cover
    pytest = None


try:  # pragma: no cover - optional dependency guard
    import torch
except ModuleNotFoundError:  # pragma: no cover
    torch = None

try:  # pragma: no cover - optional dependency guard
    import numpy as np
except ModuleNotFoundError:  # pragma: no cover
    np = None

from src.model.config import estimate_params, get_small_config, get_tiny_config

if torch is not None:
    from src.model.gqa import GroupedQueryAttention
    from src.model.rope import RotaryPositionEmbedding, apply_rotary_pos_emb
    from src.model.transformer import MiniGPT
else:  # pragma: no cover - executed when optional dependencies missing
    GroupedQueryAttention = None
    RotaryPositionEmbedding = None
    apply_rotary_pos_emb = None
    MiniGPT = None

BACKEND_AVAILABLE = torch is not None and MiniGPT is not None

if pytest is not None:
    pytestmark = pytest.mark.skipif(not BACKEND_AVAILABLE, reason="PyTorch not available")
else:  # pragma: no cover - executed when running as a plain script
    pytestmark = []


def test_rope_implementation():
    """æµ‹è¯•RoPEä½ç½®ç¼–ç å®ç°"""
    print("Testing RoPE Position Encoding...")

    batch_size = 2
    seq_len = 128
    head_dim = 64

    # åˆ›å»ºRoPE
    rope = RotaryPositionEmbedding(head_dim, max_position_embeddings=256)

    # æµ‹è¯•è¾“å…¥
    hidden_states = torch.randn(batch_size, seq_len, head_dim)

    # è·å–coså’Œsin
    cos, sin = rope(hidden_states)

    assert cos.shape == (seq_len, head_dim), f"Expected cos shape ({seq_len}, {head_dim}), got {cos.shape}"
    assert sin.shape == (seq_len, head_dim), f"Expected sin shape ({seq_len}, {head_dim}), got {sin.shape}"

    # æµ‹è¯•åº”ç”¨RoPE
    q = torch.randn(batch_size, 8, seq_len, head_dim)
    k = torch.randn(batch_size, 8, seq_len, head_dim)

    q_rot, k_rot = apply_rotary_pos_emb(q, k, cos, sin)

    assert q_rot.shape == q.shape, f"RoPE changed query shape: {q.shape} -> {q_rot.shape}"
    assert k_rot.shape == k.shape, f"RoPE changed key shape: {k.shape} -> {k_rot.shape}"

    # éªŒè¯æ—‹è½¬ä¸å˜æ€§ï¼ˆé•¿åº¦åº”è¯¥ä¿æŒä¸å˜ï¼‰
    q_norm_before = torch.norm(q, dim=-1)
    q_norm_after = torch.norm(q_rot, dim=-1)
    assert torch.allclose(q_norm_before, q_norm_after, atol=1e-6), "RoPE should preserve vector norms"

    print("âœ… RoPE implementation test passed!")
    return True


def test_gqa_implementation():
    """æµ‹è¯•åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›å®ç°"""
    print("Testing Grouped-Query Attention...")

    batch_size = 2
    seq_len = 128
    d_model = 384
    num_heads = 12
    num_kv_heads = 3

    # åˆ›å»ºGQA
    gqa = GroupedQueryAttention(
        d_model=d_model,
        num_heads=num_heads,
        num_key_value_heads=num_kv_heads,
        use_rope=True
    )

    # æµ‹è¯•è¾“å…¥
    hidden_states = torch.randn(batch_size, seq_len, d_model)

    # å‰å‘ä¼ æ’­
    output, _ = gqa(hidden_states)

    assert output.shape == hidden_states.shape, f"GQA output shape mismatch: {hidden_states.shape} vs {output.shape}"

    # éªŒè¯KVå¤´æ•°é…ç½®
    assert gqa.num_queries_per_kv == num_heads // num_kv_heads, "Incorrect queries per KV head ratio"

    # æµ‹è¯•å‚æ•°æ•°é‡å‡å°‘
    total_params = sum(p.numel() for p in gqa.parameters())

    # ä¼°ç®—ä¼ ç»ŸMHAå‚æ•°é‡
    mha_params = 4 * d_model * d_model  # Q, K, V, O projections

    print(f"GQA parameters: {total_params:,}")
    print(f"Estimated MHA parameters: {mha_params:,}")
    print(f"Parameter reduction: {(1 - total_params/mha_params)*100:.1f}%")

    print("âœ… GQA implementation test passed!")
    return True


def test_deep_thin_architecture():
    """æµ‹è¯•æ·±è€Œçª„æ¶æ„ä¼˜åŒ–"""
    print("Testing Deep-Thin Architecture Optimization...")

    # æ¯”è¾ƒä¼˜åŒ–å‰åçš„é…ç½®
    old_config = get_tiny_config()
    old_config.use_rope = False
    old_config.use_gqa = False
    old_config.hidden_size = 128
    old_config.num_hidden_layers = 4
    old_config.num_attention_heads = 2
    old_config.intermediate_size = 512

    new_config = get_tiny_config()  # å·²ç»æ˜¯æ·±è€Œçª„çš„é…ç½®

    old_params = estimate_params(old_config)
    new_params = estimate_params(new_config)

    print(f"Old architecture (wide-shallow): {old_params:,} parameters")
    print(f"New architecture (deep-thin): {new_params:,} parameters")

    # éªŒè¯æ–°æ¶æ„ç¡®å®æ›´æ·±
    assert new_config.num_hidden_layers > old_config.num_hidden_layers, "New config should be deeper"

    # åˆ›å»ºæ¨¡å‹æµ‹è¯•
    model = MiniGPT(new_config)

    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size = 2
    seq_len = 64
    input_ids = torch.randint(0, new_config.vocab_size, (batch_size, seq_len))

    with torch.no_grad():
        logits = model(input_ids)

    expected_shape = (batch_size, seq_len, new_config.vocab_size)
    assert logits.shape == expected_shape, f"Model output shape mismatch: {logits.shape} vs {expected_shape}"

    print("âœ… Deep-thin architecture test passed!")
    return True


def test_weight_sharing():
    """æµ‹è¯•æƒé‡å…±äº«å®ç°"""
    print("Testing Weight Sharing...")

    config = get_tiny_config()
    config.tie_word_embeddings = True

    model = MiniGPT(config)

    # éªŒè¯æ²¡æœ‰ç‹¬ç«‹çš„lm_head
    assert model.lm_head is None, "Model should not have separate lm_head when using weight sharing"

    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size = 2
    seq_len = 32
    input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))

    with torch.no_grad():
        logits = model(input_ids)

    # éªŒè¯è¾“å‡ºç»´åº¦æ­£ç¡®
    expected_shape = (batch_size, seq_len, config.vocab_size)
    assert logits.shape == expected_shape, f"Weight sharing model output shape: {logits.shape} vs {expected_shape}"

    # è®¡ç®—å‚æ•°èŠ‚çœ
    params_with_sharing = estimate_params(config)

    config_no_sharing = get_tiny_config()
    config_no_sharing.tie_word_embeddings = False
    params_without_sharing = estimate_params(config_no_sharing)

    saved_params = params_without_sharing - params_with_sharing
    savings_percent = (saved_params / params_without_sharing) * 100

    print(f"Parameters with sharing: {params_with_sharing:,}")
    print(f"Parameters without sharing: {params_without_sharing:,}")
    print(f"Saved parameters: {saved_params:,} ({savings_percent:.1f}%)")

    print("âœ… Weight sharing test passed!")
    return True


def test_model_compatibility():
    """æµ‹è¯•æ¨¡å‹å…¼å®¹æ€§å’Œé…ç½®ç»„åˆ"""
    print("Testing Model Compatibility...")

    configs_to_test = [
        ("tiny", get_tiny_config()),
        ("small", get_small_config()),
    ]

    for config_name, config in configs_to_test:
        print(f"  Testing {config_name} configuration...")

        try:
            model = MiniGPT(config)

            # æµ‹è¯•å‚æ•°åˆå§‹åŒ–
            total_params = sum(p.numel() for p in model.parameters())
            estimated_params = estimate_params(config)

            # å…è®¸ä¸€å®šçš„ä¼°ç®—è¯¯å·®
            param_diff = abs(total_params - estimated_params) / estimated_params
            assert param_diff < 0.1, f"Parameter estimation error too large: {param_diff:.1%}"

            # æµ‹è¯•å‰å‘ä¼ æ’­
            batch_size = 1
            seq_len = 32
            input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))

            with torch.no_grad():
                logits = model(input_ids)

            assert not torch.isnan(logits).any(), f"Model {config_name} produces NaN outputs"
            assert not torch.isinf(logits).any(), f"Model {config_name} produces infinite outputs"

            print(f"    âœ… {config_name}: {total_params:,} params, output shape {logits.shape}")

        except Exception as e:
            print(f"    âŒ {config_name} failed: {e}")
            return False

    print("âœ… Model compatibility test passed!")
    return True


def test_performance_improvements():
    """æµ‹è¯•æ€§èƒ½æ”¹è¿›"""
    print("Testing Performance Improvements...")

    # æ¯”è¾ƒGQA vs MHAçš„å†…å­˜ä½¿ç”¨
    batch_size = 4
    seq_len = 256
    d_model = 384
    num_heads = 12

    # ä¼ ç»ŸMHA
    mha_config = get_small_config()
    mha_config.use_gqa = False
    mha_config.use_rope = False

    # GQAé…ç½®
    gqa_config = get_small_config()
    gqa_config.use_gqa = True
    gqa_config.num_key_value_heads = 3

    print(f"MHA config: {estimate_params(mha_config):,} parameters")
    print(f"GQA config: {estimate_params(gqa_config):,} parameters")

    # è®¡ç®—KVç¼“å­˜å¤§å°å¯¹æ¯”
    head_dim = d_model // num_heads

    mha_kv_cache = 2 * batch_size * num_heads * seq_len * head_dim
    gqa_kv_cache = 2 * batch_size * gqa_config.num_key_value_heads * seq_len * head_dim

    kv_reduction = (1 - gqa_kv_cache / mha_kv_cache) * 100

    print(f"MHA KV cache size: {mha_kv_cache:,} elements")
    print(f"GQA KV cache size: {gqa_kv_cache:,} elements")
    print(f"KV cache reduction: {kv_reduction:.1f}%")

    assert kv_reduction > 50, "GQA should provide significant KV cache reduction"

    print("âœ… Performance improvements test passed!")
    return True


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æ¶æ„æµ‹è¯•"""
    if not BACKEND_AVAILABLE:
        print("âš ï¸ PyTorch not available, skipping architecture tests.")
        return True

    print("=" * 60)
    print("MINIGPT ARCHITECTURE UPGRADE TESTS")
    print("=" * 60)

    tests = [
        test_rope_implementation,
        test_gqa_implementation,
        test_deep_thin_architecture,
        test_weight_sharing,
        test_model_compatibility,
        test_performance_improvements,
    ]

    passed = 0
    total = len(tests)

    for test_func in tests:
        try:
            if test_func():
                passed += 1
            print()
        except Exception as e:
            print(f"âŒ {test_func.__name__} failed with error: {e}")
            print()

    print("=" * 60)
    print(f"ARCHITECTURE TESTS SUMMARY: {passed}/{total} PASSED")
    print("=" * 60)

    if passed == total:
        print("ğŸ‰ All architecture upgrades are working correctly!")
        return True
    else:
        print("âš ï¸  Some tests failed. Please check the implementation.")
        return False


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
