#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®­ç»ƒå’Œæ¨ç†æµ‹è¯•è„šæœ¬
éªŒè¯å‡çº§åçš„æ¶æ„åœ¨å®é™…è®­ç»ƒå’Œæ¨ç†ä¸­çš„è¡¨ç°
åŒ…æ‹¬å·¥å…·è°ƒç”¨ã€ultra thinkç­‰æ–°èƒ½åŠ›
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

import torch
import torch.nn as nn
import json
import time
from torch.utils.data import Dataset, DataLoader
from typing import List, Dict

from src.model.config import get_tiny_config, get_small_config
from src.model.transformer import MiniGPT


class TestDataset(Dataset):
    """æµ‹è¯•æ•°æ®é›†"""

    def __init__(self, data_path: str, tokenizer=None, max_length: int = 128):
        self.data = []
        self.max_length = max_length

        # ç®€å•çš„å­—ç¬¦çº§tokenizerç”¨äºæµ‹è¯•
        if tokenizer is None:
            self.vocab = self._build_vocab(data_path)
            self.char_to_id = {char: i for i, char in enumerate(self.vocab)}
            self.id_to_char = {i: char for char, i in self.char_to_id.items()}

        # åŠ è½½æ•°æ®
        self._load_data(data_path)

    def _build_vocab(self, data_path: str) -> List[str]:
        """æ„å»ºå­—ç¬¦çº§è¯æ±‡è¡¨"""
        chars = set()
        try:
            with open(data_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line)
                        for conv in data.get('conversations', []):
                            chars.update(conv.get('content', ''))
        except FileNotFoundError:
            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤è¯æ±‡è¡¨
            chars = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 .,!?;:()[]{}"\'-\nä½ æˆ‘ä»–çš„æ˜¯åœ¨æœ‰ä¸€ä¸ªè¿™é‚£äº†å’Œä¸º')

        # æ·»åŠ ç‰¹æ®Štoken
        vocab = ['<pad>', '<unk>', '<bos>', '<eos>'] + sorted(list(chars))
        return vocab

    def _load_data(self, data_path: str):
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        try:
            with open(data_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line)
                        text = self._extract_text(data)
                        if text:
                            tokens = self._tokenize(text)
                            if len(tokens) > 1:  # è‡³å°‘éœ€è¦è¾“å…¥å’Œç›®æ ‡
                                self.data.append(tokens)
        except FileNotFoundError:
            # ç”Ÿæˆåˆæˆæ•°æ®ç”¨äºæµ‹è¯•
            self._generate_synthetic_data()

    def _extract_text(self, data: Dict) -> str:
        """ä»å¯¹è¯æ•°æ®ä¸­æå–æ–‡æœ¬"""
        text = ""
        conversations = data.get('conversations', [])
        for conv in conversations:
            role = conv.get('role', '')
            content = conv.get('content', '')
            text += f"{role}: {content}\n"
        return text.strip()

    def _tokenize(self, text: str) -> List[int]:
        """ç®€å•çš„å­—ç¬¦çº§tokenization"""
        tokens = [self.char_to_id.get('<bos>', 2)]  # BOS token
        for char in text[:self.max_length - 2]:  # ç•™å‡ºBOSå’ŒEOSçš„ç©ºé—´
            tokens.append(self.char_to_id.get(char, self.char_to_id.get('<unk>', 1)))
        tokens.append(self.char_to_id.get('<eos>', 3))  # EOS token
        return tokens

    def _generate_synthetic_data(self):
        """ç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®"""
        synthetic_texts = [
            "Hello, how are you today?",
            "I am fine, thank you. How about you?",
            "What is the weather like?",
            "It's sunny and warm outside.",
            "Can you help me with this task?",
            "Of course! I'd be happy to help.",
            "ä½ å¥½ï¼Œä»Šå¤©æ€ä¹ˆæ ·ï¼Ÿ",
            "æˆ‘å¾ˆå¥½ï¼Œè°¢è°¢ä½ ã€‚ä½ å‘¢ï¼Ÿ",
            "å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
            "å¤–é¢é˜³å…‰æ˜åªšï¼Œå¾ˆæš–å’Œã€‚"
        ]

        for text in synthetic_texts:
            tokens = self._tokenize(text)
            if len(tokens) > 1:
                self.data.append(tokens)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        tokens = self.data[idx]

        # è¾“å…¥å’Œç›®æ ‡ï¼ˆshifted by 1ï¼‰
        if len(tokens) <= self.max_length:
            # Padding
            padded = tokens + [0] * (self.max_length - len(tokens))
            input_ids = torch.tensor(padded[:-1], dtype=torch.long)
            labels = torch.tensor(padded[1:], dtype=torch.long)
        else:
            # Truncation
            input_ids = torch.tensor(tokens[:self.max_length-1], dtype=torch.long)
            labels = torch.tensor(tokens[1:self.max_length], dtype=torch.long)

        return input_ids, labels


def test_model_training():
    """æµ‹è¯•æ¨¡å‹è®­ç»ƒ"""
    print("Testing Model Training...")

    # ä½¿ç”¨tinyé…ç½®è¿›è¡Œå¿«é€Ÿæµ‹è¯•
    config = get_tiny_config()
    config.vocab_size = 1000  # å‡å°è¯æ±‡è¡¨ä»¥åŠ å¿«æµ‹è¯•

    model = MiniGPT(config)

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    data_path = "data/dataset/minimind_dataset/sft_mini_512.jsonl"
    dataset = TestDataset(data_path, max_length=64)

    if len(dataset) == 0:
        print("âš ï¸  No data loaded, using synthetic data")

    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

    # é…ç½®è®­ç»ƒ
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    criterion = nn.CrossEntropyLoss(ignore_index=0)  # å¿½ç•¥padding

    model.train()
    initial_loss = None
    final_loss = None

    print(f"Training on {len(dataset)} samples...")

    # è®­ç»ƒå‡ ä¸ªæ­¥éª¤
    for epoch in range(2):
        epoch_loss = 0
        num_batches = 0

        for batch_idx, (input_ids, labels) in enumerate(dataloader):
            if batch_idx >= 5:  # åªè®­ç»ƒå‡ ä¸ªbatch
                break

            optimizer.zero_grad()

            # å‰å‘ä¼ æ’­
            logits = model(input_ids)

            # è®¡ç®—æŸå¤±
            loss = criterion(logits.view(-1, config.vocab_size), labels.view(-1))

            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            num_batches += 1

            if epoch == 0 and batch_idx == 0:
                initial_loss = loss.item()

            print(f"  Epoch {epoch+1}, Batch {batch_idx+1}, Loss: {loss.item():.4f}")

        if num_batches > 0:
            avg_loss = epoch_loss / num_batches
            final_loss = avg_loss
            print(f"  Epoch {epoch+1} Average Loss: {avg_loss:.4f}")

    # éªŒè¯è®­ç»ƒæ˜¯å¦æœ‰æ•ˆï¼ˆæŸå¤±åº”è¯¥ä¸‹é™ï¼‰
    if initial_loss is not None and final_loss is not None:
        loss_improvement = initial_loss - final_loss
        print(f"Loss improvement: {loss_improvement:.4f}")

        if loss_improvement > 0:
            print("âœ… Model training test passed! (Loss decreased)")
        else:
            print("âš ï¸  Model training test warning: Loss did not decrease significantly")
    else:
        print("âš ï¸  Could not evaluate loss improvement")

    return True


def test_model_inference():
    """æµ‹è¯•æ¨¡å‹æ¨ç†"""
    print("Testing Model Inference...")

    config = get_tiny_config()
    config.vocab_size = 1000

    model = MiniGPT(config)
    model.eval()

    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size = 2
    seq_len = 32
    input_ids = torch.randint(1, config.vocab_size, (batch_size, seq_len))

    print(f"Input shape: {input_ids.shape}")

    # æµ‹è¯•ç”Ÿæˆ
    with torch.no_grad():
        start_time = time.time()

        # æ™®é€šå‰å‘ä¼ æ’­
        logits = model(input_ids)
        forward_time = time.time() - start_time

        # ç”Ÿæˆæµ‹è¯•
        start_time = time.time()
        generated = model.generate(
            input_ids[:1, :10],  # ä½¿ç”¨è¾ƒçŸ­çš„prompt
            max_length=20,
            temperature=0.8,
            top_k=10
        )
        generation_time = time.time() - start_time

    print(f"Forward pass time: {forward_time:.4f}s")
    print(f"Generation time: {generation_time:.4f}s")
    print(f"Generated sequence length: {generated.shape[1]}")

    # éªŒè¯è¾“å‡º
    assert logits.shape == (batch_size, seq_len, config.vocab_size), \
        f"Unexpected logits shape: {logits.shape}"

    assert generated.shape[0] == 1, "Generated batch size should be 1"
    assert generated.shape[1] >= 10, "Generated sequence should be longer than input"

    # æ£€æŸ¥ç”Ÿæˆçš„tokenæ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
    assert torch.all(generated >= 0) and torch.all(generated < config.vocab_size), \
        "Generated tokens outside vocabulary range"

    print("âœ… Model inference test passed!")
    return True


def test_tool_calling_format():
    """æµ‹è¯•å·¥å…·è°ƒç”¨æ•°æ®æ ¼å¼"""
    print("Testing Tool Calling Data Format...")

    # æµ‹è¯•å·¥å…·è°ƒç”¨æ•°æ®åŠ è½½
    tool_data_paths = [
        "data/dataset/minimind_dataset/tool_calling_basic.jsonl",
        "data/dataset/minimind_dataset/tool_calling_advanced.jsonl",
        "data/dataset/minimind_dataset/agent_ultra_think.jsonl"
    ]

    for data_path in tool_data_paths:
        try:
            dataset = TestDataset(data_path, max_length=256)
            print(f"  {os.path.basename(data_path)}: {len(dataset)} samples")

            if len(dataset) > 0:
                # æµ‹è¯•ç¬¬ä¸€ä¸ªæ ·æœ¬
                input_ids, labels = dataset[0]
                assert input_ids.shape[0] > 0, "Empty input sequence"
                assert labels.shape[0] > 0, "Empty label sequence"
                print(f"    Sample input shape: {input_ids.shape}")

        except Exception as e:
            print(f"  âš ï¸  Failed to load {data_path}: {e}")

    print("âœ… Tool calling data format test completed!")
    return True


def test_ultra_think_capability():
    """æµ‹è¯•ultra thinkèƒ½åŠ›æ•°æ®"""
    print("Testing Ultra Think Capability...")

    config = get_tiny_config()
    model = MiniGPT(config)

    # åˆ›å»ºultra thinkæµ‹è¯•æ•°æ®
    ultra_think_data = TestDataset(
        "data/dataset/minimind_dataset/agent_ultra_think.jsonl",
        max_length=512
    )

    if len(ultra_think_data) > 0:
        print(f"Ultra think dataset loaded: {len(ultra_think_data)} samples")

        # æµ‹è¯•è®­ç»ƒä¸€ä¸ªbatch
        dataloader = DataLoader(ultra_think_data, batch_size=1, shuffle=False)
        input_ids, labels = next(iter(dataloader))

        model.eval()
        with torch.no_grad():
            logits = model(input_ids)
            print(f"Ultra think inference shape: {logits.shape}")

        print("âœ… Ultra think capability test passed!")
    else:
        print("âš ï¸  No ultra think data found, skipping test")

    return True


def test_memory_efficiency():
    """æµ‹è¯•å†…å­˜æ•ˆç‡æ”¹è¿›"""
    print("Testing Memory Efficiency...")

    # æ¯”è¾ƒGQA vs ä¼ ç»ŸMHAçš„å†…å­˜ä½¿ç”¨
    configs = {
        "MHA": get_small_config(),
        "GQA": get_small_config()
    }

    # é…ç½®MHAï¼ˆä¼ ç»Ÿï¼‰
    configs["MHA"].use_gqa = False
    configs["MHA"].use_rope = False

    # é…ç½®GQAï¼ˆä¼˜åŒ–ï¼‰
    configs["GQA"].use_gqa = True
    configs["GQA"].num_key_value_heads = 3

    results = {}

    for name, config in configs.items():
        # åˆ›å»ºæ¨¡å‹
        model = MiniGPT(config)

        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())

        # æ¨¡æ‹Ÿæ¨ç†å†…å­˜ä½¿ç”¨
        batch_size = 4
        seq_len = 256
        input_ids = torch.randint(1, config.vocab_size, (batch_size, seq_len))

        # æµ‹é‡å†…å­˜ä½¿ç”¨ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()

            model = model.cuda()
            input_ids = input_ids.cuda()

            with torch.no_grad():
                logits = model(input_ids)

            memory_used = torch.cuda.max_memory_allocated() / 1024 / 1024  # MB
        else:
            memory_used = 0  # CPUå†…å­˜æµ‹é‡è¾ƒå¤æ‚ï¼Œæš‚æ—¶è·³è¿‡

        results[name] = {
            'params': total_params,
            'memory_mb': memory_used
        }

        print(f"  {name}: {total_params:,} params, {memory_used:.1f} MB")

    # è®¡ç®—æ”¹è¿›
    if "MHA" in results and "GQA" in results:
        param_reduction = (1 - results["GQA"]["params"] / results["MHA"]["params"]) * 100
        print(f"Parameter reduction: {param_reduction:.1f}%")

        if results["GQA"]["memory_mb"] > 0 and results["MHA"]["memory_mb"] > 0:
            memory_reduction = (1 - results["GQA"]["memory_mb"] / results["MHA"]["memory_mb"]) * 100
            print(f"Memory reduction: {memory_reduction:.1f}%")

    print("âœ… Memory efficiency test completed!")
    return True


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰è®­ç»ƒå’Œæ¨ç†æµ‹è¯•"""
    print("=" * 60)
    print("MINIGPT TRAINING & INFERENCE TESTS")
    print("=" * 60)

    tests = [
        test_model_training,
        test_model_inference,
        test_tool_calling_format,
        test_ultra_think_capability,
        test_memory_efficiency,
    ]

    passed = 0
    total = len(tests)

    for test_func in tests:
        try:
            if test_func():
                passed += 1
            print()
        except Exception as e:
            print(f"âŒ {test_func.__name__} failed with error: {e}")
            import traceback
            traceback.print_exc()
            print()

    print("=" * 60)
    print(f"TRAINING & INFERENCE TESTS SUMMARY: {passed}/{total} PASSED")
    print("=" * 60)

    if passed == total:
        print("ğŸ‰ All training and inference tests passed!")
        return True
    else:
        print("âš ï¸  Some tests failed. Please check the implementation.")
        return False


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)