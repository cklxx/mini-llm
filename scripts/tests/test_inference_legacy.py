#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Legacyæ¨ç†æµ‹è¯•è„šæœ¬ (ä¼˜åŒ–ç‰ˆ)
æµ‹è¯•æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å’Œæ¨ç†æ€§èƒ½ï¼Œå…¼å®¹æ–°æ—§æ¶æ„
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

import torch
import time
import json
from typing import List, Dict, Any
from pathlib import Path

# æ–°æ¶æ„å¯¼å…¥
from src.model.config import get_tiny_config, get_small_config, MiniGPTConfig
from src.model.transformer import MiniGPT

# æ—§é…ç½®å…¼å®¹
try:
    from config.training_config import get_config as get_legacy_config
except ImportError:
    get_legacy_config = None


class LegacyInferenceTest:
    """Legacyæ¨ç†æµ‹è¯•å™¨ï¼Œå…¼å®¹æ–°æ—§æ¶æ„"""

    def __init__(self, config_name: str = "tiny", use_legacy: bool = False):
        """
        åˆå§‹åŒ–æµ‹è¯•å™¨

        Args:
            config_name: é…ç½®åç§° (tiny/small/medium)
            use_legacy: æ˜¯å¦ä½¿ç”¨legacyé…ç½®
        """
        self.config_name = config_name
        self.use_legacy = use_legacy

        # è·å–é…ç½®å’Œæ¨¡å‹
        self.config, self.model = self._setup_model()

        # è®¾å¤‡æ£€æµ‹
        self.device = self._detect_device()
        if self.device.type != "cpu":
            self.model = self.model.to(self.device)

        print(f"âœ… æµ‹è¯•å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   é…ç½®: {config_name} ({'legacy' if use_legacy else 'optimized'})")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   å‚æ•°: {sum(p.numel() for p in self.model.parameters()):,}")

    def _detect_device(self) -> torch.device:
        """æ™ºèƒ½è®¾å¤‡æ£€æµ‹"""
        if torch.cuda.is_available():
            return torch.device('cuda')
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            return torch.device('mps')
        else:
            return torch.device('cpu')

    def _setup_model(self):
        """è®¾ç½®æ¨¡å‹"""
        if self.use_legacy and get_legacy_config:
            # Legacyé…ç½®
            config = get_legacy_config(self.config_name)

            # è½¬æ¢ä¸ºæ–°é…ç½®æ ¼å¼
            model_config = MiniGPTConfig(
                vocab_size=config.vocab_size,
                hidden_size=getattr(config, 'd_model', 128),
                num_hidden_layers=getattr(config, 'n_layers', 4),
                num_attention_heads=getattr(config, 'n_heads', 2),
                intermediate_size=getattr(config, 'd_ff', 512),
                max_position_embeddings=getattr(config, 'max_seq_len', 256),
                dropout=getattr(config, 'dropout', 0.1),
                use_rope=False,  # Legacyä¸ä½¿ç”¨RoPE
                use_gqa=False,   # Legacyä¸ä½¿ç”¨GQA
                tie_word_embeddings=False
            )

            model = MiniGPT(model_config)
            return config, model
        else:
            # æ–°ä¼˜åŒ–é…ç½®
            if self.config_name == "tiny":
                config = get_tiny_config()
            elif self.config_name == "small":
                config = get_small_config()
            else:
                config = get_tiny_config()  # é»˜è®¤

            model = MiniGPT(config)
            return config, model

    def test_text_generation(self, test_prompts: List[List[int]] = None) -> List[Dict]:
        """æµ‹è¯•æ–‡æœ¬ç”ŸæˆåŠŸèƒ½"""
        print("\n=== æ–‡æœ¬ç”Ÿæˆæµ‹è¯• ===")

        if test_prompts is None:
            # åˆ›å»ºæµ‹è¯•æç¤º
            vocab_size = getattr(self.config, 'vocab_size', 1000)
            test_prompts = [
                [1, 2, 3, 4, 5],  # ç®€å•æ•°å­—åºåˆ—
                [10, 20, 30],     # å¦ä¸€ä¸ªåºåˆ—
                [min(100, vocab_size-1)],  # å•ä¸ªtoken
                [1, 1, 1, 1]      # é‡å¤token
            ]

        self.model.eval()
        results = []

        for i, prompt_tokens in enumerate(test_prompts):
            print(f"\n--- æµ‹è¯• {i+1}: è¾“å…¥é•¿åº¦ {len(prompt_tokens)} ---")

            # è½¬æ¢ä¸ºtensor
            prompt = torch.tensor([prompt_tokens], dtype=torch.long, device=self.device)
            print(f"è¾“å…¥: {prompt_tokens}")

            # ç”Ÿæˆæ–‡æœ¬
            start_time = time.time()
            with torch.no_grad():
                try:
                    if hasattr(self.model, 'generate'):
                        generated = self.model.generate(
                            prompt,
                            max_length=20,
                            temperature=0.8,
                            top_k=50
                        )
                    else:
                        # ç®€å•ç”Ÿæˆå¤‡ç”¨æ–¹æ¡ˆ
                        generated = self._simple_generate(prompt, max_length=20)
                except Exception as e:
                    print(f"âš ï¸ ç”Ÿæˆå¤±è´¥: {e}")
                    generated = prompt  # è¿”å›åŸå§‹è¾“å…¥

            end_time = time.time()

            # æå–ç”Ÿæˆçš„éƒ¨åˆ†
            generated_tokens = generated[0].tolist()
            new_tokens = generated_tokens[len(prompt_tokens):]

            generation_time = end_time - start_time
            tokens_per_second = len(new_tokens) / generation_time if generation_time > 0 else 0

            print(f"ç”Ÿæˆ: {new_tokens}")
            print(f"æ—¶é—´: {generation_time:.3f}s")
            print(f"é€Ÿåº¦: {tokens_per_second:.1f} tokens/s")

            results.append({
                'prompt': prompt_tokens,
                'generated': new_tokens,
                'time': generation_time,
                'speed': tokens_per_second,
                'success': len(new_tokens) > 0
            })

        return results

    def _simple_generate(self, prompt: torch.Tensor, max_length: int = 20) -> torch.Tensor:
        """ç®€å•ç”Ÿæˆå‡½æ•°ï¼ˆå¤‡ç”¨ï¼‰"""
        generated = prompt
        vocab_size = getattr(self.config, 'vocab_size', 1000)

        for _ in range(max_length):
            if generated.size(1) >= max_length:
                break

            # å‰å‘ä¼ æ’­
            logits = self.model(generated)
            next_token_logits = logits[0, -1, :] / 0.8  # temperature

            # ç®€å•é‡‡æ ·
            probs = torch.softmax(next_token_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)

            # æ‹¼æ¥
            generated = torch.cat([generated, next_token.unsqueeze(0)], dim=1)

        return generated

    def test_batch_generation(self) -> float:
        """æµ‹è¯•æ‰¹é‡ç”Ÿæˆ"""
        print("\n=== æ‰¹é‡ç”Ÿæˆæµ‹è¯• ===")

        batch_size = 4
        seq_len = 5
        vocab_size = getattr(self.config, 'vocab_size', 1000)

        # åˆ›å»ºæ‰¹é‡è¾“å…¥
        prompts = torch.randint(0, min(100, vocab_size), (batch_size, seq_len), device=self.device)

        print(f"æ‰¹é‡å¤§å°: {batch_size}")
        print(f"è¾“å…¥å½¢çŠ¶: {prompts.shape}")

        self.model.eval()
        start_time = time.time()

        with torch.no_grad():
            # é€ä¸ªå¤„ç†ï¼ˆå…¼å®¹æ€§è€ƒè™‘ï¼‰
            generated_batch = []
            for i in range(batch_size):
                single_prompt = prompts[i:i+1]
                try:
                    if hasattr(self.model, 'generate'):
                        generated = self.model.generate(
                            single_prompt,
                            max_length=10,
                            temperature=0.8,
                            top_k=50
                        )
                    else:
                        generated = self._simple_generate(single_prompt, max_length=10)
                    generated_batch.append(generated)
                except Exception as e:
                    print(f"âš ï¸ æ‰¹æ¬¡ {i} ç”Ÿæˆå¤±è´¥: {e}")
                    generated_batch.append(single_prompt)

        end_time = time.time()
        total_time = end_time - start_time

        print(f"æ‰¹é‡ç”Ÿæˆæ—¶é—´: {total_time:.3f}s")
        print(f"å¹³å‡æ¯æ ·æœ¬æ—¶é—´: {total_time/batch_size:.3f}s")

        return total_time

    def test_different_generation_params(self) -> List[Dict]:
        """æµ‹è¯•ä¸åŒçš„ç”Ÿæˆå‚æ•°"""
        print("\n=== ç”Ÿæˆå‚æ•°æµ‹è¯• ===")

        # å›ºå®šè¾“å…¥
        vocab_size = getattr(self.config, 'vocab_size', 1000)
        prompt = torch.tensor([[1, 2, min(3, vocab_size-1)]], dtype=torch.long, device=self.device)

        test_params = [
            {'temperature': 0.1, 'top_k': 10, 'name': 'ä½æ¸©åº¦+å°top_k'},
            {'temperature': 1.0, 'top_k': 50, 'name': 'ä¸­ç­‰æ¸©åº¦+ä¸­ç­‰top_k'},
            {'temperature': 1.5, 'top_k': 100, 'name': 'é«˜æ¸©åº¦+å¤§top_k'},
        ]

        self.model.eval()
        results = []

        for params in test_params:
            print(f"\n--- {params['name']} ---")
            print(f"æ¸©åº¦: {params['temperature']}, top_k: {params['top_k']}")

            start_time = time.time()
            with torch.no_grad():
                try:
                    if hasattr(self.model, 'generate'):
                        generated = self.model.generate(
                            prompt,
                            max_length=15,
                            temperature=params['temperature'],
                            top_k=params['top_k']
                        )
                    else:
                        generated = self._simple_generate(prompt, max_length=15)
                except Exception as e:
                    print(f"âš ï¸ å‚æ•°æµ‹è¯•å¤±è´¥: {e}")
                    generated = prompt

            end_time = time.time()

            generated_tokens = generated[0].tolist()[3:]  # å»æ‰è¾“å…¥éƒ¨åˆ†
            generation_time = end_time - start_time

            print(f"ç”Ÿæˆ: {generated_tokens}")
            print(f"æ—¶é—´: {generation_time:.3f}s")

            results.append({
                'params': params,
                'generated': generated_tokens,
                'time': generation_time,
                'success': len(generated_tokens) > 0
            })

        return results

    def test_memory_usage(self) -> Dict[str, Any]:
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        print("\n=== å†…å­˜ä½¿ç”¨æµ‹è¯• ===")

        if self.device.type == "cuda":
            torch.cuda.empty_cache()
            before_allocated = torch.cuda.memory_allocated() / 1024**3
            before_reserved = torch.cuda.memory_reserved() / 1024**3

            print(f"ç”Ÿæˆå‰ - å·²åˆ†é…: {before_allocated:.1f}GB, å·²ä¿ç•™: {before_reserved:.1f}GB")

            # ç”Ÿæˆå¤§é‡æ–‡æœ¬
            vocab_size = getattr(self.config, 'vocab_size', 1000)
            prompt = torch.tensor([[1, 2, 3, 4, min(5, vocab_size-1)]], dtype=torch.long, device=self.device)

            self.model.eval()
            with torch.no_grad():
                for i in range(5):
                    try:
                        if hasattr(self.model, 'generate'):
                            generated = self.model.generate(prompt, max_length=50)
                        else:
                            generated = self._simple_generate(prompt, max_length=50)

                        if i == 0:
                            mid_allocated = torch.cuda.memory_allocated() / 1024**3
                            mid_reserved = torch.cuda.memory_reserved() / 1024**3
                            print(f"ç”Ÿæˆä¸­ - å·²åˆ†é…: {mid_allocated:.1f}GB, å·²ä¿ç•™: {mid_reserved:.1f}GB")
                    except Exception as e:
                        print(f"âš ï¸ å†…å­˜æµ‹è¯•ç¬¬{i}è½®å¤±è´¥: {e}")

            after_allocated = torch.cuda.memory_allocated() / 1024**3
            after_reserved = torch.cuda.memory_reserved() / 1024**3

            print(f"ç”Ÿæˆå - å·²åˆ†é…: {after_allocated:.1f}GB, å·²ä¿ç•™: {after_reserved:.1f}GB")

            return {
                'before': {'allocated': before_allocated, 'reserved': before_reserved},
                'after': {'allocated': after_allocated, 'reserved': after_reserved},
                'device': 'cuda'
            }
        else:
            print(f"è®¾å¤‡ç±»å‹: {self.device.type}, è·³è¿‡æ˜¾å­˜ç›‘æ§")
            return {'device': str(self.device.type), 'monitoring': 'skipped'}

    def test_architecture_features(self) -> Dict[str, Any]:
        """æµ‹è¯•æ¶æ„ç‰¹æ€§"""
        print("\n=== æ¶æ„ç‰¹æ€§æµ‹è¯• ===")

        features = {
            'rope_enabled': getattr(self.config, 'use_rope', False),
            'gqa_enabled': getattr(self.config, 'use_gqa', False),
            'weight_sharing': getattr(self.config, 'tie_word_embeddings', False),
            'swiglu_activation': True,  # é»˜è®¤å¯ç”¨
            'rms_norm': True,  # é»˜è®¤å¯ç”¨
        }

        # æ£€æŸ¥æ¨¡å‹å®é™…ç»“æ„
        model_features = {}
        if hasattr(self.model, 'use_rope'):
            model_features['rope_in_model'] = self.model.use_rope
        if hasattr(self.model, 'lm_head'):
            model_features['separate_lm_head'] = self.model.lm_head is not None

        # è®¡ç®—å‚æ•°ç»Ÿè®¡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        stats = {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'model_size_mb': total_params * 4 / (1024 * 1024),  # å‡è®¾float32
            'config_features': features,
            'model_features': model_features
        }

        print(f"æ€»å‚æ•°: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"æ¨¡å‹å¤§å°: {stats['model_size_mb']:.1f} MB")
        print(f"RoPEå¯ç”¨: {features['rope_enabled']}")
        print(f"GQAå¯ç”¨: {features['gqa_enabled']}")
        print(f"æƒé‡å…±äº«: {features['weight_sharing']}")

        return stats

    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸ”¥ å¼€å§‹Legacyæ¨ç†æµ‹è¯•")

        results = {
            'test_config': {
                'config_name': self.config_name,
                'use_legacy': self.use_legacy,
                'device': str(self.device)
            }
        }

        try:
            # è¿è¡Œå„ç§æ¨ç†æµ‹è¯•
            results['generation'] = self.test_text_generation()
            results['batch_time'] = self.test_batch_generation()
            results['parameters'] = self.test_different_generation_params()
            results['memory'] = self.test_memory_usage()
            results['architecture'] = self.test_architecture_features()

            # è®¡ç®—æ€»ç»“ç»Ÿè®¡
            generation_results = results['generation']
            if generation_results:
                successful_tests = [r for r in generation_results if r['success']]
                if successful_tests:
                    avg_speed = sum(r['speed'] for r in successful_tests) / len(successful_tests)
                    avg_time = sum(r['time'] for r in successful_tests) / len(successful_tests)
                    results['summary'] = {
                        'avg_speed': avg_speed,
                        'avg_time': avg_time,
                        'success_rate': len(successful_tests) / len(generation_results),
                        'total_tests': len(generation_results)
                    }

            print(f"\nğŸ‰ Legacyæ¨ç†æµ‹è¯•å®Œæˆï¼")

            # æ‰“å°æ€»ç»“
            if 'summary' in results:
                summary = results['summary']
                print(f"âœ“ å¹³å‡ç”Ÿæˆé€Ÿåº¦: {summary['avg_speed']:.1f} tokens/ç§’")
                print(f"âœ“ å¹³å‡ç”Ÿæˆæ—¶é—´: {summary['avg_time']:.3f} ç§’")
                print(f"âœ“ æˆåŠŸç‡: {summary['success_rate']:.1%}")

            print(f"âœ“ æ¶æ„ç‰¹æ€§: {'Legacy' if self.use_legacy else 'Optimized'}")
            print("\næ‰€æœ‰æ¨ç†æµ‹è¯•é€šè¿‡ï¼")

        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            results['error'] = str(e)

        return results


def compare_architectures():
    """æ¯”è¾ƒæ–°æ—§æ¶æ„æ€§èƒ½"""
    print("\n" + "="*60)
    print("æ¶æ„æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("="*60)

    configs_to_test = [
        ("tiny", False, "ä¼˜åŒ–æ¶æ„"),
        ("tiny", True, "Legacyæ¶æ„") if get_legacy_config else None
    ]

    configs_to_test = [c for c in configs_to_test if c is not None]

    comparison_results = {}

    for config_name, use_legacy, description in configs_to_test:
        print(f"\n--- æµ‹è¯• {description} ---")

        try:
            tester = LegacyInferenceTest(config_name, use_legacy)
            results = tester.run_all_tests()
            comparison_results[description] = results
        except Exception as e:
            print(f"âŒ {description} æµ‹è¯•å¤±è´¥: {e}")
            comparison_results[description] = {'error': str(e)}

    # ä¿å­˜å¯¹æ¯”ç»“æœ
    output_file = "legacy_inference_comparison.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(comparison_results, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“Š å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    return comparison_results


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='Legacyæ¨ç†æµ‹è¯•è„šæœ¬')
    parser.add_argument('--config', type=str, default='tiny',
                       choices=['tiny', 'small'], help='æ¨¡å‹é…ç½®')
    parser.add_argument('--legacy', action='store_true', help='ä½¿ç”¨legacyé…ç½®')
    parser.add_argument('--compare', action='store_true', help='æ¯”è¾ƒæ–°æ—§æ¶æ„')

    args = parser.parse_args()

    if args.compare:
        compare_architectures()
    else:
        tester = LegacyInferenceTest(args.config, args.legacy)
        results = tester.run_all_tests()

        # ä¿å­˜ç»“æœ
        output_file = f"inference_test_results_{args.config}_{'legacy' if args.legacy else 'optimized'}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


if __name__ == "__main__":
    main()