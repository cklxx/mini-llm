#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Macä¼˜åŒ–è®­ç»ƒè„šæœ¬
é›†æˆèµ„æºç›‘æ§ï¼Œé˜²æ­¢ç³»ç»Ÿå¡æ­»ï¼Œä½¿ç”¨æœ€å°æ•°æ®é›†å¿«é€ŸéªŒè¯æ™ºèƒ½æ•ˆæœ
"""
import os
import sys
import argparse
import time
import signal
import threading
from datetime import datetime
import torch
from torch.utils.data import DataLoader


def print_progress_bar(current, total, prefix='', suffix='', length=40, fill='â–ˆ', empty='â–‘'):
    """æ‰“å°åŠ¨æ€è¿›åº¦æ¡"""
    percent = 100 * (current / float(total))
    filled_length = int(length * current // total)
    bar = fill * filled_length + empty * (length - filled_length)
    print(f'\r{prefix} |{bar}| {percent:.1f}% {suffix}', end='', flush=True)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•å’Œsrcç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.dirname(__file__))
sys.path.append(project_root)
sys.path.append(os.path.join(project_root, 'src'))

from model.transformer import create_model
from tokenizer.bpe_tokenizer import BPETokenizer, train_tokenizer_from_data
from data.dataset_loader import create_data_loader, DatasetConfig
from training.trainer import create_trainer, LanguageModelingDataset, ConversationDataset
from config.mac_optimized_config import (
    get_mac_tiny_config, get_mac_small_config, get_mac_medium_config, MacResourceConfig, 
    MacResourceMonitor, estimate_model_size, validate_config_for_mac,
    get_system_info
)


class OptimizedTrainer:
    """ä¼˜åŒ–çš„è®­ç»ƒå™¨ï¼Œå¸¦èµ„æºç›‘æ§"""
    
    def __init__(self, config, resource_config: MacResourceConfig):
        self.config = config
        self.resource_config = resource_config
        self.resource_monitor = MacResourceMonitor(resource_config)
        self.should_stop = False
        self.pause_training = False
        self.training_thread = None
        
        # æ³¨å†Œä¿¡å·å¤„ç†å™¨ï¼Œä¼˜é›…é€€å‡º
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
    def _signal_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†å™¨"""
        print(f"\næ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨å®‰å…¨é€€å‡º...")
        self.should_stop = True
        
    def _monitor_resources(self):
        """èµ„æºç›‘æ§çº¿ç¨‹"""
        while not self.should_stop:
            try:
                resources = self.resource_monitor.check_resources()
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æš‚åœè®­ç»ƒ
                if self.resource_monitor.should_pause_training():
                    if not self.pause_training:
                        print(f"\nâš ï¸  èµ„æºä½¿ç”¨è¿‡é«˜ï¼Œæš‚åœè®­ç»ƒ:")
                        print(f"   CPU: {resources['cpu_percent']:.1f}% (é™åˆ¶: {self.resource_config.max_cpu_percent}%)")
                        print(f"   å†…å­˜: {resources['memory_percent']:.1f}% (é™åˆ¶: {self.resource_config.max_memory_percent}%)")
                        self.pause_training = True
                else:
                    if self.pause_training:
                        print(f"\nâœ… èµ„æºä½¿ç”¨æ¢å¤æ­£å¸¸ï¼Œç»§ç»­è®­ç»ƒ")
                        self.pause_training = False
                
                # åªåœ¨èµ„æºå¼‚å¸¸æ—¶è®°å½•
                if hasattr(self, 'step_count') and (
                    resources['cpu_percent'] > 80 or resources['memory_percent'] > 80
                ) and self.step_count % 50 == 0:
                    print(f"\nâš ï¸  é«˜èµ„æºä½¿ç”¨ - CPU: {resources['cpu_percent']:.1f}%, å†…å­˜: {resources['memory_percent']:.1f}%")
                
                time.sleep(self.resource_config.monitoring_interval)
                
            except Exception as e:
                print(f"èµ„æºç›‘æ§é”™è¯¯: {e}")
                time.sleep(5)
    
    def train(self):
        """æ‰§è¡Œè®­ç»ƒ"""
        try:
            # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
            self._print_system_info()
            
            # éªŒè¯é…ç½®
            warnings = validate_config_for_mac(self.config)
            if warnings:
                print("âš ï¸  é…ç½®è­¦å‘Š:")
                for warning in warnings:
                    print(f"   - {warning}")
                input("æŒ‰å›è½¦é”®ç»§ç»­ï¼Œæˆ–Ctrl+Cé€€å‡º...")
            
            # å¯åŠ¨èµ„æºç›‘æ§
            if self.resource_config.enable_monitoring:
                print("ğŸ” å¯åŠ¨èµ„æºç›‘æ§...")
                monitor_thread = threading.Thread(target=self._monitor_resources, daemon=True)
                monitor_thread.start()
            
            # åŠ è½½æˆ–è®­ç»ƒåˆ†è¯å™¨
            tokenizer = self._setup_tokenizer()
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            data_loader = self._setup_data_loader(tokenizer)
            
            # åˆ›å»ºæ¨¡å‹
            model = self._setup_model(tokenizer)
            
            # æ‰§è¡Œè®­ç»ƒ
            self._run_training(model, data_loader, tokenizer)
            
        except KeyboardInterrupt:
            print("\nç”¨æˆ·ä¸­æ–­è®­ç»ƒ")
        except Exception as e:
            print(f"\nè®­ç»ƒé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        finally:
            self.should_stop = True
            print("è®­ç»ƒç»“æŸ")
    
    def _print_system_info(self):
        """æ‰“å°ç³»ç»Ÿä¿¡æ¯"""
        print("=" * 60)
        print("ğŸš€ Macä¼˜åŒ–è®­ç»ƒ - æ™ºèƒ½æ•ˆæœéªŒè¯")
        print("=" * 60)
        
        sys_info = get_system_info()
        print(f"ç³»ç»Ÿå¹³å°: {sys_info['platform']}")
        print(f"CPUæ ¸å¿ƒæ•°: {sys_info['cpu_count']}")
        print(f"æ€»å†…å­˜: {sys_info['memory_gb']:.1f}GB")
        print(f"å¯ç”¨å†…å­˜: {sys_info['available_memory_gb']:.1f}GB")
        
        model_info = estimate_model_size(self.config)
        print(f"\nğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"é¢„ä¼°å‚æ•°é‡: {model_info['total_params']:,}")
        print(f"æ¨¡å‹å†…å­˜: {model_info['model_memory_mb']:.1f}MB")
        print(f"è®­ç»ƒå†…å­˜: {model_info['training_memory_mb']:.1f}MB")
        
        print(f"\nâš™ï¸  è®­ç»ƒé…ç½®:")
        print(f"æ•°æ®æ–‡ä»¶: {self.config.data.train_files}")
        print(f"æ‰¹æ¬¡å¤§å°: {self.config.data.batch_size}")
        print(f"æœ€å¤§æ­¥æ•°: {self.config.pretrain.max_steps}")
        print(f"å­¦ä¹ ç‡: {self.config.pretrain.learning_rate}")
        print(f"è®¾å¤‡: {self.config.device}")
        print("-" * 60)
    
    def _setup_tokenizer(self):
        """è®¾ç½®åˆ†è¯å™¨"""
        print("ğŸ”§ è®¾ç½®åˆ†è¯å™¨...")
        
        tokenizer_path = os.path.join(self.config.output_dir, "tokenizer.pkl")
        os.makedirs(self.config.output_dir, exist_ok=True)
        
        if os.path.exists(tokenizer_path):
            print(f"åŠ è½½ç°æœ‰åˆ†è¯å™¨: {tokenizer_path}")
            tokenizer = BPETokenizer(vocab_size=self.config.tokenizer.vocab_size)
            tokenizer.load(tokenizer_path)
        else:
            print("è®­ç»ƒæ–°çš„åˆ†è¯å™¨...")
            # ä»è®­ç»ƒæ•°æ®æ„å»ºåˆ†è¯å™¨
            data_path = os.path.join(self.config.data.data_dir, self.config.data.train_files[0])
            
            # è¯»å–æ–‡æœ¬æ•°æ®
            texts = []
            import json
            with open(data_path, 'r', encoding='utf-8') as f:
                for line in f:
                    data = json.loads(line.strip())
                    texts.append(data['text'])
            
            print(f"ä» {len(texts)} æ¡æ–‡æœ¬è®­ç»ƒåˆ†è¯å™¨...")
            tokenizer = BPETokenizer(vocab_size=self.config.tokenizer.vocab_size)
            tokenizer.train(texts)
            
            tokenizer.save(tokenizer_path)
            print(f"åˆ†è¯å™¨å·²ä¿å­˜: {tokenizer_path}")
        
        print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
        return tokenizer
    
    def _setup_data_loader(self, tokenizer):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        print("ğŸ“š è®¾ç½®æ•°æ®åŠ è½½å™¨...")
        
        data_path = os.path.join(self.config.data.data_dir, self.config.data.train_files[0])
        
        # è¯»å–è®­ç»ƒæ•°æ®
        texts = []
        import json
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line.strip())
                texts.append(data['text'])
        
        print(f"åŠ è½½ {len(texts)} æ¡è®­ç»ƒæ–‡æœ¬")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = LanguageModelingDataset(
            texts=texts,
            tokenizer=tokenizer,
            max_length=self.config.data.max_seq_len
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        data_loader = DataLoader(
            dataset,
            batch_size=self.config.data.batch_size,
            shuffle=self.config.data.shuffle,
            num_workers=self.config.data.num_workers,
            pin_memory=False  # Macä¸Šé¿å…å†…å­˜é—®é¢˜
        )
        
        print(f"æ•°æ®æ‰¹æ¬¡æ•°: {len(data_loader)}")
        return data_loader
    
    def _setup_model(self, tokenizer):
        """è®¾ç½®æ¨¡å‹"""
        print("ğŸ§  åˆ›å»ºæ¨¡å‹...")
        
        # è®¾ç½®è®¾å¤‡
        device = torch.device(self.config.device)
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        # åˆ›å»ºæ¨¡å‹
        model = create_model(tokenizer.vocab_size, self.config.model.model_size)
        model = model.to(device)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"æ€»å‚æ•°é‡: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        
        return model
    
    def _run_training(self, model, data_loader, tokenizer):
        """æ‰§è¡Œè®­ç»ƒå¾ªç¯"""
        print("ğŸƒ å¼€å§‹è®­ç»ƒ...")
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=self.config.pretrain.learning_rate,
            weight_decay=self.config.pretrain.weight_decay
        )
        
        # è®¾ç½®æŸå¤±å‡½æ•°
        criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_id)
        
        # è®­ç»ƒç»Ÿè®¡
        self.step_count = 0
        best_loss = float('inf')
        start_time = time.time()
        
        model.train()
        
        for epoch in range(1000):  # æœ€å¤§epochæ•°
            if self.should_stop:
                break
                
            epoch_loss = 0
            epoch_steps = 0
            
            for batch_idx, batch in enumerate(data_loader):
                if self.should_stop:
                    break
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æš‚åœ
                while self.pause_training and not self.should_stop:
                    time.sleep(1)
                
                if self.should_stop:
                    break
                
                try:
                    # æ•°æ®ç§»åˆ°è®¾å¤‡
                    device = next(model.parameters()).device
                    batch = batch.to(device)
                    
                    # å‡†å¤‡è¾“å…¥å’Œç›®æ ‡
                    input_ids = batch[:, :-1]
                    target_ids = batch[:, 1:]
                    
                    # å‰å‘ä¼ æ’­
                    optimizer.zero_grad()
                    outputs = model(input_ids)
                    
                    # è®¡ç®—æŸå¤±
                    loss = criterion(outputs.reshape(-1, outputs.size(-1)), target_ids.reshape(-1))
                    
                    # åå‘ä¼ æ’­
                    loss.backward()
                    
                    # æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(model.parameters(), self.config.pretrain.max_grad_norm)
                    
                    # æ›´æ–°å‚æ•°
                    optimizer.step()
                    
                    # æ›´æ–°ç»Ÿè®¡
                    epoch_loss += loss.item()
                    epoch_steps += 1
                    self.step_count += 1
                    
                    # è®°å½•æ—¥å¿— - ä½¿ç”¨è¦†ç›–å¼æ˜¾ç¤º
                    if self.step_count % self.config.logging_steps == 0:
                        avg_loss = epoch_loss / epoch_steps
                        elapsed = time.time() - start_time
                        steps_per_sec = self.step_count / elapsed
                        progress = (self.step_count / self.config.pretrain.max_steps) * 100
                        
                        # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤º
                        suffix = f"æŸå¤± {loss.item():.4f} | å¹³å‡ {avg_loss:.4f} | {steps_per_sec:.1f} steps/s"
                        print_progress_bar(self.step_count, self.config.pretrain.max_steps, 
                                         prefix=f'ğŸƒ è®­ç»ƒä¸­ (è½®æ¬¡ {epoch})', suffix=suffix)
                    
                    # ä¿å­˜æ£€æŸ¥ç‚¹
                    if self.step_count % self.config.pretrain.save_steps == 0:
                        print(f"\nğŸ’¾ æ­¥éª¤ {self.step_count} - ä¿å­˜æ£€æŸ¥ç‚¹...")
                        self._save_checkpoint(model, tokenizer, optimizer, self.step_count, loss.item())
                        print()  # ç©ºè¡Œåç»§ç»­æ˜¾ç¤ºè¿›åº¦æ¡
                    
                    # ç®€å•éªŒè¯ï¼ˆç”Ÿæˆä¸€æ®µæ–‡æœ¬ï¼‰
                    if self.step_count % self.config.pretrain.eval_steps == 0:
                        print(f"\nğŸ§ª æ­¥éª¤ {self.step_count} - æ¨¡å‹è¯„ä¼°...")
                        self._evaluate_model(model, tokenizer)
                        print()  # ç©ºè¡Œåç»§ç»­æ˜¾ç¤ºè¿›åº¦æ¡
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ­¥æ•°
                    if self.step_count >= self.config.pretrain.max_steps:
                        print(f"è¾¾åˆ°æœ€å¤§è®­ç»ƒæ­¥æ•°: {self.config.pretrain.max_steps}")
                        self.should_stop = True
                        break
                        
                except Exception as e:
                    print(f"è®­ç»ƒæ­¥éª¤é”™è¯¯: {e}")
                    continue
            
            if epoch_steps > 0:
                avg_epoch_loss = epoch_loss / epoch_steps
                print(f"\nğŸ“ˆ è½®æ¬¡ {epoch} å®Œæˆ - å¹³å‡æŸå¤±: {avg_epoch_loss:.4f}")
                
                if avg_epoch_loss < best_loss:
                    best_loss = avg_epoch_loss
                    print(f"ğŸ‰ æ–°çš„æœ€ä½³æŸå¤±: {best_loss:.4f}")
        
        print(f"\nğŸ è®­ç»ƒå®Œæˆ!")
        print(f"æ€»æ­¥æ•°: {self.step_count}")
        print(f"æœ€ä½³æŸå¤±: {best_loss:.4f}")
        print(f"è®­ç»ƒæ—¶é—´: {(time.time() - start_time) / 60:.1f}åˆ†é’Ÿ")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self._save_checkpoint(model, tokenizer, optimizer, self.step_count, best_loss, final=True)
    
    def _save_checkpoint(self, model, tokenizer, optimizer, step, loss, final=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_dir = self.config.output_dir
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        if final:
            checkpoint_path = os.path.join(checkpoint_dir, "final_model.pt")
        else:
            checkpoint_path = os.path.join(checkpoint_dir, f"checkpoint_step_{step}.pt")
        
        torch.save({
            'step': step,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': loss,
            'config': self.config,
        }, checkpoint_path)
        
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
    
    def _evaluate_model(self, model, tokenizer):
        """ç®€å•è¯„ä¼°æ¨¡å‹ï¼ˆç”Ÿæˆæµ‹è¯•ï¼‰"""
        model.eval()
        
        try:
            # ç®€å•çš„ç”Ÿæˆæµ‹è¯•
            test_prompt = "ä½ å¥½ï¼Œ"
            print(f"\nğŸ§ª ç”Ÿæˆæµ‹è¯• - è¾“å…¥: '{test_prompt}'")
            
            # ç¼–ç è¾“å…¥
            input_ids = tokenizer.encode(test_prompt, add_special_tokens=True)
            device = next(model.parameters()).device  # è·å–æ¨¡å‹è®¾å¤‡
            input_tensor = torch.tensor([input_ids], device=device)
            
            # ç”Ÿæˆ
            with torch.no_grad():
                for _ in range(10):  # ç”Ÿæˆ10ä¸ªtoken
                    outputs = model(input_tensor)
                    next_token_logits = outputs[0, -1, :]
                    next_token = torch.argmax(next_token_logits).item()
                    input_tensor = torch.cat([input_tensor, torch.tensor([[next_token]], device=device)], dim=1)
            
            # è§£ç ç»“æœ
            generated_ids = input_tensor[0].cpu().tolist()
            generated_text = tokenizer.decode(generated_ids)
            print(f"ğŸ¤– ç”Ÿæˆç»“æœ: '{generated_text}'")
            
        except Exception as e:
            print(f"ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        finally:
            model.train()


def main():
    parser = argparse.ArgumentParser(description='Macä¼˜åŒ–è®­ç»ƒè„šæœ¬')
    parser.add_argument('--config', choices=['tiny', 'small', 'medium'], default='tiny',
                        help='é€‰æ‹©é…ç½® (tiny: è¶…å°æ¨¡å‹, small: å°æ¨¡å‹, medium: ä¸­æ¨¡å‹)')
    parser.add_argument('--max-cpu', type=float, default=85.0,
                        help='æœ€å¤§CPUä½¿ç”¨ç‡ (%)')
    parser.add_argument('--max-memory', type=float, default=85.0,
                        help='æœ€å¤§å†…å­˜ä½¿ç”¨ç‡ (%)')
    parser.add_argument('--disable-monitoring', action='store_true',
                        help='ç¦ç”¨èµ„æºç›‘æ§')
    
    args = parser.parse_args()
    
    # è·å–é…ç½®
    if args.config == 'tiny':
        config = get_mac_tiny_config()
        print("ä½¿ç”¨è¶…å°æ¨¡å‹é…ç½® (æœ€å¿«éªŒè¯)")
    elif args.config == 'small':
        config = get_mac_small_config()
        print("ä½¿ç”¨å°æ¨¡å‹é…ç½® (å¹³è¡¡æ€§èƒ½)")
    else: # medium
        config = get_mac_medium_config()
        print("ä½¿ç”¨ä¸­æ¨¡å‹é…ç½® (æ€§èƒ½ä¸èµ„æºå¹³è¡¡)")
    
    # èµ„æºç›‘æ§é…ç½®
    resource_config = MacResourceConfig(
        max_cpu_percent=args.max_cpu,
        max_memory_percent=args.max_memory,
        enable_monitoring=not args.disable_monitoring
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = OptimizedTrainer(config, resource_config)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()


if __name__ == "__main__":
    main() 