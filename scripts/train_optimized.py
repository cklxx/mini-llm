#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Macä¼˜åŒ–è®­ç»ƒè„šæœ¬
é›†æˆèµ„æºç›‘æ§ï¼Œé˜²æ­¢ç³»ç»Ÿå¡æ­»ï¼Œä½¿ç”¨æœ€å°æ•°æ®é›†å¿«é€ŸéªŒè¯æ™ºèƒ½æ•ˆæœ
"""
import os
import sys
import argparse
import time
import signal
import threading
from datetime import datetime
import torch
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯ï¼Œé€‚åˆæœåŠ¡å™¨ç¯å¢ƒ
import multiprocessing
import psutil


def print_progress_bar(current, total, prefix='', suffix='', length=40, fill='â–ˆ', empty='â–‘'):
    """æ‰“å°åŠ¨æ€è¿›åº¦æ¡"""
    percent = 100 * (current / float(total))
    filled_length = int(length * current // total)
    bar = fill * filled_length + empty * (length - filled_length)
    print(f'\r{prefix} |{bar}| {percent:.1f}% {suffix}', end='', flush=True)


def optimize_pytorch_performance(num_threads=None, enable_optimizations=True):
    """ä¼˜åŒ–PyTorchæ€§èƒ½è®¾ç½®"""
    print("ğŸš€ ä¼˜åŒ–PyTorchæ€§èƒ½è®¾ç½®...")
    
    # è·å–ç³»ç»Ÿä¿¡æ¯
    cpu_count = multiprocessing.cpu_count()
    physical_cores = psutil.cpu_count(logical=False)
    
    # è‡ªåŠ¨ç¡®å®šæœ€ä½³çº¿ç¨‹æ•°
    if num_threads is None:
        # ä½¿ç”¨ç‰©ç†æ ¸å¿ƒæ•°ï¼Œä½†ä¸è¶…è¿‡8ä¸ªçº¿ç¨‹ï¼ˆé¿å…è¿‡åº¦å¹¶è¡Œï¼‰
        num_threads = min(physical_cores or cpu_count, 8)
    
    print(f"   ç³»ç»ŸCPUæ ¸å¿ƒ: {cpu_count} (ç‰©ç†: {physical_cores})")
    print(f"   è®¾ç½®çº¿ç¨‹æ•°: {num_threads}")
    
    if enable_optimizations:
        # è®¾ç½®PyTorchçº¿ç¨‹æ•°
        torch.set_num_threads(num_threads)
        torch.set_num_interop_threads(num_threads)
        
        # å¯ç”¨PyTorchä¼˜åŒ–
        torch.backends.cudnn.benchmark = True  # å¦‚æœæœ‰CUDA
        torch.backends.cudnn.deterministic = False  # æé«˜æ€§èƒ½ï¼Œç‰ºç‰²ä¸€å®šçš„ç¡®å®šæ€§
        
        # å¯ç”¨JITèåˆä¼˜åŒ–
        try:
            torch.jit.set_fusion_strategy([('STATIC', 2), ('DYNAMIC', 2)])
            print("   âœ… JITèåˆä¼˜åŒ–å·²å¯ç”¨")
        except:
            print("   âš ï¸  JITèåˆä¼˜åŒ–ä¸å¯ç”¨")
        
            # CUDAä¼˜åŒ–ï¼ˆè‹±ä¼Ÿè¾¾GPUï¼‰
    if torch.cuda.is_available():
        print("   âœ… CUDA å¯ç”¨")
        device_count = torch.cuda.device_count()
        current_device = torch.cuda.current_device()
        device_name = torch.cuda.get_device_name(current_device)
        print(f"   ğŸ”§ CUDA è®¾å¤‡: {device_name} (è®¾å¤‡ {current_device}/{device_count})")
        
        # CUDAç‰¹å®šä¼˜åŒ–
        torch.backends.cudnn.enabled = True
        torch.backends.cudnn.benchmark = True  # è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜å·ç§¯ç®—æ³•
        torch.backends.cudnn.deterministic = False  # å…è®¸éç¡®å®šæ€§æé«˜æ€§èƒ½
        
        # è®¾ç½®CUDAå†…å­˜åˆ†é…ç­–ç•¥
        try:
            torch.cuda.empty_cache()  # æ¸…ç©ºç¼“å­˜
            # è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥ä¸ºæ‰©å±•åˆ†é…
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
            print("   ğŸ”§ CUDA å†…å­˜ä¼˜åŒ–å·²å¯ç”¨")
        except Exception as e:
            print(f"   âš ï¸  CUDA å†…å­˜ä¼˜åŒ–è®¾ç½®å¤±è´¥: {e}")
        
        print("   ğŸ”§ CUDA ä¼˜åŒ–é…ç½®å·²å¯ç”¨")
    
    # MPSä¼˜åŒ–ï¼ˆMac GPUï¼‰
    elif torch.backends.mps.is_available():
        print("   âœ… MPS (Metal Performance Shaders) å¯ç”¨")
        # MPSç‰¹å®šä¼˜åŒ–
        os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'  # å‡å°‘å†…å­˜ä½¿ç”¨
        # å¯ç”¨MPSç‰¹å®šçš„ä¼˜åŒ–
        os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'  # å…è®¸CPUå›é€€
        # è®¾ç½®MPSå†…å­˜åˆ†é…ç­–ç•¥
        if hasattr(torch.mps, 'set_per_process_memory_fraction'):
            torch.mps.set_per_process_memory_fraction(0.8)  # é™åˆ¶GPUå†…å­˜ä½¿ç”¨
        print("   ğŸ”§ MPS ä¼˜åŒ–é…ç½®å·²å¯ç”¨")
        
        # Intel MKLä¼˜åŒ–
        try:
            torch.backends.mkldnn.enabled = True
            print("   âœ… Intel MKL-DNN ä¼˜åŒ–å·²å¯ç”¨")
        except:
            print("   âš ï¸  Intel MKL-DNN ä¸å¯ç”¨")
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ä¼˜åŒ–
        os.environ['OMP_NUM_THREADS'] = str(num_threads)
        os.environ['MKL_NUM_THREADS'] = str(num_threads)
        os.environ['OPENBLAS_NUM_THREADS'] = str(num_threads)
        os.environ['VECLIB_MAXIMUM_THREADS'] = str(num_threads)
        os.environ['NUMEXPR_NUM_THREADS'] = str(num_threads)
        
        print("   âœ… ç¯å¢ƒå˜é‡å·²ä¼˜åŒ–")
    
    return num_threads


def get_optimal_dataloader_workers(batch_size, device_type="cpu"):
    """è·å–æœ€ä½³çš„DataLoader workeræ•°é‡"""
    cpu_count = multiprocessing.cpu_count()
    
    # å¯¹äºbatch_size=1çš„æµ‹è¯•æƒ…å†µï¼Œç¦ç”¨å¤šè¿›ç¨‹é¿å…workerå´©æºƒ
    if batch_size == 1:
        print(f"   ğŸ”§ æµ‹è¯•æ¨¡å¼ (batch_size=1): ç¦ç”¨DataLoaderå¤šè¿›ç¨‹ (workers=0)")
        return 0
    
    if device_type == "cuda":
        # CUDAè®¾å¤‡å¯ä»¥ä½¿ç”¨æ›´å¤šworkerï¼Œå› ä¸ºGPUå¤„ç†é€Ÿåº¦å¿«
        optimal_workers = min(8, cpu_count)  # CUDAå¯ä»¥ä½¿ç”¨æ›´å¤šworker
        print(f"   ğŸ”§ CUDAè®¾å¤‡ä¼˜åŒ–: ä½¿ç”¨ {optimal_workers} ä¸ªDataLoader worker")
    elif device_type == "mps":
        # MPSè®¾å¤‡æ—¶ï¼Œå‡å°‘workeræ•°é‡é¿å…ç“¶é¢ˆ
        optimal_workers = min(1, cpu_count // 4)  # MPSä½¿ç”¨æ›´å°‘worker
        print(f"   ğŸ”§ MPSè®¾å¤‡ä¼˜åŒ–: ä½¿ç”¨ {optimal_workers} ä¸ªDataLoader worker")
    elif batch_size <= 4:
        # å°æ‰¹æ¬¡æ—¶ä½¿ç”¨æ›´å°‘çš„worker
        optimal_workers = min(2, cpu_count // 2)
    else:
        # å¤§æ‰¹æ¬¡æ—¶å¯ä»¥ä½¿ç”¨æ›´å¤šworker
        optimal_workers = min(4, cpu_count // 2)
    
    print(f"   ğŸ“Š æ¨èDataLoader workers: {optimal_workers} (CPUæ ¸å¿ƒ: {cpu_count}, è®¾å¤‡: {device_type}, batch_size: {batch_size})")
    return max(0, optimal_workers)


def compile_model_if_supported(model, device_type="cpu"):
    """å¦‚æœæ”¯æŒï¼Œç¼–è¯‘æ¨¡å‹ä»¥æé«˜æ€§èƒ½"""
    try:
        # PyTorch 2.0+ çš„torch.compile
        if hasattr(torch, 'compile'):
            print("ğŸ”¥ ç¼–è¯‘æ¨¡å‹ä»¥æé«˜æ€§èƒ½...")
            # ä½¿ç”¨é€‚åˆçš„ç¼–è¯‘æ¨¡å¼
            if device_type == "cuda":
                # CUDAè®¾å¤‡ä½¿ç”¨æœ€æ¿€è¿›çš„ç¼–è¯‘ä¼˜åŒ–
                try:
                    compiled_model = torch.compile(model, mode="max-autotune", dynamic=True)
                    print(f"   âœ… CUDAæ¨¡å‹ç¼–è¯‘æˆåŠŸï¼Œä½¿ç”¨ max-autotune æ¨¡å¼")
                except Exception as e:
                    print(f"   âš ï¸  CUDAæ¨¡å‹ç¼–è¯‘å¤±è´¥ï¼Œå°è¯• reduce-overhead æ¨¡å¼: {e}")
                    try:
                        compiled_model = torch.compile(model, mode="reduce-overhead")
                        print(f"   âœ… CUDAæ¨¡å‹ç¼–è¯‘æˆåŠŸï¼Œä½¿ç”¨ reduce-overhead æ¨¡å¼")
                    except Exception as e2:
                        print(f"   âš ï¸  CUDAæ¨¡å‹ç¼–è¯‘å®Œå…¨å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹: {e2}")
                        compiled_model = model
            elif device_type == "mps":
                # MPSè®¾å¤‡ä½¿ç”¨ä¸“é—¨çš„ç¼–è¯‘ä¼˜åŒ–
                try:
                    # å°è¯•ä½¿ç”¨MPSç‰¹å®šçš„ä¼˜åŒ–æ¨¡å¼
                    compiled_model = torch.compile(model, mode="reduce-overhead", dynamic=False)
                    print(f"   âœ… MPSæ¨¡å‹ç¼–è¯‘æˆåŠŸï¼Œä½¿ç”¨ reduce-overhead æ¨¡å¼")
                except Exception as e:
                    print(f"   âš ï¸  MPSæ¨¡å‹ç¼–è¯‘å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹: {e}")
                    compiled_model = model
            else:
                # CPUè®¾å¤‡ä½¿ç”¨è½»é‡çº§ç¼–è¯‘
                try:
                    compiled_model = torch.compile(model, mode="reduce-overhead")
                    print(f"   âœ… CPUæ¨¡å‹ç¼–è¯‘æˆåŠŸï¼Œä½¿ç”¨ reduce-overhead æ¨¡å¼")
                except Exception as e:
                    print(f"   âš ï¸  CPUæ¨¡å‹ç¼–è¯‘å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹: {e}")
                    compiled_model = model
            
            print("   âœ… æ¨¡å‹ç¼–è¯‘æˆåŠŸ")
            return compiled_model
        else:
            print("   â„¹ï¸  æ¨¡å‹ç¼–è¯‘ä¸å¯ç”¨æˆ–ä¸é€‚ç”¨")
            return model
    except Exception as e:
        print(f"   âš ï¸  æ¨¡å‹ç¼–è¯‘å¤±è´¥: {e}")
        return model

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•å’Œsrcç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.dirname(__file__))
sys.path.append(project_root)
sys.path.append(os.path.join(project_root, 'src'))

from model.transformer import create_model
from tokenizer.bpe_tokenizer import BPETokenizer, train_tokenizer_from_data
from data.dataset_loader import create_data_loader, DatasetConfig
from training.trainer import create_trainer, LanguageModelingDataset, ConversationDataset
from config.mac_optimized_config import (
    get_mac_tiny_config, get_mac_small_config, get_mac_medium_config, MacResourceConfig, 
    MacResourceMonitor, estimate_model_size, validate_config_for_mac,
    get_system_info
)


class OptimizedTrainer:
    """ä¼˜åŒ–çš„è®­ç»ƒå™¨ï¼Œå¸¦èµ„æºç›‘æ§"""
    
    def __init__(self, config, resource_config: MacResourceConfig):
        self.config = config
        self.resource_config = resource_config
        self.resource_monitor = MacResourceMonitor(resource_config)
        self.should_stop = False
        self.pause_training = False
        self.training_thread = None
        
        # æŸå¤±å†å²è®°å½•
        self.loss_history = []
        self.step_history = []
        
        # æ¢å¤è®­ç»ƒç›¸å…³
        self.start_step = 0
        self.resume_checkpoint_path = None
        
        # æ³¨å†Œä¿¡å·å¤„ç†å™¨ï¼Œä¼˜é›…é€€å‡º
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
    def _signal_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†å™¨"""
        print(f"\næ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨å®‰å…¨é€€å‡º...")
        self.should_stop = True
        
    def _monitor_resources(self):
        """èµ„æºç›‘æ§çº¿ç¨‹"""
        while not self.should_stop:
            try:
                resources = self.resource_monitor.check_resources()
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æš‚åœè®­ç»ƒ
                if self.resource_monitor.should_pause_training():
                    if not self.pause_training:
                        print(f"\nâš ï¸  èµ„æºä½¿ç”¨è¿‡é«˜ï¼Œæš‚åœè®­ç»ƒ:")
                        print(f"   CPU: {resources['cpu_percent']:.1f}% (é™åˆ¶: {self.resource_config.max_cpu_percent}%)")
                        print(f"   å†…å­˜: {resources['memory_percent']:.1f}% (é™åˆ¶: {self.resource_config.max_memory_percent}%)")
                        self.pause_training = True
                else:
                    if self.pause_training:
                        print(f"\nâœ… èµ„æºä½¿ç”¨æ¢å¤æ­£å¸¸ï¼Œç»§ç»­è®­ç»ƒ")
                        self.pause_training = False
                
                # åªåœ¨èµ„æºå¼‚å¸¸æ—¶è®°å½•
                if hasattr(self, 'step_count') and (
                    resources['cpu_percent'] > 80 or resources['memory_percent'] > 80
                ) and self.step_count % 50 == 0:
                    print(f"\nâš ï¸  é«˜èµ„æºä½¿ç”¨ - CPU: {resources['cpu_percent']:.1f}%, å†…å­˜: {resources['memory_percent']:.1f}%")
                
                time.sleep(self.resource_config.monitoring_interval)
                
            except Exception as e:
                print(f"èµ„æºç›‘æ§é”™è¯¯: {e}")
                time.sleep(5)
    
    def train(self, resume_checkpoint_path=None, auto_resume=False):
        """æ‰§è¡Œè®­ç»ƒ"""
        try:
            # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
            self._print_system_info()
            
            # å¤„ç†checkpointæ¢å¤
            checkpoint = None
            if resume_checkpoint_path:
                checkpoint = self.load_checkpoint(resume_checkpoint_path)
            elif auto_resume:
                latest_checkpoint = self.find_latest_checkpoint(self.config.output_dir)
                if latest_checkpoint:
                    print(f"ğŸ”„ å‘ç°å·²æœ‰æ£€æŸ¥ç‚¹ï¼Œè‡ªåŠ¨æ¢å¤è®­ç»ƒ...")
                    checkpoint = self.load_checkpoint(latest_checkpoint)
                else:
                    print("ğŸ“ æœªæ‰¾åˆ°å·²æœ‰æ£€æŸ¥ç‚¹ï¼Œå¼€å§‹å…¨æ–°è®­ç»ƒ")
            
            # éªŒè¯é…ç½®
            warnings = validate_config_for_mac(self.config)
            if warnings:
                print("âš ï¸  é…ç½®è­¦å‘Š:")
                for warning in warnings:
                    print(f"   - {warning}")
                input("æŒ‰å›è½¦é”®ç»§ç»­ï¼Œæˆ–Ctrl+Cé€€å‡º...")
            
            # å¯åŠ¨èµ„æºç›‘æ§
            if self.resource_config.enable_monitoring:
                print("ğŸ” å¯åŠ¨èµ„æºç›‘æ§...")
                monitor_thread = threading.Thread(target=self._monitor_resources, daemon=True)
                monitor_thread.start()
            
            # åŠ è½½æˆ–è®­ç»ƒåˆ†è¯å™¨
            tokenizer = self._setup_tokenizer()
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            data_loader = self._setup_data_loader(tokenizer)
            
            # åˆ›å»ºæ¨¡å‹
            model = self._setup_model(tokenizer)
            
            # å¦‚æœæœ‰checkpointï¼Œæ¢å¤æ¨¡å‹çŠ¶æ€
            if checkpoint and 'model_state_dict' in checkpoint:
                print("ğŸ”§ æ¢å¤æ¨¡å‹çŠ¶æ€...")
                model.load_state_dict(checkpoint['model_state_dict'])
            
            # æ‰§è¡Œè®­ç»ƒ
            self._run_training(model, data_loader, tokenizer)
            
        except KeyboardInterrupt:
            print("\nç”¨æˆ·ä¸­æ–­è®­ç»ƒ")
        except Exception as e:
            print(f"\nè®­ç»ƒé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        finally:
            self.should_stop = True
            print("è®­ç»ƒç»“æŸ")
    
    def _print_system_info(self):
        """æ‰“å°ç³»ç»Ÿä¿¡æ¯"""
        print("=" * 60)
        print("ğŸš€ Macä¼˜åŒ–è®­ç»ƒ - æ™ºèƒ½æ•ˆæœéªŒè¯")
        print("=" * 60)
        
        sys_info = get_system_info()
        print(f"ç³»ç»Ÿå¹³å°: {sys_info['platform']}")
        print(f"CPUæ ¸å¿ƒæ•°: {sys_info['cpu_count']}")
        print(f"æ€»å†…å­˜: {sys_info['memory_gb']:.1f}GB")
        print(f"å¯ç”¨å†…å­˜: {sys_info['available_memory_gb']:.1f}GB")
        
        model_info = estimate_model_size(self.config)
        print(f"\nğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"é¢„ä¼°å‚æ•°é‡: {model_info['total_params']:,}")
        print(f"æ¨¡å‹å†…å­˜: {model_info['model_memory_mb']:.1f}MB")
        print(f"è®­ç»ƒå†…å­˜: {model_info['training_memory_mb']:.1f}MB")
        
        print(f"\nâš™ï¸  è®­ç»ƒé…ç½®:")
        print(f"æ•°æ®æ–‡ä»¶: {self.config.data.train_files}")
        print(f"æ‰¹æ¬¡å¤§å°: {self.config.data.batch_size}")
        print(f"æœ€å¤§æ­¥æ•°: {self.config.pretrain.max_steps}")
        print(f"å­¦ä¹ ç‡: {self.config.pretrain.learning_rate}")
        print(f"è®¾å¤‡: {self.config.device}")
        print("-" * 60)
    
    def _setup_tokenizer(self):
        """è®¾ç½®åˆ†è¯å™¨"""
        print("ğŸ”§ è®¾ç½®åˆ†è¯å™¨...")
        
        tokenizer_path = os.path.join(self.config.output_dir, "tokenizer.pkl")
        os.makedirs(self.config.output_dir, exist_ok=True)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è®­ç»ƒtokenizer
        retrain_tokenizer = getattr(self.config, 'retrain_tokenizer', False)
        
        if os.path.exists(tokenizer_path) and not retrain_tokenizer:
            print(f"åŠ è½½ç°æœ‰åˆ†è¯å™¨: {tokenizer_path}")
            tokenizer = BPETokenizer(vocab_size=self.config.tokenizer.vocab_size)
            tokenizer.load(tokenizer_path)
        else:
            if retrain_tokenizer:
                print("ğŸ”„ é‡æ–°è®­ç»ƒåˆ†è¯å™¨...")
            else:
                print("è®­ç»ƒæ–°çš„åˆ†è¯å™¨...")
            
            # ä»å¤šä¸ªè®­ç»ƒæ•°æ®æ–‡ä»¶æ„å»ºåˆ†è¯å™¨
            texts = []
            import json
            
            # è·å–è®­ç»ƒæ ·æœ¬æ•°é‡é™åˆ¶
            tokenizer_samples = getattr(self.config, 'tokenizer_samples', 100000)
            
            for train_file in self.config.data.train_files:
                data_path = os.path.join(self.config.data.data_dir, train_file)
                print(f"   è¯»å–æ–‡ä»¶: {train_file}")
                
                if not os.path.exists(data_path):
                    print(f"   âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {data_path}")
                    continue
                
                file_texts = []
                try:
                    with open(data_path, 'r', encoding='utf-8') as f:
                        for line_no, line in enumerate(f):
                            try:
                                data = json.loads(line.strip())
                                if 'text' in data and len(data['text'].strip()) > 10:
                                    file_texts.append(data['text'])
                                
                                # é™åˆ¶æ¯ä¸ªæ–‡ä»¶çš„æ ·æœ¬æ•°é‡
                                if len(file_texts) >= tokenizer_samples // len(self.config.data.train_files):
                                    break
                            except json.JSONDecodeError:
                                if line_no < 10:  # åªæŠ¥å‘Šå‰10ä¸ªé”™è¯¯
                                    print(f"   è·³è¿‡æ— æ•ˆè¡Œ {line_no}")
                                continue
                    
                    texts.extend(file_texts)
                    print(f"     åŠ è½½äº† {len(file_texts)} æ¡æ–‡æœ¬")
                    
                except Exception as e:
                    print(f"   âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
                    continue
            
            if not texts:
                raise RuntimeError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è®­ç»ƒæ–‡æœ¬ï¼Œæ— æ³•è®­ç»ƒåˆ†è¯å™¨")
            
            print(f"ğŸ”¤ ä» {len(texts)} æ¡æ–‡æœ¬è®­ç»ƒåˆ†è¯å™¨ (è¯æ±‡è¡¨å¤§å°: {self.config.tokenizer.vocab_size})...")
            tokenizer = BPETokenizer(vocab_size=self.config.tokenizer.vocab_size)
            tokenizer.train(texts)
            
            tokenizer.save(tokenizer_path)
            print(f"âœ… åˆ†è¯å™¨å·²ä¿å­˜: {tokenizer_path}")
        
        print(f"ğŸ“Š è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
        return tokenizer
    
    def _setup_data_loader(self, tokenizer):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        print("ğŸ“š è®¾ç½®æ•°æ®åŠ è½½å™¨...")
        
        # è·å–æ•°æ®é‡é™åˆ¶
        max_data_size = getattr(self.config, 'max_data_size', 0)
        
        # è¯»å–å¤šä¸ªè®­ç»ƒæ•°æ®æ–‡ä»¶
        all_texts = []
        import json
        
        for train_file in self.config.data.train_files:
            data_path = os.path.join(self.config.data.data_dir, train_file)
            print(f"ğŸ“– åŠ è½½æ•°æ®æ–‡ä»¶: {train_file}")
            
            if not os.path.exists(data_path):
                print(f"   âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {data_path}")
                continue
            
            file_texts = []
            try:
                with open(data_path, 'r', encoding='utf-8') as f:
                    for line_no, line in enumerate(f):
                        try:
                            data = json.loads(line.strip())
                            text = None
                            
                            # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
                            if 'text' in data:
                                text = data['text']
                            elif 'conversation' in data:
                                # å¤„ç†å¯¹è¯æ ¼å¼
                                conv_text = ""
                                for turn in data['conversation']:
                                    conv_text += f"{turn.get('human', '')} {turn.get('assistant', '')} "
                                text = conv_text.strip()
                            elif 'input' in data and 'output' in data:
                                # å¤„ç†è¾“å…¥è¾“å‡ºæ ¼å¼
                                text = f"{data['input']} {data['output']}"
                            
                            if text and len(text.strip()) > 10:
                                file_texts.append(text.strip())
                            
                            # å¦‚æœæœ‰æ•°æ®é‡é™åˆ¶ï¼Œæ£€æŸ¥æ˜¯å¦è¾¾åˆ°ä¸Šé™
                            if max_data_size > 0:
                                per_file_limit = max_data_size // len(self.config.data.train_files)
                                if len(file_texts) >= per_file_limit:
                                    break
                                    
                        except json.JSONDecodeError:
                            if line_no < 10:  # åªæŠ¥å‘Šå‰10ä¸ªé”™è¯¯
                                print(f"   è·³è¿‡æ— æ•ˆè¡Œ {line_no}")
                            continue
                
                all_texts.extend(file_texts)
                print(f"     åŠ è½½äº† {len(file_texts)} æ¡æ–‡æœ¬")
                
            except Exception as e:
                print(f"   âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
                continue
        
        if not all_texts:
            raise RuntimeError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è®­ç»ƒæ–‡æœ¬")
        
        # å¦‚æœæœ‰æ€»æ•°æ®é‡é™åˆ¶ï¼Œæˆªæ–­æ•°æ®
        if max_data_size > 0 and len(all_texts) > max_data_size:
            import random
            random.shuffle(all_texts)
            all_texts = all_texts[:max_data_size]
            print(f"ğŸ“Š é™åˆ¶æ•°æ®é‡ä¸º {max_data_size} æ¡")
        
        print(f"ğŸ“Š æ€»å…±åŠ è½½ {len(all_texts)} æ¡è®­ç»ƒæ–‡æœ¬")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = LanguageModelingDataset(
            texts=all_texts,
            tokenizer=tokenizer,
            max_length=self.config.data.max_seq_len
        )
        
        # ä¼˜åŒ–DataLoader workeræ•°é‡
        dataloader_workers = getattr(self.config, 'dataloader_workers', None)
        if dataloader_workers is None:
            device_type = self.config.device if hasattr(self.config, 'device') else 'cpu'
            dataloader_workers = get_optimal_dataloader_workers(
                batch_size=self.config.data.batch_size,
                device_type=device_type
            )
        else:
            # ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„workeræ•°é‡
            pass
        
        print(f"ğŸ“Š DataLoader workers: {dataloader_workers}")
        
        # æ£€æµ‹è®¾å¤‡ç±»å‹ä»¥ä¼˜åŒ–DataLoaderè®¾ç½®
        device_type = getattr(self.config, 'device', 'cpu')
        if device_type.startswith('cuda'):
            pin_memory = True  # CUDAè®¾å¤‡å¯ç”¨pin_memoryåŠ é€Ÿæ•°æ®ä¼ è¾“
            print("   ğŸ”§ CUDAè®¾å¤‡: å¯ç”¨pin_memoryåŠ é€Ÿæ•°æ®ä¼ è¾“")
        else:
            pin_memory = False  # CPU/MPSè®¾å¤‡é¿å…å†…å­˜é—®é¢˜
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        data_loader = DataLoader(
            dataset,
            batch_size=self.config.data.batch_size,
            shuffle=self.config.data.shuffle,
            num_workers=dataloader_workers,
            pin_memory=pin_memory,  # CUDAè®¾å¤‡å¯ç”¨ï¼Œå…¶ä»–è®¾å¤‡ç¦ç”¨
            persistent_workers=dataloader_workers > 0,  # æŒä¹…åŒ–workeræé«˜æ•ˆç‡
            prefetch_factor=2 if dataloader_workers > 0 else None,  # é¢„å–å› å­
            drop_last=True  # ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´çš„batchï¼Œé¿å…å°ºå¯¸é—®é¢˜
        )
        
        print(f"æ•°æ®æ‰¹æ¬¡æ•°: {len(data_loader)}")
        return data_loader
    
    def _setup_model(self, tokenizer):
        """è®¾ç½®æ¨¡å‹"""
        print("ğŸ§  åˆ›å»ºæ¨¡å‹...")
        
        # è®¾ç½®è®¾å¤‡
        device = torch.device(self.config.device)
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        # åˆ›å»ºæ¨¡å‹
        model = create_model(tokenizer.vocab_size, self.config.model.model_size)
        model = model.to(device)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"æ€»å‚æ•°é‡: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        
        # åº”ç”¨æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        enable_compile = getattr(self.config, 'enable_compile', False)
        if enable_compile:
            device_type = str(device).split(':')[0]  # è·å–è®¾å¤‡ç±»å‹ (cpu, cuda, mps)
            model = compile_model_if_supported(model, device_type)
        
        return model
    
    def _run_training(self, model, data_loader, tokenizer):
        """æ‰§è¡Œè®­ç»ƒå¾ªç¯"""
        print("ğŸƒ å¼€å§‹è®­ç»ƒ...")
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=self.config.pretrain.learning_rate,
            weight_decay=self.config.pretrain.weight_decay
        )
        
        # å¦‚æœæœ‰checkpointï¼Œæ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€
        if hasattr(self, 'resume_checkpoint_path') and self.resume_checkpoint_path:
            checkpoint = torch.load(self.resume_checkpoint_path, map_location='cpu', weights_only=False)
            if 'optimizer_state_dict' in checkpoint:
                print("ğŸ”§ æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€...")
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # è®¾ç½®æŸå¤±å‡½æ•°
        criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_id)
        
        # è®­ç»ƒç»Ÿè®¡
        # åˆå§‹åŒ–æ­¥æ•°è®¡æ•°å™¨ï¼Œè€ƒè™‘checkpointæ¢å¤
        self.step_count = self.start_step
        best_loss = float('inf')
        start_time = time.time()
        
        model.train()
        
        for epoch in range(1000):  # æœ€å¤§epochæ•°
            if self.should_stop:
                break
                
            epoch_loss = 0
            epoch_steps = 0
            
            for batch_idx, batch in enumerate(data_loader):
                if self.should_stop:
                    break
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æš‚åœ
                while self.pause_training and not self.should_stop:
                    time.sleep(1)
                
                if self.should_stop:
                    break
                
                try:
                    # æ•°æ®ç§»åˆ°è®¾å¤‡
                    device = next(model.parameters()).device
                    batch = batch.to(device)
                    
                    # éªŒè¯batchå°ºå¯¸
                    if batch.size(1) < 2:
                        print(f"âš ï¸  è·³è¿‡å¤ªçŸ­çš„batch: {batch.size()}")
                        continue
                    
                    # å‡†å¤‡è¾“å…¥å’Œç›®æ ‡
                    input_ids = batch[:, :-1]
                    target_ids = batch[:, 1:]
                    
                    # éªŒè¯è¾“å…¥å°ºå¯¸
                    if input_ids.size(1) == 0 or target_ids.size(1) == 0:
                        print(f"âš ï¸  è·³è¿‡ç©ºçš„è¾“å…¥æˆ–ç›®æ ‡: input={input_ids.size()}, target={target_ids.size()}")
                        continue
                    
                    # å‰å‘ä¼ æ’­
                    optimizer.zero_grad()
                    outputs = model(input_ids)
                    
                    # éªŒè¯è¾“å‡ºå°ºå¯¸
                    if outputs.size(0) == 0 or outputs.size(-1) == 0:
                        print(f"âš ï¸  è·³è¿‡æ— æ•ˆçš„æ¨¡å‹è¾“å‡º: {outputs.size()}")
                        continue
                    
                    # è®¡ç®—æŸå¤±
                    loss = criterion(outputs.reshape(-1, outputs.size(-1)), target_ids.reshape(-1))
                    
                    # æ£€æŸ¥æŸå¤±æ˜¯å¦æœ‰æ•ˆ
                    if torch.isnan(loss) or torch.isinf(loss):
                        print(f"âš ï¸  è·³è¿‡æ— æ•ˆæŸå¤±: {loss.item()}")
                        continue
                    
                    # åå‘ä¼ æ’­
                    loss.backward()
                    
                    # æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(model.parameters(), self.config.pretrain.max_grad_norm)
                    
                    # æ›´æ–°å‚æ•°
                    optimizer.step()
                    
                    # æ›´æ–°ç»Ÿè®¡
                    epoch_loss += loss.item()
                    epoch_steps += 1
                    self.step_count += 1
                    
                    # è®°å½•æŸå¤±å†å²
                    self.loss_history.append(loss.item())
                    self.step_history.append(self.step_count)
                    
                    # è®°å½•æ—¥å¿— - ä½¿ç”¨è¦†ç›–å¼æ˜¾ç¤º
                    if self.step_count % self.config.logging_steps == 0:
                        avg_loss = epoch_loss / epoch_steps
                        elapsed = time.time() - start_time
                        steps_per_sec = self.step_count / elapsed
                        progress = (self.step_count / self.config.pretrain.max_steps) * 100
                        
                        # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤º
                        suffix = f"æŸå¤± {loss.item():.4f} | å¹³å‡ {avg_loss:.4f} | {steps_per_sec:.1f} steps/s"
                        print_progress_bar(self.step_count, self.config.pretrain.max_steps, 
                                         prefix=f'ğŸƒ è®­ç»ƒä¸­ (è½®æ¬¡ {epoch})', suffix=suffix)
                    
                    # ä¿å­˜æ£€æŸ¥ç‚¹
                    if self.step_count % self.config.pretrain.save_steps == 0:
                        print(f"\nğŸ’¾ æ­¥éª¤ {self.step_count} - ä¿å­˜æ£€æŸ¥ç‚¹...")
                        self._save_checkpoint(model, tokenizer, optimizer, self.step_count, loss.item())
                        print()  # ç©ºè¡Œåç»§ç»­æ˜¾ç¤ºè¿›åº¦æ¡
                    
                    # ç®€å•éªŒè¯ï¼ˆç”Ÿæˆä¸€æ®µæ–‡æœ¬ï¼‰
                    if self.step_count % self.config.pretrain.eval_steps == 0:
                        print(f"\nğŸ§ª æ­¥éª¤ {self.step_count} - æ¨¡å‹è¯„ä¼°...")
                        self._evaluate_model(model, tokenizer)
                        print()  # ç©ºè¡Œåç»§ç»­æ˜¾ç¤ºè¿›åº¦æ¡
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ­¥æ•°
                    if self.step_count >= self.config.pretrain.max_steps:
                        print(f"è¾¾åˆ°æœ€å¤§è®­ç»ƒæ­¥æ•°: {self.config.pretrain.max_steps}")
                        self.should_stop = True
                        break
                        
                except Exception as e:
                    print(f"è®­ç»ƒæ­¥éª¤é”™è¯¯: {e}")
                    continue
            
            if epoch_steps > 0:
                avg_epoch_loss = epoch_loss / epoch_steps
                print(f"\nğŸ“ˆ è½®æ¬¡ {epoch} å®Œæˆ - å¹³å‡æŸå¤±: {avg_epoch_loss:.4f}")
                
                if avg_epoch_loss < best_loss:
                    best_loss = avg_epoch_loss
                    print(f"ğŸ‰ æ–°çš„æœ€ä½³æŸå¤±: {best_loss:.4f}")
        
        print(f"\nğŸ è®­ç»ƒå®Œæˆ!")
        print(f"æ€»æ­¥æ•°: {self.step_count}")
        print(f"æœ€ä½³æŸå¤±: {best_loss:.4f}")
        print(f"è®­ç»ƒæ—¶é—´: {(time.time() - start_time) / 60:.1f}åˆ†é’Ÿ")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self._save_checkpoint(model, tokenizer, optimizer, self.step_count, best_loss, final=True)
    
    def load_checkpoint(self, checkpoint_path):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        print(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
        
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        
        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
        
        print(f"âœ… æ£€æŸ¥ç‚¹ä¿¡æ¯:")
        print(f"   æ­¥æ•°: {checkpoint.get('step', 'unknown')}")
        
        loss_value = checkpoint.get('loss', 'unknown')
        if isinstance(loss_value, (int, float)):
            print(f"   æŸå¤±: {loss_value:.4f}")
        else:
            print(f"   æŸå¤±: {loss_value}")
        
        # æ¢å¤æŸå¤±å†å²
        if 'loss_history' in checkpoint and 'step_history' in checkpoint:
            self.loss_history = checkpoint['loss_history']
            self.step_history = checkpoint['step_history']
            print(f"   å·²æ¢å¤ {len(self.loss_history)} ä¸ªå†å²æŸå¤±è®°å½•")
            
            # ç»˜åˆ¶æ¢å¤çš„æŸå¤±æ›²çº¿
            if len(self.loss_history) > 0:
                current_step = checkpoint.get('step', 0)
                print(f"ğŸ“Š ç»˜åˆ¶æ¢å¤çš„æŸå¤±æ›²çº¿...")
                plot_loss = getattr(self.config, 'plot_loss', False)
                if plot_loss:
                    self._plot_and_save_loss_curve(current_step, recovered=True)
        else:
            print("   æœªæ‰¾åˆ°å†å²æŸå¤±è®°å½•ï¼Œä»å½“å‰æ­¥å¼€å§‹è®°å½•")
        
        # è®¾ç½®èµ·å§‹æ­¥æ•°
        self.start_step = checkpoint.get('step', 0)
        self.resume_checkpoint_path = checkpoint_path
        
        return checkpoint
    
    def find_latest_checkpoint(self, output_dir):
        """æ‰¾åˆ°æœ€æ–°çš„æ£€æŸ¥ç‚¹æ–‡ä»¶"""
        if not os.path.exists(output_dir):
            return None
        
        # æŸ¥æ‰¾æ‰€æœ‰æ£€æŸ¥ç‚¹æ–‡ä»¶
        checkpoint_files = []
        for file in os.listdir(output_dir):
            if file.startswith('checkpoint_step_') and file.endswith('.pt'):
                try:
                    step = int(file.replace('checkpoint_step_', '').replace('.pt', ''))
                    checkpoint_files.append((step, os.path.join(output_dir, file)))
                except ValueError:
                    continue
        
        # æ£€æŸ¥final_model.pt
        final_model_path = os.path.join(output_dir, 'final_model.pt')
        if os.path.exists(final_model_path):
            checkpoint_files.append((float('inf'), final_model_path))
        
        if checkpoint_files:
            # è¿”å›æœ€æ–°çš„æ£€æŸ¥ç‚¹
            latest = max(checkpoint_files, key=lambda x: x[0])
            return latest[1]
        
        return None

    def _save_checkpoint(self, model, tokenizer, optimizer, step, loss, final=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_dir = self.config.output_dir
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        if final:
            checkpoint_path = os.path.join(checkpoint_dir, "final_model.pt")
        else:
            checkpoint_path = os.path.join(checkpoint_dir, f"checkpoint_step_{step}.pt")
        
        torch.save({
            'step': step,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': loss,
            'config': self.config,
            'loss_history': self.loss_history,
            'step_history': self.step_history,
        }, checkpoint_path)
        
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
        
        # ç»˜åˆ¶å¹¶ä¿å­˜æŸå¤±æ›²çº¿ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        plot_loss = getattr(self.config, 'plot_loss', False)
        if plot_loss and len(self.loss_history) > 0:
            self._plot_and_save_loss_curve(step, final=final)
    
    def _plot_and_save_loss_curve(self, current_step, final=False):
        """ç»˜åˆ¶å¹¶ä¿å­˜æŸå¤±æ›²çº¿"""
        try:
            plt.figure(figsize=(12, 8))
            
            # ä¸»æŸå¤±æ›²çº¿
            plt.subplot(2, 1, 1)
            plt.plot(self.step_history, self.loss_history, 'b-', linewidth=1.5, alpha=0.8, label='Training Loss')
            plt.title(f'Training Loss Curve (Step {current_step})', fontsize=14, fontweight='bold')
            plt.xlabel('Training Step', fontsize=12)
            plt.ylabel('Loss Value', fontsize=12)
            plt.grid(True, alpha=0.3)
            plt.legend()
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            if len(self.loss_history) > 1:
                current_loss = self.loss_history[-1]
                min_loss = min(self.loss_history)
                max_loss = max(self.loss_history)
                avg_loss = sum(self.loss_history) / len(self.loss_history)
                
                stats_text = f'Current: {current_loss:.4f} | Min: {min_loss:.4f} | Max: {max_loss:.4f} | Avg: {avg_loss:.4f}'
                plt.figtext(0.5, 0.46, stats_text, ha='center', fontsize=10, 
                           bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"))
            
            # æœ€è¿‘1000æ­¥æŸå¤±æ›²çº¿
            plt.subplot(2, 1, 2)
            if len(self.loss_history) > 1000:
                recent_steps = self.step_history[-1000:]
                recent_losses = self.loss_history[-1000:]
                plt.plot(recent_steps, recent_losses, 'r-', linewidth=1.5, label='Recent 1000 Steps')
                plt.title('Recent 1000 Steps Loss', fontsize=12)
            else:
                plt.plot(self.step_history, self.loss_history, 'r-', linewidth=1.5, label='All Steps')
                plt.title('Loss (Detailed View)', fontsize=12)
            
            plt.xlabel('Training Step', fontsize=12)
            plt.ylabel('Loss Value', fontsize=12)
            plt.grid(True, alpha=0.3)
            plt.legend()
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾ç‰‡
            plot_dir = os.path.join(self.config.output_dir, "plots")
            os.makedirs(plot_dir, exist_ok=True)
            
            # ä¿å­˜å½“å‰æŸå¤±æ›²çº¿
            if final:
                current_plot_path = os.path.join(plot_dir, f"loss_curve_final_step_{current_step}.png")
            else:
                current_plot_path = os.path.join(plot_dir, f"loss_curve_step_{current_step}.png")
            plt.savefig(current_plot_path, dpi=300, bbox_inches='tight')
            
            # æ€»æ˜¯æ›´æ–°æœ€æ–°çš„æŸå¤±æ›²çº¿
            latest_plot_path = os.path.join(plot_dir, "loss_curve_latest.png")
            plt.savefig(latest_plot_path, dpi=300, bbox_inches='tight')
            
            plt.close()  # é‡Šæ”¾å†…å­˜
            
            if final:
                print(f"ğŸ¯ Final loss curve saved: {current_plot_path}")
            else:
                print(f"ğŸ“Š Loss curve saved: {current_plot_path}")
                
        except Exception as e:
            print(f"âš ï¸  Plot generation failed: {e}")
    
    def _evaluate_model(self, model, tokenizer):
        """ç®€å•è¯„ä¼°æ¨¡å‹ï¼ˆç”Ÿæˆæµ‹è¯•ï¼‰"""
        model.eval()
        
        try:
            # ç®€å•çš„ç”Ÿæˆæµ‹è¯•
            test_prompt = "ä½ å¥½ï¼Œ"
            print(f"\nğŸ§ª ç”Ÿæˆæµ‹è¯• - è¾“å…¥: '{test_prompt}'")
            
            # ç¼–ç è¾“å…¥
            input_ids = tokenizer.encode(test_prompt, add_special_tokens=True)
            device = next(model.parameters()).device  # è·å–æ¨¡å‹è®¾å¤‡
            input_tensor = torch.tensor([input_ids], device=device)
            
            # ç”Ÿæˆ
            with torch.no_grad():
                for _ in range(10):  # ç”Ÿæˆ10ä¸ªtoken
                    outputs = model(input_tensor)
                    next_token_logits = outputs[0, -1, :]
                    next_token = torch.argmax(next_token_logits).item()
                    input_tensor = torch.cat([input_tensor, torch.tensor([[next_token]], device=device)], dim=1)
            
            # è§£ç ç»“æœ
            generated_ids = input_tensor[0].cpu().tolist()
            generated_text = tokenizer.decode(generated_ids)
            print(f"ğŸ¤– ç”Ÿæˆç»“æœ: '{generated_text}'")
            
        except Exception as e:
            print(f"ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        finally:
            model.train()


def main():
    parser = argparse.ArgumentParser(description='Macä¼˜åŒ–è®­ç»ƒè„šæœ¬ - æ”¯æŒå…¨é‡æ•°æ®å’Œè‡ªå®šä¹‰é…ç½®')
    parser.add_argument('--config', choices=['tiny', 'small', 'medium'], default='medium',
                        help='é€‰æ‹©é…ç½® (tiny: è¶…å°æ¨¡å‹, small: å°æ¨¡å‹, medium: ä¸­æ¨¡å‹)')
    
    # æ•°æ®ç›¸å…³å‚æ•°
    parser.add_argument('--use-full-data', action='store_true',
                        help='ä½¿ç”¨å…¨é‡æ•°æ®é›†è¿›è¡Œè®­ç»ƒ')
    parser.add_argument('--data-files', nargs='+', 
                        default=['pretrain_hq.jsonl', 'sft_mini_512.jsonl'],
                        help='æŒ‡å®šè®­ç»ƒæ•°æ®æ–‡ä»¶åˆ—è¡¨')
    parser.add_argument('--max-data-size', type=int, default=500000,
                        help='æœ€å¤§æ•°æ®æ¡æ•°é™åˆ¶ (0è¡¨ç¤ºä¸é™åˆ¶)')
    
    # Tokenizerç›¸å…³å‚æ•°
    parser.add_argument('--retrain-tokenizer', action='store_true',
                        help='é‡æ–°è®­ç»ƒtokenizer')
    parser.add_argument('--tokenizer-vocab-size', type=int, default=15000,
                        help='Tokenizerè¯æ±‡è¡¨å¤§å°')
    parser.add_argument('--tokenizer-samples', type=int, default=100000,
                        help='è®­ç»ƒtokenizerä½¿ç”¨çš„æ ·æœ¬æ•°é‡')
    
    # è®­ç»ƒå‚æ•°è°ƒæ•´
    parser.add_argument('--learning-rate', type=float, default=None,
                        help='å­¦ä¹ ç‡ (ä¸æŒ‡å®šåˆ™ä½¿ç”¨é…ç½®é»˜è®¤å€¼)')
    parser.add_argument('--max-steps', type=int, default=None,
                        help='æœ€å¤§è®­ç»ƒæ­¥æ•° (ä¸æŒ‡å®šåˆ™ä½¿ç”¨é…ç½®é»˜è®¤å€¼)')
    parser.add_argument('--batch-size', type=int, default=None,
                        help='æ‰¹æ¬¡å¤§å° (ä¸æŒ‡å®šåˆ™ä½¿ç”¨é…ç½®é»˜è®¤å€¼)')
    parser.add_argument('--warmup-steps', type=int, default=None,
                        help='é¢„çƒ­æ­¥æ•° (ä¸æŒ‡å®šåˆ™ä½¿ç”¨é…ç½®é»˜è®¤å€¼)')
    
    # ç³»ç»Ÿèµ„æºå‚æ•°
    parser.add_argument('--max-cpu', type=float, default=85.0,
                        help='æœ€å¤§CPUä½¿ç”¨ç‡ (%%)')
    parser.add_argument('--max-memory', type=float, default=85.0,
                        help='æœ€å¤§å†…å­˜ä½¿ç”¨ç‡ (%%)')
    parser.add_argument('--disable-monitoring', action='store_true',
                        help='ç¦ç”¨èµ„æºç›‘æ§')
    
    # å¤šçº¿ç¨‹å’Œæ€§èƒ½ä¼˜åŒ–å‚æ•°
    parser.add_argument('--num-threads', type=int, default=None,
                        help='PyTorchçº¿ç¨‹æ•° (ä¸æŒ‡å®šåˆ™è‡ªåŠ¨ä¼˜åŒ–)')
    parser.add_argument('--dataloader-workers', type=int, default=None,
                        help='DataLoader workeræ•°é‡ (ä¸æŒ‡å®šåˆ™è‡ªåŠ¨ä¼˜åŒ–)')
    parser.add_argument('--enable-compile', action='store_true',
                        help='å¯ç”¨PyTorchæ¨¡å‹ç¼–è¯‘ä¼˜åŒ– (éœ€è¦PyTorch 2.0+)')
    parser.add_argument('--disable-optimizations', action='store_true',
                        help='ç¦ç”¨æ‰€æœ‰æ€§èƒ½ä¼˜åŒ–')
    
    # è¾“å‡ºå’Œä¿å­˜å‚æ•°
    parser.add_argument('--output-dir', type=str, default=None,
                        help='è¾“å‡ºç›®å½• (ä¸æŒ‡å®šåˆ™ä½¿ç”¨é…ç½®é»˜è®¤å€¼)')
    parser.add_argument('--save-steps', type=int, default=None,
                        help='ä¿å­˜æ£€æŸ¥ç‚¹çš„æ­¥æ•°é—´éš”')
    parser.add_argument('--plot-loss', action='store_true',
                        help='å®æ—¶ç»˜åˆ¶å’Œä¿å­˜æŸå¤±æ›²çº¿')
    
    # Checkpoint resume options
    parser.add_argument('--resume-from-checkpoint', type=str, default=None,
                        help='ä»æŒ‡å®šçš„æ£€æŸ¥ç‚¹æ–‡ä»¶æ¢å¤è®­ç»ƒ (ä¾‹å¦‚: checkpoints/mac_medium/checkpoint_step_4000.pt)')
    parser.add_argument('--auto-resume', action='store_true',
                        help='è‡ªåŠ¨ä»æœ€æ–°çš„æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ')
    
    args = parser.parse_args()
    
    # è·å–åŸºç¡€é…ç½®
    if args.config == 'tiny':
        config = get_mac_tiny_config()
        print("ä½¿ç”¨è¶…å°æ¨¡å‹é…ç½® (æœ€å¿«éªŒè¯)")
    elif args.config == 'small':
        config = get_mac_small_config()
        print("ä½¿ç”¨å°æ¨¡å‹é…ç½® (å¹³è¡¡æ€§èƒ½)")
    else: # medium
        config = get_mac_medium_config()
        print("ä½¿ç”¨ä¸­æ¨¡å‹é…ç½® (æ€§èƒ½ä¸èµ„æºå¹³è¡¡)")
    
    # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
    print("\nğŸ“ åº”ç”¨è‡ªå®šä¹‰é…ç½®...")
    
    # æ•°æ®é…ç½®
    if args.use_full_data:
        print("âœ… å¯ç”¨å…¨é‡æ•°æ®è®­ç»ƒ")
        config.data.train_files = [
            "pretrain_hq.jsonl",
            "sft_1024.jsonl", 
            "sft_512.jsonl",
            "r1_mix_1024.jsonl"
        ]
        args.max_data_size = 0  # ä¸é™åˆ¶æ•°æ®é‡
    else:
        config.data.train_files = args.data_files
        print(f"ä½¿ç”¨æŒ‡å®šæ•°æ®æ–‡ä»¶: {args.data_files}")
    
    # Tokenizeré…ç½®
    if args.retrain_tokenizer:
        print("âœ… å¯ç”¨é‡æ–°è®­ç»ƒtokenizer")
        config.tokenizer.vocab_size = args.tokenizer_vocab_size
        print(f"Tokenizerè¯æ±‡è¡¨å¤§å°: {args.tokenizer_vocab_size}")
    
    # è®­ç»ƒå‚æ•°è¦†ç›–
    # è®¾å¤‡æ£€æµ‹å’Œç‰¹æ®Šä¼˜åŒ–é…ç½®
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print("ğŸ”§ æ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œåº”ç”¨CUDAç‰¹å®šä¼˜åŒ–...")
        # CUDAè®¾å¤‡ä¼˜åŒ–é…ç½®
        if args.batch_size is None and hasattr(config, 'pretrain') and hasattr(config.pretrain, 'batch_size'):
            original_batch = config.pretrain.batch_size
            # CUDAè®¾å¤‡å¯ä»¥ä½¿ç”¨æ›´å¤§çš„batch_size
            config.pretrain.batch_size = min(original_batch * 2, 16)
            print(f"   ğŸ“Š CUDAä¼˜åŒ–: batch_sizeä» {original_batch} è°ƒæ•´ä¸º {config.pretrain.batch_size}")
        
        # CUDAè®¾å¤‡å¯ä»¥ä½¿ç”¨æ›´é«˜çš„å­¦ä¹ ç‡
        if args.learning_rate is None and hasattr(config, 'pretrain') and hasattr(config.pretrain, 'learning_rate'):
            original_lr = config.pretrain.learning_rate
            # CUDAè®¾å¤‡å¯ä»¥ä½¿ç”¨ç¨é«˜çš„å­¦ä¹ ç‡
            config.pretrain.learning_rate = max(original_lr, 1e-4)
            print(f"   ğŸ“Š CUDAä¼˜åŒ–: learning_rateä» {original_lr} è°ƒæ•´ä¸º {config.pretrain.learning_rate}")
            
    elif torch.backends.mps.is_available():
        device = torch.device("mps")
        print("ğŸ”§ æ£€æµ‹åˆ°MPSè®¾å¤‡ï¼Œåº”ç”¨MPSç‰¹å®šä¼˜åŒ–...")
        # MPSè®¾å¤‡ä¼˜åŒ–é…ç½®ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        if args.batch_size is None and hasattr(config, 'pretrain') and hasattr(config.pretrain, 'batch_size'):
            original_batch = config.pretrain.batch_size
            # MPSè®¾å¤‡æ¨èä½¿ç”¨è¾ƒå°çš„batch_sizeä»¥é¿å…å†…å­˜é—®é¢˜
            config.pretrain.batch_size = min(original_batch, 4)
            print(f"   ğŸ“Š MPSä¼˜åŒ–: batch_sizeä» {original_batch} è°ƒæ•´ä¸º {config.pretrain.batch_size}")
        
        # ä¸ºMPSè®¾å¤‡è°ƒæ•´å­¦ä¹ ç‡
        if args.learning_rate is None and hasattr(config, 'pretrain') and hasattr(config.pretrain, 'learning_rate'):
            # MPSè®¾å¤‡å¯èƒ½éœ€è¦ç¨ä½çš„å­¦ä¹ ç‡ä»¥ä¿æŒç¨³å®šæ€§
            original_lr = config.pretrain.learning_rate
            config.pretrain.learning_rate = min(original_lr, 5e-5)
            print(f"   ğŸ“Š MPSä¼˜åŒ–: learning_rateä» {original_lr} è°ƒæ•´ä¸º {config.pretrain.learning_rate}")
    else:
        device = torch.device("cpu")
        print("ğŸ”§ ä½¿ç”¨CPUè®¾å¤‡ï¼Œåº”ç”¨CPUç‰¹å®šä¼˜åŒ–...")
        # CPUè®¾å¤‡ä¼˜åŒ–é…ç½®
        if args.batch_size is None and hasattr(config, 'pretrain') and hasattr(config.pretrain, 'batch_size'):
            original_batch = config.pretrain.batch_size
            # CPUè®¾å¤‡ä½¿ç”¨æ›´å°çš„batch_sizeä»¥é¿å…å†…å­˜å‹åŠ›
            config.pretrain.batch_size = min(original_batch, 2)
            print(f"   ğŸ“Š CPUä¼˜åŒ–: batch_sizeä» {original_batch} è°ƒæ•´ä¸º {config.pretrain.batch_size}")

    if args.learning_rate is not None:
        config.pretrain.learning_rate = args.learning_rate
        print(f"å­¦ä¹ ç‡: {args.learning_rate}")
    
    if args.max_steps is not None:
        config.pretrain.max_steps = args.max_steps
        print(f"æœ€å¤§è®­ç»ƒæ­¥æ•°: {args.max_steps}")
    
    if args.batch_size is not None:
        config.data.batch_size = args.batch_size
        print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    
    if args.warmup_steps is not None:
        config.pretrain.warmup_steps = args.warmup_steps
        print(f"é¢„çƒ­æ­¥æ•°: {args.warmup_steps}")
    
    if args.output_dir is not None:
        config.output_dir = args.output_dir
        print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    
    if args.save_steps is not None:
        config.pretrain.save_steps = args.save_steps
        print(f"ä¿å­˜é—´éš”: {args.save_steps}")
    
    # ä¸ºé…ç½®æ·»åŠ é¢å¤–å±æ€§
    config.retrain_tokenizer = args.retrain_tokenizer
    config.tokenizer_samples = args.tokenizer_samples
    config.max_data_size = args.max_data_size
    config.plot_loss = args.plot_loss
    config.enable_compile = args.enable_compile
    config.num_threads = args.num_threads
    config.dataloader_workers = args.dataloader_workers
    
    # åº”ç”¨PyTorchæ€§èƒ½ä¼˜åŒ–
    print("\nğŸš€ åº”ç”¨æ€§èƒ½ä¼˜åŒ–...")
    enable_optimizations = not args.disable_optimizations
    optimized_threads = optimize_pytorch_performance(
        num_threads=args.num_threads,
        enable_optimizations=enable_optimizations
    )
    
    # èµ„æºç›‘æ§é…ç½®
    resource_config = MacResourceConfig(
        max_cpu_percent=args.max_cpu,
        max_memory_percent=args.max_memory,
        enable_monitoring=not args.disable_monitoring
    )
    
    print("\n" + "="*60)
    print("ğŸš€ å¼€å§‹è®­ç»ƒ")
    print("="*60)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = OptimizedTrainer(config, resource_config)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(
        resume_checkpoint_path=args.resume_from_checkpoint,
        auto_resume=args.auto_resume
    )


if __name__ == "__main__":
    main() 