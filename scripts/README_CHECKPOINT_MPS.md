# Checkpointæ¢å¤ä¸å¤šè®¾å¤‡ä¼˜åŒ–åŠŸèƒ½

æœ¬æ–‡æ¡£ä»‹ç» `train_optimized.py` è„šæœ¬æ–°å¢çš„checkpointæ¢å¤åŠŸèƒ½å’Œå¤šè®¾å¤‡ä¼˜åŒ–åŠŸèƒ½ï¼ˆæ”¯æŒCUDAã€MPSã€CPUï¼‰ã€‚

## Checkpointæ¢å¤åŠŸèƒ½

### åŠŸèƒ½ç‰¹æ€§

1. **æ‰‹åŠ¨æŒ‡å®šcheckpointæ¢å¤**
   ```bash
   python scripts/train_optimized.py --resume-from-checkpoint checkpoints/mac_medium/checkpoint_step_4000.pt
   ```

2. **è‡ªåŠ¨æ¢å¤æœ€æ–°checkpoint**
   ```bash
   python scripts/train_optimized.py --auto-resume
   ```

3. **æ™ºèƒ½checkpointæŸ¥æ‰¾**
   - è‡ªåŠ¨æ‰«æè¾“å‡ºç›®å½•ä¸­çš„æ‰€æœ‰checkpointæ–‡ä»¶
   - æŒ‰æ­¥æ•°æ’åºï¼Œé€‰æ‹©æœ€æ–°çš„checkpoint
   - æ”¯æŒ`final_model.pt`å’Œ`checkpoint_step_*.pt`æ–‡ä»¶

### æ¢å¤çš„çŠ¶æ€ä¿¡æ¯

- âœ… æ¨¡å‹æƒé‡çŠ¶æ€ (`model_state_dict`)
- âœ… ä¼˜åŒ–å™¨çŠ¶æ€ (`optimizer_state_dict`)
- âœ… è®­ç»ƒæ­¥æ•° (`step`)
- âœ… æŸå¤±å†å² (`loss_history`)
- âœ… æ­¥æ•°å†å² (`step_history`)
- âœ… è®­ç»ƒé…ç½® (`config`)

### ä½¿ç”¨ç¤ºä¾‹

#### 1. ä»ç‰¹å®šcheckpointæ¢å¤
```bash
python scripts/train_optimized.py \
    --config medium \
    --resume-from-checkpoint checkpoints/mac_medium/checkpoint_step_4000.pt \
    --max-steps 8000
```

#### 2. è‡ªåŠ¨æ¢å¤æœ€æ–°checkpoint
```bash
python scripts/train_optimized.py \
    --config medium \
    --output-dir checkpoints/mac_medium \
    --auto-resume \
    --max-steps 8000
```

## å¤šè®¾å¤‡ä¼˜åŒ–æ”¯æŒ

### è®¾å¤‡æ£€æµ‹ä¼˜å…ˆçº§

è„šæœ¬ä¼šæŒ‰ä¼˜å…ˆçº§è‡ªåŠ¨æ£€æµ‹å’Œé€‰æ‹©æœ€ä½³è®¾å¤‡ï¼š

1. **CUDAè®¾å¤‡** (è‹±ä¼Ÿè¾¾GPU) - æœ€é«˜ä¼˜å…ˆçº§
2. **MPSè®¾å¤‡** (Mac Apple Silicon GPU)  
3. **CPUè®¾å¤‡** - æœ€ä½ä¼˜å…ˆçº§

```python
if torch.cuda.is_available():
    device = torch.device("cuda")
elif torch.backends.mps.is_available():
    device = torch.device("mps")
else:
    device = torch.device("cpu")
```

## CUDAè®¾å¤‡ä¼˜åŒ–

### CUDAè®¾å¤‡æ£€æµ‹å’Œé…ç½®

è„šæœ¬ä¼šè‡ªåŠ¨æ£€æµ‹è‹±ä¼Ÿè¾¾GPUçš„CUDAæ”¯æŒå¹¶åº”ç”¨ä¸“é—¨ä¼˜åŒ–ï¼š

```python
if torch.cuda.is_available():
    device_count = torch.cuda.device_count()
    device_name = torch.cuda.get_device_name()
    print(f"æ£€æµ‹åˆ°CUDAè®¾å¤‡: {device_name}")
```

### CUDAç‰¹å®šä¼˜åŒ–

#### 1. cuDNNä¼˜åŒ–
```python
torch.backends.cudnn.enabled = True
torch.backends.cudnn.benchmark = True  # è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜ç®—æ³•
torch.backends.cudnn.deterministic = False  # å…è®¸éç¡®å®šæ€§æé«˜æ€§èƒ½
```

#### 2. CUDAå†…å­˜ç®¡ç†
```python
torch.cuda.empty_cache()  # æ¸…ç©ºç¼“å­˜
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
```

#### 3. æ‰¹é‡å¤§å°å’Œå­¦ä¹ ç‡ä¼˜åŒ–
```python
# CUDAè®¾å¤‡å¯ä»¥ä½¿ç”¨æ›´å¤§çš„batch_size
config.pretrain.batch_size = min(original_batch * 2, 16)
# CUDAè®¾å¤‡å¯ä»¥ä½¿ç”¨æ›´é«˜çš„å­¦ä¹ ç‡
config.pretrain.learning_rate = max(original_lr, 1e-4)
```

#### 4. DataLoaderä¼˜åŒ–
```python
# CUDAè®¾å¤‡ä½¿ç”¨æ›´å¤šworkerå’Œpin_memory
optimal_workers = min(8, cpu_count)
pin_memory = True  # åŠ é€ŸGPU-CPUæ•°æ®ä¼ è¾“
```

#### 5. æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–
```python
# CUDAè®¾å¤‡ä½¿ç”¨æœ€æ¿€è¿›çš„ç¼–è¯‘æ¨¡å¼
compiled_model = torch.compile(model, mode="max-autotune", dynamic=True)
```

### CUDAä¼˜åŒ–å¯åŠ¨è„šæœ¬
```bash
./scripts/start_cuda_optimized_training.sh
```

è¯¥è„šæœ¬åŒ…å«ä»¥ä¸‹CUDAä¼˜åŒ–é…ç½®ï¼š
- `--batch-size 8` - é€‚åˆCUDAè®¾å¤‡çš„è¾ƒå¤§æ‰¹é‡å¤§å°
- `--learning-rate 1e-4` - ç¨³å®šçš„å­¦ä¹ ç‡
- `--dataloader-workers 8` - å……åˆ†åˆ©ç”¨CPU-GPUå¹¶è¡Œæ€§
- `--num-threads 8` - å¤šçº¿ç¨‹ä¼˜åŒ–
- `--enable-compile` - å¯ç”¨CUDAæ¨¡å‹ç¼–è¯‘
- `--auto-resume` - è‡ªåŠ¨æ¢å¤checkpoint

## MPSè®¾å¤‡ä¼˜åŒ–

### MPSè®¾å¤‡æ£€æµ‹

è„šæœ¬ä¼šè‡ªåŠ¨æ£€æµ‹Macè®¾å¤‡çš„MPSï¼ˆMetal Performance Shadersï¼‰æ”¯æŒï¼š

### MPSç‰¹å®šä¼˜åŒ–

#### 1. å†…å­˜ç®¡ç†ä¼˜åŒ–
```python
# å‡å°‘å†…å­˜ä½¿ç”¨
os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
# å¯ç”¨CPUå›é€€
os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
# é™åˆ¶GPUå†…å­˜ä½¿ç”¨
torch.mps.set_per_process_memory_fraction(0.8)
```

#### 2. æ‰¹é‡å¤§å°è‡ªåŠ¨è°ƒæ•´
```python
# MPSè®¾å¤‡æ¨èä½¿ç”¨è¾ƒå°çš„batch_size
config.pretrain.batch_size = min(original_batch, 4)
```

#### 3. å­¦ä¹ ç‡è‡ªåŠ¨ä¼˜åŒ–
```python
# MPSè®¾å¤‡ä½¿ç”¨ç¨ä½çš„å­¦ä¹ ç‡ä¿æŒç¨³å®šæ€§
config.pretrain.learning_rate = min(original_lr, 5e-5)
```

#### 4. DataLoaderä¼˜åŒ–
```python
# MPSè®¾å¤‡ä½¿ç”¨æ›´å°‘çš„workeré¿å…ç“¶é¢ˆ
recommended_workers = min(recommended_workers, 1)
```

#### 5. æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–
```python
# MPSè®¾å¤‡ä½¿ç”¨ä¸“é—¨çš„ç¼–è¯‘æ¨¡å¼
compiled_model = torch.compile(model, mode="reduce-overhead", dynamic=False)
```

### MPSä¼˜åŒ–å¯åŠ¨è„šæœ¬

ä¸ºäº†æ–¹ä¾¿ä½¿ç”¨ï¼Œæä¾›äº†ä¸“é—¨çš„MPSä¼˜åŒ–å¯åŠ¨è„šæœ¬ï¼š

```bash
./scripts/start_mps_optimized_training.sh
```

è¯¥è„šæœ¬åŒ…å«ä»¥ä¸‹ä¼˜åŒ–é…ç½®ï¼š
- `--batch-size 2` - é€‚åˆMPSè®¾å¤‡çš„æ‰¹é‡å¤§å°
- `--learning-rate 3e-5` - ç¨³å®šçš„å­¦ä¹ ç‡
- `--dataloader-workers 1` - å‡å°‘workeræ•°é‡
- `--num-threads 4` - é€‚åˆApple Siliconçš„çº¿ç¨‹æ•°
- `--enable-compile` - å¯ç”¨MPSæ¨¡å‹ç¼–è¯‘
- `--auto-resume` - è‡ªåŠ¨æ¢å¤checkpoint

## æ€§èƒ½å¯¹æ¯”

### å¤šè®¾å¤‡è®­ç»ƒæ€§èƒ½å¯¹æ¯”

| è®¾å¤‡ç±»å‹ | æ‰¹é‡å¤§å° | æ­¥æ•°/ç§’ | å†…å­˜ä½¿ç”¨ | DataLoader Workers | æ¨èç”¨é€” |
|---------|---------|---------|---------|-------------------|---------|
| CUDA    | 8-16    | ~8.0    | 4-24GB  | 8               | é«˜æ€§èƒ½è®­ç»ƒ |
| MPS     | 2-4     | ~2.5    | 8-12GB  | 1               | æ—¥å¸¸è®­ç»ƒ |
| CPU     | 1-2     | ~1.0    | 4-8GB   | 2-4             | è½»é‡è®­ç»ƒ |

### è®¾å¤‡ç‰¹å®šä¼˜åŒ–å»ºè®®

1. **CUDAè®¾å¤‡**
   - ä½¿ç”¨è¾ƒå¤§çš„batch_size (8-16)
   - å¯ç”¨æœ€æ¿€è¿›çš„æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–
   - ä½¿ç”¨æ›´å¤šDataLoader worker (8)
   - å¯ç”¨pin_memoryåŠ é€Ÿæ•°æ®ä¼ è¾“
   - ç›‘æ§GPUå†…å­˜å’Œæ˜¾å­˜ä½¿ç”¨æƒ…å†µ

2. **MPSè®¾å¤‡**
   - ä½¿ç”¨è¾ƒå°çš„batch_size (2-4)
   - å¯ç”¨æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–
   - ä½¿ç”¨1ä¸ªDataLoader worker
   - ç›‘æ§GPUå†…å­˜ä½¿ç”¨æƒ…å†µ

3. **CPUè®¾å¤‡**
   - ä½¿ç”¨æœ€å°çš„batch_size (1-2)
   - å¯ä»¥ä½¿ç”¨æ›´å¤šçš„worker (2-4)
   - å…³æ³¨CPUå’Œå†…å­˜ä½¿ç”¨ç‡

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **CUDAç›¸å…³é—®é¢˜**
   
   **CUDAä¸å¯ç”¨**
   ```
   CUDA available: False
   ```
   è§£å†³æ–¹æ¡ˆï¼š
   - æ£€æŸ¥æ˜¯å¦å®‰è£…äº†æ”¯æŒCUDAçš„PyTorchç‰ˆæœ¬
   - éªŒè¯NVIDIA GPUé©±åŠ¨ç¨‹åºæ˜¯å¦æ­£ç¡®å®‰è£…
   - ä½¿ç”¨ `nvidia-smi` æ£€æŸ¥GPUçŠ¶æ€

   **CUDAå†…å­˜ä¸è¶³**
   ```
   RuntimeError: CUDA out of memory
   ```
   è§£å†³æ–¹æ¡ˆï¼š
   - å‡å°‘batch_size (ä¾‹å¦‚ä»8é™åˆ°4æˆ–2)
   - ä½¿ç”¨ `torch.cuda.empty_cache()` æ¸…ç†ç¼“å­˜
   - å‡å°‘æ¨¡å‹å¤§å°æˆ–åºåˆ—é•¿åº¦

   **CUDAç¼–è¯‘å¤±è´¥**
   ```
   âš ï¸  CUDAæ¨¡å‹ç¼–è¯‘å¤±è´¥ï¼Œå°è¯• reduce-overhead æ¨¡å¼
   ```
   è§£å†³æ–¹æ¡ˆï¼šè„šæœ¬ä¼šè‡ªåŠ¨é™çº§ç¼–è¯‘æ¨¡å¼æˆ–ä½¿ç”¨åŸå§‹æ¨¡å‹ã€‚

2. **MPSç›¸å…³é—®é¢˜**

   **MPSç¼–è¯‘å¤±è´¥**
   ```
   âš ï¸  MPSæ¨¡å‹ç¼–è¯‘å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹: xxx
   ```
   è§£å†³æ–¹æ¡ˆï¼šè„šæœ¬ä¼šè‡ªåŠ¨å›é€€åˆ°éç¼–è¯‘æ¨¡å¼ï¼Œè®­ç»ƒå¯ä»¥ç»§ç»­è¿›è¡Œã€‚

   **MPSå†…å­˜ä¸è¶³**
   ```
   RuntimeError: MPS backend out of memory
   ```
   è§£å†³æ–¹æ¡ˆï¼šå‡å°‘batch_sizeæˆ–é‡å¯Pythonè¿›ç¨‹é‡Šæ”¾MPSå†…å­˜ã€‚

3. **é€šç”¨é—®é¢˜**

   **CheckpointåŠ è½½å¤±è´¥**
   ```
   æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: xxx.pt
   ```
   è§£å†³æ–¹æ¡ˆï¼šæ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œæˆ–ä½¿ç”¨`--auto-resume`è‡ªåŠ¨æŸ¥æ‰¾ã€‚

### è°ƒè¯•å‘½ä»¤

æŸ¥çœ‹CUDAè®¾å¤‡çŠ¶æ€ï¼š
```python
import torch
print("CUDA available:", torch.cuda.is_available())
print("CUDA version:", torch.version.cuda)
print("GPU count:", torch.cuda.device_count())
if torch.cuda.is_available():
    print("GPU name:", torch.cuda.get_device_name(0))
    print("GPU memory:", torch.cuda.get_device_properties(0).total_memory / 1024**3, "GB")
```

æŸ¥çœ‹MPSè®¾å¤‡çŠ¶æ€ï¼š
```python
import torch
print("MPS available:", torch.backends.mps.is_available())
print("MPS built:", torch.backends.mps.is_built())
```

æŸ¥çœ‹è®¾å¤‡é€‰æ‹©é€»è¾‘ï¼š
```python
import torch
if torch.cuda.is_available():
    device = torch.device("cuda")
    print("é€‰æ‹©CUDAè®¾å¤‡")
elif torch.backends.mps.is_available():
    device = torch.device("mps")
    print("é€‰æ‹©MPSè®¾å¤‡")
else:
    device = torch.device("cpu")
    print("é€‰æ‹©CPUè®¾å¤‡")
print(f"å½“å‰è®¾å¤‡: {device}")
```

æŸ¥çœ‹å¯ç”¨çš„checkpointï¼š
```bash
ls -la checkpoints/mac_medium*/checkpoint_step_*.pt
```

## æ€»ç»“

æ–°çš„checkpointæ¢å¤å’ŒMPSä¼˜åŒ–åŠŸèƒ½å¤§å¤§æå‡äº†è®­ç»ƒçš„ä¾¿åˆ©æ€§å’Œæ•ˆç‡ï¼š

- ğŸ”„ **æ— ç¼æ¢å¤è®­ç»ƒ** - æ”¯æŒæ‰‹åŠ¨å’Œè‡ªåŠ¨checkpointæ¢å¤
- ğŸš€ **MPSè®¾å¤‡ä¼˜åŒ–** - ä¸“é—¨é’ˆå¯¹Apple Siliconè®¾å¤‡ä¼˜åŒ–
- ğŸ“Š **æ™ºèƒ½å‚æ•°è°ƒæ•´** - è‡ªåŠ¨ä¼˜åŒ–batch_sizeå’Œå­¦ä¹ ç‡
- ğŸ›¡ï¸ **ç¨³å®šæ€§ä¿éšœ** - å¼‚å¸¸å¤„ç†å’Œå›é€€æœºåˆ¶
- ğŸ“ˆ **æ€§èƒ½æå‡** - æ¨¡å‹ç¼–è¯‘å’Œå†…å­˜ç®¡ç†ä¼˜åŒ–

è¿™äº›åŠŸèƒ½è®©åœ¨Macè®¾å¤‡ä¸Šè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹å˜å¾—æ›´åŠ é«˜æ•ˆå’Œå¯é ã€‚