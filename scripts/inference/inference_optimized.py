#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆæ¨ç†è„šæœ¬
å±•ç¤ºæ‰€æœ‰æ–°èƒ½åŠ›ï¼šå·¥å…·è°ƒç”¨ã€ultra thinkã€é«˜æ•ˆæ¨ç†ç­‰
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

import torch
import torch.nn.functional as F
import json
import argparse
import time
from typing import List, Dict, Optional

from src.model.config import MiniGPTConfig
from src.model.transformer import MiniGPT


class OptimizedInference:
    """ä¼˜åŒ–çš„æ¨ç†å¼•æ“"""

    def __init__(self, model_path: str, device: str = 'auto'):
        """
        åˆå§‹åŒ–æ¨ç†å¼•æ“

        Args:
            model_path: æ¨¡å‹è·¯å¾„
            device: è®¾å¤‡ç±»å‹
        """
        self.device = self._setup_device(device)
        self.model, self.config, self.tokenizer = self._load_model(model_path)

        print(f"Loaded model with {sum(p.numel() for p in self.model.parameters()):,} parameters")
        print(f"Using device: {self.device}")
        print(f"Model config: {self.config}")

    def _setup_device(self, device: str) -> torch.device:
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if device == 'auto':
            if torch.cuda.is_available():
                return torch.device('cuda')
            elif torch.backends.mps.is_available():
                return torch.device('mps')
            else:
                return torch.device('cpu')
        else:
            return torch.device(device)

    def _load_model(self, model_path: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(model_path, map_location=self.device)

        # åŠ è½½é…ç½®
        if 'config' in checkpoint:
            config_dict = checkpoint['config']
            config = MiniGPTConfig.from_dict(config_dict)
        else:
            # é»˜è®¤é…ç½®
            from src.model.config import get_small_config
            config = get_small_config()

        # åˆ›å»ºæ¨¡å‹
        model = MiniGPT(config)
        model.load_state_dict(checkpoint['model_state_dict'])
        model = model.to(self.device)
        model.eval()

        # åŠ è½½tokenizer
        tokenizer = checkpoint.get('vocab', None)
        if tokenizer is None:
            # åˆ›å»ºé»˜è®¤tokenizer
            tokenizer = self._create_default_tokenizer()

        return model, config, tokenizer

    def _create_default_tokenizer(self):
        """åˆ›å»ºé»˜è®¤tokenizer"""
        chars = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 .,!?;:()[]{}"\'-\nä½ æˆ‘ä»–çš„æ˜¯åœ¨æœ‰ä¸€ä¸ªè¿™é‚£äº†å’Œä¸º'
        vocab = ['<pad>', '<unk>', '<bos>', '<eos>', '<|tool_call|>', '<|ultra_think|>'] + list(chars)

        return {
            'vocab': vocab,
            'char_to_id': {char: i for i, char in enumerate(vocab)},
            'id_to_char': {i: char for char, i in enumerate(vocab)}
        }

    def tokenize(self, text: str) -> List[int]:
        """æ–‡æœ¬åˆ†è¯"""
        tokens = [self.tokenizer['char_to_id'].get('<bos>', 2)]

        for char in text:
            tokens.append(self.tokenizer['char_to_id'].get(char, 1))  # UNK=1

        return tokens

    def detokenize(self, tokens: List[int]) -> str:
        """tokenè§£ç """
        text = ""
        for token in tokens:
            if token in self.tokenizer['id_to_char']:
                char = self.tokenizer['id_to_char'][token]
                if char not in ['<pad>', '<bos>', '<eos>']:
                    text += char
        return text

    def generate(self,
                prompt: str,
                max_length: int = 100,
                temperature: float = 0.8,
                top_k: int = 50,
                top_p: float = 0.9,
                do_sample: bool = True) -> str:
        """æ–‡æœ¬ç”Ÿæˆ"""

        # åˆ†è¯
        input_tokens = self.tokenize(prompt)
        input_ids = torch.tensor([input_tokens], dtype=torch.long, device=self.device)

        print(f"Prompt tokens: {len(input_tokens)}")

        generated_tokens = input_tokens.copy()

        with torch.no_grad():
            for _ in range(max_length):
                # é™åˆ¶è¾“å…¥é•¿åº¦
                if input_ids.size(1) > self.config.max_position_embeddings - 1:
                    input_ids = input_ids[:, -self.config.max_position_embeddings + 1:]

                # å‰å‘ä¼ æ’­
                logits = self.model(input_ids)
                next_token_logits = logits[0, -1, :] / temperature

                # åº”ç”¨top-kè¿‡æ»¤
                if top_k > 0:
                    top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)
                    next_token_logits = torch.full_like(next_token_logits, -float('inf'))
                    next_token_logits.scatter_(0, top_k_indices, top_k_logits)

                # åº”ç”¨top-pè¿‡æ»¤
                if top_p < 1.0:
                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

                    sorted_indices_to_remove = cumulative_probs > top_p
                    sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
                    sorted_indices_to_remove[0] = 0

                    indices_to_remove = sorted_indices[sorted_indices_to_remove]
                    next_token_logits[indices_to_remove] = -float('inf')

                # é‡‡æ ·
                if do_sample:
                    probs = F.softmax(next_token_logits, dim=-1)
                    next_token = torch.multinomial(probs, num_samples=1)
                else:
                    next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)

                # æ£€æŸ¥ç»“æŸæ¡ä»¶
                if next_token.item() == self.tokenizer['char_to_id'].get('<eos>', 3):
                    break

                # æ·»åŠ åˆ°åºåˆ—
                generated_tokens.append(next_token.item())
                input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)

        # è§£ç 
        generated_text = self.detokenize(generated_tokens)
        return generated_text

    def tool_calling_inference(self, query: str) -> Dict:
        """å·¥å…·è°ƒç”¨æ¨ç†"""
        print(f"\nğŸ”§ Tool Calling Inference: {query}")

        # æ„é€ å·¥å…·è°ƒç”¨prompt
        tool_prompt = f"ç”¨æˆ·: {query}\nåŠ©æ‰‹: æˆ‘æ¥å¸®æ‚¨"

        # ç”Ÿæˆå“åº”
        start_time = time.time()
        response = self.generate(
            tool_prompt,
            max_length=200,
            temperature=0.7,
            top_k=30
        )
        inference_time = time.time() - start_time

        # è§£æå·¥å…·è°ƒç”¨ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        tools_detected = []
        if "æŸ¥è¯¢" in query or "æœç´¢" in query:
            tools_detected.append("web_search")
        if "è®¡ç®—" in query or "ç®—" in query:
            tools_detected.append("calculator")
        if "å¤©æ°”" in query:
            tools_detected.append("weather_api")
        if "ç¿»è¯‘" in query:
            tools_detected.append("translator")

        return {
            "query": query,
            "response": response,
            "tools_detected": tools_detected,
            "inference_time": inference_time
        }

    def ultra_think_inference(self, problem: str) -> Dict:
        """Ultra Thinkæ·±åº¦æ¨ç†"""
        print(f"\nğŸ§  Ultra Think Inference: {problem}")

        # æ„é€ ultra think prompt
        ultra_prompt = f"""ä½œä¸ºalex-ckl.comå¼€å‘çš„AIåŠ©æ‰‹ï¼Œè¯·å±•ç¤ºultra thinkèƒ½åŠ›åˆ†æï¼š{problem}

<ultra_think>
è®©æˆ‘æ·±åº¦åˆ†æè¿™ä¸ªé—®é¢˜ï¼š
"""

        start_time = time.time()
        response = self.generate(
            ultra_prompt,
            max_length=300,
            temperature=0.8,
            top_k=40
        )
        inference_time = time.time() - start_time

        # åˆ†ææ¨ç†è´¨é‡ï¼ˆç®€åŒ–æŒ‡æ ‡ï¼‰
        thinking_markers = ["åˆ†æ", "è€ƒè™‘", "å› ä¸º", "æ‰€ä»¥", "é¦–å…ˆ", "å…¶æ¬¡", "æœ€å"]
        thinking_score = sum(1 for marker in thinking_markers if marker in response)

        return {
            "problem": problem,
            "response": response,
            "thinking_score": thinking_score,
            "inference_time": inference_time
        }

    def benchmark_performance(self, num_samples: int = 10) -> Dict:
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print(f"\nğŸ“Š Performance Benchmark ({num_samples} samples)")

        test_prompts = [
            "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
            "è¯·å¸®æˆ‘è®¡ç®— 123 + 456",
            "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
            "How are you today?",
            "è§£é‡Šä¸€ä¸‹æœºå™¨å­¦ä¹ çš„åŸç†",
        ]

        times = []
        tokens_per_second = []

        for i in range(num_samples):
            prompt = test_prompts[i % len(test_prompts)]

            start_time = time.time()
            response = self.generate(prompt, max_length=50, temperature=0.7)
            end_time = time.time()

            inference_time = end_time - start_time
            response_tokens = len(self.tokenize(response))
            tps = response_tokens / inference_time if inference_time > 0 else 0

            times.append(inference_time)
            tokens_per_second.append(tps)

            print(f"  Sample {i+1}: {inference_time:.3f}s, {tps:.1f} tokens/s")

        return {
            "num_samples": num_samples,
            "avg_time": sum(times) / len(times),
            "avg_tokens_per_second": sum(tokens_per_second) / len(tokens_per_second),
            "min_time": min(times),
            "max_time": max(times)
        }


def interactive_mode(inference_engine: OptimizedInference):
    """äº¤äº’å¼æ¨¡å¼"""
    print("\nğŸ¯ è¿›å…¥äº¤äº’å¼æ¨¡å¼ (è¾“å…¥ 'quit' é€€å‡º)")
    print("å‘½ä»¤:")
    print("  - normal: <text>     # æ™®é€šç”Ÿæˆ")
    print("  - tool: <query>      # å·¥å…·è°ƒç”¨")
    print("  - think: <problem>   # Ultra Think")
    print("  - benchmark          # æ€§èƒ½æµ‹è¯•")
    print("  - quit               # é€€å‡º")

    while True:
        try:
            user_input = input("\n> ").strip()

            if user_input.lower() == 'quit':
                break
            elif user_input.lower() == 'benchmark':
                result = inference_engine.benchmark_performance()
                print(f"å¹³å‡æ¨ç†æ—¶é—´: {result['avg_time']:.3f}s")
                print(f"å¹³å‡ç”Ÿæˆé€Ÿåº¦: {result['avg_tokens_per_second']:.1f} tokens/s")
            elif user_input.startswith('tool:'):
                query = user_input[5:].strip()
                result = inference_engine.tool_calling_inference(query)
                print(f"å·¥å…·æ£€æµ‹: {result['tools_detected']}")
                print(f"å“åº”: {result['response']}")
                print(f"æ¨ç†æ—¶é—´: {result['inference_time']:.3f}s")
            elif user_input.startswith('think:'):
                problem = user_input[6:].strip()
                result = inference_engine.ultra_think_inference(problem)
                print(f"å“åº”: {result['response']}")
                print(f"æ€ç»´æ·±åº¦è¯„åˆ†: {result['thinking_score']}")
                print(f"æ¨ç†æ—¶é—´: {result['inference_time']:.3f}s")
            elif user_input.startswith('normal:'):
                text = user_input[7:].strip()
                response = inference_engine.generate(text, max_length=100)
                print(f"å“åº”: {response}")
            else:
                # é»˜è®¤æ™®é€šç”Ÿæˆ
                response = inference_engine.generate(user_input, max_length=100)
                print(f"å“åº”: {response}")

        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"é”™è¯¯: {e}")

    print("é€€å‡ºäº¤äº’å¼æ¨¡å¼")


def main():
    parser = argparse.ArgumentParser(description='ä¼˜åŒ–ç‰ˆMiniGPTæ¨ç†è„šæœ¬')
    parser.add_argument('--model-path', type=str, required=True, help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--device', type=str, default='auto', help='è®¾å¤‡ç±»å‹')
    parser.add_argument('--mode', type=str, default='interactive',
                       choices=['interactive', 'single', 'tool', 'think', 'benchmark'],
                       help='æ¨ç†æ¨¡å¼')
    parser.add_argument('--prompt', type=str, help='è¾“å…¥promptï¼ˆsingleæ¨¡å¼ï¼‰')
    parser.add_argument('--max-length', type=int, default=100, help='æœ€å¤§ç”Ÿæˆé•¿åº¦')
    parser.add_argument('--temperature', type=float, default=0.8, help='é‡‡æ ·æ¸©åº¦')

    args = parser.parse_args()

    # åˆå§‹åŒ–æ¨ç†å¼•æ“
    print("ğŸš€ Initializing Optimized Inference Engine...")
    inference_engine = OptimizedInference(args.model_path, args.device)

    if args.mode == 'interactive':
        interactive_mode(inference_engine)

    elif args.mode == 'single':
        if not args.prompt:
            print("é”™è¯¯: singleæ¨¡å¼éœ€è¦æä¾› --prompt å‚æ•°")
            return

        response = inference_engine.generate(
            args.prompt,
            max_length=args.max_length,
            temperature=args.temperature
        )
        print(f"è¾“å…¥: {args.prompt}")
        print(f"è¾“å‡º: {response}")

    elif args.mode == 'tool':
        if not args.prompt:
            print("é”™è¯¯: toolæ¨¡å¼éœ€è¦æä¾› --prompt å‚æ•°")
            return

        result = inference_engine.tool_calling_inference(args.prompt)
        print(json.dumps(result, ensure_ascii=False, indent=2))

    elif args.mode == 'think':
        if not args.prompt:
            print("é”™è¯¯: thinkæ¨¡å¼éœ€è¦æä¾› --prompt å‚æ•°")
            return

        result = inference_engine.ultra_think_inference(args.prompt)
        print(json.dumps(result, ensure_ascii=False, indent=2))

    elif args.mode == 'benchmark':
        result = inference_engine.benchmark_performance()
        print(json.dumps(result, indent=2))


if __name__ == "__main__":
    main()