#!/usr/bin/env python3
"""
æ¨¡å‹è¯„ä¼°è„šæœ¬
å…¨é¢è¯„ä¼°æ¶æ„å‡çº§åçš„æ¨¡å‹æ€§èƒ½ï¼šå›°æƒ‘åº¦ã€ç”Ÿæˆè´¨é‡ã€å·¥å…·è°ƒç”¨ã€ultra thinkç­‰
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

import torch
import torch.nn.functional as F
import json
import argparse
import time
import math
from pathlib import Path
from typing import List, Dict, Any
import numpy as np
from collections import defaultdict

from src.model.config import MiniGPTConfig
from src.model.transformer import MiniGPT


class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""

    def __init__(self, model_path: str, device: str = 'auto'):
        self.device = self._setup_device(device)
        self.model, self.config, self.tokenizer = self._load_model(model_path)
        self.model.eval()

        print(f"Loaded model with {sum(p.numel() for p in self.model.parameters()):,} parameters")
        print(f"Using device: {self.device}")

    def _setup_device(self, device: str) -> torch.device:
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if device == 'auto':
            if torch.cuda.is_available():
                return torch.device('cuda')
            elif torch.backends.mps.is_available():
                return torch.device('mps')
            else:
                return torch.device('cpu')
        else:
            return torch.device(device)

    def _load_model(self, model_path: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(model_path, map_location=self.device)

        # åŠ è½½é…ç½®
        if 'config' in checkpoint:
            config_dict = checkpoint['config']
            config = MiniGPTConfig.from_dict(config_dict)
        else:
            from src.model.config import get_small_config
            config = get_small_config()

        # åˆ›å»ºæ¨¡å‹
        model = MiniGPT(config)
        model.load_state_dict(checkpoint['model_state_dict'])
        model = model.to(self.device)

        # åŠ è½½tokenizer
        tokenizer = checkpoint.get('vocab', None)
        if tokenizer is None:
            tokenizer = self._create_default_tokenizer()

        return model, config, tokenizer

    def _create_default_tokenizer(self):
        """åˆ›å»ºé»˜è®¤tokenizer"""
        chars = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 .,!?;:()[]{}"\'-\nä½ æˆ‘ä»–çš„æ˜¯åœ¨æœ‰ä¸€ä¸ªè¿™é‚£äº†å’Œä¸º'
        vocab = ['<pad>', '<unk>', '<bos>', '<eos>', '<|tool_call|>', '<|ultra_think|>'] + list(chars)

        return {
            'vocab': vocab,
            'char_to_id': {char: i for i, char in enumerate(vocab)},
            'id_to_char': {i: char for char, i in enumerate(vocab)}
        }

    def tokenize(self, text: str) -> List[int]:
        """æ–‡æœ¬åˆ†è¯"""
        tokens = [self.tokenizer['char_to_id'].get('<bos>', 2)]
        for char in text:
            tokens.append(self.tokenizer['char_to_id'].get(char, 1))
        tokens.append(self.tokenizer['char_to_id'].get('<eos>', 3))
        return tokens

    def detokenize(self, tokens: List[int]) -> str:
        """tokenè§£ç """
        text = ""
        for token in tokens:
            if token in self.tokenizer['id_to_char']:
                char = self.tokenizer['id_to_char'][token]
                if char not in ['<pad>', '<bos>', '<eos>']:
                    text += char
        return text

    def calculate_perplexity(self, test_data: List[str]) -> float:
        """è®¡ç®—å›°æƒ‘åº¦"""
        print("Calculating perplexity...")

        total_log_prob = 0
        total_tokens = 0

        with torch.no_grad():
            for text in test_data:
                tokens = self.tokenize(text)
                if len(tokens) < 2:
                    continue

                input_ids = torch.tensor([tokens[:-1]], dtype=torch.long, device=self.device)
                targets = torch.tensor([tokens[1:]], dtype=torch.long, device=self.device)

                if input_ids.size(1) > self.config.max_position_embeddings:
                    continue

                logits = self.model(input_ids)
                log_probs = F.log_softmax(logits, dim=-1)

                # è®¡ç®—æ¯ä¸ªtokençš„logæ¦‚ç‡
                for i, target in enumerate(targets[0]):
                    if target.item() != 0:  # å¿½ç•¥padding
                        token_log_prob = log_probs[0, i, target.item()].item()
                        total_log_prob += token_log_prob
                        total_tokens += 1

        if total_tokens == 0:
            return float('inf')

        avg_log_prob = total_log_prob / total_tokens
        perplexity = math.exp(-avg_log_prob)

        print(f"Perplexity: {perplexity:.2f} (lower is better)")
        return perplexity

    def evaluate_generation_quality(self, prompts: List[str]) -> Dict[str, Any]:
        """è¯„ä¼°ç”Ÿæˆè´¨é‡"""
        print("Evaluating generation quality...")

        results = {
            'num_prompts': len(prompts),
            'avg_generation_time': 0,
            'avg_tokens_per_second': 0,
            'response_lengths': [],
            'diversity_scores': [],
            'coherence_scores': []
        }

        generation_times = []
        tokens_per_second = []

        for prompt in prompts:
            start_time = time.time()

            # ç”Ÿæˆå“åº”
            response = self._generate_text(prompt, max_length=100)

            end_time = time.time()
            generation_time = end_time - start_time

            # è®¡ç®—æŒ‡æ ‡
            response_tokens = len(self.tokenize(response))
            tps = response_tokens / generation_time if generation_time > 0 else 0

            generation_times.append(generation_time)
            tokens_per_second.append(tps)
            results['response_lengths'].append(response_tokens)

            # ç®€å•çš„å¤šæ ·æ€§è¯„åˆ†ï¼ˆä¸é‡å¤n-gramæ¯”ä¾‹ï¼‰
            diversity = self._calculate_diversity(response)
            results['diversity_scores'].append(diversity)

            # ç®€å•çš„è¿è´¯æ€§è¯„åˆ†
            coherence = self._calculate_coherence(prompt, response)
            results['coherence_scores'].append(coherence)

        # è®¡ç®—å¹³å‡å€¼
        results['avg_generation_time'] = sum(generation_times) / len(generation_times)
        results['avg_tokens_per_second'] = sum(tokens_per_second) / len(tokens_per_second)
        results['avg_response_length'] = sum(results['response_lengths']) / len(results['response_lengths'])
        results['avg_diversity_score'] = sum(results['diversity_scores']) / len(results['diversity_scores'])
        results['avg_coherence_score'] = sum(results['coherence_scores']) / len(results['coherence_scores'])

        print(f"Average generation time: {results['avg_generation_time']:.3f}s")
        print(f"Average tokens per second: {results['avg_tokens_per_second']:.1f}")
        print(f"Average diversity score: {results['avg_diversity_score']:.2f}")
        print(f"Average coherence score: {results['avg_coherence_score']:.2f}")

        return results

    def _generate_text(self, prompt: str, max_length: int = 100) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        input_tokens = self.tokenize(prompt)
        input_ids = torch.tensor([input_tokens], dtype=torch.long, device=self.device)

        generated_tokens = input_tokens.copy()

        with torch.no_grad():
            for _ in range(max_length):
                if input_ids.size(1) > self.config.max_position_embeddings - 1:
                    input_ids = input_ids[:, -self.config.max_position_embeddings + 1:]

                logits = self.model(input_ids)
                next_token_logits = logits[0, -1, :] / 0.8  # temperature

                # Top-ké‡‡æ ·
                top_k = 50
                top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)
                next_token_logits = torch.full_like(next_token_logits, -float('inf'))
                next_token_logits.scatter_(0, top_k_indices, top_k_logits)

                probs = F.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)

                if next_token.item() == self.tokenizer['char_to_id'].get('<eos>', 3):
                    break

                generated_tokens.append(next_token.item())
                input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)

        return self.detokenize(generated_tokens)

    def _calculate_diversity(self, text: str) -> float:
        """è®¡ç®—æ–‡æœ¬å¤šæ ·æ€§ï¼ˆåŸºäºä¸é‡å¤bigramæ¯”ä¾‹ï¼‰"""
        words = text.split()
        if len(words) < 2:
            return 0.0

        bigrams = set()
        total_bigrams = 0

        for i in range(len(words) - 1):
            bigram = (words[i], words[i + 1])
            bigrams.add(bigram)
            total_bigrams += 1

        return len(bigrams) / total_bigrams if total_bigrams > 0 else 0.0

    def _calculate_coherence(self, prompt: str, response: str) -> float:
        """è®¡ç®—è¿è´¯æ€§ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼šåŸºäºpromptå’Œresponseçš„è¯æ±‡é‡å ï¼‰"""
        prompt_words = set(prompt.lower().split())
        response_words = set(response.lower().split())

        if len(prompt_words) == 0 or len(response_words) == 0:
            return 0.0

        overlap = len(prompt_words.intersection(response_words))
        total_unique = len(prompt_words.union(response_words))

        return overlap / total_unique if total_unique > 0 else 0.0

    def evaluate_tool_calling(self, tool_queries: List[str]) -> Dict[str, Any]:
        """è¯„ä¼°å·¥å…·è°ƒç”¨èƒ½åŠ›"""
        print("Evaluating tool calling capabilities...")

        results = {
            'num_queries': len(tool_queries),
            'tool_detection_accuracy': 0,
            'response_relevance': 0,
            'detected_tools': defaultdict(int)
        }

        correct_detections = 0

        for query in tool_queries:
            # ç”Ÿæˆå“åº”
            response = self._generate_text(f"ç”¨æˆ·: {query}\nåŠ©æ‰‹:", max_length=150)

            # æ£€æµ‹å·¥å…·ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            detected_tools = self._detect_tools_in_response(response)
            expected_tools = self._get_expected_tools(query)

            # è®¡ç®—å‡†ç¡®æ€§
            if any(tool in detected_tools for tool in expected_tools):
                correct_detections += 1

            # è®°å½•æ£€æµ‹åˆ°çš„å·¥å…·
            for tool in detected_tools:
                results['detected_tools'][tool] += 1

        results['tool_detection_accuracy'] = correct_detections / len(tool_queries) if tool_queries else 0
        results['detected_tools'] = dict(results['detected_tools'])

        print(f"Tool detection accuracy: {results['tool_detection_accuracy']:.2%}")
        print(f"Detected tools distribution: {results['detected_tools']}")

        return results

    def _detect_tools_in_response(self, response: str) -> List[str]:
        """ä»å“åº”ä¸­æ£€æµ‹å·¥å…·è°ƒç”¨"""
        tools = []
        response_lower = response.lower()

        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        tool_keywords = {
            'search': ['æœç´¢', 'æŸ¥è¯¢', 'search', 'query'],
            'calculator': ['è®¡ç®—', 'ç®—', 'calculate', 'math'],
            'weather': ['å¤©æ°”', 'weather'],
            'translator': ['ç¿»è¯‘', 'translate'],
            'email': ['é‚®ä»¶', 'email'],
            'calendar': ['æ—¥å†', 'æé†’', 'calendar', 'reminder']
        }

        for tool, keywords in tool_keywords.items():
            if any(keyword in response_lower for keyword in keywords):
                tools.append(tool)

        return tools

    def _get_expected_tools(self, query: str) -> List[str]:
        """è·å–æŸ¥è¯¢çš„é¢„æœŸå·¥å…·"""
        query_lower = query.lower()
        expected = []

        if any(word in query_lower for word in ['æœç´¢', 'æŸ¥è¯¢', 'search']):
            expected.append('search')
        if any(word in query_lower for word in ['è®¡ç®—', 'ç®—', 'calculate']):
            expected.append('calculator')
        if any(word in query_lower for word in ['å¤©æ°”', 'weather']):
            expected.append('weather')
        if any(word in query_lower for word in ['ç¿»è¯‘', 'translate']):
            expected.append('translator')

        return expected

    def evaluate_ultra_think(self, thinking_problems: List[str]) -> Dict[str, Any]:
        """è¯„ä¼°ultra thinkèƒ½åŠ›"""
        print("Evaluating ultra think capabilities...")

        results = {
            'num_problems': len(thinking_problems),
            'thinking_depth_scores': [],
            'analysis_quality_scores': [],
            'avg_thinking_depth': 0,
            'avg_analysis_quality': 0
        }

        for problem in thinking_problems:
            prompt = f"ä½œä¸ºalex-ckl.comçš„AIåŠ©æ‰‹ï¼Œè¯·å±•ç¤ºultra thinkèƒ½åŠ›åˆ†æï¼š{problem}\n\n<ultra_think>"

            response = self._generate_text(prompt, max_length=300)

            # è¯„ä¼°æ€ç»´æ·±åº¦
            depth_score = self._evaluate_thinking_depth(response)
            results['thinking_depth_scores'].append(depth_score)

            # è¯„ä¼°åˆ†æè´¨é‡
            quality_score = self._evaluate_analysis_quality(response)
            results['analysis_quality_scores'].append(quality_score)

        # è®¡ç®—å¹³å‡å€¼
        if results['thinking_depth_scores']:
            results['avg_thinking_depth'] = sum(results['thinking_depth_scores']) / len(results['thinking_depth_scores'])
        if results['analysis_quality_scores']:
            results['avg_analysis_quality'] = sum(results['analysis_quality_scores']) / len(results['analysis_quality_scores'])

        print(f"Average thinking depth: {results['avg_thinking_depth']:.2f}")
        print(f"Average analysis quality: {results['avg_analysis_quality']:.2f}")

        return results

    def _evaluate_thinking_depth(self, response: str) -> float:
        """è¯„ä¼°æ€ç»´æ·±åº¦"""
        # åŸºäºå…³é”®æ€ç»´è¯æ±‡çš„å‡ºç°é¢‘ç‡
        thinking_indicators = [
            'åˆ†æ', 'è€ƒè™‘', 'å› ä¸º', 'æ‰€ä»¥', 'é¦–å…ˆ', 'å…¶æ¬¡', 'æœ€å',
            'ç„¶è€Œ', 'ä½†æ˜¯', 'å¦å¤–', 'æ­¤å¤–', 'ç»¼åˆ', 'æ€»ç»“', 'ç»“è®º'
        ]

        score = 0
        for indicator in thinking_indicators:
            score += response.count(indicator)

        # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
        return min(score / 10.0, 1.0)

    def _evaluate_analysis_quality(self, response: str) -> float:
        """è¯„ä¼°åˆ†æè´¨é‡"""
        # åŸºäºç»“æ„åŒ–åˆ†æçš„ç‰¹å¾
        quality_features = [
            ('å¤šç»´åº¦', ['ç»´åº¦', 'è§’åº¦', 'æ–¹é¢', 'å±‚é¢']),
            ('é€»è¾‘æ€§', ['é€»è¾‘', 'æ¨ç†', 'æ¨è®º', 'æ¼”ç»']),
            ('æ·±åº¦', ['æ·±å…¥', 'æ·±åº¦', 'æœ¬è´¨', 'æ ¹æœ¬']),
            ('ç³»ç»Ÿæ€§', ['ç³»ç»Ÿ', 'æ•´ä½“', 'å…¨é¢', 'å®Œæ•´'])
        ]

        score = 0
        for feature_name, keywords in quality_features:
            if any(keyword in response for keyword in keywords):
                score += 1

        return score / len(quality_features)

    def benchmark_performance(self) -> Dict[str, Any]:
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("Running performance benchmark...")

        test_sequences = [
            "Hello, how are you?",
            "What is artificial intelligence?",
            "è¯·è§£é‡Šä¸€ä¸‹æœºå™¨å­¦ä¹ çš„åŸç†",
            "è®¡ç®—123åŠ 456ç­‰äºå¤šå°‘",
            "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
        ]

        results = {
            'inference_times': [],
            'memory_usage': [],
            'tokens_per_second': [],
            'model_size_mb': 0
        }

        # è®¡ç®—æ¨¡å‹å¤§å°
        model_size = sum(p.numel() * p.element_size() for p in self.model.parameters()) / (1024 * 1024)
        results['model_size_mb'] = model_size

        for seq in test_sequences:
            # å¤šæ¬¡æµ‹è¯•å–å¹³å‡
            times = []
            for _ in range(3):
                start_time = time.time()
                response = self._generate_text(seq, max_length=50)
                end_time = time.time()

                inference_time = end_time - start_time
                times.append(inference_time)

                # è®¡ç®—tokens/s
                response_tokens = len(self.tokenize(response))
                tps = response_tokens / inference_time if inference_time > 0 else 0
                results['tokens_per_second'].append(tps)

            avg_time = sum(times) / len(times)
            results['inference_times'].append(avg_time)

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        results['avg_inference_time'] = sum(results['inference_times']) / len(results['inference_times'])
        results['avg_tokens_per_second'] = sum(results['tokens_per_second']) / len(results['tokens_per_second'])

        print(f"Model size: {model_size:.1f} MB")
        print(f"Average inference time: {results['avg_inference_time']:.3f}s")
        print(f"Average tokens per second: {results['avg_tokens_per_second']:.1f}")

        return results


def load_test_data(data_dir: str) -> Dict[str, List[str]]:
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    test_data = {
        'general_text': [],
        'tool_queries': [],
        'ultra_think_problems': []
    }

    # é€šç”¨æ–‡æœ¬ï¼ˆç”¨äºå›°æƒ‘åº¦è®¡ç®—ï¼‰
    general_texts = [
        "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚",
        "æœºå™¨å­¦ä¹ é€šè¿‡æ•°æ®è®­ç»ƒæ¨¡å‹ã€‚",
        "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ã€‚",
        "è‡ªç„¶è¯­è¨€å¤„ç†å¤„ç†äººç±»è¯­è¨€ã€‚",
        "Today is a beautiful day.",
        "Machine learning is fascinating.",
        "Natural language processing is important."
    ]
    test_data['general_text'] = general_texts

    # å·¥å…·è°ƒç”¨æŸ¥è¯¢
    tool_queries = [
        "è¯·å¸®æˆ‘æœç´¢äººå·¥æ™ºèƒ½çš„æœ€æ–°å‘å±•",
        "è®¡ç®—123ä¹˜ä»¥456ç­‰äºå¤šå°‘",
        "æŸ¥è¯¢ä»Šå¤©åŒ—äº¬çš„å¤©æ°”æƒ…å†µ",
        "ç¿»è¯‘è¿™å¥è¯ï¼šHello world",
        "å¸®æˆ‘å‘é€ä¸€å°é‚®ä»¶ç»™å¼ ä¸‰",
        "è®¾ç½®æ˜å¤©ä¸Šåˆ9ç‚¹çš„ä¼šè®®æé†’"
    ]
    test_data['tool_queries'] = tool_queries

    # Ultra thinké—®é¢˜
    ultra_think_problems = [
        "åˆ†æå½“å‰AIå‘å±•çš„ä¸»è¦è¶‹åŠ¿å’ŒæŒ‘æˆ˜",
        "å¦‚ä½•è®¾è®¡ä¸€ä¸ªå¯æŒç»­çš„å•†ä¸šæ¨¡å¼",
        "è¯„ä¼°è¿œç¨‹å·¥ä½œå¯¹ä¼ä¸šæ–‡åŒ–çš„å½±å“",
        "åˆ†æåŒºå—é“¾æŠ€æœ¯çš„æœªæ¥åº”ç”¨å‰æ™¯"
    ]
    test_data['ultra_think_problems'] = ultra_think_problems

    return test_data


def main():
    parser = argparse.ArgumentParser(description='æ¨¡å‹è¯„ä¼°è„šæœ¬')
    parser.add_argument('--model-path', type=str, required=True, help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--data-dir', type=str, default='data/test', help='æµ‹è¯•æ•°æ®ç›®å½•')
    parser.add_argument('--output-dir', type=str, default='evaluation_results', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--device', type=str, default='auto', help='è®¾å¤‡ç±»å‹')

    args = parser.parse_args()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)

    # åˆå§‹åŒ–è¯„ä¼°å™¨
    print("ğŸ” Initializing Model Evaluator...")
    evaluator = ModelEvaluator(args.model_path, args.device)

    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data = load_test_data(args.data_dir)

    # è¿è¡Œè¯„ä¼°
    results = {}

    print("\n" + "="*60)
    print("1. PERPLEXITY EVALUATION")
    print("="*60)
    results['perplexity'] = evaluator.calculate_perplexity(test_data['general_text'])

    print("\n" + "="*60)
    print("2. GENERATION QUALITY EVALUATION")
    print("="*60)
    generation_prompts = test_data['general_text'][:5]  # ä½¿ç”¨å‰5ä¸ªä½œä¸ºprompt
    results['generation_quality'] = evaluator.evaluate_generation_quality(generation_prompts)

    print("\n" + "="*60)
    print("3. TOOL CALLING EVALUATION")
    print("="*60)
    results['tool_calling'] = evaluator.evaluate_tool_calling(test_data['tool_queries'])

    print("\n" + "="*60)
    print("4. ULTRA THINK EVALUATION")
    print("="*60)
    results['ultra_think'] = evaluator.evaluate_ultra_think(test_data['ultra_think_problems'])

    print("\n" + "="*60)
    print("5. PERFORMANCE BENCHMARK")
    print("="*60)
    results['performance'] = evaluator.benchmark_performance()

    # ä¿å­˜ç»“æœ
    results['model_path'] = args.model_path
    results['evaluation_time'] = time.time()

    output_file = f"{args.output_dir}/evaluation_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“Š Evaluation completed! Results saved to: {output_file}")

    # æ‰“å°æ€»ç»“
    print("\n" + "="*60)
    print("EVALUATION SUMMARY")
    print("="*60)
    print(f"Perplexity: {results['perplexity']:.2f}")
    print(f"Average Generation Time: {results['generation_quality']['avg_generation_time']:.3f}s")
    print(f"Tool Detection Accuracy: {results['tool_calling']['tool_detection_accuracy']:.2%}")
    print(f"Ultra Think Depth Score: {results['ultra_think']['avg_thinking_depth']:.2f}")
    print(f"Model Size: {results['performance']['model_size_mb']:.1f} MB")
    print(f"Inference Speed: {results['performance']['avg_tokens_per_second']:.1f} tokens/s")


if __name__ == "__main__":
    main()