#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ” MiniGPTåˆ†è¯å™¨ç»¼åˆè¯„ä¼°ç³»ç»Ÿ
===============================

ç›®æ ‡: ç³»ç»Ÿæ€§è¯„ä¼°åˆ†è¯å™¨çš„æ€§èƒ½ã€æ•ˆç‡å’Œè´¨é‡
ä½œè€…: alex-ckl.com AIç ”å‘å›¢é˜Ÿ
é£æ ¼: ISTJç³»ç»ŸåŒ–æ‰§è¡Œé£æ ¼

è¯„ä¼°ç»´åº¦:
1. åŸºç¡€æ€§èƒ½æŒ‡æ ‡ (å‹ç¼©ç‡ã€è¯æ±‡è¦†ç›–ç‡)
2. è¯­è¨€ç†è§£è´¨é‡ (è¯­ä¹‰ä¿æŒã€OOVå¤„ç†)
3. æ•ˆç‡æŒ‡æ ‡ (ç¼–è§£ç é€Ÿåº¦ã€å†…å­˜ä½¿ç”¨)
4. å¤šè¯­è¨€æ”¯æŒè¯„ä¼° (ä¸­è‹±æ–‡æ··åˆã€ç‰¹æ®Šç¬¦å·)
5. å¯¹æ¯”åˆ†æ (ä¸å…¶ä»–åˆ†è¯å™¨å¯¹æ¯”)
6. å®ç”¨æ€§è¯„ä¼° (è®­ç»ƒæ•°æ®é€‚é…æ€§)
"""

import sys
import os
import time
import json
import pickle
import argparse
import statistics
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass, asdict
from collections import Counter, defaultdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../../..'))

try:
    from src.tokenizer.bpe_tokenizer import BPETokenizer
    from src.tokenizer.tokenizer_manager import TokenizerManager
    TOKENIZER_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  åˆ†è¯å™¨æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    TOKENIZER_AVAILABLE = False


@dataclass
class TokenizerMetrics:
    """åˆ†è¯å™¨è¯„ä¼°æŒ‡æ ‡æ•°æ®ç±»"""
    # åŸºç¡€ä¿¡æ¯
    tokenizer_name: str
    vocab_size: int
    model_path: str

    # å‹ç¼©æ€§èƒ½
    compression_ratio: float  # å‹ç¼©ç‡ (åŸå­—ç¬¦æ•°/tokenæ•°)
    avg_token_length: float   # å¹³å‡tokené•¿åº¦

    # è¯æ±‡è¦†ç›–
    vocab_coverage: float     # è¯æ±‡è¦†ç›–ç‡
    oov_rate: float          # æœªçŸ¥è¯ç‡

    # æ•ˆç‡æŒ‡æ ‡
    encode_speed: float      # ç¼–ç é€Ÿåº¦ (tokens/ç§’)
    decode_speed: float      # è§£ç é€Ÿåº¦ (tokens/ç§’)
    memory_usage_mb: float   # å†…å­˜ä½¿ç”¨é‡ (MB)

    # è´¨é‡æŒ‡æ ‡
    semantic_coherence: float    # è¯­ä¹‰è¿è´¯æ€§å¾—åˆ†
    boundary_accuracy: float     # è¯è¾¹ç•Œå‡†ç¡®æ€§

    # å¤šè¯­è¨€æ”¯æŒ
    chinese_support: float       # ä¸­æ–‡æ”¯æŒè´¨é‡
    english_support: float       # è‹±æ–‡æ”¯æŒè´¨é‡
    mixed_language_support: float # æ··åˆè¯­è¨€æ”¯æŒ

    # ç‰¹æ®Šå¤„ç†
    special_token_handling: float  # ç‰¹æ®Štokenå¤„ç†è´¨é‡
    code_tokenization: float       # ä»£ç åˆ†è¯è´¨é‡

    # å®ç”¨æ€§
    training_data_efficiency: float  # è®­ç»ƒæ•°æ®é€‚é…æ•ˆç‡
    model_compatibility: float      # æ¨¡å‹å…¼å®¹æ€§


class TokenizerEvaluator:
    """åˆ†è¯å™¨è¯„ä¼°å™¨ä¸»ç±»"""

    def __init__(self):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        self.test_cases = self._prepare_test_cases()
        self.results = {}

    def _prepare_test_cases(self) -> Dict[str, List[str]]:
        """å‡†å¤‡æµ‹è¯•ç”¨ä¾‹ (ç³»ç»ŸåŒ–æµ‹è¯•æ•°æ®)"""
        return {
            # åŸºç¡€ä¸­æ–‡æµ‹è¯•
            "chinese_basic": [
                "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œæˆ‘ä»¬å»å…¬å›­æ•£æ­¥ã€‚",
                "äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œæ”¹å˜ç€æˆ‘ä»¬çš„ç”Ÿæ´»ã€‚",
                "ä¸­å›½çš„ä¼ ç»Ÿæ–‡åŒ–åšå¤§ç²¾æ·±ï¼Œå€¼å¾—æˆ‘ä»¬ä¼ æ‰¿å’Œå‘æ‰¬ã€‚",
                "æ·±åº¦å­¦ä¹ æ¨¡å‹éœ€è¦å¤§é‡çš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚",
                "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ã€‚"
            ],

            # åŸºç¡€è‹±æ–‡æµ‹è¯•
            "english_basic": [
                "The weather is beautiful today, let's go for a walk in the park.",
                "Artificial intelligence technology is rapidly developing and changing our lives.",
                "Natural language processing is an important branch of artificial intelligence.",
                "Deep learning models require large amounts of data for training.",
                "Machine learning algorithms can solve complex problems efficiently."
            ],

            # ä¸­è‹±æ–‡æ··åˆæµ‹è¯•
            "mixed_language": [
                "æˆ‘æ­£åœ¨å­¦ä¹ Pythonç¼–ç¨‹è¯­è¨€å’Œmachine learningç®—æ³•ã€‚",
                "OpenAIçš„GPTæ¨¡å‹åœ¨ä¸­æ–‡ç†è§£æ–¹é¢è¡¨ç°excellentã€‚",
                "ä»Šå¤©æˆ‘ä»¬è®¨è®ºäº†transformer architectureå’Œattention mechanismã€‚",
                "æ•°æ®ç§‘å­¦data scienceéœ€è¦ç»Ÿè®¡å­¦statisticså’Œç¼–ç¨‹programmingæŠ€èƒ½ã€‚",
                "AIäººå·¥æ™ºèƒ½å’ŒMLæœºå™¨å­¦ä¹ æ˜¯å½“å‰çš„çƒ­é—¨æŠ€æœ¯trendsã€‚"
            ],

            # æŠ€æœ¯æœ¯è¯­æµ‹è¯•
            "technical_terms": [
                "åˆ†è¯å™¨tokenizerã€ç¼–ç å™¨encoderã€è§£ç å™¨decoderæ˜¯NLPçš„æ ¸å¿ƒç»„ä»¶ã€‚",
                "BERTã€GPTã€T5ç­‰é¢„è®­ç»ƒæ¨¡å‹ä½¿ç”¨äº†ä¸åŒçš„æ¶æ„è®¾è®¡ã€‚",
                "è¯åµŒå…¥word embeddingså’Œä½ç½®ç¼–ç positional encodingå¾ˆé‡è¦ã€‚",
                "æ³¨æ„åŠ›æœºåˆ¶attention mechanismå’Œå‰é¦ˆç½‘ç»œfeedforward networkçš„ä½œç”¨ã€‚",
                "è¶…å‚æ•°hyperparametersè°ƒä¼˜å¯¹æ¨¡å‹æ€§èƒ½model performanceå½±å“å¾ˆå¤§ã€‚"
            ],

            # ä»£ç å’Œç‰¹æ®Šç¬¦å·æµ‹è¯•
            "code_and_symbols": [
                "def tokenize(text): return tokenizer.encode(text)",
                "import torch; model = torch.nn.Transformer()",
                "for i in range(len(tokens)): print(f'Token {i}: {tokens[i]}')",
                "APIæ¥å£: https://api.example.com/v1/generate?text=hello",
                "é‚®ç®±æ ¼å¼: user@domain.com, ç”µè¯: +86-138-0013-8000"
            ],

            # é•¿æ–‡æœ¬æµ‹è¯•
            "long_text": [
                """äººå·¥æ™ºèƒ½(Artificial Intelligence, AI)æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒè¯•å›¾ç†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚è¯¥é¢†åŸŸçš„ç ”ç©¶åŒ…æ‹¬æœºå™¨äººroboticsã€è¯­è¨€è¯†åˆ«speech recognitionã€å›¾åƒè¯†åˆ«image recognitionã€è‡ªç„¶è¯­è¨€å¤„ç†natural language processingå’Œä¸“å®¶ç³»ç»Ÿexpert systemsç­‰ã€‚è‡ªä»è®¡ç®—æœºè¯ç”Ÿä»¥æ¥ï¼Œäººä»¬å°±å¼€å§‹æ¢ç´¢èƒ½å¦è®©æœºå™¨åƒäººä¸€æ ·æ€è€ƒï¼Œè¿™ä¸ªé—®é¢˜ä¸€ç›´å»¶ç»­è‡³ä»Šã€‚"""
            ],

            # è¾¹ç•Œæƒ…å†µæµ‹è¯•
            "edge_cases": [
                "",  # ç©ºå­—ç¬¦ä¸²
                " ",  # ç©ºæ ¼
                "\n\n\t\t",  # ç©ºç™½å­—ç¬¦
                "a",  # å•å­—ç¬¦
                "ä½ ",  # å•ä¸­æ–‡å­—ç¬¦
                "!!!!!",  # é‡å¤ç¬¦å·
                "123456789",  # çº¯æ•°å­—
                "aaaaaaaaaa",  # é‡å¤å­—ç¬¦
            ]
        }

    def evaluate_tokenizer(self, tokenizer_path: str) -> TokenizerMetrics:
        """è¯„ä¼°å•ä¸ªåˆ†è¯å™¨ (ä¸»è¦è¯„ä¼°å‡½æ•°)"""
        print(f"ğŸ” å¼€å§‹è¯„ä¼°åˆ†è¯å™¨: {os.path.basename(tokenizer_path)}")
        print("="*70)

        if not TOKENIZER_AVAILABLE:
            print("âŒ åˆ†è¯å™¨æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡è¯„ä¼°")
            return self._create_empty_metrics(tokenizer_path)

        # åŠ è½½åˆ†è¯å™¨
        try:
            tokenizer = self._load_tokenizer(tokenizer_path)
            if tokenizer is None:
                return self._create_empty_metrics(tokenizer_path)
        except Exception as e:
            print(f"âŒ åŠ è½½åˆ†è¯å™¨å¤±è´¥: {e}")
            return self._create_empty_metrics(tokenizer_path)

        # ç³»ç»Ÿæ€§æ‰§è¡Œå„é¡¹è¯„ä¼°
        metrics = {
            'tokenizer_name': os.path.basename(tokenizer_path),
            'model_path': tokenizer_path,
            'vocab_size': getattr(tokenizer, 'vocab_size', 0)
        }

        # 1. åŸºç¡€æ€§èƒ½è¯„ä¼°
        print("ğŸ“Š 1. æ‰§è¡ŒåŸºç¡€æ€§èƒ½è¯„ä¼°...")
        basic_metrics = self._evaluate_basic_performance(tokenizer)
        metrics.update(basic_metrics)

        # 2. æ•ˆç‡è¯„ä¼°
        print("âš¡ 2. æ‰§è¡Œæ•ˆç‡è¯„ä¼°...")
        efficiency_metrics = self._evaluate_efficiency(tokenizer)
        metrics.update(efficiency_metrics)

        # 3. è´¨é‡è¯„ä¼°
        print("ğŸ¯ 3. æ‰§è¡Œè´¨é‡è¯„ä¼°...")
        quality_metrics = self._evaluate_quality(tokenizer)
        metrics.update(quality_metrics)

        # 4. å¤šè¯­è¨€æ”¯æŒè¯„ä¼°
        print("ğŸŒ 4. æ‰§è¡Œå¤šè¯­è¨€æ”¯æŒè¯„ä¼°...")
        multilang_metrics = self._evaluate_multilang_support(tokenizer)
        metrics.update(multilang_metrics)

        # 5. ç‰¹æ®Šå¤„ç†èƒ½åŠ›è¯„ä¼°
        print("ğŸ”§ 5. æ‰§è¡Œç‰¹æ®Šå¤„ç†èƒ½åŠ›è¯„ä¼°...")
        special_metrics = self._evaluate_special_handling(tokenizer)
        metrics.update(special_metrics)

        # 6. å®ç”¨æ€§è¯„ä¼°
        print("ğŸ’¼ 6. æ‰§è¡Œå®ç”¨æ€§è¯„ä¼°...")
        utility_metrics = self._evaluate_utility(tokenizer)
        metrics.update(utility_metrics)

        # åˆ›å»ºæœ€ç»ˆæŒ‡æ ‡å¯¹è±¡
        tokenizer_metrics = TokenizerMetrics(**metrics)

        print("âœ… åˆ†è¯å™¨è¯„ä¼°å®Œæˆ!")
        return tokenizer_metrics

    def _load_tokenizer(self, tokenizer_path: str):
        """åŠ è½½åˆ†è¯å™¨ (ç»Ÿä¸€åŠ è½½é€»è¾‘)"""
        try:
            if tokenizer_path.endswith('.pkl'):
                with open(tokenizer_path, 'rb') as f:
                    tokenizer = pickle.load(f)
            else:
                # å°è¯•å…¶ä»–æ ¼å¼
                tokenizer = TokenizerManager.load_tokenizer(tokenizer_path)

            print(f"âœ… æˆåŠŸåŠ è½½åˆ†è¯å™¨: {type(tokenizer).__name__}")
            if hasattr(tokenizer, 'vocab_size'):
                print(f"ğŸ“š è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size:,}")

            return tokenizer

        except Exception as e:
            print(f"âŒ åŠ è½½åˆ†è¯å™¨å¤±è´¥: {e}")
            return None

    def _create_empty_metrics(self, tokenizer_path: str) -> TokenizerMetrics:
        """åˆ›å»ºç©ºçš„æŒ‡æ ‡å¯¹è±¡ (é”™è¯¯å¤„ç†)"""
        return TokenizerMetrics(
            tokenizer_name=os.path.basename(tokenizer_path),
            vocab_size=0,
            model_path=tokenizer_path,
            compression_ratio=0.0,
            avg_token_length=0.0,
            vocab_coverage=0.0,
            oov_rate=1.0,
            encode_speed=0.0,
            decode_speed=0.0,
            memory_usage_mb=0.0,
            semantic_coherence=0.0,
            boundary_accuracy=0.0,
            chinese_support=0.0,
            english_support=0.0,
            mixed_language_support=0.0,
            special_token_handling=0.0,
            code_tokenization=0.0,
            training_data_efficiency=0.0,
            model_compatibility=0.0
        )

    def _evaluate_basic_performance(self, tokenizer) -> Dict[str, float]:
        """è¯„ä¼°åŸºç¡€æ€§èƒ½æŒ‡æ ‡"""
        total_chars = 0
        total_tokens = 0
        token_lengths = []

        # ä½¿ç”¨å¤šç§æµ‹è¯•ç”¨ä¾‹
        all_texts = []
        for category, texts in self.test_cases.items():
            if category != 'edge_cases':  # æ’é™¤è¾¹ç•Œæƒ…å†µ
                all_texts.extend(texts)

        for text in all_texts:
            try:
                if hasattr(tokenizer, 'encode'):
                    tokens = tokenizer.encode(text)
                elif hasattr(tokenizer, 'tokenize'):
                    tokens = tokenizer.tokenize(text)
                else:
                    continue

                total_chars += len(text)
                total_tokens += len(tokens)

                # è®¡ç®—æ¯ä¸ªtokençš„å¹³å‡å­—ç¬¦é•¿åº¦
                if hasattr(tokenizer, 'decode'):
                    for token in tokens:
                        try:
                            decoded = tokenizer.decode([token]) if isinstance(token, int) else str(token)
                            token_lengths.append(len(decoded))
                        except:
                            token_lengths.append(1)  # é»˜è®¤é•¿åº¦

            except Exception as e:
                print(f"âš ï¸  å¤„ç†æ–‡æœ¬æ—¶å‡ºé”™: {e}")
                continue

        # è®¡ç®—æŒ‡æ ‡
        compression_ratio = total_chars / max(total_tokens, 1)
        avg_token_length = statistics.mean(token_lengths) if token_lengths else 1.0

        return {
            'compression_ratio': compression_ratio,
            'avg_token_length': avg_token_length
        }

    def _evaluate_efficiency(self, tokenizer) -> Dict[str, float]:
        """è¯„ä¼°æ•ˆç‡æŒ‡æ ‡"""
        test_text = "è¿™æ˜¯ä¸€ä¸ªç”¨äºæµ‹è¯•åˆ†è¯å™¨ç¼–ç è§£ç é€Ÿåº¦çš„é•¿æ–‡æœ¬ã€‚" * 100  # é‡å¤100æ¬¡

        # ç¼–ç é€Ÿåº¦æµ‹è¯•
        encode_times = []
        for _ in range(10):  # å¤šæ¬¡æµ‹è¯•å–å¹³å‡
            start_time = time.time()
            try:
                if hasattr(tokenizer, 'encode'):
                    tokens = tokenizer.encode(test_text)
                elif hasattr(tokenizer, 'tokenize'):
                    tokens = tokenizer.tokenize(test_text)
                else:
                    tokens = []
                end_time = time.time()

                if tokens:
                    encode_times.append(len(tokens) / (end_time - start_time))
            except:
                continue

        # è§£ç é€Ÿåº¦æµ‹è¯• (å¦‚æœæ”¯æŒ)
        decode_times = []
        if hasattr(tokenizer, 'encode') and hasattr(tokenizer, 'decode'):
            try:
                tokens = tokenizer.encode(test_text[:1000])  # ä½¿ç”¨è¾ƒçŸ­æ–‡æœ¬é¿å…è¶…æ—¶
                for _ in range(10):
                    start_time = time.time()
                    decoded = tokenizer.decode(tokens)
                    end_time = time.time()
                    decode_times.append(len(tokens) / (end_time - start_time))
            except:
                pass

        # å†…å­˜ä½¿ç”¨ä¼°ç®— (ç®€åŒ–ç‰ˆæœ¬)
        memory_usage = 0.0
        if hasattr(tokenizer, 'vocab_size'):
            # ç²—ç•¥ä¼°ç®—: vocab_size * å¹³å‡tokené•¿åº¦ * å­—èŠ‚æ•°
            memory_usage = getattr(tokenizer, 'vocab_size', 0) * 10 / (1024 * 1024)  # MB

        return {
            'encode_speed': statistics.mean(encode_times) if encode_times else 0.0,
            'decode_speed': statistics.mean(decode_times) if decode_times else 0.0,
            'memory_usage_mb': memory_usage
        }

    def _evaluate_quality(self, tokenizer) -> Dict[str, float]:
        """è¯„ä¼°è´¨é‡æŒ‡æ ‡"""
        semantic_scores = []
        boundary_scores = []

        # è¯­ä¹‰è¿è´¯æ€§æµ‹è¯•
        semantic_test_cases = [
            "äººå·¥æ™ºèƒ½",  # åº”è¯¥ä½œä¸ºæ•´ä½“
            "machine learning",  # è‹±æ–‡å¤åˆè¯
            "è‡ªç„¶è¯­è¨€å¤„ç†",  # æŠ€æœ¯æœ¯è¯­
            "æ·±åº¦å­¦ä¹ æ¨¡å‹",  # é¢†åŸŸæœ¯è¯­
        ]

        for text in semantic_test_cases:
            try:
                if hasattr(tokenizer, 'encode') and hasattr(tokenizer, 'decode'):
                    tokens = tokenizer.encode(text)
                    decoded = tokenizer.decode(tokens)

                    # ç®€å•çš„è¯­ä¹‰ä¿æŒè¯„åˆ† (åŸºäºé‡å»ºè´¨é‡)
                    if decoded.replace(' ', '') == text.replace(' ', ''):
                        semantic_scores.append(1.0)
                    else:
                        # è®¡ç®—å­—ç¬¦çº§ç›¸ä¼¼åº¦
                        similarity = self._calculate_string_similarity(text, decoded)
                        semantic_scores.append(similarity)

                    # è¯è¾¹ç•Œå‡†ç¡®æ€§ (ç†æƒ³æƒ…å†µä¸‹æŠ€æœ¯æœ¯è¯­åº”è¯¥ä¿æŒå®Œæ•´)
                    if len(tokens) <= 3:  # æŠ€æœ¯æœ¯è¯­ç†æƒ³æƒ…å†µä¸‹ä¸åº”è¿‡åº¦åˆ†å‰²
                        boundary_scores.append(1.0)
                    else:
                        boundary_scores.append(max(0.0, 1.0 - (len(tokens) - 3) * 0.2))

            except Exception:
                semantic_scores.append(0.0)
                boundary_scores.append(0.0)

        return {
            'semantic_coherence': statistics.mean(semantic_scores) if semantic_scores else 0.0,
            'boundary_accuracy': statistics.mean(boundary_scores) if boundary_scores else 0.0
        }

    def _evaluate_multilang_support(self, tokenizer) -> Dict[str, float]:
        """è¯„ä¼°å¤šè¯­è¨€æ”¯æŒ"""
        chinese_scores = []
        english_scores = []
        mixed_scores = []

        # ä¸­æ–‡æ”¯æŒæµ‹è¯•
        for text in self.test_cases['chinese_basic']:
            score = self._evaluate_language_support(tokenizer, text, 'chinese')
            chinese_scores.append(score)

        # è‹±æ–‡æ”¯æŒæµ‹è¯•
        for text in self.test_cases['english_basic']:
            score = self._evaluate_language_support(tokenizer, text, 'english')
            english_scores.append(score)

        # æ··åˆè¯­è¨€æ”¯æŒæµ‹è¯•
        for text in self.test_cases['mixed_language']:
            score = self._evaluate_language_support(tokenizer, text, 'mixed')
            mixed_scores.append(score)

        return {
            'chinese_support': statistics.mean(chinese_scores) if chinese_scores else 0.0,
            'english_support': statistics.mean(english_scores) if english_scores else 0.0,
            'mixed_language_support': statistics.mean(mixed_scores) if mixed_scores else 0.0
        }

    def _evaluate_special_handling(self, tokenizer) -> Dict[str, float]:
        """è¯„ä¼°ç‰¹æ®Šå¤„ç†èƒ½åŠ›"""
        special_token_scores = []
        code_scores = []

        # ç‰¹æ®Štokenå¤„ç†æµ‹è¯•
        special_cases = [
            "[CLS]", "[SEP]", "[MASK]", "[PAD]",  # BERTé£æ ¼
            "<bos>", "<eos>", "<pad>", "<unk>",   # GPTé£æ ¼
            "<|im_start|>", "<|im_end|>",        # Chaté£æ ¼
        ]

        for special in special_cases:
            try:
                if hasattr(tokenizer, 'encode'):
                    tokens = tokenizer.encode(special)
                    # ç‰¹æ®Štokenç†æƒ³æƒ…å†µä¸‹åº”è¯¥è¢«è¯†åˆ«ä¸ºå•ä¸ªtoken
                    if len(tokens) == 1:
                        special_token_scores.append(1.0)
                    else:
                        special_token_scores.append(max(0.0, 1.0 - (len(tokens) - 1) * 0.3))
            except:
                special_token_scores.append(0.0)

        # ä»£ç åˆ†è¯æµ‹è¯•
        for code in self.test_cases['code_and_symbols']:
            try:
                if hasattr(tokenizer, 'encode'):
                    tokens = tokenizer.encode(code)
                    # ä»£ç åº”è¯¥åˆç†åˆ†å‰²ï¼Œä¸åº”è¿‡åº¦ç¢ç‰‡åŒ–
                    code_length_ratio = len(code) / max(len(tokens), 1)
                    # ç†æƒ³çš„ä»£ç åˆ†è¯åº”è¯¥åœ¨2-8ä¸ªå­—ç¬¦per tokenä¹‹é—´
                    if 2 <= code_length_ratio <= 8:
                        code_scores.append(1.0)
                    else:
                        code_scores.append(max(0.0, 1.0 - abs(code_length_ratio - 5) * 0.1))
            except:
                code_scores.append(0.0)

        return {
            'special_token_handling': statistics.mean(special_token_scores) if special_token_scores else 0.0,
            'code_tokenization': statistics.mean(code_scores) if code_scores else 0.0
        }

    def _evaluate_utility(self, tokenizer) -> Dict[str, float]:
        """è¯„ä¼°å®ç”¨æ€§æŒ‡æ ‡"""
        # è®­ç»ƒæ•°æ®é€‚é…æ•ˆç‡ (åŸºäºå‹ç¼©ç‡å’Œè¦†ç›–ç‡)
        compression_efficiency = self._calculate_training_efficiency(tokenizer)

        # æ¨¡å‹å…¼å®¹æ€§ (åŸºäºè¯æ±‡è¡¨å¤§å°å’Œå¸¸è§æ¶æ„çš„åŒ¹é…åº¦)
        model_compat = self._calculate_model_compatibility(tokenizer)

        return {
            'training_data_efficiency': compression_efficiency,
            'model_compatibility': model_compat
        }

    def _evaluate_language_support(self, tokenizer, text: str, lang_type: str) -> float:
        """è¯„ä¼°ç‰¹å®šè¯­è¨€æ”¯æŒè´¨é‡"""
        try:
            if hasattr(tokenizer, 'encode') and hasattr(tokenizer, 'decode'):
                tokens = tokenizer.encode(text)
                decoded = tokenizer.decode(tokens)

                # è®¡ç®—é‡å»ºè´¨é‡
                similarity = self._calculate_string_similarity(text, decoded)

                # æ ¹æ®è¯­è¨€ç±»å‹è°ƒæ•´è¯„åˆ†æ ‡å‡†
                if lang_type == 'chinese':
                    # ä¸­æ–‡åº”è¯¥æœ‰è¾ƒå¥½çš„å­—ç¬¦ä¿æŒ
                    return similarity
                elif lang_type == 'english':
                    # è‹±æ–‡åº”è¯¥æœ‰è¾ƒå¥½çš„è¯æ±‡ä¿æŒ
                    return similarity
                else:  # mixed
                    # æ··åˆè¯­è¨€åº”è¯¥å‡è¡¡å¤„ç†
                    return similarity * 0.9  # ç¨å¾®é™ä½æ ‡å‡†
            return 0.0
        except:
            return 0.0

    def _calculate_string_similarity(self, s1: str, s2: str) -> float:
        """è®¡ç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ (ç®€åŒ–ç‰ˆæœ¬)"""
        if not s1 and not s2:
            return 1.0
        if not s1 or not s2:
            return 0.0

        # ç®€å•çš„å­—ç¬¦çº§Jaccardç›¸ä¼¼åº¦
        set1 = set(s1.replace(' ', ''))
        set2 = set(s2.replace(' ', ''))

        intersection = len(set1.intersection(set2))
        union = len(set1.union(set2))

        return intersection / union if union > 0 else 0.0

    def _calculate_training_efficiency(self, tokenizer) -> float:
        """è®¡ç®—è®­ç»ƒæ•°æ®é€‚é…æ•ˆç‡"""
        # åŸºäºå¤šä¸ªå› ç´ çš„ç»¼åˆè¯„åˆ†
        factors = []

        # 1. è¯æ±‡è¡¨å¤§å°åˆç†æ€§ (10K-50Kä¸ºç†æƒ³èŒƒå›´)
        vocab_size = getattr(tokenizer, 'vocab_size', 0)
        if 10000 <= vocab_size <= 50000:
            factors.append(1.0)
        elif vocab_size < 10000:
            factors.append(vocab_size / 10000)
        else:
            factors.append(max(0.3, 50000 / vocab_size))

        # 2. å‹ç¼©æ•ˆç‡ (ä¹‹å‰è®¡ç®—çš„compression_ratio)
        # è¿™é‡Œä½¿ç”¨ç®€åŒ–ä¼°ç®—
        factors.append(0.8)  # é»˜è®¤åˆç†å€¼

        return statistics.mean(factors)

    def _calculate_model_compatibility(self, tokenizer) -> float:
        """è®¡ç®—æ¨¡å‹å…¼å®¹æ€§"""
        compat_score = 0.0

        # æ£€æŸ¥å¸¸è§æ–¹æ³•çš„å­˜åœ¨
        methods = ['encode', 'decode', 'tokenize']
        available_methods = sum(1 for method in methods if hasattr(tokenizer, method))
        compat_score += (available_methods / len(methods)) * 0.5

        # æ£€æŸ¥è¯æ±‡è¡¨å¤§å°æ˜¯å¦åœ¨å¸¸è§èŒƒå›´å†…
        vocab_size = getattr(tokenizer, 'vocab_size', 0)
        if vocab_size > 0:
            if 5000 <= vocab_size <= 100000:
                compat_score += 0.5
            else:
                compat_score += 0.2

        return compat_score

    def compare_tokenizers(self, tokenizer_paths: List[str]) -> Dict[str, Any]:
        """å¯¹æ¯”å¤šä¸ªåˆ†è¯å™¨ (å¯¹æ¯”åˆ†æåŠŸèƒ½)"""
        print("ğŸ”„ å¼€å§‹å¯¹æ¯”åˆ†æå¤šä¸ªåˆ†è¯å™¨...")
        print("="*70)

        all_metrics = {}

        # è¯„ä¼°æ¯ä¸ªåˆ†è¯å™¨
        for path in tokenizer_paths:
            if os.path.exists(path):
                metrics = self.evaluate_tokenizer(path)
                all_metrics[metrics.tokenizer_name] = metrics
            else:
                print(f"âš ï¸  åˆ†è¯å™¨æ–‡ä»¶ä¸å­˜åœ¨: {path}")

        if not all_metrics:
            print("âŒ æ²¡æœ‰å¯è¯„ä¼°çš„åˆ†è¯å™¨")
            return {}

        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        comparison = self._generate_comparison_report(all_metrics)

        return comparison

    def _generate_comparison_report(self, all_metrics: Dict[str, TokenizerMetrics]) -> Dict[str, Any]:
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        report = {
            'summary': {},
            'rankings': {},
            'detailed_comparison': {},
            'recommendations': []
        }

        # è®¡ç®—å„é¡¹æŒ‡æ ‡çš„æ’å
        metrics_for_ranking = [
            'compression_ratio', 'vocab_coverage', 'encode_speed',
            'semantic_coherence', 'chinese_support', 'english_support'
        ]

        for metric in metrics_for_ranking:
            values = [(name, getattr(metrics, metric)) for name, metrics in all_metrics.items()]
            values.sort(key=lambda x: x[1], reverse=True)
            report['rankings'][metric] = values

        # ç”Ÿæˆæ¨è
        if all_metrics:
            best_overall = self._find_best_overall_tokenizer(all_metrics)
            report['recommendations'].append(f"ç»¼åˆæ€§èƒ½æœ€ä½³: {best_overall}")

        report['detailed_comparison'] = {
            name: asdict(metrics) for name, metrics in all_metrics.items()
        }

        return report

    def _find_best_overall_tokenizer(self, all_metrics: Dict[str, TokenizerMetrics]) -> str:
        """æ‰¾å‡ºç»¼åˆæ€§èƒ½æœ€ä½³çš„åˆ†è¯å™¨"""
        scores = {}

        # å…³é”®æŒ‡æ ‡æƒé‡
        weights = {
            'compression_ratio': 0.2,
            'encode_speed': 0.15,
            'semantic_coherence': 0.25,
            'chinese_support': 0.15,
            'english_support': 0.15,
            'training_data_efficiency': 0.1
        }

        for name, metrics in all_metrics.items():
            score = 0.0
            for metric, weight in weights.items():
                value = getattr(metrics, metric, 0.0)
                score += value * weight
            scores[name] = score

        return max(scores, key=scores.get) if scores else "æœªçŸ¥"

    def save_evaluation_report(self, results: Dict[str, Any], output_path: str):
        """ä¿å­˜è¯„ä¼°æŠ¥å‘Š (ç»“æœæŒä¹…åŒ–)"""
        report_data = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'evaluation_results': results,
            'system_info': {
                'python_version': sys.version,
                'platform': os.name
            }
        }

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2, default=str)

        print(f"ğŸ“„ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {output_path}")


def main():
    """ä¸»å‡½æ•° - ç³»ç»ŸåŒ–æ‰§è¡Œæµç¨‹"""
    parser = argparse.ArgumentParser(description='MiniGPTåˆ†è¯å™¨ç»¼åˆè¯„ä¼°ç³»ç»Ÿ')
    parser.add_argument('--tokenizer', '-t', type=str, help='å•ä¸ªåˆ†è¯å™¨æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--directory', '-d', type=str, help='åˆ†è¯å™¨ç›®å½•è·¯å¾„')
    parser.add_argument('--compare', '-c', action='store_true', help='å¯¹æ¯”æ¨¡å¼')
    parser.add_argument('--output', '-o', type=str, default='scripts/evaluation/tokenizer/reports/evaluation_report.json',
                       help='è¾“å‡ºæŠ¥å‘Šè·¯å¾„')

    args = parser.parse_args()

    print("ğŸ¯ MiniGPTåˆ†è¯å™¨ç»¼åˆè¯„ä¼°ç³»ç»Ÿ")
    print("ğŸ“‹ æ‰§è¡Œæ–¹å¼: ISTJç³»ç»ŸåŒ–é£æ ¼")
    print("ğŸ”¬ alex-ckl.com AIç ”å‘å›¢é˜Ÿ")
    print("="*70)

    evaluator = TokenizerEvaluator()

    if args.tokenizer:
        # å•ä¸ªåˆ†è¯å™¨è¯„ä¼°
        if not os.path.exists(args.tokenizer):
            print(f"âŒ åˆ†è¯å™¨æ–‡ä»¶ä¸å­˜åœ¨: {args.tokenizer}")
            return

        metrics = evaluator.evaluate_tokenizer(args.tokenizer)
        results = {'single_evaluation': asdict(metrics)}

    elif args.directory:
        # ç›®å½•æ‰¹é‡è¯„ä¼°
        if not os.path.exists(args.directory):
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {args.directory}")
            return

        # æŸ¥æ‰¾æ‰€æœ‰åˆ†è¯å™¨æ–‡ä»¶
        tokenizer_files = []
        for ext in ['.pkl', '.json']:
            tokenizer_files.extend(Path(args.directory).glob(f'**/*{ext}'))

        if not tokenizer_files:
            print(f"âŒ åœ¨ç›®å½• {args.directory} ä¸­æœªæ‰¾åˆ°åˆ†è¯å™¨æ–‡ä»¶")
            return

        tokenizer_paths = [str(f) for f in tokenizer_files]

        if args.compare:
            # å¯¹æ¯”æ¨¡å¼
            results = evaluator.compare_tokenizers(tokenizer_paths)
        else:
            # æ‰¹é‡è¯„ä¼°æ¨¡å¼
            batch_results = {}
            for path in tokenizer_paths:
                metrics = evaluator.evaluate_tokenizer(path)
                batch_results[metrics.tokenizer_name] = asdict(metrics)
            results = {'batch_evaluation': batch_results}

    else:
        # é»˜è®¤ï¼šè¯„ä¼°é¡¹ç›®ä¸­çš„æ‰€æœ‰åˆ†è¯å™¨
        print("ğŸ” è‡ªåŠ¨æœç´¢é¡¹ç›®ä¸­çš„åˆ†è¯å™¨æ–‡ä»¶...")

        # æœç´¢å¸¸è§ä½ç½®
        search_paths = [
            'tokenizers/models/',
            'checkpoints/',
            'src/tokenizer/'
        ]

        tokenizer_files = []
        for search_path in search_paths:
            if os.path.exists(search_path):
                for ext in ['.pkl', '.json']:
                    tokenizer_files.extend(Path(search_path).glob(f'**/*{ext}'))

        if tokenizer_files:
            tokenizer_paths = [str(f) for f in tokenizer_files[:5]]  # é™åˆ¶æ•°é‡é¿å…è¿‡å¤š
            print(f"ğŸ“‹ æ‰¾åˆ° {len(tokenizer_paths)} ä¸ªåˆ†è¯å™¨æ–‡ä»¶è¿›è¡Œè¯„ä¼°")
            results = evaluator.compare_tokenizers(tokenizer_paths)
        else:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•åˆ†è¯å™¨æ–‡ä»¶")
            return

    # ä¿å­˜ç»“æœ
    evaluator.save_evaluation_report(results, args.output)

    # æ‰“å°ç®€è¦æ€»ç»“
    print("\nğŸ“Š è¯„ä¼°å®Œæˆæ€»ç»“:")
    if 'single_evaluation' in results:
        metrics = results['single_evaluation']
        print(f"  åˆ†è¯å™¨: {metrics['tokenizer_name']}")
        print(f"  è¯æ±‡è¡¨å¤§å°: {metrics['vocab_size']:,}")
        print(f"  å‹ç¼©ç‡: {metrics['compression_ratio']:.2f}")
        print(f"  ä¸­æ–‡æ”¯æŒ: {metrics['chinese_support']:.2f}")
        print(f"  è‹±æ–‡æ”¯æŒ: {metrics['english_support']:.2f}")
    elif 'batch_evaluation' in results:
        print(f"  è¯„ä¼°äº† {len(results['batch_evaluation'])} ä¸ªåˆ†è¯å™¨")
    elif 'rankings' in results:
        print("  å¯¹æ¯”åˆ†æå®Œæˆï¼Œè¯¦ç»†ç»“æœè¯·æŸ¥çœ‹æŠ¥å‘Šæ–‡ä»¶")

    print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {args.output}")
    print("âœ… ç³»ç»ŸåŒ–è¯„ä¼°æµç¨‹æ‰§è¡Œå®Œæ¯•")


if __name__ == "__main__":
    main()