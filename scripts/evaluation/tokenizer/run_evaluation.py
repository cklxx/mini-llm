#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ MiniGPTåˆ†è¯å™¨ä¸€é”®è¯„æµ‹è„šæœ¬
==============================

å¿«é€Ÿå¯åŠ¨åˆ†è¯å™¨å…¨é¢è¯„ä¼°çš„ä¾¿æ·å·¥å…·
æ‰§è¡Œé£æ ¼: ISTJç³»ç»ŸåŒ–è‡ªåŠ¨åŒ–æ‰§è¡Œ

åŠŸèƒ½ç‰¹æ€§:
1. ä¸€é”®å®Œæ•´è¯„ä¼°æ‰€æœ‰è®­ç»ƒå¥½çš„åˆ†è¯å™¨
2. è‡ªåŠ¨ç”Ÿæˆå¯¹æ¯”åˆ†ææŠ¥å‘Š
3. è¾“å‡ºæ ‡å‡†åŒ–çš„è¯„ä¼°ç»“æœ
4. æ”¯æŒè‡ªå®šä¹‰è¯„ä¼°å‚æ•°
"""

import sys
import os
import argparse
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.append(str(project_root))

try:
    from scripts.evaluation.tokenizer.comprehensive_tokenizer_evaluation import TokenizerEvaluator
    from scripts.evaluation.tokenizer.comparison.tokenizer_comparison import TokenizerComparison
    from scripts.evaluation.tokenizer.benchmarks.tokenizer_benchmark import TokenizerBenchmark
except ImportError as e:
    print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿åœ¨MiniGPTé¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)


class OneClickEvaluator:
    """ä¸€é”®è¯„æµ‹ç®¡ç†å™¨"""

    def __init__(self):
        self.project_root = project_root
        self.tokenizer_dir = self.project_root / "tokenizers" / "trained_models"
        self.output_dir = self.project_root / "results" / "tokenizer_evaluation"

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)
        (self.output_dir / "charts").mkdir(exist_ok=True)
        (self.output_dir / "reports").mkdir(exist_ok=True)

    def find_tokenizers(self):
        """è‡ªåŠ¨å‘ç°å¯ç”¨çš„åˆ†è¯å™¨"""
        tokenizers = []

        if self.tokenizer_dir.exists():
            for file in self.tokenizer_dir.glob("*.pkl"):
                tokenizers.append(file)

        # å¦‚æœè®­ç»ƒç›®å½•æ²¡æœ‰åˆ†è¯å™¨ï¼Œæ£€æŸ¥checkpointsç›®å½•
        if not tokenizers:
            checkpoints_dir = self.project_root / "checkpoints"
            if checkpoints_dir.exists():
                for file in checkpoints_dir.glob("*tokenizer*.pkl"):
                    tokenizers.append(file)

        return sorted(tokenizers)

    def run_complete_evaluation(self, custom_tokenizers=None, iterations=50):
        """è¿è¡Œå®Œæ•´è¯„ä¼°æµç¨‹"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        print("ğŸš€ MiniGPTåˆ†è¯å™¨ä¸€é”®è¯„æµ‹å¯åŠ¨")
        print("=" * 50)

        # 1. å‘ç°åˆ†è¯å™¨
        tokenizers = custom_tokenizers if custom_tokenizers else self.find_tokenizers()

        if not tokenizers:
            print("âŒ æœªå‘ç°ä»»ä½•åˆ†è¯å™¨æ–‡ä»¶ï¼")
            print("è¯·æ£€æŸ¥ä»¥ä¸‹ç›®å½•:")
            print(f"   - {self.tokenizer_dir}")
            print(f"   - {self.project_root / 'checkpoints'}")
            return

        print(f"ğŸ” å‘ç° {len(tokenizers)} ä¸ªåˆ†è¯å™¨:")
        for i, tokenizer in enumerate(tokenizers, 1):
            print(f"   {i}. {tokenizer.name}")

        print()

        # 2. è¿è¡Œç»¼åˆè¯„ä¼°
        print("ğŸ“Š å¼€å§‹ç»¼åˆè¯„ä¼°...")
        evaluator = TokenizerEvaluator()
        evaluation_results = {}

        for tokenizer_path in tokenizers:
            print(f"  âš¡ è¯„ä¼°: {tokenizer_path.name}")
            try:
                result = evaluator.evaluate_tokenizer(str(tokenizer_path))
                if result:
                    evaluation_results[tokenizer_path.name] = result.__dict__
                    print(f"     âœ… å®Œæˆ")
                else:
                    print(f"     âŒ å¤±è´¥")
            except Exception as e:
                print(f"     âŒ é”™è¯¯: {e}")

        if not evaluation_results:
            print("âŒ æ‰€æœ‰åˆ†è¯å™¨è¯„ä¼°éƒ½å¤±è´¥äº†ï¼")
            return

        print(f"\nâœ… ç»¼åˆè¯„ä¼°å®Œæˆï¼ŒæˆåŠŸè¯„ä¼° {len(evaluation_results)} ä¸ªåˆ†è¯å™¨")

        # 3. ç”Ÿæˆå¯¹æ¯”åˆ†æ
        print("\nğŸ“ˆ ç”Ÿæˆå¯¹æ¯”åˆ†æ...")
        comparison = TokenizerComparison()

        # åˆ›å»ºå¯¹æ¯”çŸ©é˜µ
        df = comparison.create_comparison_matrix(evaluation_results)

        # ç”Ÿæˆå›¾è¡¨
        charts_dir = self.output_dir / "charts" / timestamp
        comparison.generate_comparison_charts(df, str(charts_dir))
        print(f"   ğŸ“Š å›¾è¡¨å·²ä¿å­˜åˆ°: {charts_dir}")

        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        report_path = self.output_dir / "reports" / f"comparison_report_{timestamp}.json"
        comparison_report = comparison.generate_comparison_report(evaluation_results, str(report_path))
        print(f"   ğŸ“‹ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

        # 4. æ˜¾ç¤ºå…³é”®ç»“æœ
        print("\nğŸ† è¯„ä¼°ç»“æœæ‘˜è¦:")
        print("-" * 40)

        for name, metrics in evaluation_results.items():
            print(f"\nğŸ“ {name}:")
            print(f"   è¯æ±‡è¡¨å¤§å°: {metrics.get('vocab_size', 'N/A'):,}")
            print(f"   å‹ç¼©ç‡: {metrics.get('compression_ratio', 0):.2f}")
            print(f"   ç¼–ç é€Ÿåº¦: {metrics.get('encode_speed', 0):.0f} tokens/sec")
            print(f"   ä¸­æ–‡æ”¯æŒ: {metrics.get('chinese_support', 0):.2f}")
            print(f"   è‹±æ–‡æ”¯æŒ: {metrics.get('english_support', 0):.2f}")

        # 5. æ˜¾ç¤ºæœ€ä½³æ¨è
        if 'analysis' in comparison_report and 'best_overall' in comparison_report['analysis']:
            best = comparison_report['analysis']['best_overall']
            print(f"\nğŸŒŸ ç»¼åˆæœ€ä½³: {best['tokenizer']}")
            print(f"   ç»¼åˆå¾—åˆ†: {best['score']:.3f}")
            if 'strengths' in best:
                print(f"   ä¼˜åŠ¿ç‰¹ç‚¹: {', '.join(best['strengths'])}")

        print(f"\nğŸ“‚ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")
        print("ğŸ‰ ä¸€é”®è¯„æµ‹å®Œæˆï¼")

        return evaluation_results, comparison_report


def main():
    """ä¸»å…¥å£å‡½æ•°"""
    parser = argparse.ArgumentParser(description="MiniGPTåˆ†è¯å™¨ä¸€é”®è¯„æµ‹å·¥å…·")
    parser.add_argument("--tokenizers", nargs="+", help="æŒ‡å®šè¦è¯„ä¼°çš„åˆ†è¯å™¨æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--iterations", type=int, default=50, help="åŸºå‡†æµ‹è¯•è¿­ä»£æ¬¡æ•° (é»˜è®¤: 50)")
    parser.add_argument("--output", help="è¾“å‡ºç›®å½• (é»˜è®¤: results/tokenizer_evaluation)")

    args = parser.parse_args()

    evaluator = OneClickEvaluator()

    # å¦‚æœæŒ‡å®šäº†è¾“å‡ºç›®å½•
    if args.output:
        evaluator.output_dir = Path(args.output)
        evaluator.output_dir.mkdir(parents=True, exist_ok=True)

    # è½¬æ¢tokenizerè·¯å¾„
    custom_tokenizers = None
    if args.tokenizers:
        custom_tokenizers = [Path(p) for p in args.tokenizers]

        # éªŒè¯æ–‡ä»¶å­˜åœ¨
        missing_files = [p for p in custom_tokenizers if not p.exists()]
        if missing_files:
            print("âŒ ä»¥ä¸‹æ–‡ä»¶ä¸å­˜åœ¨:")
            for f in missing_files:
                print(f"   {f}")
            return 1

    # è¿è¡Œè¯„ä¼°
    try:
        evaluator.run_complete_evaluation(custom_tokenizers, args.iterations)
        return 0
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­è¯„ä¼°")
        return 1
    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹å‡ºç°é”™è¯¯: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())