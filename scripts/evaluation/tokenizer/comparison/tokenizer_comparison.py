#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ“Š MiniGPTåˆ†è¯å™¨å¯¹æ¯”åˆ†æç³»ç»Ÿ
============================

ç³»ç»ŸåŒ–å¯¹æ¯”ä¸åŒåˆ†è¯å™¨çš„æ€§èƒ½ç‰¹å¾
æ‰§è¡Œé£æ ¼: ISTJè¯¦ç»†è®°å½•ä¸åˆ†æ

å¯¹æ¯”ç»´åº¦:
1. è¯æ±‡è¡¨æ•ˆç‡å¯¹æ¯”
2. è¯­è¨€æ”¯æŒèƒ½åŠ›å¯¹æ¯”
3. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
4. èµ„æºä½¿ç”¨å¯¹æ¯”
5. é€‚ç”¨åœºæ™¯åˆ†æ
"""

import sys
import os
import json
import time
from typing import Dict, List, Any, Tuple

# å¯é€‰ä¾èµ–å¯¼å…¥
try:
    import matplotlib
    matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
    import matplotlib.pyplot as plt
    import pandas as pd
    import numpy as np
    VISUALIZATION_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  å¯è§†åŒ–æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    print("ğŸ“ æç¤º: è¿è¡Œ 'pip install matplotlib pandas numpy' å®‰è£…ä¾èµ–")
    VISUALIZATION_AVAILABLE = False

sys.path.append(os.path.join(os.path.dirname(__file__), '../../../..'))


class TokenizerComparison:
    """åˆ†è¯å™¨å¯¹æ¯”åˆ†æå™¨"""

    def __init__(self):
        self.comparison_categories = [
            'compression_ratio',
            'vocab_size',
            'encode_speed',
            'chinese_support',
            'english_support',
            'memory_usage_mb'
        ]

    def create_comparison_matrix(self, evaluation_results: Dict[str, Any]):
        """åˆ›å»ºå¯¹æ¯”çŸ©é˜µ"""
        data = []

        for tokenizer_name, metrics in evaluation_results.items():
            if isinstance(metrics, dict):
                row = {'tokenizer': tokenizer_name}
                for category in self.comparison_categories:
                    row[category] = metrics.get(category, 0.0)
                data.append(row)

        if VISUALIZATION_AVAILABLE:
            import pandas as pd
            return pd.DataFrame(data)
        else:
            return data  # è¿”å›åˆ—è¡¨å­—å…¸æ ¼å¼

    def generate_comparison_charts(self, df, output_dir: str):
        """ç”Ÿæˆå¯¹æ¯”å›¾è¡¨"""
        if not VISUALIZATION_AVAILABLE:
            print("âš ï¸  å¯è§†åŒ–åŠŸèƒ½ä¸å¯ç”¨ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
            return

        os.makedirs(output_dir, exist_ok=True)

        # 1. ç»¼åˆæ€§èƒ½é›·è¾¾å›¾
        self._create_radar_chart(df, os.path.join(output_dir, 'performance_radar.png'))

        # 2. æ€§èƒ½æŒ‡æ ‡æ¡å½¢å›¾
        self._create_performance_bars(df, os.path.join(output_dir, 'performance_bars.png'))

        # 3. è¯æ±‡è¡¨æ•ˆç‡æ•£ç‚¹å›¾
        self._create_efficiency_scatter(df, os.path.join(output_dir, 'efficiency_scatter.png'))

    def _create_radar_chart(self, df, output_path: str):
        """åˆ›å»ºé›·è¾¾å›¾"""
        if df.empty:
            return

        fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))

        # é€‰æ‹©å…³é”®æŒ‡æ ‡
        metrics = ['compression_ratio', 'encode_speed', 'chinese_support', 'english_support']

        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False)
        angles = np.concatenate((angles, [angles[0]]))  # é—­åˆé›·è¾¾å›¾

        for _, row in df.iterrows():
            values = []
            for metric in metrics:
                # æ ‡å‡†åŒ–åˆ°0-1èŒƒå›´
                max_val = df[metric].max()
                normalized = row[metric] / max_val if max_val > 0 else 0
                values.append(normalized)

            values = np.concatenate((values, [values[0]]))  # é—­åˆæ•°æ®

            ax.plot(angles, values, 'o-', linewidth=2, label=row['tokenizer'])
            ax.fill(angles, values, alpha=0.25)

        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(metrics)
        ax.set_ylim(0, 1)
        ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))

        plt.title('åˆ†è¯å™¨æ€§èƒ½é›·è¾¾å›¾', size=16, pad=20)
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()

    def _create_performance_bars(self, df, output_path: str):
        """åˆ›å»ºæ€§èƒ½æ¡å½¢å›¾"""
        if df.empty:
            return

        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        axes = axes.ravel()

        metrics = ['compression_ratio', 'encode_speed', 'chinese_support', 'english_support']
        titles = ['å‹ç¼©ç‡', 'ç¼–ç é€Ÿåº¦', 'ä¸­æ–‡æ”¯æŒ', 'è‹±æ–‡æ”¯æŒ']

        for i, (metric, title) in enumerate(zip(metrics, titles)):
            ax = axes[i]
            bars = ax.bar(df['tokenizer'], df[metric])
            ax.set_title(title, fontsize=12)
            ax.tick_params(axis='x', rotation=45)

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar in bars:
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{height:.2f}', ha='center', va='bottom')

        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()

    def _create_efficiency_scatter(self, df, output_path: str):
        """åˆ›å»ºæ•ˆç‡æ•£ç‚¹å›¾"""
        if df.empty or len(df) < 2:
            return

        plt.figure(figsize=(10, 6))

        scatter = plt.scatter(df['vocab_size'], df['compression_ratio'],
                             s=df['encode_speed']*10, alpha=0.6, c=range(len(df)))

        for i, row in df.iterrows():
            plt.annotate(row['tokenizer'],
                        (row['vocab_size'], row['compression_ratio']),
                        xytext=(5, 5), textcoords='offset points')

        plt.xlabel('è¯æ±‡è¡¨å¤§å°')
        plt.ylabel('å‹ç¼©ç‡')
        plt.title('åˆ†è¯å™¨æ•ˆç‡åˆ†æ (æ°”æ³¡å¤§å°è¡¨ç¤ºç¼–ç é€Ÿåº¦)')
        plt.colorbar(scatter, label='åˆ†è¯å™¨ç¼–å·')

        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()

    def generate_comparison_report(self, evaluation_results: Dict[str, Any], output_path: str):
        """ç”Ÿæˆè¯¦ç»†å¯¹æ¯”æŠ¥å‘Š"""

        data = self.create_comparison_matrix(evaluation_results)

        if VISUALIZATION_AVAILABLE:
            df = data
            total_tokenizers = len(df)
            df_empty = df.empty
        else:
            df = data  # åˆ—è¡¨æ ¼å¼
            total_tokenizers = len(data)
            df_empty = len(data) == 0

        report = {
            'summary': {
                'total_tokenizers': total_tokenizers,
                'evaluation_date': time.strftime('%Y-%m-%d %H:%M:%S')
            },
            'rankings': {},
            'analysis': {},
            'recommendations': []
        }

        # ç”Ÿæˆå„æŒ‡æ ‡æ’å
        if VISUALIZATION_AVAILABLE and not df_empty:
            for metric in self.comparison_categories:
                if metric in df.columns:
                    ranking = df.nlargest(len(df), metric)[['tokenizer', metric]].to_dict('records')
                    report['rankings'][metric] = ranking
        else:
            # ç®€åŒ–ç‰ˆæ’å (æ— pandas)
            for metric in self.comparison_categories:
                ranking = sorted(data, key=lambda x: x.get(metric, 0), reverse=True)
                report['rankings'][metric] = [{'tokenizer': r['tokenizer'], metric: r.get(metric, 0)} for r in ranking]

        # åˆ†ææœ€ä½³é€‰æ‹©
        if not df_empty:
            # ç»¼åˆå¾—åˆ† (åŠ æƒ)
            weights = {
                'compression_ratio': 0.25,
                'encode_speed': 0.20,
                'chinese_support': 0.25,
                'english_support': 0.15,
                'memory_usage_mb': -0.15  # å†…å­˜ä½¿ç”¨è¶Šå°‘è¶Šå¥½
            }

            if VISUALIZATION_AVAILABLE:
                # ä½¿ç”¨pandasè®¡ç®—
                df_norm = df.copy()
                for metric, weight in weights.items():
                    if metric in df.columns:
                        if weight > 0:
                            df_norm[f'{metric}_score'] = df[metric] / df[metric].max() * weight
                        else:
                            df_norm[f'{metric}_score'] = (1 - df[metric] / df[metric].max()) * abs(weight)

                score_columns = [f'{m}_score' for m in weights.keys() if f'{m}_score' in df_norm.columns]
                df_norm['total_score'] = df_norm[score_columns].sum(axis=1)
                best_overall = df_norm.loc[df_norm['total_score'].idxmax()]

                report['analysis']['best_overall'] = {
                    'tokenizer': best_overall['tokenizer'],
                    'score': best_overall['total_score'],
                    'strengths': self._analyze_strengths(best_overall, df)
                }
            else:
                # ç®€åŒ–ç‰ˆè®¡ç®— (æ— pandas)
                best_tokenizer, best_score = self._calculate_best_simple(data, weights)
                if best_tokenizer:
                    report['analysis']['best_overall'] = {
                        'tokenizer': best_tokenizer,
                        'score': best_score,
                        'strengths': ['ç»¼åˆæ€§èƒ½ä¼˜å¼‚']
                    }

        # ä¿å­˜æŠ¥å‘Š
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        return report

    def _analyze_strengths(self, best_tokenizer, df) -> List[str]:
        """åˆ†ææœ€ä½³åˆ†è¯å™¨çš„ä¼˜åŠ¿"""
        if not VISUALIZATION_AVAILABLE:
            return ['ç»¼åˆæ€§èƒ½ä¼˜å¼‚']

        strengths = []

        metrics_analysis = {
            'compression_ratio': 'å‹ç¼©æ•ˆç‡',
            'encode_speed': 'ç¼–ç é€Ÿåº¦',
            'chinese_support': 'ä¸­æ–‡å¤„ç†',
            'english_support': 'è‹±æ–‡å¤„ç†'
        }

        for metric, description in metrics_analysis.items():
            if metric in df.columns and metric in best_tokenizer:
                if best_tokenizer[metric] >= df[metric].quantile(0.75):
                    strengths.append(f'{description}è¡¨ç°ä¼˜å¼‚')

        return strengths if strengths else ['ç»¼åˆæ€§èƒ½ä¼˜å¼‚']

    def _calculate_best_simple(self, data: List[Dict], weights: Dict[str, float]) -> Tuple[str, float]:
        """ç®€åŒ–ç‰ˆæœ€ä½³åˆ†è¯å™¨è®¡ç®— (æ— pandas)"""
        if not data:
            return None, 0.0

        # è®¡ç®—æ¯ä¸ªæŒ‡æ ‡çš„æœ€å¤§å€¼ç”¨äºæ ‡å‡†åŒ–
        max_values = {}
        for metric in weights.keys():
            values = [d.get(metric, 0) for d in data]
            max_values[metric] = max(values) if values else 1.0

        best_tokenizer = None
        best_score = -1

        for tokenizer_data in data:
            total_score = 0
            for metric, weight in weights.items():
                value = tokenizer_data.get(metric, 0)
                max_val = max_values[metric]

                if max_val > 0:
                    if weight > 0:
                        normalized = value / max_val
                    else:
                        normalized = 1 - (value / max_val)
                    total_score += normalized * abs(weight)

            if total_score > best_score:
                best_score = total_score
                best_tokenizer = tokenizer_data['tokenizer']

        return best_tokenizer, best_score