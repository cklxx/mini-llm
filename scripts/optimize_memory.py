#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPUå†…å­˜ä¼˜åŒ–å·¥å…·
è‡ªåŠ¨åˆ†æGPUå†…å­˜å¹¶å»ºè®®æœ€ä¼˜é…ç½®
"""
import os
import sys
import torch
import argparse

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(__file__))
sys.path.append(project_root)

from config.training_config import get_gpu_info


def analyze_memory():
    """åˆ†æå½“å‰GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    if not torch.cuda.is_available():
        print("âŒ æœªæ£€æµ‹åˆ°CUDA GPU")
        return None

    gpu_info = get_gpu_info()

    print("=" * 60)
    print("ğŸ” GPUå†…å­˜åˆ†ææŠ¥å‘Š")
    print("=" * 60)

    for device in gpu_info['devices']:
        print(f"\nğŸ“Š GPU {device['id']}: {device['name']}")
        print(f"   è®¡ç®—èƒ½åŠ›: {device['compute_capability']}")
        print(f"   æ˜¾å­˜æ€»é‡: {device['memory_total']:.2f} GB")
        print(f"   å·²åˆ†é…: {device['memory_allocated']:.2f} GB")
        print(f"   å·²ä¿ç•™: {device['memory_reserved']:.2f} GB")

        available = device['memory_total'] - device['memory_allocated']
        print(f"   å¯ç”¨æ˜¾å­˜: {available:.2f} GB")

        # è®¡ç®—å»ºè®®é…ç½®
        total_mem = device['memory_total']

        print(f"\nğŸ’¡ é’ˆå¯¹ {device['name']} çš„ä¼˜åŒ–å»ºè®®:")

        if total_mem >= 40:  # A6000, A100ç­‰
            print("   GPUç±»å‹: é«˜ç«¯è®­ç»ƒå¡")
            print("   æ¨èé…ç½® (Mediumæ¨¡å‹, 512 hidden, 16 layers):")
            print("   â€¢ batch_size: 12-16")
            print("   â€¢ gradient_accumulation_steps: 8-12")
            print("   â€¢ max_seq_len: 2048")
            print("   â€¢ æœ‰æ•ˆæ‰¹é‡: 128-192")
            print("\n   ä¿å®ˆé…ç½® (é¿å…OOM):")
            print("   â€¢ batch_size: 8")
            print("   â€¢ gradient_accumulation_steps: 16")
            print("   â€¢ max_seq_len: 1024")
        elif total_mem >= 24:  # RTX 3090/4090
            print("   GPUç±»å‹: æ¶ˆè´¹çº§é«˜ç«¯å¡")
            print("   æ¨èé…ç½® (Smallæ¨¡å‹):")
            print("   â€¢ batch_size: 8")
            print("   â€¢ gradient_accumulation_steps: 16")
            print("   â€¢ max_seq_len: 1024")
        elif total_mem >= 12:  # RTX 3060Ti/4060Ti
            print("   GPUç±»å‹: æ¶ˆè´¹çº§ä¸­ç«¯å¡")
            print("   æ¨èé…ç½® (Tinyæ¨¡å‹):")
            print("   â€¢ batch_size: 4")
            print("   â€¢ gradient_accumulation_steps: 32")
            print("   â€¢ max_seq_len: 512")
        else:
            print("   GPUç±»å‹: å…¥é—¨çº§")
            print("   å»ºè®®ä½¿ç”¨CPUè®­ç»ƒæˆ–é€‰æ‹©æ›´å°çš„æ¨¡å‹")

        print("\nğŸ› ï¸  é¢å¤–ä¼˜åŒ–æŠ€å·§:")
        print("   1. å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (FP16/BF16)")
        print("   2. ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ (gradient checkpointing)")
        print("   3. è®¾ç½®ç¯å¢ƒå˜é‡:")
        print("      export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True")
        print("   4. å‡å°æ¨¡å‹ç»´åº¦ (d_model, n_layers)")
        print("   5. ä½¿ç”¨æ›´å°çš„è¯æ±‡è¡¨å¤§å°")

    return gpu_info


def suggest_config(model_size="medium"):
    """æ ¹æ®GPUå»ºè®®é…ç½®"""
    if not torch.cuda.is_available():
        print("âŒ æœªæ£€æµ‹åˆ°CUDA GPU")
        return

    gpu_info = get_gpu_info()
    device = gpu_info['devices'][0]
    total_mem = device['memory_total']

    print("\n" + "=" * 60)
    print(f"ğŸ“‹ {model_size.upper()}æ¨¡å‹æ¨èè®­ç»ƒå‘½ä»¤")
    print("=" * 60)

    if model_size == "medium":
        if total_mem >= 40:
            cmd = """
# A6000/A100 - ä¿å®ˆé…ç½®
python scripts/train.py \\
    --mode pretrain \\
    --config medium \\
    --batch-size 12 \\
    --max-steps 50000

# å¦‚æœä»ç„¶OOMï¼Œè¿›ä¸€æ­¥é™ä½batch_size:
python scripts/train.py \\
    --mode pretrain \\
    --config medium \\
    --batch-size 8 \\
    --max-steps 50000
"""
        elif total_mem >= 24:
            cmd = """
# RTX 3090/4090
python scripts/train.py \\
    --mode pretrain \\
    --config small \\
    --batch-size 8 \\
    --max-steps 50000
"""
        else:
            cmd = """
# æ˜¾å­˜ä¸è¶³ï¼Œå»ºè®®ä½¿ç”¨smallæˆ–tinyé…ç½®
python scripts/train.py \\
    --mode pretrain \\
    --config small \\
    --batch-size 4 \\
    --max-steps 50000
"""

    print(cmd)

    print("\nğŸ”§ ç¯å¢ƒå˜é‡ä¼˜åŒ–:")
    print("export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True")
    print("export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512")


def check_oom_solutions():
    """æ˜¾ç¤ºOOMé—®é¢˜çš„è§£å†³æ–¹æ¡ˆ"""
    print("\n" + "=" * 60)
    print("ğŸš¨ CUDA OOM å¸¸è§è§£å†³æ–¹æ¡ˆ")
    print("=" * 60)

    solutions = [
        ("é™ä½batch size", "ä»32 â†’ 16 â†’ 8 â†’ 4 é€æ­¥é™ä½"),
        ("å¢åŠ æ¢¯åº¦ç´¯ç§¯", "ä¿æŒæœ‰æ•ˆbatch sizeï¼Œé™ä½å†…å­˜å³°å€¼"),
        ("å‡å°åºåˆ—é•¿åº¦", "ä»2048 â†’ 1024 â†’ 512"),
        ("ä½¿ç”¨æ›´å°çš„æ¨¡å‹", "å‡å°‘å±‚æ•°æˆ–éšè—ç»´åº¦"),
        ("å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹", "ç‰ºç‰²é€Ÿåº¦æ¢å†…å­˜"),
        ("ä½¿ç”¨æ··åˆç²¾åº¦", "FP16/BF16è®­ç»ƒ"),
        ("æ¸…ç†GPUç¼“å­˜", "torch.cuda.empty_cache()"),
        ("è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥", "PYTORCH_CUDA_ALLOC_CONF"),
    ]

    for i, (solution, detail) in enumerate(solutions, 1):
        print(f"\n{i}. {solution}")
        print(f"   â†’ {detail}")

    print("\n" + "=" * 60)
    print("ğŸ“ å®é™…æ“ä½œæ­¥éª¤:")
    print("=" * 60)

    print("""
1. ç«‹å³å°è¯• (æ— éœ€é‡å¯):
   python scripts/train.py --mode pretrain --config medium --batch-size 8

2. å¦‚æœä»ç„¶OOM:
   python scripts/train.py --mode pretrain --config medium --batch-size 4

3. è®¾ç½®ç¯å¢ƒå˜é‡åé‡è¯•:
   export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
   python scripts/train.py --mode pretrain --config medium --batch-size 8

4. ç»ˆææ–¹æ¡ˆ - ä½¿ç”¨smallé…ç½®:
   python scripts/train.py --mode pretrain --config small --batch-size 8
""")


def main():
    parser = argparse.ArgumentParser(description="GPUå†…å­˜ä¼˜åŒ–å·¥å…·")
    parser.add_argument("--analyze", action="store_true", help="åˆ†æå½“å‰GPUå†…å­˜")
    parser.add_argument("--suggest", type=str, choices=["tiny", "small", "medium"],
                       help="å»ºè®®æŒ‡å®šæ¨¡å‹çš„é…ç½®")
    parser.add_argument("--oom-help", action="store_true", help="æ˜¾ç¤ºOOMè§£å†³æ–¹æ¡ˆ")

    args = parser.parse_args()

    if args.analyze:
        analyze_memory()
    elif args.suggest:
        analyze_memory()
        suggest_config(args.suggest)
    elif args.oom_help:
        check_oom_solutions()
    else:
        # é»˜è®¤æ˜¾ç¤ºæ‰€æœ‰ä¿¡æ¯
        analyze_memory()
        check_oom_solutions()


if __name__ == "__main__":
    main()
