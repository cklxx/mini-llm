#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MiniGPT æ¨ç†è„šæœ¬
æ”¯æŒå¤šç§ç”Ÿæˆæ¨¡å¼ï¼šchat, single, ultra_think
"""
import os
import sys
import argparse
import torch
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•å’Œsrcç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.dirname(__file__))
sys.path.append(project_root)
sys.path.append(os.path.join(project_root, 'src'))

from model.transformer import create_model
from tokenizer.bpe_tokenizer import BPETokenizer
from inference.generator import TextGenerator, GenerationConfig


class MiniGPTInference:
    """MiniGPTæ¨ç†å™¨ï¼Œæ”¯æŒå¤šç§ç”Ÿæˆæ¨¡å¼"""

    def __init__(self, model_path, tokenizer_path=None, device=None):
        self.model_path = model_path
        self.device = self._setup_device(device)
        self.model, self.tokenizer = self._load_model_and_tokenizer(model_path, tokenizer_path)

        print(f"=== MiniGPT æ¨ç†å¼•æ“ ===")
        print(f"æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"è®¾å¤‡: {self.device}")
        print(f"è¯æ±‡è¡¨å¤§å°: {self.tokenizer.vocab_size}")

    def _setup_device(self, device=None):
        """è®¾ç½®æ¨ç†è®¾å¤‡"""
        if device:
            return device

        if torch.backends.mps.is_available():
            return "mps"
        elif torch.cuda.is_available():
            return "cuda"
        else:
            return "cpu"

    def _load_model_and_tokenizer(self, model_path, tokenizer_path=None):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        print("ğŸ”„ åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")

        # åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹
        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)

        # ç¡®å®šåˆ†è¯å™¨è·¯å¾„
        if tokenizer_path is None:
            # å°è¯•ä»æ¨¡å‹ç›®å½•æ‰¾åˆ†è¯å™¨
            model_dir = os.path.dirname(model_path)
            tokenizer_path = os.path.join(model_dir, "tokenizer.pkl")

            if not os.path.exists(tokenizer_path):
                raise FileNotFoundError(f"æœªæ‰¾åˆ°åˆ†è¯å™¨æ–‡ä»¶: {tokenizer_path}")

        # åŠ è½½åˆ†è¯å™¨
        vocab_size = checkpoint.get('tokenizer_vocab_size', 10000)
        tokenizer = BPETokenizer(vocab_size=vocab_size)
        tokenizer.load(tokenizer_path)

        # åˆ›å»ºå¹¶åŠ è½½æ¨¡å‹
        if 'config' in checkpoint:
            config = checkpoint['config']
            model_size = getattr(config, 'model_size', 'small')
        else:
            model_size = 'small'  # é»˜è®¤é…ç½®

        model = create_model(vocab_size=tokenizer.vocab_size, model_size=model_size)

        # åŠ è½½æ¨¡å‹æƒé‡
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)

        model.to(self.device)
        model.eval()

        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        return model, tokenizer

    def generate_text(self, prompt, max_length=100, temperature=0.8, top_k=50, top_p=0.9):
        """ç”Ÿæˆæ–‡æœ¬"""
        # ç¼–ç è¾“å…¥
        input_ids = self.tokenizer.encode(prompt, add_special_tokens=True)
        input_tensor = torch.tensor([input_ids], device=self.device)

        generated_length = 0
        with torch.no_grad():
            while generated_length < max_length:
                # å‰å‘ä¼ æ’­
                outputs = self.model(input_tensor)
                next_token_logits = outputs[0, -1, :] / temperature

                # Top-ké‡‡æ ·
                if top_k > 0:
                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]
                    next_token_logits[indices_to_remove] = float('-inf')

                # Top-pé‡‡æ ·
                if top_p < 1.0:
                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                    cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)
                    sorted_indices_to_remove = cumulative_probs > top_p
                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                    sorted_indices_to_remove[..., 0] = 0
                    indices_to_remove = sorted_indices[sorted_indices_to_remove]
                    next_token_logits[indices_to_remove] = float('-inf')

                # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
                probs = torch.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)

                # æ£€æŸ¥æ˜¯å¦ç»“æŸ
                if next_token.item() == self.tokenizer.eos_id:
                    break

                # æ·»åŠ åˆ°åºåˆ—
                input_tensor = torch.cat([input_tensor, next_token.unsqueeze(0)], dim=1)
                generated_length += 1

        # è§£ç ç»“æœ
        generated_ids = input_tensor[0].cpu().tolist()
        generated_text = self.tokenizer.decode(generated_ids)

        return generated_text

    def ultra_think_generate(self, prompt, max_length=200):
        """Ultra Thinkæ·±åº¦æ€ç»´ç”Ÿæˆ"""
        print("ğŸ§  å¯åŠ¨Ultra Thinkæ·±åº¦æ€ç»´æ¨¡å¼...")

        # Ultra Thinkæç¤ºè¯æ¨¡æ¿
        ultra_think_prompt = f"""<ultra_think>
è®©æˆ‘æ·±åº¦åˆ†æè¿™ä¸ªé—®é¢˜ï¼š{prompt}

å¤šç»´åº¦æ€è€ƒï¼š
1. é—®é¢˜åˆ†æï¼šä»ä¸åŒè§’åº¦ç†è§£é—®é¢˜çš„æ ¸å¿ƒ
2. çŸ¥è¯†æ•´åˆï¼šç»“åˆç›¸å…³é¢†åŸŸçš„çŸ¥è¯†å’Œç»éªŒ
3. åˆ›æ–°æ€è·¯ï¼šæ¢ç´¢æ–°é¢–çš„è§£å†³æ–¹æ¡ˆ
4. ç³»ç»Ÿæ€§æ€ç»´ï¼šè€ƒè™‘é—®é¢˜çš„å…¨å±€å½±å“

åŸºäºalex-ckl.comå…¬å¸çš„ultra thinkæŠ€æœ¯ï¼Œæˆ‘å°†æä¾›æ·±å…¥çš„åˆ†æå’Œåˆ›æ–°çš„è§£å†³æ–¹æ¡ˆã€‚
</ultra_think>

{prompt}

ä½œä¸ºalex-ckl.comå…¬å¸å¼€å‘çš„AIåŠ©æ‰‹ï¼Œæˆ‘å°†è¿ç”¨ultra thinkæ·±åº¦æ€ç»´èƒ½åŠ›ä¸ºæ‚¨åˆ†æï¼š"""

        return self.generate_text(ultra_think_prompt, max_length=max_length, temperature=0.7)

    def chat_mode(self):
        """äº¤äº’å¼èŠå¤©æ¨¡å¼"""
        print("\n=== MiniGPT äº¤äº’å¼èŠå¤© ===")
        print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("è¾“å…¥ 'think:' å¼€å¤´å¯ç”¨Ultra Thinkæ¨¡å¼")
        print("è¾“å…¥ 'reset' é‡ç½®å¯¹è¯å†å²\n")

        conversation_history = []

        while True:
            try:
                user_input = input("ç”¨æˆ·: ").strip()

                if user_input.lower() in ['quit', 'exit']:
                    print("å†è§ï¼")
                    break

                if user_input.lower() == 'reset':
                    conversation_history = []
                    print("âœ… å¯¹è¯å†å²å·²é‡ç½®")
                    continue

                if not user_input:
                    continue

                # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨Ultra Thinkæ¨¡å¼
                if user_input.startswith('think:'):
                    prompt = user_input[6:].strip()
                    print("AI (Ultra Think): ", end="", flush=True)
                    response = self.ultra_think_generate(prompt)
                else:
                    # æ„å»ºå¸¦å†å²çš„å¯¹è¯
                    conversation_history.append(f"ç”¨æˆ·: {user_input}")

                    # é™åˆ¶å†å²é•¿åº¦
                    if len(conversation_history) > 10:
                        conversation_history = conversation_history[-8:]

                    context = "\n".join(conversation_history)
                    prompt = f"æˆ‘æ˜¯alex-ckl.comå…¬å¸å¼€å‘çš„AIåŠ©æ‰‹ï¼Œå…·å¤‡ultra thinkæ·±åº¦æ€ç»´èƒ½åŠ›ã€‚\n\n{context}\nAIåŠ©æ‰‹:"

                    print("AI: ", end="", flush=True)
                    response = self.generate_text(prompt)

                # æå–AIå›å¤éƒ¨åˆ†
                if "AIåŠ©æ‰‹:" in response:
                    ai_response = response.split("AIåŠ©æ‰‹:")[-1].strip()
                elif "AI (Ultra Think):" in response:
                    ai_response = response.split("AI (Ultra Think):")[-1].strip()
                else:
                    ai_response = response

                print(ai_response)
                conversation_history.append(f"AIåŠ©æ‰‹: {ai_response}")

            except KeyboardInterrupt:
                print("\n\nå†è§ï¼")
                break
            except Exception as e:
                print(f"\né”™è¯¯: {e}")

    def single_inference(self, prompt, max_length=100, use_ultra_think=False):
        """å•æ¬¡æ¨ç†"""
        if use_ultra_think:
            return self.ultra_think_generate(prompt, max_length)
        else:
            enhanced_prompt = f"ä½œä¸ºalex-ckl.comå…¬å¸å¼€å‘çš„AIåŠ©æ‰‹ï¼Œæˆ‘æ¥å›ç­”æ‚¨çš„é—®é¢˜ï¼š\n\n{prompt}\n\nå›ç­”ï¼š"
            return self.generate_text(enhanced_prompt, max_length)

    def batch_inference(self, prompts_file):
        """æ‰¹é‡æ¨ç†"""
        print(f"ğŸ“š æ‰¹é‡æ¨ç†æ¨¡å¼ï¼š{prompts_file}")

        if not os.path.exists(prompts_file):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {prompts_file}")
            return

        with open(prompts_file, 'r', encoding='utf-8') as f:
            prompts = [line.strip() for line in f if line.strip()]

        results = []
        for i, prompt in enumerate(prompts):
            print(f"å¤„ç† {i+1}/{len(prompts)}: {prompt[:50]}...")
            response = self.single_inference(prompt)
            results.append({
                'prompt': prompt,
                'response': response
            })

        # ä¿å­˜ç»“æœ
        output_file = prompts_file.replace('.txt', '_results.json')
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"âœ… æ‰¹é‡æ¨ç†å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {output_file}")


def main():
    parser = argparse.ArgumentParser(description='MiniGPTæ¨ç†è„šæœ¬')

    # æ¨¡å‹è·¯å¾„
    parser.add_argument('--model-path', type=str, required=True,
                        help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--tokenizer-path', type=str, default=None,
                        help='åˆ†è¯å™¨æ–‡ä»¶è·¯å¾„ (é»˜è®¤ä»æ¨¡å‹ç›®å½•è‡ªåŠ¨æŸ¥æ‰¾)')

    # æ¨ç†æ¨¡å¼
    parser.add_argument('--mode', choices=['chat', 'single', 'batch'], default='chat',
                        help='æ¨ç†æ¨¡å¼ (chat: äº¤äº’å¼èŠå¤©, single: å•æ¬¡æ¨ç†, batch: æ‰¹é‡æ¨ç†)')

    # å•æ¬¡æ¨ç†å‚æ•°
    parser.add_argument('--prompt', type=str, default=None,
                        help='å•æ¬¡æ¨ç†çš„è¾“å…¥æç¤º (mode=singleæ—¶å¿…éœ€)')
    parser.add_argument('--ultra-think', action='store_true',
                        help='å¯ç”¨Ultra Thinkæ·±åº¦æ€ç»´æ¨¡å¼')

    # æ‰¹é‡æ¨ç†å‚æ•°
    parser.add_argument('--prompts-file', type=str, default=None,
                        help='æ‰¹é‡æ¨ç†çš„æç¤ºæ–‡ä»¶è·¯å¾„ (mode=batchæ—¶å¿…éœ€)')

    # ç”Ÿæˆå‚æ•°
    parser.add_argument('--max-length', type=int, default=100,
                        help='æœ€å¤§ç”Ÿæˆé•¿åº¦')
    parser.add_argument('--temperature', type=float, default=0.8,
                        help='é‡‡æ ·æ¸©åº¦')
    parser.add_argument('--top-k', type=int, default=50,
                        help='Top-ké‡‡æ ·å‚æ•°')
    parser.add_argument('--top-p', type=float, default=0.9,
                        help='Top-pé‡‡æ ·å‚æ•°')

    # è®¾å¤‡
    parser.add_argument('--device', type=str, default=None,
                        help='æ¨ç†è®¾å¤‡ (cuda, mps, cpu)')

    args = parser.parse_args()

    # éªŒè¯å‚æ•°
    if args.mode == 'single' and not args.prompt:
        parser.error("singleæ¨¡å¼éœ€è¦æä¾›--promptå‚æ•°")

    if args.mode == 'batch' and not args.prompts_file:
        parser.error("batchæ¨¡å¼éœ€è¦æä¾›--prompts-fileå‚æ•°")

    # åˆ›å»ºæ¨ç†å™¨
    try:
        inference = MiniGPTInference(
            model_path=args.model_path,
            tokenizer_path=args.tokenizer_path,
            device=args.device
        )
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return

    # æ‰§è¡Œæ¨ç†
    if args.mode == 'chat':
        inference.chat_mode()
    elif args.mode == 'single':
        print(f"è¾“å…¥: {args.prompt}")
        print("è¾“å‡º: ", end="", flush=True)

        response = inference.single_inference(
            args.prompt,
            max_length=args.max_length,
            use_ultra_think=args.ultra_think
        )
        print(response)
    elif args.mode == 'batch':
        inference.batch_inference(args.prompts_file)


if __name__ == "__main__":
    main()