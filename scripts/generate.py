#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MiniGPT æ¨ç†è„šæœ¬
æ”¯æŒå¤šç§ç”Ÿæˆæ¨¡å¼ï¼šchat, single, ultra_think
"""
import os
import sys
import argparse
import torch
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•å’Œsrcç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.dirname(__file__))
sys.path.append(project_root)
sys.path.append(os.path.join(project_root, 'src'))

from model.transformer import create_model
from tokenizer.bpe_tokenizer import BPETokenizer


class MiniGPTInference:
    """MiniGPTæ¨ç†å™¨ï¼Œæ”¯æŒå¤šç§ç”Ÿæˆæ¨¡å¼"""

    def __init__(self, model_path, tokenizer_path=None, device=None, generation_kwargs=None):
        self.model_path = model_path
        self.device = self._setup_device(device)
        self.model, self.tokenizer = self._load_model_and_tokenizer(model_path, tokenizer_path)

        defaults = {
            "max_new_tokens": 128,
            "temperature": 0.7,
            "top_k": 50,
            "top_p": 0.9,
            "repetition_penalty": 1.05,
        }
        if generation_kwargs:
            defaults.update({k: v for k, v in generation_kwargs.items() if v is not None})
        self.generation_defaults = defaults

        print(f"=== MiniGPT æ¨ç†å¼•æ“ ===")
        print(f"æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"è®¾å¤‡: {self.device}")
        print(f"è¯æ±‡è¡¨å¤§å°: {self.tokenizer.vocab_size}")

    def _setup_device(self, device=None):
        """è®¾ç½®æ¨ç†è®¾å¤‡"""
        if device:
            return device

        if torch.backends.mps.is_available():
            return "mps"
        elif torch.cuda.is_available():
            return "cuda"
        else:
            return "cpu"

    def _load_model_and_tokenizer(self, model_path, tokenizer_path=None):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        print("ğŸ”„ åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")

        # åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹
        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)

        # ç¡®å®šåˆ†è¯å™¨è·¯å¾„
        if tokenizer_path is None:
            # å°è¯•ä»æ¨¡å‹ç›®å½•æ‰¾åˆ†è¯å™¨
            model_dir = os.path.dirname(model_path)
            tokenizer_path = os.path.join(model_dir, "tokenizer.pkl")

            if not os.path.exists(tokenizer_path):
                raise FileNotFoundError(f"æœªæ‰¾åˆ°åˆ†è¯å™¨æ–‡ä»¶: {tokenizer_path}")

        # åŠ è½½åˆ†è¯å™¨
        vocab_size = checkpoint.get('tokenizer_vocab_size', 10000)
        tokenizer = BPETokenizer(vocab_size=vocab_size)
        tokenizer.load(tokenizer_path)

        expected_config = checkpoint.get('tokenizer_config')
        if expected_config:
            actual_config = tokenizer.get_config()
            mismatches = {
                key: (expected_config[key], actual_config.get(key))
                for key in expected_config
                if actual_config.get(key) != expected_config[key]
            }
            if mismatches:
                mismatch_info = ", ".join(
                    f"{k}: ckpt={v[0]} vs tokenizer={v[1]}" for k, v in mismatches.items()
                )
                raise ValueError(
                    "åˆ†è¯å™¨é…ç½®ä¸checkpointä¸ä¸€è‡´ï¼Œè¯·ç¡®è®¤æ¨ç†ç«¯ä½¿ç”¨çš„tokenizeræ–‡ä»¶æ­£ç¡®ã€‚"
                    f"å·®å¼‚: {mismatch_info}"
                )

        expected_special = checkpoint.get('tokenizer_special_tokens')
        if expected_special:
            special_mismatches = tokenizer.diff_special_tokens(expected_special)
            if special_mismatches:
                mismatch_info = ", ".join(
                    f"{name}: ckpt={exp} vs tokenizer={act}" for name, (exp, act) in special_mismatches.items()
                )
                raise ValueError(
                    "åˆ†è¯å™¨ç‰¹æ®Štokenæ˜ å°„ä¸checkpointä¸ä¸€è‡´ï¼Œè¯·æ£€æŸ¥ tokenizer.pkl æ˜¯å¦åŒ¹é…è®­ç»ƒè¾“å‡ºã€‚"
                    f"å·®å¼‚: {mismatch_info}"
                )

        expected_checksum = checkpoint.get('tokenizer_checksum')
        if expected_checksum:
            actual_checksum = tokenizer.checksum()
            if actual_checksum and actual_checksum != expected_checksum:
                raise ValueError(
                    "åˆ†è¯å™¨æ ¡éªŒå¤±è´¥ï¼šchecksumä¸åŒ¹é…ã€‚è¯·ç¡®ä¿ä½¿ç”¨è®­ç»ƒæ—¶å¯¼å‡ºçš„tokenizer.pklã€‚"
                )
        print("âœ… åˆ†è¯å™¨é…ç½®æ ¡éªŒé€šè¿‡")

        # åˆ›å»ºå¹¶åŠ è½½æ¨¡å‹
        if 'config' in checkpoint:
            config = checkpoint['config']
            model_size = getattr(config, 'model_size', 'small')
        else:
            model_size = 'small'  # é»˜è®¤é…ç½®

        model = create_model(vocab_size=tokenizer.vocab_size, model_size=model_size)

        # åŠ è½½æ¨¡å‹æƒé‡
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)

        model.to(self.device)
        model.eval()

        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        return model, tokenizer

    def generate_text(self, prompt, **overrides):
        """ç”Ÿæˆæ–‡æœ¬"""

        config = self.generation_defaults.copy()
        config.update({k: v for k, v in overrides.items() if v is not None})

        max_new_tokens = int(config.get("max_new_tokens", 0))
        temperature = float(config.get("temperature", 1.0))
        top_k = int(config.get("top_k", 0))
        top_p = float(config.get("top_p", 1.0))
        repetition_penalty = float(config.get("repetition_penalty", 1.0))

        if max_new_tokens <= 0:
            return ""

        input_ids = self.tokenizer.encode(prompt, add_special_tokens=True)
        prompt_length = len(input_ids)
        generated_ids = list(input_ids)
        input_tensor = torch.tensor([generated_ids], device=self.device, dtype=torch.long)

        with torch.no_grad():
            for _ in range(max_new_tokens):
                outputs = self.model(input_tensor)
                next_token_logits = outputs[0, -1, :]

                adjusted_logits = next_token_logits.clone()
                if temperature > 0:
                    adjusted_logits = adjusted_logits / temperature

                if repetition_penalty and repetition_penalty != 1.0:
                    for token_id in set(generated_ids):
                        logit = adjusted_logits[token_id]
                        if logit < 0:
                            adjusted_logits[token_id] *= repetition_penalty
                        else:
                            adjusted_logits[token_id] /= repetition_penalty

                if top_k and top_k > 0:
                    values, _ = torch.topk(adjusted_logits, min(top_k, adjusted_logits.size(-1)))
                    min_values = values[..., -1, None]
                    adjusted_logits[adjusted_logits < min_values] = float('-inf')

                if top_p and 0 < top_p < 1.0:
                    sorted_logits, sorted_indices = torch.sort(adjusted_logits, descending=True)
                    cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)
                    sorted_indices_to_remove = cumulative_probs > top_p
                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                    sorted_indices_to_remove[..., 0] = 0
                    indices_to_remove = sorted_indices[sorted_indices_to_remove]
                    adjusted_logits[indices_to_remove] = float('-inf')

                if temperature <= 0:
                    next_token_id = int(torch.argmax(adjusted_logits).item())
                else:
                    probs = torch.softmax(adjusted_logits, dim=-1)
                    next_token_id = int(torch.multinomial(probs, num_samples=1).item())

                generated_ids.append(next_token_id)
                next_token_tensor = torch.tensor([[next_token_id]], device=self.device, dtype=torch.long)
                input_tensor = torch.cat([input_tensor, next_token_tensor], dim=1)

                if next_token_id == self.tokenizer.eos_id:
                    break

        new_token_ids = generated_ids[prompt_length:]
        generated_text = self.tokenizer.decode(new_token_ids)
        return generated_text.strip()

    def ultra_think_generate(self, prompt, **generation_kwargs):
        """Ultra Thinkæ·±åº¦æ€ç»´ç”Ÿæˆ"""
        print("ğŸ§  å¯åŠ¨Ultra Thinkæ·±åº¦æ€ç»´æ¨¡å¼...")

        ultra_think_prompt = f"""<ultra_think>
æˆ‘å°†é€æ­¥æ·±å…¥åˆ†æè¯¥é—®é¢˜ï¼š{prompt}

æ€è€ƒæ–¹å‘ï¼š
1. æ ¸å¿ƒé—®é¢˜ä¸èƒŒæ™¯
2. ç›¸å…³çŸ¥è¯†ä¸äº‹å®
3. æ½œåœ¨æ–¹æ¡ˆä¸åˆ©å¼Š
4. é•¿æœŸå½±å“ä¸å»¶ä¼¸æ€è€ƒ
</ultra_think>

è¯·æ ¹æ®ä»¥ä¸Šåˆ†æç»™å‡ºæœ€ç»ˆå›ç­”ï¼š"""

        max_tokens = generation_kwargs.pop(
            "max_new_tokens",
            max(self.generation_defaults.get("max_new_tokens", 128), 200),
        )
        return self.generate_text(
            ultra_think_prompt,
            max_new_tokens=max_tokens,
            **generation_kwargs,
        )

    def chat_mode(self):
        """äº¤äº’å¼èŠå¤©æ¨¡å¼"""
        print("\n=== MiniGPT äº¤äº’å¼èŠå¤© ===")
        print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("è¾“å…¥ 'think:' å¼€å¤´å¯ç”¨Ultra Thinkæ¨¡å¼")
        print("è¾“å…¥ 'reset' é‡ç½®å¯¹è¯å†å²\n")

        conversation_history = []

        while True:
            try:
                user_input = input("ç”¨æˆ·: ").strip()

                if user_input.lower() in ['quit', 'exit']:
                    print("å†è§ï¼")
                    break

                if user_input.lower() == 'reset':
                    conversation_history = []
                    print("âœ… å¯¹è¯å†å²å·²é‡ç½®")
                    continue

                if not user_input:
                    continue

                # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨Ultra Thinkæ¨¡å¼
                if user_input.startswith('think:'):
                    prompt = user_input[6:].strip()
                    print("AI (Ultra Think): ", end="", flush=True)
                    response = self.ultra_think_generate(prompt)
                else:
                    # æ„å»ºå¸¦å†å²çš„å¯¹è¯
                    conversation_history.append(f"ç”¨æˆ·: {user_input}")

                    # é™åˆ¶å†å²é•¿åº¦
                    if len(conversation_history) > 10:
                        conversation_history = conversation_history[-8:]

                    context = "\n".join(conversation_history)
                    prompt = f"{context}\nåŠ©æ‰‹:"

                    print("åŠ©æ‰‹: ", end="", flush=True)
                    response = self.generate_text(prompt)

                # æå–AIå›å¤éƒ¨åˆ†
                ai_response = response.strip()
                if ai_response.lower().startswith("åŠ©æ‰‹:"):
                    ai_response = ai_response.split("åŠ©æ‰‹:", 1)[-1].strip()

                print(ai_response)
                conversation_history.append(f"åŠ©æ‰‹: {ai_response}")

            except KeyboardInterrupt:
                print("\n\nå†è§ï¼")
                break
            except Exception as e:
                print(f"\né”™è¯¯: {e}")

    def single_inference(self, prompt, use_ultra_think=False, **generation_kwargs):
        """å•æ¬¡æ¨ç†"""
        if use_ultra_think:
            return self.ultra_think_generate(prompt, **generation_kwargs)
        return self.generate_text(prompt, **generation_kwargs)

    def batch_inference(self, prompts_file, **generation_kwargs):
        """æ‰¹é‡æ¨ç†"""
        print(f"ğŸ“š æ‰¹é‡æ¨ç†æ¨¡å¼ï¼š{prompts_file}")

        if not os.path.exists(prompts_file):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {prompts_file}")
            return

        with open(prompts_file, 'r', encoding='utf-8') as f:
            prompts = [line.strip() for line in f if line.strip()]

        results = []
        for i, prompt in enumerate(prompts):
            print(f"å¤„ç† {i+1}/{len(prompts)}: {prompt[:50]}...")
            response = self.single_inference(prompt, **generation_kwargs)
            results.append({
                'prompt': prompt,
                'response': response
            })

        # ä¿å­˜ç»“æœ
        output_file = prompts_file.replace('.txt', '_results.json')
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"âœ… æ‰¹é‡æ¨ç†å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {output_file}")


def main():
    parser = argparse.ArgumentParser(description='MiniGPTæ¨ç†è„šæœ¬')

    # æ¨¡å‹è·¯å¾„
    parser.add_argument('--model-path', type=str, required=True,
                        help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--tokenizer-path', type=str, default=None,
                        help='åˆ†è¯å™¨æ–‡ä»¶è·¯å¾„ (é»˜è®¤ä»æ¨¡å‹ç›®å½•è‡ªåŠ¨æŸ¥æ‰¾)')

    # æ¨ç†æ¨¡å¼
    parser.add_argument('--mode', choices=['chat', 'single', 'batch'], default='chat',
                        help='æ¨ç†æ¨¡å¼ (chat: äº¤äº’å¼èŠå¤©, single: å•æ¬¡æ¨ç†, batch: æ‰¹é‡æ¨ç†)')

    # å•æ¬¡æ¨ç†å‚æ•°
    parser.add_argument('--prompt', type=str, default=None,
                        help='å•æ¬¡æ¨ç†çš„è¾“å…¥æç¤º (mode=singleæ—¶å¿…éœ€)')
    parser.add_argument('--ultra-think', action='store_true',
                        help='å¯ç”¨Ultra Thinkæ·±åº¦æ€ç»´æ¨¡å¼')

    # æ‰¹é‡æ¨ç†å‚æ•°
    parser.add_argument('--prompts-file', type=str, default=None,
                        help='æ‰¹é‡æ¨ç†çš„æç¤ºæ–‡ä»¶è·¯å¾„ (mode=batchæ—¶å¿…éœ€)')

    # ç”Ÿæˆå‚æ•°
    parser.add_argument('--max-new-tokens', '--max-length', type=int, default=128, dest='max_new_tokens',
                        help='æœ€å¤§ç”Ÿæˆtokenæ•° (åŒ…å«åˆ«å --max-length)')
    parser.add_argument('--temperature', type=float, default=0.7,
                        help='é‡‡æ ·æ¸©åº¦')
    parser.add_argument('--top-k', type=int, default=50,
                        help='Top-ké‡‡æ ·å‚æ•°')
    parser.add_argument('--top-p', type=float, default=0.9,
                        help='Top-pé‡‡æ ·å‚æ•°')
    parser.add_argument('--repetition-penalty', type=float, default=1.05,
                        help='é‡å¤æƒ©ç½šç³»æ•° (>1 ä¼šæŠ‘åˆ¶é‡å¤)')

    # è®¾å¤‡
    parser.add_argument('--device', type=str, default=None,
                        help='æ¨ç†è®¾å¤‡ (cuda, mps, cpu)')

    args = parser.parse_args()

    # éªŒè¯å‚æ•°
    if args.mode == 'single' and not args.prompt:
        parser.error("singleæ¨¡å¼éœ€è¦æä¾›--promptå‚æ•°")

    if args.mode == 'batch' and not args.prompts_file:
        parser.error("batchæ¨¡å¼éœ€è¦æä¾›--prompts-fileå‚æ•°")

    # åˆ›å»ºæ¨ç†å™¨
    try:
        generation_kwargs = {
            "max_new_tokens": args.max_new_tokens,
            "temperature": args.temperature,
            "top_k": args.top_k,
            "top_p": args.top_p,
            "repetition_penalty": args.repetition_penalty,
        }
        inference = MiniGPTInference(
            model_path=args.model_path,
            tokenizer_path=args.tokenizer_path,
            device=args.device,
            generation_kwargs=generation_kwargs,
        )
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return

    # æ‰§è¡Œæ¨ç†
    if args.mode == 'chat':
        inference.chat_mode()
    elif args.mode == 'single':
        print(f"è¾“å…¥: {args.prompt}")
        print("è¾“å‡º: ", end="", flush=True)

        response = inference.single_inference(
            args.prompt,
            use_ultra_think=args.ultra_think,
            max_new_tokens=args.max_new_tokens,
            temperature=args.temperature,
            top_k=args.top_k,
            top_p=args.top_p,
            repetition_penalty=args.repetition_penalty,
        )
        print(response)
    elif args.mode == 'batch':
        inference.batch_inference(
            args.prompts_file,
            max_new_tokens=args.max_new_tokens,
            temperature=args.temperature,
            top_k=args.top_k,
            top_p=args.top_p,
            repetition_penalty=args.repetition_penalty,
        )


if __name__ == "__main__":
    main()