#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MiniGPT è®­ç»ƒè„šæœ¬
æ”¯æŒå®Œæ•´çš„è®­ç»ƒæµæ°´çº¿ï¼špretrain â†’ sft â†’ dpo â†’ rlhf
æ”¯æŒä»checkpointæ¢å¤è®­ç»ƒ
"""
import os
import sys
import argparse
import time
import json
import glob
import math
import signal
import torch
from torch.utils.data import DataLoader
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•å’Œsrcç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.dirname(__file__))
sys.path.append(project_root)
sys.path.append(os.path.join(project_root, 'src'))

from model.transformer import create_model
from tokenizer.bpe_tokenizer import BPETokenizer
from training.trainer import create_trainer, LanguageModelingDataset, ConversationDataset
from training.training_monitor import TrainingMonitor
from config.training_config import get_config


class MiniGPTTrainer:
    """MiniGPTè®­ç»ƒå™¨ï¼Œæ”¯æŒå¤šç§è®­ç»ƒæ¨¡å¼"""

    def __init__(self, config, mode="pretrain"):
        self.config = config
        self.mode = mode
        self.device = self._setup_device()
        self.output_dir = os.path.join(config.checkpoint_dir, f"{mode}_{config.model_size}")
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ä¼˜é›…ä¸­æ–­æ ‡å¿—
        self.interrupted = False
        self.save_on_interrupt = True  # ä¸­æ–­æ—¶ä¿å­˜æ¨¡å‹

        print(f"=== MiniGPT {mode.upper()} è®­ç»ƒ ===")
        print(f"æ¨¡å‹é…ç½®: {config.model_size}")
        print(f"è®¾å¤‡: {self.device}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")

    def _setup_device(self):
        """è®¾ç½®è®­ç»ƒè®¾å¤‡"""
        if torch.backends.mps.is_available():
            device = "mps"
            print("ğŸ”§ ä½¿ç”¨Apple Silicon GPU (MPS)")
        elif torch.cuda.is_available():
            device = "cuda"
            print(f"ğŸ”§ ä½¿ç”¨CUDA GPU: {torch.cuda.get_device_name()}")
        else:
            device = "cpu"
            print("ğŸ”§ ä½¿ç”¨CPU")
        return device

    def setup_tokenizer(self, retrain=False):
        """è®¾ç½®åˆ†è¯å™¨ - æ”¯æŒè·¨é˜¶æ®µå¤ç”¨"""
        print("ğŸ”¤ è®¾ç½®åˆ†è¯å™¨...")

        # å½“å‰æ¨¡å¼çš„åˆ†è¯å™¨è·¯å¾„
        tokenizer_path = os.path.join(self.output_dir, "tokenizer.pkl")
        
        # å¦‚æœæ˜¯ SFT/DPO/RLHFï¼Œä¼˜å…ˆä» pretrain checkpoint åŠ è½½åˆ†è¯å™¨
        pretrain_tokenizer_path = None
        if self.mode in ["sft", "dpo", "rlhf"]:
            # å°è¯•æ‰¾åˆ° pretrain çš„åˆ†è¯å™¨
            pretrain_dir = os.path.join(self.config.checkpoint_dir, f"pretrain_{self.config.model_size}")
            pretrain_tokenizer_path = os.path.join(pretrain_dir, "tokenizer.pkl")
            
            if os.path.exists(pretrain_tokenizer_path):
                print(f"âœ… ä» pretrain checkpoint åŠ è½½åˆ†è¯å™¨: {pretrain_tokenizer_path}")
                tokenizer = BPETokenizer(vocab_size=self.config.vocab_size)
                tokenizer.load(pretrain_tokenizer_path)
                
                # å¤åˆ¶åˆ°å½“å‰ç›®å½•ä»¥ä¾¿æ¨ç†æ—¶ä½¿ç”¨
                import shutil
                shutil.copy2(pretrain_tokenizer_path, tokenizer_path)
                print(f"ğŸ“‹ åˆ†è¯å™¨å·²å¤åˆ¶åˆ°: {tokenizer_path}")
                print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
                return tokenizer
            else:
                print(f"âš ï¸  æœªæ‰¾åˆ° pretrain åˆ†è¯å™¨: {pretrain_tokenizer_path}")
                print(f"   å°†è®­ç»ƒæ–°çš„åˆ†è¯å™¨ï¼ˆå»ºè®®å…ˆè¿è¡Œ pretrain æ¨¡å¼ï¼‰")

        # æ£€æŸ¥å½“å‰ç›®å½•æ˜¯å¦å·²æœ‰åˆ†è¯å™¨
        if os.path.exists(tokenizer_path) and not retrain:
            print(f"åŠ è½½ç°æœ‰åˆ†è¯å™¨: {tokenizer_path}")
            tokenizer = BPETokenizer(vocab_size=self.config.vocab_size)
            tokenizer.load(tokenizer_path)
        else:
            print("è®­ç»ƒæ–°çš„åˆ†è¯å™¨...")
            # æ”¶é›†è®­ç»ƒæ–‡æœ¬
            texts = self._collect_texts_for_tokenizer()

            tokenizer = BPETokenizer(vocab_size=self.config.vocab_size)
            tokenizer.train(texts)
            tokenizer.save(tokenizer_path)
            print(f"åˆ†è¯å™¨å·²ä¿å­˜: {tokenizer_path}")

        print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
        return tokenizer

    def _collect_texts_for_tokenizer(self):
        """æ”¶é›†ç”¨äºè®­ç»ƒåˆ†è¯å™¨çš„æ–‡æœ¬"""
        texts = []
        data_paths = self._get_data_paths()

        for data_path in data_paths:
            if not os.path.exists(data_path):
                print(f"âš ï¸  æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {data_path}")
                continue

            with open(data_path, 'r', encoding='utf-8') as f:
                for line_no, line in enumerate(f):
                    try:
                        data = json.loads(line.strip())
                        text = self._extract_text_from_data(data)
                        if text:
                            texts.append(text)

                        # é™åˆ¶åˆ†è¯å™¨è®­ç»ƒæ ·æœ¬æ•°é‡
                        if len(texts) >= 50000:
                            break
                    except json.JSONDecodeError:
                        continue

            if len(texts) >= 50000:
                break

        print(f"æ”¶é›†äº† {len(texts)} æ¡æ–‡æœ¬ç”¨äºè®­ç»ƒåˆ†è¯å™¨")
        return texts

    def _get_data_paths(self):
        """æ ¹æ®è®­ç»ƒæ¨¡å¼è·å–æ•°æ®è·¯å¾„"""
        base_dir = self.config.data_dir

        if self.mode == "pretrain":
            return [
                os.path.join(base_dir, "pretrain_hq.jsonl"),
                os.path.join(base_dir, "sft_mini_512.jsonl")  # è¡¥å……æ•°æ®
            ]
        elif self.mode == "sft":
            return [
                os.path.join(base_dir, "sft_mini_512.jsonl"),
                os.path.join(base_dir, "alex_identity.jsonl"),
                os.path.join(base_dir, "ultra_think.jsonl")
            ]
        elif self.mode == "dpo":
            return [
                os.path.join(base_dir, "dpo.jsonl")
            ]
        elif self.mode == "rlhf":
            return [
                os.path.join(base_dir, "alex_identity.jsonl"),
                os.path.join(base_dir, "ultra_think.jsonl")
            ]
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è®­ç»ƒæ¨¡å¼: {self.mode}")

    def _extract_text_from_data(self, data):
        """ä»æ•°æ®ä¸­æå–æ–‡æœ¬"""
        if 'text' in data:
            return data['text']
        elif 'conversations' in data:
            # å¯¹è¯æ ¼å¼
            text = ""
            for turn in data['conversations']:
                if 'content' in turn:
                    text += turn['content'] + " "
            return text.strip()
        elif 'input' in data and 'output' in data:
            return f"{data['input']} {data['output']}"
        elif 'chosen' in data and 'rejected' in data:
            # DPOæ ¼å¼
            return data['chosen']
        return None

    def setup_data_loader(self, tokenizer):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        print(f"ğŸ“š è®¾ç½®{self.mode}æ•°æ®åŠ è½½å™¨...")

        # åŠ è½½æ•°æ®
        all_data = []
        data_paths = self._get_data_paths()

        for data_path in data_paths:
            if not os.path.exists(data_path):
                print(f"âš ï¸  æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {data_path}")
                continue

            file_data = []
            with open(data_path, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        data = json.loads(line.strip())
                        file_data.append(data)
                    except json.JSONDecodeError:
                        continue

            all_data.extend(file_data)
            print(f"ä» {os.path.basename(data_path)} åŠ è½½äº† {len(file_data)} æ¡æ•°æ®")

        print(f"æ€»å…±åŠ è½½ {len(all_data)} æ¡{self.mode}è®­ç»ƒæ•°æ®")

        # æ ¹æ®è®­ç»ƒæ¨¡å¼åˆ›å»ºæ•°æ®é›†
        if self.mode == "pretrain":
            dataset = self._create_pretrain_dataset(all_data, tokenizer)
        elif self.mode == "sft":
            dataset = self._create_sft_dataset(all_data, tokenizer)
        elif self.mode == "dpo":
            dataset = self._create_dpo_dataset(all_data, tokenizer)
        elif self.mode == "rlhf":
            dataset = self._create_rlhf_dataset(all_data, tokenizer)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è®­ç»ƒæ¨¡å¼: {self.mode}")

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - å¤šè¿›ç¨‹ä¼˜åŒ–
        data_loader = DataLoader(
            dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            num_workers=self.config.num_workers,
            pin_memory=self.config.pin_memory,
            persistent_workers=self.config.persistent_workers,
            prefetch_factor=getattr(self.config, 'prefetch_factor', 2),
            drop_last=True
        )

        print(f"æ•°æ®æ‰¹æ¬¡æ•°: {len(data_loader)}")
        return data_loader

    def _create_pretrain_dataset(self, data, tokenizer):
        """åˆ›å»ºé¢„è®­ç»ƒæ•°æ®é›†"""
        texts = []
        for item in data:
            text = self._extract_text_from_data(item)
            if text and len(text.strip()) > 10:
                texts.append(text)

        return LanguageModelingDataset(
            texts=texts,
            tokenizer=tokenizer,
            max_length=self.config.max_seq_len
        )

    def _create_sft_dataset(self, data, tokenizer):
        """åˆ›å»ºSFTæ•°æ®é›† - æ”¯æŒå¤šç§æ•°æ®æ ¼å¼"""
        conversations = []
        for item in data:
            if 'conversations' in item:
                # æ ¼å¼1: {'conversations': [{'role': 'user', 'content': ...}, ...]}
                conversations.append(item['conversations'])
            elif 'input' in item and 'output' in item:
                # æ ¼å¼2: {'input': ..., 'output': ...} - ç›´æ¥ä½¿ç”¨å­—å…¸æ ¼å¼
                conversations.append({
                    'input': item['input'],
                    'output': item['output']
                })
            else:
                # è·³è¿‡æ— æ³•è¯†åˆ«çš„æ ¼å¼
                continue

        print(f"ğŸ“Š SFTæ•°æ®é›†åŒ…å« {len(conversations)} ä¸ªå¯¹è¯")
        return ConversationDataset(
            conversations=conversations,
            tokenizer=tokenizer,
            max_length=self.config.max_seq_len
        )

    def _create_dpo_dataset(self, data, tokenizer):
        """åˆ›å»ºDPOæ•°æ®é›†"""
        # ç®€åŒ–å®ç°ï¼Œå°†chosenä½œä¸ºæ­£ä¾‹è®­ç»ƒ
        texts = []
        for item in data:
            if 'chosen' in item:
                texts.append(item['chosen'])

        return LanguageModelingDataset(
            texts=texts,
            tokenizer=tokenizer,
            max_length=self.config.max_seq_len
        )

    def _create_rlhf_dataset(self, data, tokenizer):
        """åˆ›å»ºRLHFæ•°æ®é›†"""
        # ä½¿ç”¨å¯¹è¯æ ¼å¼è¿›è¡Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒ
        return self._create_sft_dataset(data, tokenizer)

    def setup_model(self, tokenizer, resume_from=None):
        """è®¾ç½®æ¨¡å‹"""
        print("ğŸ§  åˆ›å»ºæ¨¡å‹...")

        model = create_model(vocab_size=tokenizer.vocab_size, model_size=self.config.model_size)
        model = model.to(self.device)

        # å¦‚æœæœ‰é¢„è®­ç»ƒæ¨¡å‹ï¼ŒåŠ è½½æƒé‡
        if resume_from:
            print(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹: {resume_from}")
            checkpoint = torch.load(resume_from, map_location=self.device, weights_only=False)
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
            else:
                model.load_state_dict(checkpoint)

        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"æ€»å‚æ•°é‡: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

        return model

    def _signal_handler(self, signum, frame):
        """å¤„ç†ä¸­æ–­ä¿¡å· (Ctrl+C)"""
        print(f"\n\nâš ï¸  æ”¶åˆ°ä¸­æ–­ä¿¡å· (Ctrl+C)")
        if not self.interrupted:
            self.interrupted = True
            print("ğŸ”„ æ­£åœ¨ä¼˜é›…åœ°åœæ­¢è®­ç»ƒ...")
            print("ğŸ’¾ å°†ä¿å­˜å½“å‰æ¨¡å‹çŠ¶æ€...")
            print("   (å†æ¬¡æŒ‰ Ctrl+C å¯å¼ºåˆ¶é€€å‡º)")
        else:
            print("âš¡ å¼ºåˆ¶é€€å‡ºï¼")
            sys.exit(1)

    def train(self, resume_from=None, auto_resume=False, retrain_tokenizer=False):
        """æ‰§è¡Œè®­ç»ƒ
        
        å‚æ•°:
            resume_from: æŒ‡å®šcheckpointæ–‡ä»¶è·¯å¾„
            auto_resume: è‡ªåŠ¨ä»æœ€æ–°checkpointæ¢å¤
            retrain_tokenizer: æ˜¯å¦é‡æ–°è®­ç»ƒåˆ†è¯å™¨
        """
        print(f"ğŸš€ å¼€å§‹{self.mode}è®­ç»ƒ...")
        
        # è®¾ç½®ä¿¡å·å¤„ç†å™¨
        signal.signal(signal.SIGINT, self._signal_handler)
        print("ğŸ’¡ æŒ‰ Ctrl+C å¯ä¼˜é›…åœ°åœæ­¢è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹")
        
        start_time = time.time()

        # è®¾ç½®åˆ†è¯å™¨
        tokenizer = self.setup_tokenizer(retrain=retrain_tokenizer)

        # è®¾ç½®æ•°æ®åŠ è½½å™¨
        data_loader = self.setup_data_loader(tokenizer)

        # è®¾ç½®æ¨¡å‹
        model = self.setup_model(tokenizer, resume_from=None)  # ç¨åä¼šåŠ è½½å®Œæ•´checkpoint

        # è®¾ç½®ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
            betas=(self.config.beta1, self.config.beta2),
            eps=self.config.eps
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨: Warmup + Cosine Decay
        def get_lr_scheduler(optimizer, warmup_steps, max_steps, last_epoch=-1):
            """
            åˆ›å»ºå¸¦Warmupçš„Cosineé€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨

            - 0 ~ warmup_steps: çº¿æ€§å¢é•¿ä»0åˆ°peak_lr
            - warmup_steps ~ max_steps: Cosineé€€ç«åˆ°min_lr
            
            Args:
                last_epoch: ä¸Šæ¬¡è®­ç»ƒçš„epoch/stepæ•°ï¼Œç”¨äºæ¢å¤è®­ç»ƒ (default: -1è¡¨ç¤ºä»å¤´å¼€å§‹)
            """
            def lr_lambda(current_step):
                if current_step < warmup_steps:
                    # Warmupé˜¶æ®µ: çº¿æ€§å¢é•¿
                    return float(current_step) / float(max(1, warmup_steps))
                else:
                    # Cosineé€€ç«é˜¶æ®µ
                    progress = float(current_step - warmup_steps) / float(max(1, max_steps - warmup_steps))
                    return max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))  # æœ€ä½é™åˆ°10%

            return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)

        # Note: scheduler will be created after checkpoint loading to use correct last_epoch
        scheduler = None
        warmup_ratio = self.config.warmup_steps / self.config.max_steps * 100
        print(f"âœ… å­¦ä¹ ç‡è°ƒåº¦å™¨: Warmup({self.config.warmup_steps}æ­¥, {warmup_ratio:.1f}%) + Cosine Decay")
        print(f"   åˆå§‹LR: 0 -> å³°å€¼LR: {self.config.learning_rate:.2e} -> æœ€ä½LR: {self.config.learning_rate * 0.1:.2e}")

        criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_id)

        # è®¾ç½®æ··åˆç²¾åº¦è®­ç»ƒ
        scaler = None
        if self.config.mixed_precision and self.device == "cuda":
            scaler = torch.cuda.amp.GradScaler()
            print("âœ… å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (FP16)")

        # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        if self.config.gradient_checkpointing and hasattr(model, 'gradient_checkpointing_enable'):
            model.gradient_checkpointing_enable()
            print("âœ… å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")

        # å¤„ç†checkpointæ¢å¤
        start_step = 0
        checkpoint_loaded = False
        
        if auto_resume:
            # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°checkpoint
            latest_checkpoint = self._find_latest_checkpoint()
            if latest_checkpoint:
                print(f"ğŸ” æ‰¾åˆ°checkpoint: {latest_checkpoint}")
                start_step = self._load_checkpoint(latest_checkpoint, model, optimizer)
                checkpoint_loaded = True
            else:
                print("â„¹ï¸  æœªæ‰¾åˆ°å½“å‰æ¨¡å¼çš„checkpoint")
        elif resume_from:
            # ä»æŒ‡å®šcheckpointæ¢å¤
            if os.path.exists(resume_from):
                start_step = self._load_checkpoint(resume_from, model, optimizer)
                checkpoint_loaded = True
            else:
                print(f"âš ï¸  Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {resume_from}")
        
        # å¦‚æœæ˜¯ SFT/DPO/RLHF æ¨¡å¼ä¸”æ²¡æœ‰åŠ è½½åˆ°checkpointï¼Œå°è¯•ä» pretrain åŠ è½½åˆå§‹æƒé‡
        if not checkpoint_loaded and self.mode in ["sft", "dpo", "rlhf"]:
            pretrain_dir = os.path.join(self.config.checkpoint_dir, f"pretrain_{self.config.model_size}")
            pretrain_model_path = os.path.join(pretrain_dir, "final_model.pt")
            
            if os.path.exists(pretrain_model_path):
                print(f"\nğŸ¯ {self.mode.upper()} æ¨¡å¼ï¼šä» pretrain checkpoint åŠ è½½åˆå§‹æƒé‡")
                print(f"   åŠ è½½è·¯å¾„: {pretrain_model_path}")
                try:
                    checkpoint = torch.load(pretrain_model_path, map_location=self.device, weights_only=False)
                    if 'model_state_dict' in checkpoint:
                        model.load_state_dict(checkpoint['model_state_dict'])
                    else:
                        model.load_state_dict(checkpoint)
                    print(f"âœ… æˆåŠŸåŠ è½½ pretrain æ¨¡å‹æƒé‡")
                    print(f"   ğŸ’¡ å°†åœ¨é¢„è®­ç»ƒåŸºç¡€ä¸Šè¿›è¡Œ {self.mode} è®­ç»ƒ")
                    checkpoint_loaded = True
                except Exception as e:
                    print(f"âš ï¸  åŠ è½½ pretrain æƒé‡å¤±è´¥: {e}")
                    print(f"   å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
            else:
                print(f"\nâš ï¸  æœªæ‰¾åˆ° pretrain æ¨¡å‹: {pretrain_model_path}")
                print(f"   å»ºè®®å…ˆè¿è¡Œ pretrain æ¨¡å¼è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼š")
                print(f"   uv run python scripts/train.py --mode pretrain --config {self.config.model_size}")
                print(f"   ç°åœ¨å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹è¿›è¡Œ {self.mode} è®­ç»ƒ")
        
        if not checkpoint_loaded and self.mode == "pretrain":
            print("\nğŸ“š Pretrain æ¨¡å¼ï¼šä»éšæœºåˆå§‹åŒ–å¼€å§‹è®­ç»ƒ")
        
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œä½¿ç”¨æ­£ç¡®çš„last_epochå‚æ•°
        # å½“æ¢å¤checkpointæ—¶ï¼Œlast_epochåº”è¯¥æ˜¯start_step - 1
        scheduler = get_lr_scheduler(
            optimizer,
            warmup_steps=self.config.warmup_steps,
            max_steps=self.config.max_steps,
            last_epoch=start_step - 1 if start_step > 0 else -1
        )
        
        # æ˜¾ç¤ºå½“å‰å­¦ä¹ ç‡çŠ¶æ€
        if start_step > 0:
            print(f"ğŸ“Š å­¦ä¹ ç‡è°ƒåº¦å™¨å·²æ¢å¤åˆ°ç¬¬ {start_step} æ­¥")
            current_lr = optimizer.param_groups[0]['lr']
            if start_step >= self.config.warmup_steps:
                phase = "Cosine Decay"
                progress = (start_step - self.config.warmup_steps) / (self.config.max_steps - self.config.warmup_steps) * 100
                print(f"   å½“å‰é˜¶æ®µ: {phase} (å·²å®Œæˆ{progress:.1f}%)")
            else:
                phase = "Warmup"
                progress = start_step / self.config.warmup_steps * 100
                print(f"   å½“å‰é˜¶æ®µ: {phase} (å·²å®Œæˆ{progress:.1f}%)")
            print(f"   å½“å‰å­¦ä¹ ç‡: {current_lr:.2e}")

        # åˆå§‹åŒ–è®­ç»ƒç›‘æ§å™¨ï¼ˆè½»é‡çº§æ¨¡å¼ï¼‰
        # TensorBoardæ—¥å¿—ç»Ÿä¸€å­˜å‚¨åœ¨ runs/{mode}_{size}_{timestamp}/
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        tensorboard_dir = os.path.join(
            self.config.tensorboard_dir,
            f"{self.mode}_{self.config.model_size}_{timestamp}"
        )

        monitor = TrainingMonitor(
            model=model,
            log_dir=tensorboard_dir,
            enable_tensorboard=self.config.enable_tensorboard,
            enable_real_time_plots=False,  # ç¦ç”¨å®æ—¶ç»˜å›¾ä»¥èŠ‚çœæ€§èƒ½
            lightweight_mode=True,         # å¯ç”¨è½»é‡çº§æ¨¡å¼
            log_interval=10                # æ¯10æ­¥è®°å½•ä¸€æ¬¡å®Œæ•´æŒ‡æ ‡
        )

        # æ¸…ç†GPUç¼“å­˜
        if self.device == "cuda":
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
            print(f"ğŸ§¹ GPUç¼“å­˜å·²æ¸…ç†")

        # è®­ç»ƒå¾ªç¯
        model.train()
        step = start_step  # ä»checkpointçš„æ­¥æ•°ç»§ç»­
        best_loss = float('inf')
        accumulation_steps = self.config.gradient_accumulation_steps

        print(f"å¼€å§‹è®­ç»ƒï¼Œæœ€å¤§æ­¥æ•°: {self.config.max_steps}")
        print(f"Batch size: {self.config.batch_size}, æ¢¯åº¦ç´¯ç§¯: {accumulation_steps}, æœ‰æ•ˆbatch: {self.config.batch_size * accumulation_steps}")

        # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨æƒ…å†µ
        if self.device == "cuda":
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            print(f"ğŸ’¾ åˆå§‹GPUå†…å­˜: å·²åˆ†é…={allocated:.2f}GB, å·²ä¿ç•™={reserved:.2f}GB")

        for epoch in range(1000):  # æœ€å¤§epochæ•°
            epoch_loss = 0
            epoch_steps = 0
            optimizer.zero_grad()  # åœ¨epochå¼€å§‹æ—¶æ¸…ç©ºæ¢¯åº¦

            for batch_idx, batch in enumerate(data_loader):
                # æ£€æŸ¥ä¸­æ–­æ ‡å¿—
                if self.interrupted:
                    print(f"\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­ï¼ˆæ­¥éª¤ {step}ï¼‰")
                    break
                
                if step >= self.config.max_steps:
                    break

                try:
                    # æ•°æ®ç§»åˆ°è®¾å¤‡ - å¤„ç†å­—å…¸å’Œtensorä¸¤ç§æ ¼å¼
                    if isinstance(batch, dict):
                        # å­—å…¸æ ¼å¼ (ConversationDataset)
                        input_ids = batch['input_ids'].to(self.device, non_blocking=True)
                        if 'labels' in batch:
                            target_ids = batch['labels'].to(self.device, non_blocking=True)
                        else:
                            # å¦‚æœæ²¡æœ‰labelsï¼Œä»input_idsç”Ÿæˆ
                            target_ids = torch.cat([
                                input_ids[:, 1:],
                                torch.full((input_ids.size(0), 1), tokenizer.pad_id, 
                                         dtype=torch.long, device=self.device)
                            ], dim=1)
                    else:
                        # Tensoræ ¼å¼ (LanguageModelingDataset)
                        batch = batch.to(self.device, non_blocking=True)
                        
                        # éªŒè¯batchå°ºå¯¸
                        if batch.size(1) < 2:
                            continue
                        
                        # å‡†å¤‡è¾“å…¥å’Œç›®æ ‡
                        input_ids = batch[:, :-1]
                        target_ids = batch[:, 1:]

                    # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
                    if scaler is not None:
                        with torch.amp.autocast('cuda'):
                            outputs = model(input_ids)
                            loss = criterion(outputs.reshape(-1, outputs.size(-1)), target_ids.reshape(-1))
                            # æ¢¯åº¦ç´¯ç§¯ï¼šæŸå¤±é™¤ä»¥ç´¯ç§¯æ­¥æ•°
                            loss = loss / accumulation_steps
                    else:
                        outputs = model(input_ids)
                        loss = criterion(outputs.reshape(-1, outputs.size(-1)), target_ids.reshape(-1))
                        loss = loss / accumulation_steps

                    # åå‘ä¼ æ’­
                    if scaler is not None:
                        scaler.scale(loss).backward()
                    else:
                        loss.backward()

                except torch.cuda.OutOfMemoryError as e:
                    print(f"\nâŒ CUDA OOMé”™è¯¯åœ¨æ­¥éª¤ {step}!")
                    # è·å–batchä¿¡æ¯ - å¤„ç†å­—å…¸å’Œtensoræ ¼å¼
                    if isinstance(batch, dict):
                        batch_size = batch['input_ids'].size(0)
                        seq_length = batch['input_ids'].size(1)
                    else:
                        batch_size = batch.size(0)
                        seq_length = batch.size(1)
                    print(f"   å½“å‰æ‰¹æ¬¡å¤§å°: {batch_size}")
                    print(f"   åºåˆ—é•¿åº¦: {seq_length}")
                    if self.device == "cuda":
                        allocated = torch.cuda.memory_allocated() / 1024**3
                        reserved = torch.cuda.memory_reserved() / 1024**3
                        print(f"   GPUå†…å­˜: å·²åˆ†é…={allocated:.2f}GB, å·²ä¿ç•™={reserved:.2f}GB")

                    # æ¸…ç†æ˜¾å­˜
                    optimizer.zero_grad()
                    if self.device == "cuda":
                        torch.cuda.empty_cache()

                    print(f"\nğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
                    print(f"   1. é™ä½batch_size: --batch-size {self.config.batch_size // 2}")
                    print(f"   2. å¢åŠ æ¢¯åº¦ç´¯ç§¯: å½“å‰={accumulation_steps}, å»ºè®®={accumulation_steps * 2}")
                    print(f"   3. å‡å°åºåˆ—é•¿åº¦: å½“å‰max_seq_len={self.config.max_seq_len}")
                    print(f"   4. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ (gradient checkpointing)")
                    print(f"   5. è®¾ç½®ç¯å¢ƒå˜é‡: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True")

                    raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ä»¥ç»ˆæ­¢è®­ç»ƒ

                # æ¢¯åº¦ç´¯ç§¯ï¼šåªåœ¨ç´¯ç§¯æ­¥æ•°è¾¾åˆ°æ—¶æ›´æ–°å‚æ•°
                if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(data_loader):
                    if scaler is not None:
                        scaler.unscale_(optimizer)
                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                        optimizer.step()

                    # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
                    scheduler.step()

                    optimizer.zero_grad()
                    step += 1

                    # æ›´æ–°ç»Ÿè®¡ï¼ˆä½¿ç”¨å®é™…æŸå¤±å€¼ï¼‰
                    actual_loss = loss.item() * accumulation_steps
                    epoch_loss += actual_loss
                    epoch_steps += 1

                    # ä½¿ç”¨ç›‘æ§å™¨è®°å½•æŒ‡æ ‡
                    # è·å–batch size - å¤„ç†å­—å…¸å’Œtensoræ ¼å¼
                    if isinstance(batch, dict):
                        current_batch_size = batch['input_ids'].size(0)
                    else:
                        current_batch_size = batch.size(0)
                    
                    monitor.log_step(
                        step=step,
                        epoch=epoch,
                        loss=actual_loss,
                        learning_rate=optimizer.param_groups[0]['lr'],
                        batch_size=current_batch_size * accumulation_steps
                    )

                    # è®°å½•æ—¥å¿— - æ¯æ­¥éƒ½æ˜¾ç¤ºå­¦ä¹ ç‡å’Œloss
                    avg_loss = epoch_loss / epoch_steps
                    elapsed = time.time() - start_time
                    current_lr = optimizer.param_groups[0]['lr']
                    
                    # æ˜¾ç¤ºå­¦ä¹ ç‡é˜¶æ®µ
                    lr_phase = "Warmup" if step < self.config.warmup_steps else "Decay"
                    lr_progress = f"{step}/{self.config.warmup_steps}" if step < self.config.warmup_steps else f"{step}/{self.config.max_steps}"
                    
                    print(f"Step {step:5d} | Loss: {actual_loss:.4f} | Avg: {avg_loss:.4f} | LR: {current_lr:.2e} ({lr_phase} {lr_progress}) | Time: {elapsed/60:.1f}min")

                    # ä¿å­˜æ£€æŸ¥ç‚¹ - æ¯100æ­¥è‡ªåŠ¨ä¿å­˜
                    if step % 100 == 0:
                        self._save_checkpoint(model, tokenizer, optimizer, step, actual_loss)

                    # è¯„ä¼°æ¨¡å‹
                    if step % self.config.eval_steps == 0:
                        self._evaluate_model(model, tokenizer)

                    if step >= self.config.max_steps:
                        break

            # æ£€æŸ¥æ˜¯å¦éœ€è¦é€€å‡ºepochå¾ªç¯
            if step >= self.config.max_steps or self.interrupted:
                break

        # å…³é—­ç›‘æ§å™¨å¹¶ä¿å­˜æ€»ç»“
        monitor.close()

        # å¦‚æœæ˜¯ä¸­æ–­ï¼Œå…ˆä¿å­˜checkpointä»¥ä¾¿æ¢å¤
        if self.interrupted:
            print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜ä¸­æ–­checkpoint...")
            checkpoint_path = os.path.join(self.output_dir, f"checkpoint_step_{step}.pt")
            torch.save({
                'step': step,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': epoch_loss / max(epoch_steps, 1),
                'config': self.config,
                'mode': self.mode
            }, checkpoint_path)
            print(f"âœ… Checkpointå·²ä¿å­˜: {checkpoint_path}")
            print(f"ğŸ’¡ å¯ä½¿ç”¨ --auto-resume ä»æ­¤å¤„æ¢å¤è®­ç»ƒ")

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_path = os.path.join(self.output_dir, "final_model.pt")
        print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
        torch.save({
            'model_state_dict': model.state_dict(),
            'tokenizer_vocab_size': tokenizer.vocab_size,
            'config': self.config,
            'mode': self.mode,
            'step': step
        }, final_path)

        # æ ¹æ®æ˜¯å¦ä¸­æ–­æ˜¾ç¤ºä¸åŒçš„æ¶ˆæ¯
        if self.interrupted:
            print(f"\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
            print(f"âœ… å·²æˆåŠŸä¿å­˜ä¸­æ–­æ—¶çš„æ¨¡å‹çŠ¶æ€")
        else:
            print(f"\nğŸ‰ {self.mode}è®­ç»ƒå®Œæˆï¼")
        
        print(f"æ€»æ­¥æ•°: {step}")
        print(f"è®­ç»ƒæ—¶é—´: {(time.time() - start_time)/60:.1f}åˆ†é’Ÿ")
        print(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_path}")
        print(f"ğŸ“Š TensorBoardæ—¥å¿—: {tensorboard_dir}")
        print(f"ğŸ’¡ æŸ¥çœ‹è®­ç»ƒè¿‡ç¨‹: tensorboard --logdir={tensorboard_dir}")

        return final_path

    def _save_checkpoint(self, model, tokenizer, optimizer, step, loss):
        """ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆåªä¿ç•™æœ€æ–°çš„ä¸€ä¸ªï¼‰"""
        # åˆ é™¤æ—§çš„checkpointæ–‡ä»¶
        checkpoint_pattern = os.path.join(self.output_dir, "checkpoint_step_*.pt")
        old_checkpoints = glob.glob(checkpoint_pattern)
        for old_ckpt in old_checkpoints:
            try:
                os.remove(old_ckpt)
                print(f"ğŸ—‘ï¸  åˆ é™¤æ—§checkpoint: {os.path.basename(old_ckpt)}")
            except Exception as e:
                print(f"âš ï¸  åˆ é™¤æ—§checkpointå¤±è´¥: {e}")
        
        # ä¿å­˜æ–°çš„checkpoint
        checkpoint_path = os.path.join(self.output_dir, f"checkpoint_step_{step}.pt")
        torch.save({
            'step': step,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': loss,
            'config': self.config,
            'mode': self.mode
        }, checkpoint_path)
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")

    def _evaluate_model(self, model, tokenizer):
        """ç®€å•è¯„ä¼°æ¨¡å‹"""
        model.eval()
        try:
            test_prompt = "ä½ å¥½ï¼Œæˆ‘æ˜¯"
            input_ids = tokenizer.encode(test_prompt, add_special_tokens=True)
            input_tensor = torch.tensor([input_ids], device=self.device)

            with torch.no_grad():
                for _ in range(10):
                    outputs = model(input_tensor)
                    next_token_logits = outputs[0, -1, :]
                    next_token = torch.argmax(next_token_logits).item()
                    input_tensor = torch.cat([input_tensor, torch.tensor([[next_token]], device=self.device)], dim=1)

            generated_text = tokenizer.decode(input_tensor[0].cpu().tolist())
            print(f"ğŸ§ª ç”Ÿæˆæµ‹è¯•: '{generated_text}'")
        except Exception as e:
            print(f"ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        finally:
            model.train()

    def _find_latest_checkpoint(self, search_pretrain_if_not_found=True):
        """æŸ¥æ‰¾æœ€æ–°çš„checkpointæ–‡ä»¶
        
        å‚æ•°:
            search_pretrain_if_not_found: å¦‚æœæ˜¯SFT/DPO/RLHFæ¨¡å¼ä¸”æ‰¾ä¸åˆ°checkpointï¼Œ
                                         æ˜¯å¦æŸ¥æ‰¾pretrainçš„checkpoint
        """
        checkpoint_pattern = os.path.join(self.output_dir, "checkpoint_step_*.pt")
        checkpoint_files = glob.glob(checkpoint_pattern)
        
        if not checkpoint_files:
            # å°è¯•æŸ¥æ‰¾ final_model.pt
            final_model = os.path.join(self.output_dir, "final_model.pt")
            if os.path.exists(final_model):
                return final_model
            
            # å¦‚æœæ˜¯ SFT/DPO/RLHF æ¨¡å¼ï¼Œå°è¯•ä» pretrain æŸ¥æ‰¾
            if search_pretrain_if_not_found and self.mode in ["sft", "dpo", "rlhf"]:
                print(f"   å½“å‰æ¨¡å¼({self.mode})æœªæ‰¾åˆ°checkpointï¼Œå°è¯•æŸ¥æ‰¾ pretrain checkpoint...")
                pretrain_dir = os.path.join(self.config.checkpoint_dir, f"pretrain_{self.config.model_size}")
                
                # æŸ¥æ‰¾ pretrain çš„ä¸­é—´ checkpoint
                pretrain_pattern = os.path.join(pretrain_dir, "checkpoint_step_*.pt")
                pretrain_checkpoints = glob.glob(pretrain_pattern)
                
                if pretrain_checkpoints:
                    # æŒ‰æ­¥æ•°æ’åºï¼Œè¿”å›æœ€æ–°çš„
                    def get_step_num(filename):
                        try:
                            basename = os.path.basename(filename)
                            step_str = basename.replace("checkpoint_step_", "").replace(".pt", "")
                            return int(step_str)
                        except:
                            return 0
                    
                    pretrain_checkpoints.sort(key=get_step_num, reverse=True)
                    print(f"   âœ… æ‰¾åˆ° pretrain checkpoint: {pretrain_checkpoints[0]}")
                    return pretrain_checkpoints[0]
                
                # æŸ¥æ‰¾ pretrain çš„ final_model.pt
                pretrain_final = os.path.join(pretrain_dir, "final_model.pt")
                if os.path.exists(pretrain_final):
                    print(f"   âœ… æ‰¾åˆ° pretrain final_model: {pretrain_final}")
                    return pretrain_final
            
            return None
        
        # æŒ‰æ­¥æ•°æ’åºï¼Œè¿”å›æœ€æ–°çš„
        def get_step_num(filename):
            try:
                # ä»æ–‡ä»¶åä¸­æå–æ­¥æ•°: checkpoint_step_5000.pt -> 5000
                basename = os.path.basename(filename)
                step_str = basename.replace("checkpoint_step_", "").replace(".pt", "")
                return int(step_str)
            except:
                return 0
        
        checkpoint_files.sort(key=get_step_num, reverse=True)
        return checkpoint_files[0]
    
    def _load_checkpoint(self, checkpoint_path, model, optimizer):
        """åŠ è½½checkpointå¹¶æ¢å¤è®­ç»ƒçŠ¶æ€
        
        è¿”å›:
            start_step: ä»å“ªä¸€æ­¥å¼€å§‹ç»§ç»­è®­ç»ƒ
        """
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½checkpoint: {checkpoint_path}")
        
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
            
            # åŠ è½½æ¨¡å‹æƒé‡
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
                print("âœ… æ¨¡å‹æƒé‡å·²åŠ è½½")
            else:
                model.load_state_dict(checkpoint)
                print("âœ… æ¨¡å‹æƒé‡å·²åŠ è½½ï¼ˆæ—§æ ¼å¼ï¼‰")
            
            # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
            if 'optimizer_state_dict' in checkpoint and optimizer is not None:
                try:
                    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                    print("âœ… ä¼˜åŒ–å™¨çŠ¶æ€å·²åŠ è½½")
                except Exception as e:
                    print(f"âš ï¸  ä¼˜åŒ–å™¨çŠ¶æ€åŠ è½½å¤±è´¥: {e}")
                    print("   å°†ä½¿ç”¨æ–°çš„ä¼˜åŒ–å™¨çŠ¶æ€")
            
            # è·å–è®­ç»ƒæ­¥æ•°
            start_step = checkpoint.get('step', 0)
            if start_step > 0:
                print(f"âœ… å°†ä»ç¬¬ {start_step} æ­¥ç»§ç»­è®­ç»ƒ")
            
            # æ˜¾ç¤ºcheckpointä¿¡æ¯
            if 'loss' in checkpoint:
                print(f"ğŸ“Š CheckpointæŸå¤±: {checkpoint['loss']:.4f}")
            if 'mode' in checkpoint:
                print(f"ğŸ“ è®­ç»ƒæ¨¡å¼: {checkpoint['mode']}")
            
            return start_step
            
        except Exception as e:
            print(f"âŒ åŠ è½½checkpointå¤±è´¥: {e}")
            print("   å°†ä»å¤´å¼€å§‹è®­ç»ƒ")
            return 0


def main():
    parser = argparse.ArgumentParser(description='MiniGPTè®­ç»ƒè„šæœ¬')

    # è®­ç»ƒæ¨¡å¼
    parser.add_argument('--mode', choices=['pretrain', 'sft', 'dpo', 'rlhf'], default='sft',
                        help='è®­ç»ƒæ¨¡å¼ (pretrain: é¢„è®­ç»ƒ, sft: ç›‘ç£å¾®è°ƒ, dpo: ç›´æ¥åå¥½ä¼˜åŒ–, rlhf: å¼ºåŒ–å­¦ä¹ )')

    # æ¨¡å‹é…ç½®
    parser.add_argument('--config', choices=['medium', 'large'], default='medium',
                        help='æ¨¡å‹é…ç½®å¤§å° (medium: ~200Må‚æ•°, large: ~500Må‚æ•°)')

    # æ•°æ®ç›¸å…³
    parser.add_argument('--retrain-tokenizer', action='store_true',
                        help='é‡æ–°è®­ç»ƒåˆ†è¯å™¨')

    # Checkpointæ¢å¤
    parser.add_argument('--resume', '--resume-from-checkpoint', type=str, default=None,
                        dest='resume_from_checkpoint',
                        help='ä»æŒ‡å®šcheckpointæ–‡ä»¶ç»§ç»­è®­ç»ƒï¼ˆä¾‹å¦‚: checkpoints/sft_medium/checkpoint_step_5000.ptï¼‰')
    parser.add_argument('--auto-resume', action='store_true',
                        help='è‡ªåŠ¨ä»æœ€æ–°çš„checkpointæ¢å¤è®­ç»ƒã€‚æ³¨æ„ï¼šSFT/DPO/RLHFæ¨¡å¼ä¼šè‡ªåŠ¨åŠ è½½pretrainæƒé‡ä½œä¸ºåˆå§‹åŒ–')

    # è®­ç»ƒå‚æ•°è¦†ç›–
    parser.add_argument('--learning-rate', type=float, default=None,
                        help='å­¦ä¹ ç‡')
    parser.add_argument('--max-steps', type=int, default=None,
                        help='æœ€å¤§è®­ç»ƒæ­¥æ•°')
    parser.add_argument('--batch-size', type=int, default=None,
                        help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--warmup-steps', type=int, default=None,
                        help='å­¦ä¹ ç‡warmupæ­¥æ•°ï¼ˆå¦‚æœä¸æŒ‡å®šï¼Œå°†æ ¹æ®è®­ç»ƒæ¨¡å¼è‡ªåŠ¨è®¾ç½®ï¼‰')

    args = parser.parse_args()

    # è·å–é…ç½®
    config = get_config(args.config)

    # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–ï¼ˆä¼˜å…ˆçº§æœ€ä½ï¼‰
    if args.learning_rate is not None:
        config.learning_rate = args.learning_rate
    if args.max_steps is not None:
        config.max_steps = args.max_steps
    if args.batch_size is not None:
        config.batch_size = args.batch_size

    # æ ¹æ®è®­ç»ƒæ¨¡å¼è°ƒæ•´é…ç½®ï¼ˆä¼šè‡ªåŠ¨è®¾ç½®warmup_stepsï¼‰
    if args.mode == "pretrain":
        config.max_steps = config.max_steps or 50000
        if args.learning_rate is None:
            config.learning_rate = 1e-4  # é¢„è®­ç»ƒåŸºç¡€å­¦ä¹ ç‡
        # é¢„è®­ç»ƒï¼šä»é›¶å¼€å§‹ï¼Œéœ€è¦è¾ƒé•¿warmupç¨³å®šè®­ç»ƒ
        config.warmup_steps = min(500, int(config.max_steps * 0.05))  # 5% æˆ–æœ€å¤š500æ­¥
        print("ğŸ“š é¢„è®­ç»ƒæ¨¡å¼ï¼šå»ºç«‹åŸºç¡€è¯­è¨€ç†è§£èƒ½åŠ›")
        print(f"   å­¦ä¹ ç‡: {config.learning_rate:.2e}")
        print(f"   Warmup steps: {config.warmup_steps} (å‰{config.warmup_steps/config.max_steps*100:.1f}%)")
    elif args.mode == "sft":
        config.max_steps = config.max_steps or 10000
        # SFTå¾®è°ƒï¼šä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡é¿å…ç ´åé¢„è®­ç»ƒçŸ¥è¯†
        if args.learning_rate is None:  # åªåœ¨ç”¨æˆ·æœªæŒ‡å®šæ—¶è®¾ç½®é»˜è®¤å€¼
            config.learning_rate = 5e-5  # æ¯”é¢„è®­ç»ƒä½ä¸€ä¸ªæ•°é‡çº§
        # SFTï¼šå·²æœ‰é¢„è®­ç»ƒåŸºç¡€ï¼Œä½¿ç”¨è¾ƒçŸ­warmupå¿«é€Ÿé€‚åº”
        config.warmup_steps = min(200, int(config.max_steps * 0.02))  # 2% æˆ–æœ€å¤š200æ­¥
        print("ğŸ¯ ç›‘ç£å¾®è°ƒæ¨¡å¼ï¼šè®­ç»ƒå¯¹è¯å’Œç‰¹å®šä»»åŠ¡èƒ½åŠ›")
        print(f"   å­¦ä¹ ç‡: {config.learning_rate:.2e} (æ¯”é¢„è®­ç»ƒä½ï¼Œä¿æŠ¤å·²å­¦çŸ¥è¯†)")
        print(f"   Warmup steps: {config.warmup_steps} (å‰{config.warmup_steps/config.max_steps*100:.1f}%)")
        print(f"   ğŸ’¡ æ¨¡å‹å·²æœ‰é¢„è®­ç»ƒåŸºç¡€ï¼Œä½¿ç”¨çŸ­warmupå¿«é€Ÿè¿›å…¥è¡°å‡é˜¶æ®µ")
    elif args.mode == "dpo":
        config.max_steps = config.max_steps or 5000
        if args.learning_rate is None:
            config.learning_rate = 1e-5  # DPOä½¿ç”¨æ›´å°å­¦ä¹ ç‡
        # DPOï¼šåœ¨SFTåŸºç¡€ä¸Šå¾®è°ƒï¼Œä½¿ç”¨æçŸ­warmup
        config.warmup_steps = min(100, int(config.max_steps * 0.02))  # 2% æˆ–æœ€å¤š100æ­¥
        print("âš–ï¸  ç›´æ¥åå¥½ä¼˜åŒ–æ¨¡å¼ï¼šæ ¹æ®äººç±»åå¥½è°ƒæ•´å“åº”")
        print(f"   å­¦ä¹ ç‡: {config.learning_rate:.2e}")
        print(f"   Warmup steps: {config.warmup_steps} (å‰{config.warmup_steps/config.max_steps*100:.1f}%)")
        print(f"   ğŸ’¡ åœ¨SFTåŸºç¡€ä¸Šä¼˜åŒ–ï¼Œä½¿ç”¨æçŸ­warmup")
    elif args.mode == "rlhf":
        config.max_steps = config.max_steps or 3000
        if args.learning_rate is None:
            config.learning_rate = 1e-5  # RLHFä½¿ç”¨å°å­¦ä¹ ç‡
        # RLHFï¼šåœ¨å·²è®­ç»ƒæ¨¡å‹ä¸Šå¼ºåŒ–å­¦ä¹ ï¼Œä½¿ç”¨æçŸ­warmup
        config.warmup_steps = min(100, int(config.max_steps * 0.02))  # 2% æˆ–æœ€å¤š100æ­¥
        print("ğŸ”„ å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ¨¡å¼ï¼šé€šè¿‡å¥–åŠ±æ¨¡å‹ä¼˜åŒ–")
        print(f"   å­¦ä¹ ç‡: {config.learning_rate:.2e}")
        print(f"   Warmup steps: {config.warmup_steps} (å‰{config.warmup_steps/config.max_steps*100:.1f}%)")
        print(f"   ğŸ’¡ åœ¨å·²è®­ç»ƒæ¨¡å‹ä¸Šå¼ºåŒ–å­¦ä¹ ï¼Œä½¿ç”¨æçŸ­warmup")

    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–warmup_stepsï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
    if args.warmup_steps is not None:
        config.warmup_steps = args.warmup_steps
        print(f"âš™ï¸  ä½¿ç”¨è‡ªå®šä¹‰warmupæ­¥æ•°: {config.warmup_steps} (å‰{config.warmup_steps/config.max_steps*100:.1f}%)")

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = MiniGPTTrainer(config, mode=args.mode)

    # æ˜¾ç¤ºæ¢å¤ä¿¡æ¯
    if args.auto_resume:
        print("ğŸ”„ å¯ç”¨è‡ªåŠ¨æ¢å¤æ¨¡å¼")
    elif args.resume_from_checkpoint:
        print(f"ğŸ”„ å°†ä»checkpointæ¢å¤: {args.resume_from_checkpoint}")

    # å¼€å§‹è®­ç»ƒ
    final_model_path = trainer.train(
        resume_from=args.resume_from_checkpoint,
        auto_resume=args.auto_resume,
        retrain_tokenizer=args.retrain_tokenizer
    )

    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: {final_model_path}")

    # æç¤ºä¸‹ä¸€æ­¥è®­ç»ƒå»ºè®®
    if args.mode == "pretrain":
        print("\nğŸ’¡ å»ºè®®ä¸‹ä¸€æ­¥è¿è¡ŒSFTè®­ç»ƒ:")
        print(f"uv run python scripts/train.py --mode sft --config {args.config} --resume {final_model_path}")
    elif args.mode == "sft":
        print("\nğŸ’¡ å»ºè®®ä¸‹ä¸€æ­¥è¿è¡ŒDPOè®­ç»ƒ:")
        print(f"uv run python scripts/train.py --mode dpo --config {args.config} --resume {final_model_path}")


if __name__ == "__main__":
    main()