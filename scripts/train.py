#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MiniGPT è®­ç»ƒè„šæœ¬
æ”¯æŒå®Œæ•´çš„è®­ç»ƒæµæ°´çº¿ï¼špretrain â†’ sft â†’ dpo â†’ rlhf
æ”¯æŒä»checkpointæ¢å¤è®­ç»ƒ
"""
import os
import sys
import argparse
import time
import json
import glob
import torch
from torch.utils.data import DataLoader
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•å’Œsrcç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.dirname(__file__))
sys.path.append(project_root)
sys.path.append(os.path.join(project_root, 'src'))

from model.transformer import create_model
from tokenizer.bpe_tokenizer import BPETokenizer
from training.trainer import create_trainer, LanguageModelingDataset, ConversationDataset
from training.training_monitor import TrainingMonitor
from config.training_config import get_config


class MiniGPTTrainer:
    """MiniGPTè®­ç»ƒå™¨ï¼Œæ”¯æŒå¤šç§è®­ç»ƒæ¨¡å¼"""

    def __init__(self, config, mode="pretrain"):
        self.config = config
        self.mode = mode
        self.device = self._setup_device()
        self.output_dir = os.path.join(config.checkpoint_dir, f"{mode}_{config.model_size}")
        os.makedirs(self.output_dir, exist_ok=True)

        print(f"=== MiniGPT {mode.upper()} è®­ç»ƒ ===")
        print(f"æ¨¡å‹é…ç½®: {config.model_size}")
        print(f"è®¾å¤‡: {self.device}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")

    def _setup_device(self):
        """è®¾ç½®è®­ç»ƒè®¾å¤‡"""
        if torch.backends.mps.is_available():
            device = "mps"
            print("ğŸ”§ ä½¿ç”¨Apple Silicon GPU (MPS)")
        elif torch.cuda.is_available():
            device = "cuda"
            print(f"ğŸ”§ ä½¿ç”¨CUDA GPU: {torch.cuda.get_device_name()}")
        else:
            device = "cpu"
            print("ğŸ”§ ä½¿ç”¨CPU")
        return device

    def setup_tokenizer(self, retrain=False):
        """è®¾ç½®åˆ†è¯å™¨"""
        print("ğŸ”¤ è®¾ç½®åˆ†è¯å™¨...")

        tokenizer_path = os.path.join(self.output_dir, "tokenizer.pkl")

        if os.path.exists(tokenizer_path) and not retrain:
            print(f"åŠ è½½ç°æœ‰åˆ†è¯å™¨: {tokenizer_path}")
            tokenizer = BPETokenizer(vocab_size=self.config.vocab_size)
            tokenizer.load(tokenizer_path)
        else:
            print("è®­ç»ƒæ–°çš„åˆ†è¯å™¨...")
            # æ”¶é›†è®­ç»ƒæ–‡æœ¬
            texts = self._collect_texts_for_tokenizer()

            tokenizer = BPETokenizer(vocab_size=self.config.vocab_size)
            tokenizer.train(texts)
            tokenizer.save(tokenizer_path)
            print(f"åˆ†è¯å™¨å·²ä¿å­˜: {tokenizer_path}")

        print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
        return tokenizer

    def _collect_texts_for_tokenizer(self):
        """æ”¶é›†ç”¨äºè®­ç»ƒåˆ†è¯å™¨çš„æ–‡æœ¬"""
        texts = []
        data_paths = self._get_data_paths()

        for data_path in data_paths:
            if not os.path.exists(data_path):
                print(f"âš ï¸  æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {data_path}")
                continue

            with open(data_path, 'r', encoding='utf-8') as f:
                for line_no, line in enumerate(f):
                    try:
                        data = json.loads(line.strip())
                        text = self._extract_text_from_data(data)
                        if text:
                            texts.append(text)

                        # é™åˆ¶åˆ†è¯å™¨è®­ç»ƒæ ·æœ¬æ•°é‡
                        if len(texts) >= 50000:
                            break
                    except json.JSONDecodeError:
                        continue

            if len(texts) >= 50000:
                break

        print(f"æ”¶é›†äº† {len(texts)} æ¡æ–‡æœ¬ç”¨äºè®­ç»ƒåˆ†è¯å™¨")
        return texts

    def _get_data_paths(self):
        """æ ¹æ®è®­ç»ƒæ¨¡å¼è·å–æ•°æ®è·¯å¾„"""
        base_dir = self.config.data_dir

        if self.mode == "pretrain":
            return [
                os.path.join(base_dir, "pretrain_hq.jsonl"),
                os.path.join(base_dir, "sft_mini_512.jsonl")  # è¡¥å……æ•°æ®
            ]
        elif self.mode == "sft":
            return [
                os.path.join(base_dir, "sft_mini_512.jsonl"),
                os.path.join(base_dir, "alex_identity.jsonl"),
                os.path.join(base_dir, "ultra_think.jsonl")
            ]
        elif self.mode == "dpo":
            return [
                os.path.join(base_dir, "dpo.jsonl")
            ]
        elif self.mode == "rlhf":
            return [
                os.path.join(base_dir, "alex_identity.jsonl"),
                os.path.join(base_dir, "ultra_think.jsonl")
            ]
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è®­ç»ƒæ¨¡å¼: {self.mode}")

    def _extract_text_from_data(self, data):
        """ä»æ•°æ®ä¸­æå–æ–‡æœ¬"""
        if 'text' in data:
            return data['text']
        elif 'conversations' in data:
            # å¯¹è¯æ ¼å¼
            text = ""
            for turn in data['conversations']:
                if 'content' in turn:
                    text += turn['content'] + " "
            return text.strip()
        elif 'input' in data and 'output' in data:
            return f"{data['input']} {data['output']}"
        elif 'chosen' in data and 'rejected' in data:
            # DPOæ ¼å¼
            return data['chosen']
        return None

    def setup_data_loader(self, tokenizer):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        print(f"ğŸ“š è®¾ç½®{self.mode}æ•°æ®åŠ è½½å™¨...")

        # åŠ è½½æ•°æ®
        all_data = []
        data_paths = self._get_data_paths()

        for data_path in data_paths:
            if not os.path.exists(data_path):
                print(f"âš ï¸  æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {data_path}")
                continue

            file_data = []
            with open(data_path, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        data = json.loads(line.strip())
                        file_data.append(data)
                    except json.JSONDecodeError:
                        continue

            all_data.extend(file_data)
            print(f"ä» {os.path.basename(data_path)} åŠ è½½äº† {len(file_data)} æ¡æ•°æ®")

        print(f"æ€»å…±åŠ è½½ {len(all_data)} æ¡{self.mode}è®­ç»ƒæ•°æ®")

        # æ ¹æ®è®­ç»ƒæ¨¡å¼åˆ›å»ºæ•°æ®é›†
        if self.mode == "pretrain":
            dataset = self._create_pretrain_dataset(all_data, tokenizer)
        elif self.mode == "sft":
            dataset = self._create_sft_dataset(all_data, tokenizer)
        elif self.mode == "dpo":
            dataset = self._create_dpo_dataset(all_data, tokenizer)
        elif self.mode == "rlhf":
            dataset = self._create_rlhf_dataset(all_data, tokenizer)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è®­ç»ƒæ¨¡å¼: {self.mode}")

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - å¤šè¿›ç¨‹ä¼˜åŒ–
        data_loader = DataLoader(
            dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            num_workers=self.config.num_workers,
            pin_memory=self.config.pin_memory,
            persistent_workers=self.config.persistent_workers,
            prefetch_factor=getattr(self.config, 'prefetch_factor', 2),
            drop_last=True
        )

        print(f"æ•°æ®æ‰¹æ¬¡æ•°: {len(data_loader)}")
        return data_loader

    def _create_pretrain_dataset(self, data, tokenizer):
        """åˆ›å»ºé¢„è®­ç»ƒæ•°æ®é›†"""
        texts = []
        for item in data:
            text = self._extract_text_from_data(item)
            if text and len(text.strip()) > 10:
                texts.append(text)

        return LanguageModelingDataset(
            texts=texts,
            tokenizer=tokenizer,
            max_length=self.config.max_seq_len
        )

    def _create_sft_dataset(self, data, tokenizer):
        """åˆ›å»ºSFTæ•°æ®é›†"""
        conversations = []
        for item in data:
            if 'conversations' in item:
                conversations.append(item['conversations'])
            elif 'input' in item and 'output' in item:
                # è½¬æ¢ä¸ºå¯¹è¯æ ¼å¼
                conv = [
                    {"role": "user", "content": item['input']},
                    {"role": "assistant", "content": item['output']}
                ]
                conversations.append(conv)

        return ConversationDataset(
            conversations=conversations,
            tokenizer=tokenizer,
            max_length=self.config.max_seq_len
        )

    def _create_dpo_dataset(self, data, tokenizer):
        """åˆ›å»ºDPOæ•°æ®é›†"""
        # ç®€åŒ–å®ç°ï¼Œå°†chosenä½œä¸ºæ­£ä¾‹è®­ç»ƒ
        texts = []
        for item in data:
            if 'chosen' in item:
                texts.append(item['chosen'])

        return LanguageModelingDataset(
            texts=texts,
            tokenizer=tokenizer,
            max_length=self.config.max_seq_len
        )

    def _create_rlhf_dataset(self, data, tokenizer):
        """åˆ›å»ºRLHFæ•°æ®é›†"""
        # ä½¿ç”¨å¯¹è¯æ ¼å¼è¿›è¡Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒ
        return self._create_sft_dataset(data, tokenizer)

    def setup_model(self, tokenizer, resume_from=None):
        """è®¾ç½®æ¨¡å‹"""
        print("ğŸ§  åˆ›å»ºæ¨¡å‹...")

        model = create_model(vocab_size=tokenizer.vocab_size, model_size=self.config.model_size)
        model = model.to(self.device)

        # å¦‚æœæœ‰é¢„è®­ç»ƒæ¨¡å‹ï¼ŒåŠ è½½æƒé‡
        if resume_from:
            print(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹: {resume_from}")
            checkpoint = torch.load(resume_from, map_location=self.device, weights_only=False)
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
            else:
                model.load_state_dict(checkpoint)

        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"æ€»å‚æ•°é‡: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

        return model

    def train(self, resume_from=None, auto_resume=False, retrain_tokenizer=False):
        """æ‰§è¡Œè®­ç»ƒ
        
        å‚æ•°:
            resume_from: æŒ‡å®šcheckpointæ–‡ä»¶è·¯å¾„
            auto_resume: è‡ªåŠ¨ä»æœ€æ–°checkpointæ¢å¤
            retrain_tokenizer: æ˜¯å¦é‡æ–°è®­ç»ƒåˆ†è¯å™¨
        """
        print(f"ğŸš€ å¼€å§‹{self.mode}è®­ç»ƒ...")
        start_time = time.time()

        # è®¾ç½®åˆ†è¯å™¨
        tokenizer = self.setup_tokenizer(retrain=retrain_tokenizer)

        # è®¾ç½®æ•°æ®åŠ è½½å™¨
        data_loader = self.setup_data_loader(tokenizer)

        # è®¾ç½®æ¨¡å‹
        model = self.setup_model(tokenizer, resume_from=None)  # ç¨åä¼šåŠ è½½å®Œæ•´checkpoint

        # è®¾ç½®ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay
        )

        criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_id)

        # è®¾ç½®æ··åˆç²¾åº¦è®­ç»ƒ
        scaler = None
        if self.config.mixed_precision and self.device == "cuda":
            scaler = torch.cuda.amp.GradScaler()
            print("âœ… å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (FP16)")

        # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        if self.config.gradient_checkpointing and hasattr(model, 'gradient_checkpointing_enable'):
            model.gradient_checkpointing_enable()
            print("âœ… å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")

        # å¤„ç†checkpointæ¢å¤
        start_step = 0
        if auto_resume:
            # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°checkpoint
            latest_checkpoint = self._find_latest_checkpoint()
            if latest_checkpoint:
                print(f"ğŸ” æ‰¾åˆ°checkpoint: {latest_checkpoint}")
                start_step = self._load_checkpoint(latest_checkpoint, model, optimizer)
            else:
                print("â„¹ï¸  æœªæ‰¾åˆ°checkpointï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
        elif resume_from:
            # ä»æŒ‡å®šcheckpointæ¢å¤
            if os.path.exists(resume_from):
                start_step = self._load_checkpoint(resume_from, model, optimizer)
            else:
                print(f"âš ï¸  Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {resume_from}")
                print("   ä»å¤´å¼€å§‹è®­ç»ƒ")

        # åˆå§‹åŒ–è®­ç»ƒç›‘æ§å™¨ï¼ˆè½»é‡çº§æ¨¡å¼ï¼‰
        monitor_dir = os.path.join(self.output_dir, "monitor_logs")
        monitor = TrainingMonitor(
            model=model,
            log_dir=monitor_dir,
            enable_tensorboard=True,
            enable_real_time_plots=False,  # ç¦ç”¨å®æ—¶ç»˜å›¾ä»¥èŠ‚çœæ€§èƒ½
            lightweight_mode=True,         # å¯ç”¨è½»é‡çº§æ¨¡å¼
            log_interval=10                # æ¯10æ­¥è®°å½•ä¸€æ¬¡å®Œæ•´æŒ‡æ ‡
        )

        # è®­ç»ƒå¾ªç¯
        model.train()
        step = start_step  # ä»checkpointçš„æ­¥æ•°ç»§ç»­
        best_loss = float('inf')
        accumulation_steps = self.config.gradient_accumulation_steps

        print(f"å¼€å§‹è®­ç»ƒï¼Œæœ€å¤§æ­¥æ•°: {self.config.max_steps}")
        print(f"Batch size: {self.config.batch_size}, æ¢¯åº¦ç´¯ç§¯: {accumulation_steps}, æœ‰æ•ˆbatch: {self.config.batch_size * accumulation_steps}")

        for epoch in range(1000):  # æœ€å¤§epochæ•°
            epoch_loss = 0
            epoch_steps = 0
            optimizer.zero_grad()  # åœ¨epochå¼€å§‹æ—¶æ¸…ç©ºæ¢¯åº¦

            for batch_idx, batch in enumerate(data_loader):
                if step >= self.config.max_steps:
                    break

                # æ•°æ®ç§»åˆ°è®¾å¤‡
                batch = batch.to(self.device, non_blocking=True)

                # éªŒè¯batchå°ºå¯¸
                if batch.size(1) < 2:
                    continue

                # å‡†å¤‡è¾“å…¥å’Œç›®æ ‡
                input_ids = batch[:, :-1]
                target_ids = batch[:, 1:]

                # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
                if scaler is not None:
                    with torch.cuda.amp.autocast():
                        outputs = model(input_ids)
                        loss = criterion(outputs.reshape(-1, outputs.size(-1)), target_ids.reshape(-1))
                        # æ¢¯åº¦ç´¯ç§¯ï¼šæŸå¤±é™¤ä»¥ç´¯ç§¯æ­¥æ•°
                        loss = loss / accumulation_steps
                else:
                    outputs = model(input_ids)
                    loss = criterion(outputs.reshape(-1, outputs.size(-1)), target_ids.reshape(-1))
                    loss = loss / accumulation_steps

                # åå‘ä¼ æ’­
                if scaler is not None:
                    scaler.scale(loss).backward()
                else:
                    loss.backward()

                # æ¢¯åº¦ç´¯ç§¯ï¼šåªåœ¨ç´¯ç§¯æ­¥æ•°è¾¾åˆ°æ—¶æ›´æ–°å‚æ•°
                if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(data_loader):
                    if scaler is not None:
                        scaler.unscale_(optimizer)
                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                        optimizer.step()

                    optimizer.zero_grad()
                    step += 1

                    # æ›´æ–°ç»Ÿè®¡ï¼ˆä½¿ç”¨å®é™…æŸå¤±å€¼ï¼‰
                    actual_loss = loss.item() * accumulation_steps
                    epoch_loss += actual_loss
                    epoch_steps += 1

                    # ä½¿ç”¨ç›‘æ§å™¨è®°å½•æŒ‡æ ‡
                    monitor.log_step(
                        step=step,
                        epoch=epoch,
                        loss=actual_loss,
                        learning_rate=optimizer.param_groups[0]['lr'],
                        batch_size=batch.size(0) * accumulation_steps
                    )

                    # è®°å½•æ—¥å¿— - æ¯æ­¥éƒ½æ˜¾ç¤ºå­¦ä¹ ç‡å’Œloss
                    avg_loss = epoch_loss / epoch_steps
                    elapsed = time.time() - start_time
                    current_lr = optimizer.param_groups[0]['lr']
                    print(f"Step {step:5d} | Loss: {actual_loss:.4f} | Avg: {avg_loss:.4f} | LR: {current_lr:.2e} | Time: {elapsed/60:.1f}min")

                    # ä¿å­˜æ£€æŸ¥ç‚¹ - æ¯100æ­¥è‡ªåŠ¨ä¿å­˜
                    if step % 100 == 0:
                        self._save_checkpoint(model, tokenizer, optimizer, step, actual_loss)

                    # è¯„ä¼°æ¨¡å‹
                    if step % self.config.eval_steps == 0:
                        self._evaluate_model(model, tokenizer)

                    if step >= self.config.max_steps:
                        break

            if step >= self.config.max_steps:
                break

        # å…³é—­ç›‘æ§å™¨å¹¶ä¿å­˜æ€»ç»“
        monitor.close()

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_path = os.path.join(self.output_dir, "final_model.pt")
        torch.save({
            'model_state_dict': model.state_dict(),
            'tokenizer_vocab_size': tokenizer.vocab_size,
            'config': self.config,
            'mode': self.mode,
            'step': step
        }, final_path)

        print(f"ğŸ‰ {self.mode}è®­ç»ƒå®Œæˆï¼")
        print(f"æ€»æ­¥æ•°: {step}")
        print(f"è®­ç»ƒæ—¶é—´: {(time.time() - start_time)/60:.1f}åˆ†é’Ÿ")
        print(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_path}")
        print(f"ğŸ“Š è®­ç»ƒç›‘æ§æ—¥å¿—: {monitor_dir}")

        return final_path

    def _save_checkpoint(self, model, tokenizer, optimizer, step, loss):
        """ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆåªä¿ç•™æœ€æ–°çš„ä¸€ä¸ªï¼‰"""
        # åˆ é™¤æ—§çš„checkpointæ–‡ä»¶
        checkpoint_pattern = os.path.join(self.output_dir, "checkpoint_step_*.pt")
        old_checkpoints = glob.glob(checkpoint_pattern)
        for old_ckpt in old_checkpoints:
            try:
                os.remove(old_ckpt)
                print(f"ğŸ—‘ï¸  åˆ é™¤æ—§checkpoint: {os.path.basename(old_ckpt)}")
            except Exception as e:
                print(f"âš ï¸  åˆ é™¤æ—§checkpointå¤±è´¥: {e}")
        
        # ä¿å­˜æ–°çš„checkpoint
        checkpoint_path = os.path.join(self.output_dir, f"checkpoint_step_{step}.pt")
        torch.save({
            'step': step,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': loss,
            'config': self.config,
            'mode': self.mode
        }, checkpoint_path)
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")

    def _evaluate_model(self, model, tokenizer):
        """ç®€å•è¯„ä¼°æ¨¡å‹"""
        model.eval()
        try:
            test_prompt = "ä½ å¥½ï¼Œæˆ‘æ˜¯"
            input_ids = tokenizer.encode(test_prompt, add_special_tokens=True)
            input_tensor = torch.tensor([input_ids], device=self.device)

            with torch.no_grad():
                for _ in range(10):
                    outputs = model(input_tensor)
                    next_token_logits = outputs[0, -1, :]
                    next_token = torch.argmax(next_token_logits).item()
                    input_tensor = torch.cat([input_tensor, torch.tensor([[next_token]], device=self.device)], dim=1)

            generated_text = tokenizer.decode(input_tensor[0].cpu().tolist())
            print(f"ğŸ§ª ç”Ÿæˆæµ‹è¯•: '{generated_text}'")
        except Exception as e:
            print(f"ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        finally:
            model.train()

    def _find_latest_checkpoint(self):
        """æŸ¥æ‰¾æœ€æ–°çš„checkpointæ–‡ä»¶"""
        checkpoint_pattern = os.path.join(self.output_dir, "checkpoint_step_*.pt")
        checkpoint_files = glob.glob(checkpoint_pattern)
        
        if not checkpoint_files:
            # å°è¯•æŸ¥æ‰¾ final_model.pt
            final_model = os.path.join(self.output_dir, "final_model.pt")
            if os.path.exists(final_model):
                return final_model
            return None
        
        # æŒ‰æ­¥æ•°æ’åºï¼Œè¿”å›æœ€æ–°çš„
        def get_step_num(filename):
            try:
                # ä»æ–‡ä»¶åä¸­æå–æ­¥æ•°: checkpoint_step_5000.pt -> 5000
                basename = os.path.basename(filename)
                step_str = basename.replace("checkpoint_step_", "").replace(".pt", "")
                return int(step_str)
            except:
                return 0
        
        checkpoint_files.sort(key=get_step_num, reverse=True)
        return checkpoint_files[0]
    
    def _load_checkpoint(self, checkpoint_path, model, optimizer):
        """åŠ è½½checkpointå¹¶æ¢å¤è®­ç»ƒçŠ¶æ€
        
        è¿”å›:
            start_step: ä»å“ªä¸€æ­¥å¼€å§‹ç»§ç»­è®­ç»ƒ
        """
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½checkpoint: {checkpoint_path}")
        
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
            
            # åŠ è½½æ¨¡å‹æƒé‡
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
                print("âœ… æ¨¡å‹æƒé‡å·²åŠ è½½")
            else:
                model.load_state_dict(checkpoint)
                print("âœ… æ¨¡å‹æƒé‡å·²åŠ è½½ï¼ˆæ—§æ ¼å¼ï¼‰")
            
            # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
            if 'optimizer_state_dict' in checkpoint and optimizer is not None:
                try:
                    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                    print("âœ… ä¼˜åŒ–å™¨çŠ¶æ€å·²åŠ è½½")
                except Exception as e:
                    print(f"âš ï¸  ä¼˜åŒ–å™¨çŠ¶æ€åŠ è½½å¤±è´¥: {e}")
                    print("   å°†ä½¿ç”¨æ–°çš„ä¼˜åŒ–å™¨çŠ¶æ€")
            
            # è·å–è®­ç»ƒæ­¥æ•°
            start_step = checkpoint.get('step', 0)
            if start_step > 0:
                print(f"âœ… å°†ä»ç¬¬ {start_step} æ­¥ç»§ç»­è®­ç»ƒ")
            
            # æ˜¾ç¤ºcheckpointä¿¡æ¯
            if 'loss' in checkpoint:
                print(f"ğŸ“Š CheckpointæŸå¤±: {checkpoint['loss']:.4f}")
            if 'mode' in checkpoint:
                print(f"ğŸ“ è®­ç»ƒæ¨¡å¼: {checkpoint['mode']}")
            
            return start_step
            
        except Exception as e:
            print(f"âŒ åŠ è½½checkpointå¤±è´¥: {e}")
            print("   å°†ä»å¤´å¼€å§‹è®­ç»ƒ")
            return 0


def main():
    parser = argparse.ArgumentParser(description='MiniGPTè®­ç»ƒè„šæœ¬')

    # è®­ç»ƒæ¨¡å¼
    parser.add_argument('--mode', choices=['pretrain', 'sft', 'dpo', 'rlhf'], default='sft',
                        help='è®­ç»ƒæ¨¡å¼ (pretrain: é¢„è®­ç»ƒ, sft: ç›‘ç£å¾®è°ƒ, dpo: ç›´æ¥åå¥½ä¼˜åŒ–, rlhf: å¼ºåŒ–å­¦ä¹ )')

    # æ¨¡å‹é…ç½®
    parser.add_argument('--config', choices=['medium', 'large'], default='medium',
                        help='æ¨¡å‹é…ç½®å¤§å° (medium: ~200Må‚æ•°, large: ~500Må‚æ•°)')

    # æ•°æ®ç›¸å…³
    parser.add_argument('--retrain-tokenizer', action='store_true',
                        help='é‡æ–°è®­ç»ƒåˆ†è¯å™¨')

    # Checkpointæ¢å¤
    parser.add_argument('--resume', '--resume-from-checkpoint', type=str, default=None,
                        dest='resume_from_checkpoint',
                        help='ä»æŒ‡å®šcheckpointæ–‡ä»¶ç»§ç»­è®­ç»ƒï¼ˆä¾‹å¦‚: checkpoints/sft_medium/checkpoint_step_5000.ptï¼‰')
    parser.add_argument('--auto-resume', action='store_true',
                        help='è‡ªåŠ¨ä»æœ€æ–°çš„checkpointæ¢å¤è®­ç»ƒ')

    # è®­ç»ƒå‚æ•°è¦†ç›–
    parser.add_argument('--learning-rate', type=float, default=None,
                        help='å­¦ä¹ ç‡')
    parser.add_argument('--max-steps', type=int, default=None,
                        help='æœ€å¤§è®­ç»ƒæ­¥æ•°')
    parser.add_argument('--batch-size', type=int, default=None,
                        help='æ‰¹æ¬¡å¤§å°')

    args = parser.parse_args()

    # è·å–é…ç½®
    config = get_config(args.config)

    # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    if args.learning_rate is not None:
        config.learning_rate = args.learning_rate
    if args.max_steps is not None:
        config.max_steps = args.max_steps
    if args.batch_size is not None:
        config.batch_size = args.batch_size

    # æ ¹æ®è®­ç»ƒæ¨¡å¼è°ƒæ•´é…ç½®
    if args.mode == "pretrain":
        config.max_steps = config.max_steps or 50000
        config.learning_rate = config.learning_rate or 1e-4
        print("ğŸ“š é¢„è®­ç»ƒæ¨¡å¼ï¼šå»ºç«‹åŸºç¡€è¯­è¨€ç†è§£èƒ½åŠ›")
    elif args.mode == "sft":
        config.max_steps = config.max_steps or 10000
        config.learning_rate = config.learning_rate or 5e-5
        print("ğŸ¯ ç›‘ç£å¾®è°ƒæ¨¡å¼ï¼šè®­ç»ƒå¯¹è¯å’Œç‰¹å®šä»»åŠ¡èƒ½åŠ›")
    elif args.mode == "dpo":
        config.max_steps = config.max_steps or 5000
        config.learning_rate = config.learning_rate or 1e-5
        print("âš–ï¸  ç›´æ¥åå¥½ä¼˜åŒ–æ¨¡å¼ï¼šæ ¹æ®äººç±»åå¥½è°ƒæ•´å“åº”")
    elif args.mode == "rlhf":
        config.max_steps = config.max_steps or 3000
        config.learning_rate = config.learning_rate or 1e-5
        print("ğŸ”„ å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ¨¡å¼ï¼šé€šè¿‡å¥–åŠ±æ¨¡å‹ä¼˜åŒ–")

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = MiniGPTTrainer(config, mode=args.mode)

    # æ˜¾ç¤ºæ¢å¤ä¿¡æ¯
    if args.auto_resume:
        print("ğŸ”„ å¯ç”¨è‡ªåŠ¨æ¢å¤æ¨¡å¼")
    elif args.resume_from_checkpoint:
        print(f"ğŸ”„ å°†ä»checkpointæ¢å¤: {args.resume_from_checkpoint}")

    # å¼€å§‹è®­ç»ƒ
    final_model_path = trainer.train(
        resume_from=args.resume_from_checkpoint,
        auto_resume=args.auto_resume,
        retrain_tokenizer=args.retrain_tokenizer
    )

    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: {final_model_path}")

    # æç¤ºä¸‹ä¸€æ­¥è®­ç»ƒå»ºè®®
    if args.mode == "pretrain":
        print("\nğŸ’¡ å»ºè®®ä¸‹ä¸€æ­¥è¿è¡ŒSFTè®­ç»ƒ:")
        print(f"uv run python scripts/train.py --mode sft --config {args.config} --resume {final_model_path}")
    elif args.mode == "sft":
        print("\nğŸ’¡ å»ºè®®ä¸‹ä¸€æ­¥è¿è¡ŒDPOè®­ç»ƒ:")
        print(f"uv run python scripts/train.py --mode dpo --config {args.config} --resume {final_model_path}")


if __name__ == "__main__":
    main()