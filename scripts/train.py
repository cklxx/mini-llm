"""
è®­ç»ƒè„šæœ¬
æ”¯æŒé¢„è®­ç»ƒã€SFTã€DPOç­‰å¤šç§è®­ç»ƒæ¨¡å¼
"""
import os
import sys
import argparse
import torch
from torch.utils.data import DataLoader

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•å’Œsrcç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.dirname(__file__))
sys.path.append(project_root)
sys.path.append(os.path.join(project_root, 'src'))

from model.transformer import create_model
from tokenizer.bpe_tokenizer import BPETokenizer, train_tokenizer_from_data
from tokenizer.tokenizer_manager import get_tokenizer
from data.dataset_loader import create_data_loader, DatasetConfig
from training.trainer import create_trainer, LanguageModelingDataset, ConversationDataset
from config.training_config import TrainingConfig, get_small_config, get_tiny_config


def setup_device():
    """è®¾ç½®è®­ç»ƒè®¾å¤‡"""
    if torch.backends.mps.is_available():
        device = "mps"
        print("ä½¿ç”¨Apple Silicon GPU (MPS)")
    elif torch.cuda.is_available():
        device = "cuda"
        print(f"ä½¿ç”¨CUDA GPU: {torch.cuda.get_device_name()}")
    else:
        device = "cpu"
        print("ä½¿ç”¨CPU")
    
    return device


def train_or_load_tokenizer(config: TrainingConfig, force_retrain: bool = False):
    """ä½¿ç”¨æ™ºèƒ½tokenizerç®¡ç†ç³»ç»Ÿè®­ç»ƒæˆ–åŠ è½½åˆ†è¯å™¨"""
    print("ğŸ”§ Using smart tokenizer management system...")

    # é€‰æ‹©è®­ç»ƒæ•°æ®
    data_path = os.path.join(config.data.data_dir, config.data.train_files[0])

    # ä½¿ç”¨æ™ºèƒ½tokenizerç®¡ç†å™¨
    tokenizer = get_tokenizer(
        data_path=data_path,
        vocab_size=config.tokenizer.vocab_size,
        tokenizer_type="bpe",
        force_retrain=force_retrain,
        cache_dir=os.path.join(config.output_dir, "tokenizers")
    )

    # ä¸ºäº†å‘åå…¼å®¹ï¼ŒåŒæ—¶ä¿å­˜åˆ°åŸæ¥çš„ä½ç½®
    tokenizer_path = os.path.join(config.output_dir, "tokenizer.pkl")
    os.makedirs(config.output_dir, exist_ok=True)
    tokenizer.save(tokenizer_path)
    print(f"ğŸ“ Tokenizer also saved to: {tokenizer_path}")

    return tokenizer


def prepare_data(config: TrainingConfig, tokenizer, training_mode: str):
    """å‡†å¤‡è®­ç»ƒæ•°æ®"""
    if training_mode == "pretrain":
        # é¢„è®­ç»ƒæ•°æ®
        data_config = DatasetConfig(
            data_path=os.path.join(config.data.data_dir, "pretrain_hq.jsonl"),
            max_length=config.data.max_seq_len
        )
        loader = create_data_loader("pretrain", data_config)
        texts = loader.load_pretrain_data()
        
        dataset = LanguageModelingDataset(texts, tokenizer, config.data.max_seq_len)
        
    elif training_mode == "sft":
        # SFTæ•°æ®
        data_config = DatasetConfig(
            data_path=os.path.join(config.data.data_dir, config.data.train_files[0]),
            max_length=config.data.max_seq_len
        )
        loader = create_data_loader("sft", data_config)
        conversations = loader.load_conversations()
        
        dataset = ConversationDataset(conversations, tokenizer, config.data.max_seq_len)
        
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„è®­ç»ƒæ¨¡å¼: {training_mode}")
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_size = int((1 - config.data.val_split) * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=config.data.batch_size,
        shuffle=config.data.shuffle,
        num_workers=config.data.num_workers
    )
    
    val_dataloader = DataLoader(
        val_dataset,
        batch_size=config.data.batch_size,
        shuffle=False,
        num_workers=config.data.num_workers
    ) if val_size > 0 else None
    
    return train_dataloader, val_dataloader


def main():
    parser = argparse.ArgumentParser(description="MiniGPTè®­ç»ƒè„šæœ¬")
    parser.add_argument("--mode", type=str, choices=["pretrain", "sft", "dpo"], 
                       default="sft", help="è®­ç»ƒæ¨¡å¼")
    parser.add_argument("--config", type=str, choices=["tiny", "small", "medium"], 
                       default="small", help="æ¨¡å‹é…ç½®")
    parser.add_argument("--data-dir", type=str, default="data/dataset/minimind_dataset",
                       help="æ•°æ®ç›®å½•")
    parser.add_argument("--output-dir", type=str, default="checkpoints",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--resume", type=str, default=None,
                       help="ä»checkpointæ¢å¤è®­ç»ƒ")
    parser.add_argument("--retrain-tokenizer", action="store_true",
                       help="é‡æ–°è®­ç»ƒåˆ†è¯å™¨")
    
    args = parser.parse_args()
    
    # è®¾ç½®è®¾å¤‡
    device = setup_device()
    
    # åŠ è½½é…ç½®
    if args.config == "tiny":
        config = get_tiny_config()
    elif args.config == "small":
        config = get_small_config()
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„é…ç½®: {args.config}")
    
    # æ›´æ–°é…ç½®
    config.device = device
    config.data.data_dir = args.data_dir
    config.output_dir = args.output_dir
    
    print(f"è®­ç»ƒæ¨¡å¼: {args.mode}")
    print(f"æ¨¡å‹é…ç½®: {args.config}")
    print(f"è®¾å¤‡: {device}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(config.output_dir, exist_ok=True)
    os.makedirs(config.logging_dir, exist_ok=True)
    
    # è®­ç»ƒæˆ–åŠ è½½åˆ†è¯å™¨
    tokenizer = train_or_load_tokenizer(config, args.retrain_tokenizer)
    print(f"åˆ†è¯å™¨è¯æ±‡è¡¨å¤§å°: {tokenizer.get_vocab_size()}")
    
    # æ›´æ–°æ¨¡å‹è¯æ±‡è¡¨å¤§å°
    config.model.vocab_size = tokenizer.get_vocab_size()
    
    # åˆ›å»ºæ¨¡å‹
    model = create_model(
        vocab_size=config.model.vocab_size,
        model_size=config.model.model_size
    )
    print(f"æ¨¡å‹å‚æ•°é‡: {model.get_num_params():,}")
    
    # å‡†å¤‡æ•°æ®
    train_dataloader, val_dataloader = prepare_data(config, tokenizer, args.mode)
    print(f"è®­ç»ƒæ•°æ®æ‰¹æ¬¡æ•°: {len(train_dataloader)}")
    if val_dataloader:
        print(f"éªŒè¯æ•°æ®æ‰¹æ¬¡æ•°: {len(val_dataloader)}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = create_trainer(args.mode, model, tokenizer, device)
    
    # ä»checkpointæ¢å¤ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.resume:
        print(f"ä»checkpointæ¢å¤: {args.resume}")
        trainer.load_checkpoint(args.resume)
    
    # å¼€å§‹è®­ç»ƒ
    if args.mode == "pretrain":
        num_epochs = config.pretrain.max_steps // len(train_dataloader) + 1
        trainer.train(train_dataloader, val_dataloader, num_epochs, config.output_dir)
    elif args.mode == "sft":
        trainer.train(train_dataloader, val_dataloader, config.sft.max_epochs, config.output_dir)
    
    print("è®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    main()