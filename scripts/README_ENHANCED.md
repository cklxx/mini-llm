# MiniGPT å¢å¼ºè®­ç»ƒè„šæœ¬è¯´æ˜

## ğŸš€ æ–°åŠŸèƒ½æ¦‚è§ˆ

`train_optimized.py` è„šæœ¬å·²ç»è¿›è¡Œäº†é‡å¤§å¢å¼ºï¼Œæ”¯æŒï¼š

- âœ… **å…¨é‡æ•°æ®è®­ç»ƒ** - ä½¿ç”¨å®Œæ•´çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒ
- âœ… **é‡æ–°è®­ç»ƒTokenizer** - ä»å¤šä¸ªæ•°æ®æºé‡æ–°æ„å»ºè¯æ±‡è¡¨
- âœ… **è‡ªå®šä¹‰æ•°æ®é›†** - çµæ´»é€‰æ‹©è®­ç»ƒæ•°æ®æ–‡ä»¶
- âœ… **å®æ—¶æŸå¤±æ›²çº¿** - è‡ªåŠ¨ç»˜åˆ¶å¹¶ä¿å­˜è®­ç»ƒæŸå¤±å›¾è¡¨
- âœ… **çµæ´»å‚æ•°é…ç½®** - å‘½ä»¤è¡Œè¦†ç›–æ‰€æœ‰è®­ç»ƒå‚æ•°
- âœ… **èµ„æºç›‘æ§** - é˜²æ­¢ç³»ç»Ÿè¿‡è½½
- ğŸš€ **å¤šçº¿ç¨‹ä¼˜åŒ–** - PyTorchåŸç”Ÿå¤šçº¿ç¨‹å’Œæ€§èƒ½ä¼˜åŒ–
- ğŸ”¥ **æ¨¡å‹ç¼–è¯‘** - PyTorch 2.0+ torch.compile åŠ é€Ÿ
- âš¡ **æ•°æ®åŠ è½½ä¼˜åŒ–** - æ™ºèƒ½DataLoader workeré…ç½®

## ğŸ“Š ä½¿ç”¨æ–¹æ³•

### åŸºç¡€ç”¨æ³•

```bash
# ä½¿ç”¨é»˜è®¤mediumé…ç½®è®­ç»ƒ
python scripts/train_optimized.py --config medium

# ä½¿ç”¨å…¨é‡æ•°æ®è®­ç»ƒ
python scripts/train_optimized.py --config medium --use-full-data

# é‡æ–°è®­ç»ƒtokenizer
python scripts/train_optimized.py --config medium --retrain-tokenizer
```

### é«˜çº§ç”¨æ³•

```bash
# å®Œæ•´çš„è‡ªå®šä¹‰è®­ç»ƒ
python scripts/train_optimized.py \
    --config medium \
    --use-full-data \
    --retrain-tokenizer \
    --tokenizer-vocab-size 20000 \
    --learning-rate 3e-5 \
    --max-steps 8000 \
    --batch-size 2 \
    --warmup-steps 800 \
    --output-dir "checkpoints/my_model" \
    --save-steps 400 \
    --plot-loss
```

## ğŸ”§ å‚æ•°è¯´æ˜

### æ•°æ®ç›¸å…³å‚æ•°

- `--use-full-data`: ä½¿ç”¨å…¨é‡æ•°æ®é›† (pretrain_hq.jsonl, sft_1024.jsonl, sft_512.jsonl, r1_mix_1024.jsonl)
- `--data-files`: æŒ‡å®šå…·ä½“çš„æ•°æ®æ–‡ä»¶åˆ—è¡¨
- `--max-data-size`: é™åˆ¶æ•°æ®æ¡æ•° (0è¡¨ç¤ºä¸é™åˆ¶)

### Tokenizerç›¸å…³å‚æ•°

- `--retrain-tokenizer`: é‡æ–°è®­ç»ƒtokenizer
- `--tokenizer-vocab-size`: Tokenizerè¯æ±‡è¡¨å¤§å° (é»˜è®¤: 15000)
- `--tokenizer-samples`: è®­ç»ƒtokenizerä½¿ç”¨çš„æ ·æœ¬æ•°é‡ (é»˜è®¤: 100000)

### è®­ç»ƒå‚æ•°

- `--learning-rate`: å­¦ä¹ ç‡
- `--max-steps`: æœ€å¤§è®­ç»ƒæ­¥æ•°
- `--batch-size`: æ‰¹æ¬¡å¤§å°
- `--warmup-steps`: é¢„çƒ­æ­¥æ•°
- `--save-steps`: ä¿å­˜æ£€æŸ¥ç‚¹çš„æ­¥æ•°é—´éš”

### è¾“å‡ºå’Œå¯è§†åŒ–

- `--output-dir`: è¾“å‡ºç›®å½•
- `--plot-loss`: å¯ç”¨å®æ—¶æŸå¤±æ›²çº¿ç»˜åˆ¶

### ç³»ç»Ÿèµ„æº

- `--max-cpu`: æœ€å¤§CPUä½¿ç”¨ç‡ (%)
- `--max-memory`: æœ€å¤§å†…å­˜ä½¿ç”¨ç‡ (%)
- `--disable-monitoring`: ç¦ç”¨èµ„æºç›‘æ§

### å¤šçº¿ç¨‹å’Œæ€§èƒ½ä¼˜åŒ–

- `--num-threads`: PyTorchçº¿ç¨‹æ•° (ä¸æŒ‡å®šåˆ™è‡ªåŠ¨ä¼˜åŒ–)
- `--dataloader-workers`: DataLoader workeræ•°é‡ (ä¸æŒ‡å®šåˆ™è‡ªåŠ¨ä¼˜åŒ–)
- `--enable-compile`: å¯ç”¨PyTorchæ¨¡å‹ç¼–è¯‘ä¼˜åŒ– (éœ€è¦PyTorch 2.0+)
- `--disable-optimizations`: ç¦ç”¨æ‰€æœ‰æ€§èƒ½ä¼˜åŒ–

## ğŸ“ˆ æŸå¤±æ›²çº¿åŠŸèƒ½

å¯ç”¨ `--plot-loss` å‚æ•°åï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè‡ªåŠ¨ï¼š

1. **å®æ—¶ä¿å­˜æŸå¤±æ›²çº¿**: æ¯æ¬¡ä¿å­˜æ£€æŸ¥ç‚¹æ—¶ç”ŸæˆæŸå¤±å›¾è¡¨
2. **åŒé‡è§†å›¾**: åŒ…å«å®Œæ•´è®­ç»ƒå†å²å’Œæœ€è¿‘1000æ­¥çš„è¯¦ç»†è§†å›¾
3. **ç»Ÿè®¡ä¿¡æ¯**: æ˜¾ç¤ºå½“å‰ã€æœ€å°ã€æœ€å¤§ã€å¹³å‡æŸå¤±å€¼
4. **è‡ªåŠ¨ä¿å­˜**: ä¿å­˜åˆ° `{output_dir}/plots/` ç›®å½•

ç”Ÿæˆçš„æ–‡ä»¶ï¼š
- `loss_curve_step_{step}.png` - æ¯ä¸ªæ£€æŸ¥ç‚¹çš„æŸå¤±æ›²çº¿
- `loss_curve_latest.png` - æœ€æ–°çš„æŸå¤±æ›²çº¿
- `loss_curve_final_step_{step}.png` - æœ€ç»ˆè®­ç»ƒå®Œæˆçš„æŸå¤±æ›²çº¿

## ğŸ¯ æ¨èé…ç½®

### å…¨æ–°è®­ç»ƒ (æ¨èï¼Œå«å¤šçº¿ç¨‹ä¼˜åŒ–)

```bash
python scripts/train_optimized.py \
    --config medium \
    --use-full-data \
    --retrain-tokenizer \
    --tokenizer-vocab-size 20000 \
    --learning-rate 3e-5 \
    --max-steps 8000 \
    --batch-size 2 \
    --warmup-steps 800 \
    --save-steps 400 \
    --plot-loss \
    --output-dir "checkpoints/mac_medium_v2" \
    --num-threads 6 \
    --dataloader-workers 2 \
    --enable-compile
```

### å¿«é€ŸéªŒè¯

```bash
python scripts/train_optimized.py \
    --config small \
    --data-files "pretrain_200.jsonl" \
    --max-data-size 10000 \
    --max-steps 1000 \
    --batch-size 8 \
    --plot-loss \
    --output-dir "checkpoints/test_run"
```

### èµ„æºå—é™ç¯å¢ƒ

```bash
python scripts/train_optimized.py \
    --config small \
    --data-files "sft_mini_512.jsonl" \
    --max-data-size 50000 \
    --batch-size 4 \
    --max-steps 3000 \
    --max-cpu 70 \
    --max-memory 70 \
    --plot-loss
```

## ğŸš€ æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½

### å¤šçº¿ç¨‹ä¼˜åŒ–

è„šæœ¬ä¼šè‡ªåŠ¨åº”ç”¨ä»¥ä¸‹PyTorchæ€§èƒ½ä¼˜åŒ–ï¼š

1. **çº¿ç¨‹æ•°ä¼˜åŒ–**: è‡ªåŠ¨æ£€æµ‹ç³»ç»Ÿæ ¸å¿ƒæ•°å¹¶è®¾ç½®æœ€ä½³çº¿ç¨‹æ•°
2. **ç¯å¢ƒå˜é‡ä¼˜åŒ–**: è®¾ç½® OMP_NUM_THREADS, MKL_NUM_THREADS ç­‰
3. **JITèåˆä¼˜åŒ–**: å¯ç”¨ PyTorch JIT èåˆç­–ç•¥
4. **MPSä¼˜åŒ–**: Mac GPU (Metal Performance Shaders) ä¸“é¡¹ä¼˜åŒ–
5. **MKL-DNNä¼˜åŒ–**: Intel æ•°å­¦æ ¸å¿ƒåº“ä¼˜åŒ–

### æ¨¡å‹ç¼–è¯‘åŠ é€Ÿ

ä½¿ç”¨ `--enable-compile` å¯ç”¨ PyTorch 2.0+ çš„ torch.compile:

- **MPSè®¾å¤‡**: ä½¿ç”¨ "reduce-overhead" æ¨¡å¼
- **CUDAè®¾å¤‡**: ä½¿ç”¨ "max-autotune" æ¨¡å¼
- **è‡ªåŠ¨å›é€€**: ä¸æ”¯æŒæ—¶è‡ªåŠ¨è·³è¿‡

### æ•°æ®åŠ è½½ä¼˜åŒ–

- **æ™ºèƒ½Workeré…ç½®**: æ ¹æ®è®¾å¤‡ç±»å‹å’Œæ‰¹æ¬¡å¤§å°è‡ªåŠ¨ä¼˜åŒ–
- **æŒä¹…åŒ–Workers**: å‡å°‘workeré‡å¯å¼€é”€
- **é¢„å–å› å­**: ä¼˜åŒ–æ•°æ®ç®¡é“ååé‡

## ğŸš¨ é‡è¦æç¤º

1. **æ¸…ç†æ—§æ•°æ®**: å¦‚æœè¦å…¨æ–°è®­ç»ƒï¼Œå»ºè®®å…ˆåˆ é™¤æ—§çš„æ£€æŸ¥ç‚¹å’Œtokenizeræ–‡ä»¶
2. **èµ„æºç›‘æ§**: å»ºè®®ä¿æŒèµ„æºç›‘æ§å¼€å¯ï¼Œé˜²æ­¢ç³»ç»Ÿå¡æ­»
3. **æ‰¹æ¬¡å¤§å°**: Macç¯å¢ƒä¸‹å»ºè®®ä½¿ç”¨è¾ƒå°çš„æ‰¹æ¬¡å¤§å° (2-4)
4. **å­¦ä¹ ç‡**: æ–°è®­ç»ƒå»ºè®®ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ (3e-5 åˆ° 5e-5)
5. **é¢„çƒ­æ­¥æ•°**: å……åˆ†çš„é¢„çƒ­æœ‰åŠ©äºè®­ç»ƒç¨³å®šæ€§
6. **å¤šçº¿ç¨‹è®¾ç½®**: çº¿ç¨‹æ•°ä¸å®œè¿‡å¤šï¼Œå»ºè®®4-8ä¸ªçº¿ç¨‹
7. **æ¨¡å‹ç¼–è¯‘**: MPSè®¾å¤‡ä¸Šå¯ç”¨ç¼–è¯‘å¯æ˜¾è‘—æé€Ÿ

## ğŸ“‚ è¾“å‡ºæ–‡ä»¶ç»“æ„

```
checkpoints/your_model/
â”œâ”€â”€ tokenizer.pkl                    # è®­ç»ƒçš„åˆ†è¯å™¨
â”œâ”€â”€ checkpoint_step_400.pt           # å®šæœŸæ£€æŸ¥ç‚¹
â”œâ”€â”€ checkpoint_step_800.pt
â”œâ”€â”€ ...
â”œâ”€â”€ final_model.pt                   # æœ€ç»ˆæ¨¡å‹
â””â”€â”€ plots/                           # æŸå¤±æ›²çº¿å›¾ç‰‡
    â”œâ”€â”€ loss_curve_step_400.png
    â”œâ”€â”€ loss_curve_latest.png
    â””â”€â”€ loss_curve_final_step_xxx.png
```

## ğŸ”„ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **å†…å­˜ä¸è¶³**: å‡å° `--batch-size` æˆ– `--max-data-size`
2. **è®­ç»ƒä¸æ”¶æ•›**: é™ä½ `--learning-rate` æˆ–å¢åŠ  `--warmup-steps`
3. **ç³»ç»Ÿå¡æ­»**: æ£€æŸ¥ `--max-cpu` å’Œ `--max-memory` è®¾ç½®
4. **Tokenizerè®­ç»ƒå¤±è´¥**: æ£€æŸ¥æ•°æ®æ–‡ä»¶æ ¼å¼æˆ–å‡å°‘ `--tokenizer-samples`

### ç›‘æ§è®­ç»ƒè¿›åº¦

ä½¿ç”¨ç›‘æ§è„šæœ¬å®æ—¶æŸ¥çœ‹è®­ç»ƒçŠ¶æ€ï¼š

```bash
python scripts/monitor_training.py
```

æˆ–è€…æŸ¥çœ‹æŸå¤±æ›²çº¿ï¼š

```bash
python scripts/plot_training_curves.py --checkpoint-dir checkpoints/your_model
```

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼** ğŸ‰