#!/usr/bin/env python3
"""
æ¢¯åº¦è¯Šæ–­å·¥å…·
åˆ†æè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¢¯åº¦å¥åº·çŠ¶å†µ
"""
import argparse
import json
import os
import sys
from pathlib import Path

import torch

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(__file__))
sys.path.append(project_root)


def analyze_checkpoint_gradients(checkpoint_path):
    """åˆ†æcheckpointä¸­çš„æ¢¯åº¦ä¿¡æ¯"""
    print(f"\n{'='*60}")
    print(f"ğŸ“Š åˆ†æCheckpoint: {checkpoint_path}")
    print(f"{'='*60}\n")

    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)

    # åŸºæœ¬ä¿¡æ¯
    print("ğŸ“‹ åŸºæœ¬ä¿¡æ¯:")
    print(f"   è®­ç»ƒæ­¥æ•°: {checkpoint.get('step', 'N/A')}")
    print(f"   Epoch: {checkpoint.get('epoch', 'N/A')}")
    print(f"   Loss: {checkpoint.get('loss', 'N/A'):.4f}")

    # åˆ†æå‚æ•°èŒƒæ•°
    print("\nğŸ” å‚æ•°ç»Ÿè®¡:")
    if 'model_state_dict' in checkpoint:
        state_dict = checkpoint['model_state_dict']

        total_params = 0
        layer_stats = {}

        for name, param in state_dict.items():
            total_params += param.numel()

            # æŒ‰å±‚ç»Ÿè®¡
            layer_name = name.split('.')[0]
            if layer_name not in layer_stats:
                layer_stats[layer_name] = {
                    'params': 0,
                    'mean_norm': 0,
                    'count': 0
                }

            param_norm = param.norm().item()
            layer_stats[layer_name]['params'] += param.numel()
            layer_stats[layer_name]['mean_norm'] += param_norm
            layer_stats[layer_name]['count'] += 1

        print(f"   æ€»å‚æ•°é‡: {total_params:,}")

        print("\n   å„å±‚å‚æ•°èŒƒæ•°:")
        for layer, stats in sorted(layer_stats.items()):
            avg_norm = stats['mean_norm'] / stats['count']
            print(f"   â€¢ {layer:20s}: å‚æ•°={stats['params']:>10,}, "
                  f"å¹³å‡èŒƒæ•°={avg_norm:>8.4f}")

    # ä¼˜åŒ–å™¨çŠ¶æ€
    if 'optimizer_state_dict' in checkpoint:
        print("\nâš™ï¸  ä¼˜åŒ–å™¨çŠ¶æ€:")
        opt_state = checkpoint['optimizer_state_dict']
        print(f"   å­¦ä¹ ç‡: {opt_state.get('param_groups', [{}])[0].get('lr', 'N/A')}")


def analyze_training_logs(log_dir):
    """åˆ†æè®­ç»ƒæ—¥å¿—ä¸­çš„æ¢¯åº¦è¶‹åŠ¿"""
    print(f"\n{'='*60}")
    print("ğŸ“ˆ åˆ†æè®­ç»ƒæ—¥å¿—")
    print(f"{'='*60}\n")

    # æŸ¥æ‰¾æ—¥å¿—æ–‡ä»¶
    log_files = list(Path(log_dir).glob("**/*.jsonl"))

    if not log_files:
        print(f"âŒ æœªæ‰¾åˆ°æ—¥å¿—æ–‡ä»¶: {log_dir}")
        return

    print(f"æ‰¾åˆ° {len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶\n")

    # è¯»å–æ¢¯åº¦æ•°æ®
    grad_norms = []
    losses = []
    steps = []

    for log_file in log_files:
        try:
            with open(log_file) as f:
                for line in f:
                    try:
                        data = json.loads(line.strip())
                        if 'grad_norm' in data:
                            grad_norms.append(data['grad_norm'])
                            losses.append(data.get('loss', 0))
                            steps.append(data.get('step', len(steps)))
                    except json.JSONDecodeError:
                        continue
        except Exception as e:
            print(f"âš ï¸  è¯»å–{log_file}å¤±è´¥: {e}")

    if not grad_norms:
        print("âŒ æœªæ‰¾åˆ°æ¢¯åº¦æ•°æ®")
        return

    # ç»Ÿè®¡åˆ†æ
    import numpy as np
    grad_norms = np.array(grad_norms)
    losses = np.array(losses)

    print(f"ğŸ“Š æ¢¯åº¦èŒƒæ•°ç»Ÿè®¡ (å…±{len(grad_norms)}ä¸ªæ ·æœ¬):")
    print(f"   æœ€å°å€¼: {grad_norms.min():.6f}")
    print(f"   æœ€å¤§å€¼: {grad_norms.max():.6f}")
    print(f"   å¹³å‡å€¼: {grad_norms.mean():.6f}")
    print(f"   ä¸­ä½æ•°: {np.median(grad_norms):.6f}")
    print(f"   æ ‡å‡†å·®: {grad_norms.std():.6f}")

    # æ£€æµ‹å¼‚å¸¸
    print("\nğŸš¨ å¼‚å¸¸æ£€æµ‹:")
    vanishing_count = (grad_norms < 1e-6).sum()
    explosion_count = (grad_norms > 10).sum()

    print(f"   æ¢¯åº¦æ¶ˆå¤± (<1e-6): {vanishing_count} ({vanishing_count/len(grad_norms)*100:.1f}%)")
    print(f"   æ¢¯åº¦çˆ†ç‚¸ (>10): {explosion_count} ({explosion_count/len(grad_norms)*100:.1f}%)")

    # è¶‹åŠ¿åˆ†æ
    if len(grad_norms) > 10:
        window = min(10, len(grad_norms) // 4)
        recent_avg = grad_norms[-window:].mean()
        overall_avg = grad_norms.mean()

        print("\nğŸ“ˆ è¶‹åŠ¿åˆ†æ:")
        print(f"   æ•´ä½“å¹³å‡: {overall_avg:.6f}")
        print(f"   æœ€è¿‘{window}æ­¥å¹³å‡: {recent_avg:.6f}")

        if recent_avg > overall_avg * 1.5:
            print("   âš ï¸  æ¢¯åº¦å‘ˆä¸Šå‡è¶‹åŠ¿ï¼Œæ³¨æ„æ¢¯åº¦çˆ†ç‚¸é£é™©")
        elif recent_avg < overall_avg * 0.5:
            print("   âš ï¸  æ¢¯åº¦å‘ˆä¸‹é™è¶‹åŠ¿ï¼Œæ³¨æ„æ¢¯åº¦æ¶ˆå¤±é£é™©")
        else:
            print("   âœ… æ¢¯åº¦ç¨³å®š")

    # Lossåˆ†æ
    if len(losses) > 0:
        print("\nğŸ“‰ Lossç»Ÿè®¡:")
        print(f"   åˆå§‹Loss: {losses[0]:.4f}")
        print(f"   å½“å‰Loss: {losses[-1]:.4f}")
        print(f"   ä¸‹é™å¹…åº¦: {(losses[0] - losses[-1]):.4f}")

        if losses[-1] < losses[0]:
            print("   âœ… Lossæ­£å¸¸ä¸‹é™ï¼Œè®­ç»ƒæœ‰æ•ˆ")
        else:
            print("   âš ï¸  Lossæœªä¸‹é™ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜")


def check_model_architecture(checkpoint_path):
    """æ£€æŸ¥æ¨¡å‹æ¶æ„çš„æ¢¯åº¦ä¿æŠ¤æœºåˆ¶"""
    print(f"\n{'='*60}")
    print("ğŸ—ï¸  æ¨¡å‹æ¶æ„å¥åº·æ£€æŸ¥")
    print(f"{'='*60}\n")

    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)

    if 'config' in checkpoint:
        config = checkpoint['config']

        print("âœ… å·²å®ç°çš„æ¢¯åº¦ä¿æŠ¤æœºåˆ¶:")

        checks = []

        # æ£€æŸ¥æ®‹å·®è¿æ¥
        if hasattr(config, 'num_hidden_layers'):
            checks.append(("æ®‹å·®è¿æ¥ (Residual)", True, "Transformeræ ‡å‡†é…ç½®"))

        # æ£€æŸ¥å½’ä¸€åŒ–
        if hasattr(config, 'rms_norm_eps'):
            checks.append(("RMSNormå½’ä¸€åŒ–", True, "ç°ä»£LLMæ ‡é…"))

        # æ£€æŸ¥dropout
        if hasattr(config, 'dropout') and config.dropout > 0:
            checks.append(("Dropoutæ­£åˆ™åŒ–", True, f"dropout={config.dropout}"))

        # æ£€æŸ¥æ¿€æ´»å‡½æ•°
        if hasattr(config, 'hidden_act'):
            act = config.hidden_act
            if act in ['swiglu', 'silu', 'gelu']:
                checks.append(("ç°ä»£æ¿€æ´»å‡½æ•°", True, f"{act.upper()}"))
            else:
                checks.append(("æ¿€æ´»å‡½æ•°", False, f"{act} (å»ºè®®ä½¿ç”¨SwiGLU/GELU)"))

        for name, status, detail in checks:
            symbol = "âœ…" if status else "âš ï¸ "
            print(f"   {symbol} {name:20s}: {detail}")

        print("\nğŸ“‹ æ¨¡å‹é…ç½®:")
        print(f"   å±‚æ•°: {getattr(config, 'num_hidden_layers', 'N/A')}")
        print(f"   éšè—ç»´åº¦: {getattr(config, 'hidden_size', 'N/A')}")
        print(f"   æ³¨æ„åŠ›å¤´æ•°: {getattr(config, 'num_attention_heads', 'N/A')}")
    else:
        print("âš ï¸  æœªæ‰¾åˆ°é…ç½®ä¿¡æ¯")


def provide_recommendations(checkpoint_dir):
    """æä¾›ä¼˜åŒ–å»ºè®®"""
    print(f"\n{'='*60}")
    print("ğŸ’¡ ä¼˜åŒ–å»ºè®®")
    print(f"{'='*60}\n")

    recommendations = [
        ("ç»§ç»­è§‚å¯Ÿ", "æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸å¾€å¾€åœ¨è®­ç»ƒåˆæœŸå‡ºç°ï¼Œç»§ç»­è®­ç»ƒè§‚å¯Ÿè¶‹åŠ¿"),
        ("æ£€æŸ¥TensorBoard", "tensorboard --logdir=" + checkpoint_dir),
        ("è°ƒæ•´å­¦ä¹ ç‡", "å¦‚æœæ¢¯åº¦æŒç»­è¿‡å°ï¼Œè€ƒè™‘æé«˜å­¦ä¹ ç‡ (å½“å‰3e-4 â†’ 5e-4)"),
        ("å¢åŠ warmup", "å»¶é•¿warmupæ­¥æ•°å¯ä»¥è®©æ¢¯åº¦æ›´å¹³ç¨³ (4000 â†’ 8000)"),
        ("è°ƒæ•´æ£€æµ‹é˜ˆå€¼", "ä¿®æ”¹ training_monitor.py:176 é˜ˆå€¼ (1e-6 â†’ 1e-8)"),
    ]

    for i, (title, detail) in enumerate(recommendations, 1):
        print(f"{i}. {title}")
        print(f"   â†’ {detail}\n")


def main():
    parser = argparse.ArgumentParser(description="æ¢¯åº¦è¯Šæ–­å·¥å…·")
    parser.add_argument("--checkpoint", type=str, help="Checkpointæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--log-dir", type=str, help="æ—¥å¿—ç›®å½•è·¯å¾„")
    parser.add_argument("--mode", type=str, default="pretrain",
                       choices=["pretrain", "sft", "dpo"],
                       help="è®­ç»ƒæ¨¡å¼")
    parser.add_argument("--config", type=str, default="medium",
                       choices=["tiny", "small", "medium"],
                       help="æ¨¡å‹é…ç½®")

    args = parser.parse_args()

    # è‡ªåŠ¨ç¡®å®šè·¯å¾„
    if not args.checkpoint and not args.log_dir:
        base_dir = f"checkpoints/{args.mode}_{args.config}"

        # æŸ¥æ‰¾æœ€æ–°checkpoint
        checkpoint_dir = Path(base_dir)
        if checkpoint_dir.exists():
            checkpoints = list(checkpoint_dir.glob("checkpoint_*.pt"))
            if checkpoints:
                args.checkpoint = str(sorted(checkpoints)[-1])

            # æŸ¥æ‰¾æ—¥å¿—ç›®å½•
            log_dir = checkpoint_dir / "monitor_logs"
            if log_dir.exists():
                args.log_dir = str(log_dir)

    # æ‰§è¡Œè¯Šæ–­
    if args.checkpoint and Path(args.checkpoint).exists():
        check_model_architecture(args.checkpoint)
        analyze_checkpoint_gradients(args.checkpoint)
    elif args.checkpoint:
        print(f"âŒ Checkpointä¸å­˜åœ¨: {args.checkpoint}")

    if args.log_dir and Path(args.log_dir).exists():
        analyze_training_logs(args.log_dir)
    elif args.log_dir:
        print(f"âŒ æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {args.log_dir}")

    # æä¾›å»ºè®®
    if args.checkpoint or args.log_dir:
        checkpoint_dir = (Path(args.checkpoint).parent if args.checkpoint
                         else args.log_dir)
        provide_recommendations(str(checkpoint_dir))
    else:
        print("\nâŒ æœªæ‰¾åˆ°checkpointæˆ–æ—¥å¿—æ–‡ä»¶")
        print("ğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
        print("   python scripts/diagnose_gradients.py --mode pretrain --config medium")
        print("   python scripts/diagnose_gradients.py --checkpoint path/to/checkpoint.pt")
        print("   python scripts/diagnose_gradients.py --log-dir path/to/logs")


if __name__ == "__main__":
    main()
