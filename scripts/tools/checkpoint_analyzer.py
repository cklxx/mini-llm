#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Checkpointåˆ†æå·¥å…·
=================

åŠŸèƒ½ï¼š
1. åˆ†æPyTorch checkpointæ–‡ä»¶çš„å¤§å°å’Œå‚æ•°é‡
2. å¯¹æ¯”ä¸åŒä¿å­˜æ ¼å¼çš„ä½“ç§¯å·®å¼‚
3. æä¾›æ¨¡å‹å‹ç¼©å’Œä¼˜åŒ–å»ºè®®
4. æ”¯æŒå¤šç§checkpointæ ¼å¼åˆ†æ

ä½œè€…: alex-ckl.com AIç ”å‘å›¢é˜Ÿ
"""

import os
import sys
import json
import pickle
import argparse
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List
from dataclasses import dataclass

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("âš ï¸  PyTorchæœªå®‰è£…ï¼Œéƒ¨åˆ†åŠŸèƒ½å°†ä¸å¯ç”¨")

from src.model.config import get_config, MiniGPTConfig


@dataclass
class CheckpointInfo:
    """Checkpointä¿¡æ¯æ•°æ®ç±»"""
    file_path: str
    file_size_bytes: int
    file_size_mb: float
    file_size_gb: float
    format_type: str

    # æ¨¡å‹ä¿¡æ¯ (å¦‚æœå¯è§£æ)
    total_params: Optional[int] = None
    trainable_params: Optional[int] = None
    model_config: Optional[Dict] = None

    # å†…å®¹ä¿¡æ¯
    contains_model: bool = False
    contains_optimizer: bool = False
    contains_scheduler: bool = False
    contains_metadata: bool = False

    # å‹ç¼©ä¿¡æ¯
    compression_ratio: Optional[float] = None
    estimated_fp16_size: Optional[float] = None
    estimated_int8_size: Optional[float] = None


def get_file_size(file_path: str) -> Tuple[int, float, float]:
    """è·å–æ–‡ä»¶å¤§å° (bytes, MB, GB)"""
    size_bytes = os.path.getsize(file_path)
    size_mb = size_bytes / (1024 * 1024)
    size_gb = size_bytes / (1024 * 1024 * 1024)
    return size_bytes, size_mb, size_gb


def analyze_pytorch_checkpoint(checkpoint_path: str) -> CheckpointInfo:
    """åˆ†æPyTorch checkpointæ–‡ä»¶"""
    print(f"ğŸ” åˆ†æcheckpoint: {os.path.basename(checkpoint_path)}")

    # åŸºæœ¬æ–‡ä»¶ä¿¡æ¯
    size_bytes, size_mb, size_gb = get_file_size(checkpoint_path)

    info = CheckpointInfo(
        file_path=checkpoint_path,
        file_size_bytes=size_bytes,
        file_size_mb=size_mb,
        file_size_gb=size_gb,
        format_type="PyTorch (.pt/.pth)"
    )

    if not TORCH_AVAILABLE:
        print("âš ï¸  PyTorchæœªå®‰è£…ï¼Œæ— æ³•è§£æcheckpointå†…å®¹")
        return info

    try:
        # åŠ è½½checkpoint
        checkpoint = torch.load(checkpoint_path, map_location='cpu')

        # åˆ†æcheckpointå†…å®¹
        if isinstance(checkpoint, dict):
            # æ£€æŸ¥åŒ…å«çš„ç»„ä»¶
            info.contains_model = 'model' in checkpoint or 'model_state_dict' in checkpoint
            info.contains_optimizer = 'optimizer' in checkpoint or 'optimizer_state_dict' in checkpoint
            info.contains_scheduler = 'scheduler' in checkpoint or 'lr_scheduler' in checkpoint
            info.contains_metadata = any(key in checkpoint for key in ['epoch', 'step', 'config', 'args'])

            # å°è¯•è·å–æ¨¡å‹çŠ¶æ€å­—å…¸
            model_state = None
            if 'model_state_dict' in checkpoint:
                model_state = checkpoint['model_state_dict']
            elif 'model' in checkpoint:
                model_state = checkpoint['model']
            elif isinstance(checkpoint, dict) and 'transformer' in str(checkpoint.keys()):
                model_state = checkpoint

            # åˆ†ææ¨¡å‹å‚æ•°
            if model_state is not None:
                total_params = 0
                trainable_params = 0

                for name, param in model_state.items():
                    if isinstance(param, torch.Tensor):
                        param_count = param.numel()
                        total_params += param_count
                        # å‡è®¾æ‰€æœ‰å‚æ•°éƒ½æ˜¯å¯è®­ç»ƒçš„
                        trainable_params += param_count

                info.total_params = total_params
                info.trainable_params = trainable_params

                # ä¼°ç®—ä¸åŒç²¾åº¦ä¸‹çš„å¤§å°
                info.estimated_fp16_size = total_params * 2 / (1024 * 1024)  # FP16: 2 bytes per param
                info.estimated_int8_size = total_params * 1 / (1024 * 1024)  # INT8: 1 byte per param

            # å°è¯•è·å–é…ç½®ä¿¡æ¯
            if 'config' in checkpoint:
                info.model_config = checkpoint['config']
            elif 'args' in checkpoint:
                info.model_config = checkpoint['args']

        elif isinstance(checkpoint, torch.nn.Module):
            # ç›´æ¥æ˜¯æ¨¡å‹å¯¹è±¡
            info.contains_model = True

            total_params = sum(p.numel() for p in checkpoint.parameters())
            trainable_params = sum(p.numel() for p in checkpoint.parameters() if p.requires_grad)

            info.total_params = total_params
            info.trainable_params = trainable_params
            info.estimated_fp16_size = total_params * 2 / (1024 * 1024)
            info.estimated_int8_size = total_params * 1 / (1024 * 1024)

    except Exception as e:
        print(f"âŒ è§£æcheckpointå¤±è´¥: {e}")

    return info


def analyze_safetensors_checkpoint(checkpoint_path: str) -> CheckpointInfo:
    """åˆ†æSafeTensorsæ ¼å¼checkpoint"""
    print(f"ğŸ” åˆ†æSafeTensors checkpoint: {os.path.basename(checkpoint_path)}")

    size_bytes, size_mb, size_gb = get_file_size(checkpoint_path)

    info = CheckpointInfo(
        file_path=checkpoint_path,
        file_size_bytes=size_bytes,
        file_size_mb=size_mb,
        file_size_gb=size_gb,
        format_type="SafeTensors (.safetensors)"
    )

    try:
        # å°è¯•å¯¼å…¥safetensors
        from safetensors import safe_open

        total_params = 0
        with safe_open(checkpoint_path, framework="pt", device="cpu") as f:
            for key in f.keys():
                tensor = f.get_tensor(key)
                total_params += tensor.numel()

        info.total_params = total_params
        info.trainable_params = total_params  # å‡è®¾éƒ½æ˜¯å¯è®­ç»ƒçš„
        info.contains_model = True

        # ä¼°ç®—ä¸åŒç²¾åº¦å¤§å°
        info.estimated_fp16_size = total_params * 2 / (1024 * 1024)
        info.estimated_int8_size = total_params * 1 / (1024 * 1024)

    except ImportError:
        print("âš ï¸  safetensorsåº“æœªå®‰è£…ï¼Œæ— æ³•è§£æå†…å®¹")
    except Exception as e:
        print(f"âŒ è§£æSafeTensorså¤±è´¥: {e}")

    return info


def create_demo_checkpoint(config_name: str = "medium") -> str:
    """åˆ›å»ºæ¼”ç¤ºç”¨çš„checkpointæ–‡ä»¶"""
    if not TORCH_AVAILABLE:
        print("âš ï¸  PyTorchæœªå®‰è£…ï¼Œæ— æ³•åˆ›å»ºæ¼”ç¤ºcheckpoint")
        return None

    print(f"ğŸ”§ åˆ›å»º {config_name} é…ç½®çš„æ¼”ç¤ºcheckpoint...")

    try:
        from src.model.transformer import MiniGPT

        # è·å–é…ç½®
        config = get_config(config_name)

        # åˆ›å»ºæ¨¡å‹
        model = MiniGPT(config)

        # åˆ›å»ºcheckpointç›®å½•
        checkpoint_dir = Path("checkpoints/demo")
        checkpoint_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜ä¸åŒæ ¼å¼çš„checkpoint
        base_name = f"demo_{config_name}_model"

        # 1. å®Œæ•´checkpoint (åŒ…å«ä¼˜åŒ–å™¨ç­‰)
        full_checkpoint_path = checkpoint_dir / f"{base_name}_full.pt"
        full_checkpoint = {
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': torch.optim.AdamW(model.parameters()).state_dict(),
            'scheduler_state_dict': {},
            'epoch': 10,
            'step': 1000,
            'loss': 2.5,
            'config': config.to_dict(),
            'model_config': {
                'total_params': sum(p.numel() for p in model.parameters()),
                'trainable_params': sum(p.numel() for p in model.parameters() if p.requires_grad)
            }
        }
        torch.save(full_checkpoint, full_checkpoint_path)

        # 2. ä»…æ¨¡å‹æƒé‡checkpoint
        model_only_path = checkpoint_dir / f"{base_name}_weights.pt"
        torch.save(model.state_dict(), model_only_path)

        # 3. å‹ç¼©checkpoint (å¦‚æœæ”¯æŒ)
        compressed_path = checkpoint_dir / f"{base_name}_compressed.pt"
        torch.save(full_checkpoint, compressed_path, _use_new_zipfile_serialization=True)

        print(f"âœ… æ¼”ç¤ºcheckpointå·²åˆ›å»ºåœ¨: {checkpoint_dir}")
        return str(checkpoint_dir)

    except Exception as e:
        print(f"âŒ åˆ›å»ºæ¼”ç¤ºcheckpointå¤±è´¥: {e}")
        return None


def compare_checkpoint_formats(checkpoint_dir: str) -> Dict[str, CheckpointInfo]:
    """å¯¹æ¯”ä¸åŒæ ¼å¼checkpointçš„å¤§å°"""
    print(f"\nğŸ“Š å¯¹æ¯”checkpointæ ¼å¼...")

    results = {}

    # æŸ¥æ‰¾æ‰€æœ‰checkpointæ–‡ä»¶
    checkpoint_dir = Path(checkpoint_dir)
    if not checkpoint_dir.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {checkpoint_dir}")
        return results

    # æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
    extensions = ['.pt', '.pth', '.safetensors', '.ckpt']

    for ext in extensions:
        files = list(checkpoint_dir.glob(f"*{ext}"))
        for file_path in files:
            print(f"\nåˆ†ææ–‡ä»¶: {file_path.name}")

            if ext in ['.pt', '.pth', '.ckpt']:
                info = analyze_pytorch_checkpoint(str(file_path))
            elif ext == '.safetensors':
                info = analyze_safetensors_checkpoint(str(file_path))

            results[file_path.name] = info

    return results


def print_checkpoint_analysis(info: CheckpointInfo):
    """æ‰“å°checkpointåˆ†æç»“æœ"""
    print(f"\n{'='*60}")
    print(f"ğŸ“ æ–‡ä»¶: {os.path.basename(info.file_path)}")
    print(f"{'='*60}")

    # åŸºæœ¬ä¿¡æ¯
    print(f"ğŸ“ æ–‡ä»¶å¤§å°:")
    print(f"  â€¢ {info.file_size_bytes:,} bytes")
    print(f"  â€¢ {info.file_size_mb:.2f} MB")
    if info.file_size_gb >= 0.1:
        print(f"  â€¢ {info.file_size_gb:.3f} GB")

    print(f"ğŸ“‚ æ ¼å¼ç±»å‹: {info.format_type}")

    # å†…å®¹ä¿¡æ¯
    print(f"\nğŸ“¦ åŒ…å«å†…å®¹:")
    print(f"  â€¢ æ¨¡å‹æƒé‡: {'âœ…' if info.contains_model else 'âŒ'}")
    print(f"  â€¢ ä¼˜åŒ–å™¨çŠ¶æ€: {'âœ…' if info.contains_optimizer else 'âŒ'}")
    print(f"  â€¢ è°ƒåº¦å™¨çŠ¶æ€: {'âœ…' if info.contains_scheduler else 'âŒ'}")
    print(f"  â€¢ å…ƒæ•°æ®: {'âœ…' if info.contains_metadata else 'âŒ'}")

    # å‚æ•°ä¿¡æ¯
    if info.total_params:
        print(f"\nğŸ”¢ å‚æ•°ç»Ÿè®¡:")
        print(f"  â€¢ æ€»å‚æ•°é‡: {info.total_params:,}")
        if info.trainable_params:
            print(f"  â€¢ å¯è®­ç»ƒå‚æ•°: {info.trainable_params:,}")

        # ç†è®ºå¤§å°ä¼°ç®—
        print(f"\nğŸ’¾ ç†è®ºæ¨¡å‹å¤§å° (ä»…æƒé‡):")
        if info.estimated_fp16_size:
            print(f"  â€¢ FP16: {info.estimated_fp16_size:.1f} MB")
        if info.estimated_int8_size:
            print(f"  â€¢ INT8: {info.estimated_int8_size:.1f} MB")

        # å®é™…æ–‡ä»¶å¤§å° vs ç†è®ºå¤§å°
        if info.estimated_fp16_size:
            overhead = info.file_size_mb / info.estimated_fp16_size
            print(f"\nğŸ“ˆ å­˜å‚¨æ•ˆç‡:")
            print(f"  â€¢ æ–‡ä»¶å¤§å° / ç†è®ºFP16å¤§å°: {overhead:.2f}x")
            if overhead > 2.0:
                print(f"  â€¢ âš ï¸  æ–‡ä»¶åŒ…å«é¢å¤–ä¿¡æ¯ (ä¼˜åŒ–å™¨çŠ¶æ€ã€å…ƒæ•°æ®ç­‰)")
            elif overhead < 1.1:
                print(f"  â€¢ âœ… é«˜æ•ˆå‹ç¼©å­˜å‚¨")

    # é…ç½®ä¿¡æ¯
    if info.model_config:
        print(f"\nâš™ï¸  æ¨¡å‹é…ç½®:")
        for key, value in info.model_config.items():
            if isinstance(value, (int, float, str, bool)):
                print(f"  â€¢ {key}: {value}")


def print_comparison_table(results: Dict[str, CheckpointInfo]):
    """æ‰“å°å¯¹æ¯”è¡¨æ ¼"""
    if not results:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°checkpointæ–‡ä»¶")
        return

    print(f"\n{'='*80}")
    print(f"ğŸ“Š CHECKPOINTæ ¼å¼å¯¹æ¯”è¡¨")
    print(f"{'='*80}")

    # è¡¨å¤´
    print(f"{'æ–‡ä»¶å':<30} {'å¤§å°(MB)':<10} {'æ ¼å¼':<15} {'å‚æ•°é‡':<12} {'æ•ˆç‡':<8}")
    print(f"{'-'*30} {'-'*10} {'-'*15} {'-'*12} {'-'*8}")

    # æŒ‰æ–‡ä»¶å¤§å°æ’åº
    sorted_results = sorted(results.items(), key=lambda x: x[1].file_size_mb)

    for filename, info in sorted_results:
        params_str = f"{info.total_params/1e6:.1f}M" if info.total_params else "N/A"

        # è®¡ç®—æ•ˆç‡æ¯”
        efficiency = ""
        if info.total_params and info.estimated_fp16_size:
            ratio = info.file_size_mb / info.estimated_fp16_size
            if ratio <= 1.2:
                efficiency = "ä¼˜ç§€"
            elif ratio <= 2.0:
                efficiency = "è‰¯å¥½"
            else:
                efficiency = "ä¸€èˆ¬"

        print(f"{filename:<30} {info.file_size_mb:<10.1f} {info.format_type.split()[0]:<15} {params_str:<12} {efficiency:<8}")

    # æ€»ç»“
    print(f"\nğŸ’¡ æ€»ç»“:")
    min_size = min(info.file_size_mb for info in results.values())
    max_size = max(info.file_size_mb for info in results.values())
    print(f"  â€¢ æœ€å°æ–‡ä»¶: {min_size:.1f}MB")
    print(f"  â€¢ æœ€å¤§æ–‡ä»¶: {max_size:.1f}MB")
    print(f"  â€¢ å¤§å°å·®å¼‚: {max_size/min_size:.1f}x")


def provide_optimization_suggestions(results: Dict[str, CheckpointInfo]):
    """æä¾›checkpointä¼˜åŒ–å»ºè®®"""
    print(f"\n{'='*60}")
    print(f"ğŸš€ CHECKPOINTä¼˜åŒ–å»ºè®®")
    print(f"{'='*60}")

    suggestions = []

    # åˆ†æç»“æœ
    has_full_checkpoint = any(info.contains_optimizer for info in results.values())
    has_model_only = any(not info.contains_optimizer and info.contains_model for info in results.values())

    # åŸºæœ¬å»ºè®®
    print(f"ğŸ“ é€šç”¨ä¼˜åŒ–å»ºè®®:")

    if has_full_checkpoint:
        print(f"  1. ğŸ’¾ åˆ†ç¦»å­˜å‚¨ç­–ç•¥:")
        print(f"     â€¢ æ¨¡å‹æƒé‡å•ç‹¬ä¿å­˜ (ç”¨äºæ¨ç†)")
        print(f"     â€¢ è®­ç»ƒçŠ¶æ€å•ç‹¬ä¿å­˜ (ç”¨äºæ¢å¤è®­ç»ƒ)")
        print(f"     â€¢ å¯èŠ‚çœ50-80%çš„æ¨ç†éƒ¨ç½²ä½“ç§¯")

    print(f"  2. ğŸ—œï¸  ç²¾åº¦ä¼˜åŒ–:")
    print(f"     â€¢ FP16: å‡å°‘50%å­˜å‚¨ï¼Œä¿æŒç²¾åº¦")
    print(f"     â€¢ INT8é‡åŒ–: å‡å°‘75%å­˜å‚¨ï¼Œè½»å¾®ç²¾åº¦æŸå¤±")
    print(f"     â€¢ INT4é‡åŒ–: å‡å°‘87.5%å­˜å‚¨ï¼Œé€‚åˆè¾¹ç¼˜éƒ¨ç½²")

    print(f"  3. ğŸ“¦ æ ¼å¼é€‰æ‹©:")
    print(f"     â€¢ SafeTensors: æ›´å®‰å…¨ï¼ŒåŠ è½½æ›´å¿«")
    print(f"     â€¢ ONNX: è·¨å¹³å°éƒ¨ç½²ä¼˜åŒ–")
    print(f"     â€¢ TensorRT: NVIDIA GPUéƒ¨ç½²åŠ é€Ÿ")

    # é’ˆå¯¹æ€§å»ºè®®
    if results:
        largest_file = max(results.values(), key=lambda x: x.file_size_mb)

        if largest_file.file_size_mb > 500:  # å¤§äº500MB
            print(f"\nâš ï¸  å¤§æ–‡ä»¶ä¼˜åŒ–å»ºè®®:")
            print(f"     â€¢ å½“å‰æœ€å¤§æ–‡ä»¶: {largest_file.file_size_mb:.1f}MB")
            print(f"     â€¢ å»ºè®®å¯ç”¨æ¨¡å‹å¹¶è¡Œæˆ–åˆ†ç‰‡å­˜å‚¨")
            print(f"     â€¢ è€ƒè™‘ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹å‡å°‘è®­ç»ƒå†…å­˜")

        if any(info.file_size_mb / info.estimated_fp16_size > 3.0 for info in results.values() if info.estimated_fp16_size):
            print(f"\nğŸ” å­˜å‚¨æ•ˆç‡å»ºè®®:")
            print(f"     â€¢ æ£€æµ‹åˆ°ä½æ•ˆå­˜å‚¨æ–‡ä»¶")
            print(f"     â€¢ å»ºè®®æ¸…ç†ä¸å¿…è¦çš„ä¼˜åŒ–å™¨çŠ¶æ€")
            print(f"     â€¢ ä½¿ç”¨torch.save(..., _use_new_zipfile_serialization=True)")

    print(f"\nğŸ› ï¸  å®ç”¨å·¥å…·æ¨è:")
    print(f"     â€¢ æ¨¡å‹é‡åŒ–: torch.quantization")
    print(f"     â€¢ æ¨¡å‹å‰ªæ: torch.nn.utils.prune")
    print(f"     â€¢ çŸ¥è¯†è’¸é¦: å‡å°‘æ¨¡å‹å¤§å°åŒæ—¶ä¿æŒæ€§èƒ½")
    print(f"     â€¢ åŠ¨æ€é‡åŒ–: è¿è¡Œæ—¶é‡åŒ–ï¼Œæ— éœ€é‡æ–°è®­ç»ƒ")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Checkpointåˆ†æå·¥å…·")
    parser.add_argument("--checkpoint", "-c", type=str, help="checkpointæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--directory", "-d", type=str, help="checkpointç›®å½•è·¯å¾„")
    parser.add_argument("--create-demo", action="store_true", help="åˆ›å»ºæ¼”ç¤ºcheckpoint")
    parser.add_argument("--config", default="medium", help="æ¼”ç¤ºcheckpointçš„é…ç½® (tiny/small/medium)")
    parser.add_argument("--compare", action="store_true", help="å¯¹æ¯”ä¸åŒæ ¼å¼")

    args = parser.parse_args()

    print("ğŸ” MiniGPT Checkpointåˆ†æå·¥å…·")
    print("alex-ckl.com AIç ”å‘å›¢é˜Ÿ")
    print("="*60)

    if args.create_demo:
        demo_dir = create_demo_checkpoint(args.config)
        if demo_dir:
            args.directory = demo_dir
            args.compare = True

    if args.checkpoint:
        # åˆ†æå•ä¸ªcheckpointæ–‡ä»¶
        if not os.path.exists(args.checkpoint):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.checkpoint}")
            return

        if args.checkpoint.endswith('.safetensors'):
            info = analyze_safetensors_checkpoint(args.checkpoint)
        else:
            info = analyze_pytorch_checkpoint(args.checkpoint)

        print_checkpoint_analysis(info)

    elif args.directory or args.compare:
        # åˆ†æç›®å½•ä¸­çš„æ‰€æœ‰checkpoint
        directory = args.directory or "checkpoints"

        if not os.path.exists(directory):
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {directory}")
            print(f"ğŸ’¡ ä½¿ç”¨ --create-demo åˆ›å»ºæ¼”ç¤ºæ–‡ä»¶")
            return

        results = compare_checkpoint_formats(directory)

        if results:
            # æ‰“å°æ¯ä¸ªæ–‡ä»¶çš„è¯¦ç»†åˆ†æ
            for filename, info in results.items():
                print_checkpoint_analysis(info)

            # æ‰“å°å¯¹æ¯”è¡¨æ ¼
            print_comparison_table(results)

            # æä¾›ä¼˜åŒ–å»ºè®®
            provide_optimization_suggestions(results)
        else:
            print(f"âŒ åœ¨ç›®å½• {directory} ä¸­æ²¡æœ‰æ‰¾åˆ°checkpointæ–‡ä»¶")

    else:
        print("ğŸ†˜ ä½¿ç”¨å¸®åŠ©:")
        print("  python checkpoint_analyzer.py --checkpoint model.pt")
        print("  python checkpoint_analyzer.py --directory checkpoints/")
        print("  python checkpoint_analyzer.py --create-demo --config medium")
        print("  python checkpoint_analyzer.py --compare --directory checkpoints/")


if __name__ == "__main__":
    main()