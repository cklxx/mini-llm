#!/usr/bin/env python3
"""
æµ‹è¯• SwiGLU æ¨¡å‹å®ç°
ä½¿ç”¨ minimind_dataset çš„æ•°æ®æµ‹è¯•æ¨¡å‹çš„æ‰€æœ‰å±‚æ˜¯å¦æ­£å¸¸å·¥ä½œ
åŒ…å«è¯¦ç»†çš„æ—¥å¿—è¾“å‡º
"""
import os
import sys
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import json
from typing import List, Dict

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from src.model.transformer import MiniGPT, create_model
from src.tokenizer.bpe_tokenizer import BPETokenizer
from src.training.trainer import PreTrainer, LanguageModelingDataset
from src.data.dataset_loader import DatasetConfig, PretrainDataLoader

class ModelTestSuite:
    """æ¨¡å‹æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self, device='cpu'):
        self.device = device
        print(f"ğŸ”§ åˆå§‹åŒ–æµ‹è¯•å¥—ä»¶ï¼Œä½¿ç”¨è®¾å¤‡: {device}")
        
        # åˆå§‹åŒ– tokenizer
        self.tokenizer = self._load_tokenizer()
        
        # åˆ›å»ºæ¨¡å‹
        self.model = self._create_model()
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        self.test_data = self._load_test_data()
        
    def _load_tokenizer(self):
        """åŠ è½½åˆ†è¯å™¨"""
        print("ğŸ“š åŠ è½½åˆ†è¯å™¨...")
        try:
            tokenizer = BPETokenizer()
            # è®¾ç½®åŸºæœ¬çš„ç‰¹æ®Štoken
            tokenizer.pad_id = 0
            tokenizer.bos_id = 1
            tokenizer.eos_id = 2
            tokenizer.unk_id = 3
            
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„è¯æ±‡è¡¨ç”¨äºæµ‹è¯•
            vocab = {
                '<PAD>': 0, '<BOS>': 1, '<EOS>': 2, '<UNK>': 3,
                'ä½ ': 4, 'å¥½': 5, 'æˆ‘': 6, 'æ˜¯': 7, 'çš„': 8, 'äº†': 9, 'åœ¨': 10,
                'ä»€': 11, 'ä¹ˆ': 12, 'å¦‚': 13, 'ä½•': 14, 'å­¦': 15, 'ä¹ ': 16, 'ç¼–': 17, 'ç¨‹': 18,
                'æ˜¥': 19, 'é£': 20, 'èŠ±': 21, 'å¼€': 22, 'é¸Ÿ': 23, 'è¯­': 24, 'é¦™': 25
            }
            
            # æ‰©å±•è¯æ±‡è¡¨åˆ°è‡³å°‘100ä¸ªtoken
            for i in range(26, 100):
                vocab[f'token_{i}'] = i
                
            tokenizer.vocab = vocab
            tokenizer.vocab_size = len(vocab)
            
            print(f"âœ… åˆ†è¯å™¨åŠ è½½å®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
            return tokenizer
        except Exception as e:
            print(f"âŒ åˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")
            # åˆ›å»ºä¸€ä¸ªæœ€å°çš„tokenizerç”¨äºæµ‹è¯•
            class SimpleTokenizer:
                def __init__(self):
                    self.vocab_size = 1000
                    self.pad_id = 0
                    self.bos_id = 1
                    self.eos_id = 2
                    self.unk_id = 3
                
                def encode(self, text, add_special_tokens=True):
                    # ç®€å•çš„å­—ç¬¦çº§ç¼–ç 
                    tokens = [min(ord(c), 999) for c in text[:50]]  # é™åˆ¶é•¿åº¦å’Œå€¼
                    if add_special_tokens:
                        tokens = [self.bos_id] + tokens + [self.eos_id]
                    return tokens
                
                def decode(self, tokens):
                    return ''.join([chr(min(t, 127)) for t in tokens if t not in [self.pad_id, self.bos_id, self.eos_id]])
            
            return SimpleTokenizer()
    
    def _create_model(self):
        """åˆ›å»ºæ¨¡å‹"""
        print("ğŸ¤– åˆ›å»ºæ¨¡å‹...")
        try:
            # åˆ›å»ºä¸€ä¸ªå°å‹æ¨¡å‹ç”¨äºæµ‹è¯•
            model = MiniGPT(
                vocab_size=self.tokenizer.vocab_size,
                d_model=256,      # è¾ƒå°çš„ç»´åº¦ç”¨äºå¿«é€Ÿæµ‹è¯•
                n_heads=4,        # è¾ƒå°‘çš„å¤´æ•°
                n_layers=2,       # è¾ƒå°‘çš„å±‚æ•°
                d_ff=1024,        # SwiGLU éšè—ç»´åº¦
                max_len=128,      # è¾ƒçŸ­çš„åºåˆ—é•¿åº¦
                dropout=0.1
            )
            model.to(self.device)
            
            print(f"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ")
            print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {model.get_num_params():,}")
            print(f"ğŸ“Š æ¨¡å‹é…ç½®: d_model={model.d_model}, n_layers={len(model.transformer_blocks)}")
            
            return model
        except Exception as e:
            print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise e
    
    def _load_test_data(self):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        print("ğŸ“ åŠ è½½æµ‹è¯•æ•°æ®...")
        
        # é¦–å…ˆå°è¯•åŠ è½½å®é™…çš„æ•°æ®æ–‡ä»¶
        data_files = [
            "data/dataset/minimind_dataset/pretrain_minimal.jsonl",
            "data/dataset/minimind_dataset/pretrain_test.jsonl"
        ]
        
        texts = []
        for data_file in data_files:
            if os.path.exists(data_file):
                print(f"ğŸ“‚ åŠ è½½æ•°æ®æ–‡ä»¶: {data_file}")
                try:
                    with open(data_file, 'r', encoding='utf-8') as f:
                        for line_num, line in enumerate(f):
                            if line_num >= 10:  # åªåŠ è½½å‰10è¡Œç”¨äºæµ‹è¯•
                                break
                            line = line.strip()
                            if line:
                                try:
                                    data = json.loads(line)
                                    if 'text' in data:
                                        text = data['text']
                                        # æå–å®é™…æ–‡æœ¬å†…å®¹ï¼ˆå»é™¤ç‰¹æ®Šæ ‡è®°ï¼‰
                                        if '<|im_start|>' in text and '<|im_end|>' in text:
                                            text = text.replace('<|im_start|>', '').replace('<|im_end|>', '').strip()
                                        if len(text) > 20 and len(text) < 200:  # é€‰æ‹©åˆé€‚é•¿åº¦çš„æ–‡æœ¬
                                            texts.append(text)
                                except json.JSONDecodeError:
                                    continue
                    print(f"âœ… ä» {data_file} åŠ è½½äº† {len([t for t in texts])} æ¡æ–‡æœ¬")
                except Exception as e:
                    print(f"âš ï¸ åŠ è½½ {data_file} æ—¶å‡ºé”™: {e}")
                    continue
            else:
                print(f"âš ï¸ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼Œåˆ›å»ºä¸€äº›æµ‹è¯•æ•°æ®
        if not texts:
            print("ğŸ“ åˆ›å»ºæµ‹è¯•æ•°æ®...")
            texts = [
                "ä½ å¥½ï¼Œæˆ‘æƒ³å­¦ä¹ å¦‚ä½•ç¼–ç¨‹ï¼Œæœ‰ä»€ä¹ˆå»ºè®®å—ï¼Ÿ",
                "å¸®æˆ‘å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„äº”è¨€è¯—ã€‚",
                "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
                "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå‡ºå»æ•£æ­¥ã€‚",
                "äººå·¥æ™ºèƒ½æ˜¯æœªæ¥ç§‘æŠ€å‘å±•çš„é‡è¦æ–¹å‘ã€‚"
            ]
        
        print(f"âœ… æ€»å…±åŠ è½½äº† {len(texts)} æ¡æµ‹è¯•æ–‡æœ¬")
        for i, text in enumerate(texts[:3]):
            print(f"ğŸ“„ ç¤ºä¾‹ {i+1}: {text[:50]}...")
        
        return texts
    
    def test_tokenizer(self):
        """æµ‹è¯•åˆ†è¯å™¨"""
        print("\n" + "="*50)
        print("ğŸ§ª æµ‹è¯•åˆ†è¯å™¨")
        print("="*50)
        
        test_text = self.test_data[0] if self.test_data else "ä½ å¥½ä¸–ç•Œ"
        print(f"ğŸ“ æµ‹è¯•æ–‡æœ¬: {test_text}")
        
        try:
            # ç¼–ç 
            tokens = self.tokenizer.encode(test_text, add_special_tokens=True)
            print(f"ğŸ”¢ ç¼–ç ç»“æœ: {tokens[:10]}...")
            print(f"ğŸ“ Tokenæ•°é‡: {len(tokens)}")
            
            # è§£ç 
            if hasattr(self.tokenizer, 'decode'):
                decoded = self.tokenizer.decode(tokens)
                print(f"ğŸ“ è§£ç ç»“æœ: {decoded[:50]}...")
            
            print("âœ… åˆ†è¯å™¨æµ‹è¯•é€šè¿‡")
            return True
        except Exception as e:
            print(f"âŒ åˆ†è¯å™¨æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def test_model_layers(self):
        """æµ‹è¯•æ¨¡å‹å„å±‚"""
        print("\n" + "="*50)
        print("ğŸ§ª æµ‹è¯•æ¨¡å‹å„å±‚")
        print("="*50)
        
        try:
            # åˆ›å»ºæµ‹è¯•è¾“å…¥
            test_text = self.test_data[0] if self.test_data else "æµ‹è¯•æ–‡æœ¬"
            tokens = self.tokenizer.encode(test_text, add_special_tokens=True)
            if len(tokens) > 64:
                tokens = tokens[:64]
            while len(tokens) < 10:
                tokens.append(self.tokenizer.pad_id)
            
            input_ids = torch.tensor([tokens], dtype=torch.long).to(self.device)
            print(f"ğŸ“¦ è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
            
            self.model.eval()
            with torch.no_grad():
                # æµ‹è¯•è¯åµŒå…¥å±‚
                print("\nğŸ”¤ æµ‹è¯•è¯åµŒå…¥å±‚...")
                embeddings = self.model.token_embedding(input_ids)
                print(f"âœ… è¯åµŒå…¥è¾“å‡ºå½¢çŠ¶: {embeddings.shape}")
                print(f"ğŸ“Š è¯åµŒå…¥ç»Ÿè®¡: mean={embeddings.mean():.4f}, std={embeddings.std():.4f}")
                
                # æµ‹è¯•ä½ç½®ç¼–ç 
                print("\nğŸ“ æµ‹è¯•ä½ç½®ç¼–ç ...")
                pos_encoded = self.model.positional_encoding(embeddings)
                print(f"âœ… ä½ç½®ç¼–ç è¾“å‡ºå½¢çŠ¶: {pos_encoded.shape}")
                print(f"ğŸ“Š ä½ç½®ç¼–ç ç»Ÿè®¡: mean={pos_encoded.mean():.4f}, std={pos_encoded.std():.4f}")
                
                # æµ‹è¯•æ¯ä¸ªTransformerå—
                x = self.model.dropout(pos_encoded)
                causal_mask = self.model.create_causal_mask(input_ids.size(1)).to(self.device)
                
                for i, transformer_block in enumerate(self.model.transformer_blocks):
                    print(f"\nğŸ”„ æµ‹è¯•Transformerå— {i+1}...")
                    
                    # æµ‹è¯•æ³¨æ„åŠ›æœºåˆ¶
                    print(f"  ğŸ¯ æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›...")
                    attn_output = transformer_block.attention(x, x, x, causal_mask)
                    print(f"  âœ… æ³¨æ„åŠ›è¾“å‡ºå½¢çŠ¶: {attn_output.shape}")
                    print(f"  ğŸ“Š æ³¨æ„åŠ›ç»Ÿè®¡: mean={attn_output.mean():.4f}, std={attn_output.std():.4f}")
                    
                    # åº”ç”¨ç¬¬ä¸€ä¸ªæ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
                    x_after_attn = transformer_block.norm1(x + transformer_block.dropout(attn_output))
                    print(f"  âœ… æ³¨æ„åŠ›åè§„èŒƒåŒ–å½¢çŠ¶: {x_after_attn.shape}")
                    
                    # æµ‹è¯•SwiGLUå‰é¦ˆç½‘ç»œ
                    print(f"  ğŸš€ æµ‹è¯•SwiGLUå‰é¦ˆç½‘ç»œ...")
                    ff_output = transformer_block.feed_forward(x_after_attn)
                    print(f"  âœ… SwiGLUè¾“å‡ºå½¢çŠ¶: {ff_output.shape}")
                    print(f"  ğŸ“Š SwiGLUç»Ÿè®¡: mean={ff_output.mean():.4f}, std={ff_output.std():.4f}")
                    
                    # æ£€æŸ¥SwiGLUå†…éƒ¨ç»„ä»¶
                    ff = transformer_block.feed_forward
                    print(f"  ğŸ” SwiGLUç»„ä»¶:")
                    print(f"    - w_gateæƒé‡å½¢çŠ¶: {ff.w_gate.weight.shape}")
                    print(f"    - w_upæƒé‡å½¢çŠ¶: {ff.w_up.weight.shape}")
                    print(f"    - w_downæƒé‡å½¢çŠ¶: {ff.w_down.weight.shape}")
                    
                    # åº”ç”¨ç¬¬äºŒä¸ªæ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
                    x = transformer_block.norm2(x_after_attn + transformer_block.dropout(ff_output))
                    print(f"  âœ… å—è¾“å‡ºå½¢çŠ¶: {x.shape}")
                    print(f"  ğŸ“Š å—è¾“å‡ºç»Ÿè®¡: mean={x.mean():.4f}, std={x.std():.4f}")
                
                # æµ‹è¯•æœ€ç»ˆå±‚å½’ä¸€åŒ–å’Œè¾“å‡ºæŠ•å½±
                print(f"\nğŸ¯ æµ‹è¯•æœ€ç»ˆå±‚...")
                normalized = self.model.layer_norm(x)
                print(f"âœ… æœ€ç»ˆå±‚å½’ä¸€åŒ–å½¢çŠ¶: {normalized.shape}")
                
                logits = self.model.lm_head(normalized)
                print(f"âœ… æœ€ç»ˆè¾“å‡ºlogitså½¢çŠ¶: {logits.shape}")
                print(f"ğŸ“Š Logitsç»Ÿè®¡: mean={logits.mean():.4f}, std={logits.std():.4f}")
                
                # æµ‹è¯•å®Œæ•´å‰å‘ä¼ æ’­
                print(f"\nğŸ”„ æµ‹è¯•å®Œæ•´å‰å‘ä¼ æ’­...")
                full_logits = self.model(input_ids)
                print(f"âœ… å®Œæ•´å‰å‘ä¼ æ’­è¾“å‡ºå½¢çŠ¶: {full_logits.shape}")
                print(f"ğŸ“Š å®Œæ•´è¾“å‡ºç»Ÿè®¡: mean={full_logits.mean():.4f}, std={full_logits.std():.4f}")
                
                # éªŒè¯è¾“å‡ºæ˜¯å¦ä¸€è‡´
                diff = torch.abs(logits - full_logits).max()
                print(f"ğŸ” åˆ†æ­¥vså®Œæ•´å‰å‘ä¼ æ’­å·®å¼‚: {diff:.8f}")
                if diff < 1e-6:
                    print("âœ… åˆ†æ­¥å’Œå®Œæ•´å‰å‘ä¼ æ’­ç»“æœä¸€è‡´")
                else:
                    print("âš ï¸ åˆ†æ­¥å’Œå®Œæ•´å‰å‘ä¼ æ’­ç»“æœå­˜åœ¨å·®å¼‚")
                
            print("âœ… æ¨¡å‹å„å±‚æµ‹è¯•é€šè¿‡")
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹å±‚æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def test_training_step(self):
        """æµ‹è¯•è®­ç»ƒæ­¥éª¤"""
        print("\n" + "="*50)
        print("ğŸ§ª æµ‹è¯•è®­ç»ƒæ­¥éª¤")
        print("="*50)
        
        try:
            # åˆ›å»ºæ•°æ®é›†
            dataset = LanguageModelingDataset(
                texts=self.test_data[:5],  # åªä½¿ç”¨å‰5æ¡æ•°æ®
                tokenizer=self.tokenizer,
                max_length=64  # è¾ƒçŸ­çš„åºåˆ—ç”¨äºå¿«é€Ÿæµ‹è¯•
            )
            
            dataloader = DataLoader(dataset, batch_size=2, shuffle=True)
            print(f"ğŸ“¦ æ•°æ®é›†å¤§å°: {len(dataset)}")
            print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: 2")
            
            # åˆ›å»ºè®­ç»ƒå™¨
            trainer = PreTrainer(self.model, self.tokenizer, device=self.device)
            
            print("\nğŸ‹ï¸ å¼€å§‹è®­ç»ƒæµ‹è¯•...")
            
            # è®°å½•è®­ç»ƒå‰çš„å‚æ•°
            param_before = {}
            for name, param in self.model.named_parameters():
                if 'feed_forward' in name:  # é‡ç‚¹å…³æ³¨SwiGLUå‚æ•°
                    param_before[name] = param.clone().detach()
            
            print("ğŸ“Š è®­ç»ƒå‰SwiGLUå‚æ•°ç»Ÿè®¡:")
            for name, param in param_before.items():
                print(f"  {name}: mean={param.mean():.6f}, std={param.std():.6f}")
            
            # æ‰§è¡Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤
            self.model.train()
            batch = next(iter(dataloader))
            
            print(f"\nğŸ“¦ å¤„ç†æ‰¹æ¬¡æ•°æ®...")
            input_ids = batch.to(self.device)
            print(f"  è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
            
            # åˆ›å»ºæ ‡ç­¾
            labels = torch.cat([input_ids[:, 1:], 
                              torch.full((input_ids.size(0), 1), 
                                       self.tokenizer.pad_id, device=self.device)], dim=1)
            print(f"  æ ‡ç­¾å½¢çŠ¶: {labels.shape}")
            
            # å‰å‘ä¼ æ’­
            print("ğŸ”„ å‰å‘ä¼ æ’­...")
            logits = self.model(input_ids)
            print(f"âœ… å‰å‘ä¼ æ’­å®Œæˆï¼Œè¾“å‡ºå½¢çŠ¶: {logits.shape}")
            
            # è®¡ç®—æŸå¤±
            print("ğŸ“‰ è®¡ç®—æŸå¤±...")
            loss = trainer.compute_loss(logits, labels)
            print(f"âœ… æŸå¤±è®¡ç®—å®Œæˆ: {loss.item():.4f}")
            
            # åå‘ä¼ æ’­
            print("ğŸ”™ åå‘ä¼ æ’­...")
            trainer.optimizer.zero_grad()
            loss.backward()
            print("âœ… åå‘ä¼ æ’­å®Œæˆ")
            
            # æ£€æŸ¥æ¢¯åº¦
            print("ğŸ” æ£€æŸ¥æ¢¯åº¦...")
            grad_stats = {}
            for name, param in self.model.named_parameters():
                if param.grad is not None and 'feed_forward' in name:
                    grad_norm = param.grad.norm().item()
                    grad_stats[name] = grad_norm
                    print(f"  {name}: æ¢¯åº¦èŒƒæ•°={grad_norm:.6f}")
            
            if not grad_stats:
                print("âš ï¸ æ²¡æœ‰æ£€æµ‹åˆ°SwiGLUç›¸å…³æ¢¯åº¦")
            else:
                print("âœ… SwiGLUå±‚æ¢¯åº¦æ­£å¸¸")
            
            # ä¼˜åŒ–å™¨æ­¥éª¤
            print("âš¡ æ‰§è¡Œä¼˜åŒ–å™¨æ­¥éª¤...")
            trainer.optimizer.step()
            print("âœ… ä¼˜åŒ–å™¨æ­¥éª¤å®Œæˆ")
            
            # æ£€æŸ¥å‚æ•°æ›´æ–°
            print("\nğŸ“Š æ£€æŸ¥å‚æ•°æ›´æ–°...")
            param_updated = False
            for name, param_before_val in param_before.items():
                param_after = dict(self.model.named_parameters())[name]
                diff = torch.abs(param_after - param_before_val).max().item()
                print(f"  {name}: æœ€å¤§å˜åŒ–={diff:.8f}")
                if diff > 1e-8:
                    param_updated = True
            
            if param_updated:
                print("âœ… SwiGLUå‚æ•°æˆåŠŸæ›´æ–°")
            else:
                print("âš ï¸ SwiGLUå‚æ•°æ²¡æœ‰æ˜æ˜¾æ›´æ–°")
            
            print("âœ… è®­ç»ƒæ­¥éª¤æµ‹è¯•é€šè¿‡")
            return True
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒæ­¥éª¤æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def test_model_generation(self):
        """æµ‹è¯•æ¨¡å‹ç”Ÿæˆ"""
        print("\n" + "="*50)
        print("ğŸ§ª æµ‹è¯•æ¨¡å‹ç”Ÿæˆ")
        print("="*50)
        
        try:
            # å‡†å¤‡è¾“å…¥
            prompt = "ä½ å¥½"
            tokens = self.tokenizer.encode(prompt, add_special_tokens=True)
            if len(tokens) > 10:
                tokens = tokens[:10]
            
            input_ids = torch.tensor([tokens], dtype=torch.long).to(self.device)
            print(f"ğŸ“ è¾“å…¥æç¤º: {prompt}")
            print(f"ğŸ“¦ è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
            
            # ç”Ÿæˆæ–‡æœ¬
            print("ğŸ¯ å¼€å§‹ç”Ÿæˆ...")
            with torch.no_grad():
                generated = self.model.generate(
                    input_ids=input_ids,
                    max_length=5,  # åªç”Ÿæˆ5ä¸ªtokenç”¨äºæµ‹è¯•
                    temperature=1.0,
                    top_k=10
                )
            
            print(f"âœ… ç”Ÿæˆå®Œæˆï¼Œè¾“å‡ºå½¢çŠ¶: {generated.shape}")
            print(f"ğŸ“„ ç”Ÿæˆçš„tokenåºåˆ—: {generated[0].tolist()}")
            
            # å°è¯•è§£ç 
            if hasattr(self.tokenizer, 'decode'):
                try:
                    decoded_text = self.tokenizer.decode(generated[0].tolist())
                    print(f"ğŸ“ è§£ç åçš„æ–‡æœ¬: {decoded_text}")
                except:
                    print("âš ï¸ è§£ç å¤±è´¥ï¼Œä½†ç”Ÿæˆè¿‡ç¨‹æ­£å¸¸")
            
            print("âœ… æ¨¡å‹ç”Ÿæˆæµ‹è¯•é€šè¿‡")
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def run_full_test(self):
        """è¿è¡Œå®Œæ•´æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹SwiGLUæ¨¡å‹å®Œæ•´æµ‹è¯•")
        print("="*60)
        
        results = {}
        
        # 1. æµ‹è¯•åˆ†è¯å™¨
        results['tokenizer'] = self.test_tokenizer()
        
        # 2. æµ‹è¯•æ¨¡å‹å„å±‚
        results['model_layers'] = self.test_model_layers()
        
        # 3. æµ‹è¯•è®­ç»ƒæ­¥éª¤
        results['training_step'] = self.test_training_step()
        
        # 4. æµ‹è¯•æ¨¡å‹ç”Ÿæˆ
        results['generation'] = self.test_model_generation()
        
        # æ€»ç»“
        print("\n" + "="*60)
        print("ğŸ“‹ æµ‹è¯•ç»“æœæ€»ç»“")
        print("="*60)
        
        total_tests = len(results)
        passed_tests = sum(results.values())
        
        for test_name, passed in results.items():
            status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
            print(f"{test_name:20}: {status}")
        
        print(f"\nğŸ† æ€»ä½“ç»“æœ: {passed_tests}/{total_tests} é¡¹æµ‹è¯•é€šè¿‡")
        
        if passed_tests == total_tests:
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼SwiGLUæ¨¡å‹å®ç°æ­£ç¡®ï¼")
        else:
            print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")
        
        return passed_tests == total_tests


def main():
    """ä¸»å‡½æ•°"""
    # æ£€æµ‹è®¾å¤‡
    device = 'mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = ModelTestSuite(device=device)
    
    # è¿è¡Œå®Œæ•´æµ‹è¯•
    success = test_suite.run_full_test()
    
    if success:
        print("\nğŸ¯ å»ºè®®ä¸‹ä¸€æ­¥ï¼šè¿è¡Œæ›´é•¿æ—¶é—´çš„è®­ç»ƒæ¥éªŒè¯æ¨¡å‹æ”¶æ•›æ€§")
        print("ğŸ’¡ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œå®Œæ•´è®­ç»ƒ:")
        print("   python scripts/train_optimized.py")
    
    return success


if __name__ == "__main__":
    main()
