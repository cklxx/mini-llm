#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MiniGPTä¼˜åŒ–å¥—ä»¶é›†æˆæ¼”ç¤º
å±•ç¤ºé«˜æ€§èƒ½æ•°æ®åŠ è½½ã€å†…å­˜ä¼˜åŒ–ã€è®­ç»ƒç›‘æ§å’Œæ€§èƒ½åŸºå‡†æµ‹è¯•çš„ååŒä½¿ç”¨
"""
import os
import sys
import time
import json
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.append(str(project_root))
sys.path.append(str(project_root / "src"))

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader

# å¯¼å…¥æˆ‘ä»¬çš„ä¼˜åŒ–æ¨¡å—
from src.data.high_performance_loader import DataLoadingConfig, HighPerformanceDataset
from src.training.memory_optimizer import MemoryOptimizer, MemoryConfig
from src.training.training_monitor import TrainingMonitor
from src.benchmarks.performance_benchmark import PerformanceBenchmarkSuite, BenchmarkConfig, ModelFactory


class OptimizationDemo:
    """ä¼˜åŒ–å¥—ä»¶æ¼”ç¤ºç±»"""

    def __init__(self, device: str = "auto"):
        """åˆå§‹åŒ–æ¼”ç¤ºç¯å¢ƒ"""
        print("ğŸš€ MiniGPTä¼˜åŒ–å¥—ä»¶æ¼”ç¤º")
        print("=" * 60)

        # è®¾å¤‡é€‰æ‹©
        if device == "auto":
            if torch.cuda.is_available():
                self.device = torch.device("cuda")
                print(f"ğŸ¯ ä½¿ç”¨CUDA GPU: {torch.cuda.get_device_name()}")
            elif torch.backends.mps.is_available():
                self.device = torch.device("mps")
                print("ğŸ¯ ä½¿ç”¨Apple Silicon GPU (MPS)")
            else:
                self.device = torch.device("cpu")
                print("ğŸ¯ ä½¿ç”¨CPU")
        else:
            self.device = torch.device(device)
            print(f"ğŸ¯ ä½¿ç”¨æŒ‡å®šè®¾å¤‡: {device}")

        # åˆ›å»ºæ¼”ç¤ºç›®å½•
        self.demo_dir = project_root / "demo_results"
        self.demo_dir.mkdir(exist_ok=True)

        print(f"ğŸ“ æ¼”ç¤ºç»“æœå°†ä¿å­˜åˆ°: {self.demo_dir}")
        print()

    def create_dummy_dataset(self, size: int = 1000, seq_len: int = 512) -> str:
        """åˆ›å»ºè™šæ‹Ÿè®­ç»ƒæ•°æ®"""
        print(f"ğŸ“ åˆ›å»ºè™šæ‹Ÿæ•°æ®é›† ({size} æ ·æœ¬, åºåˆ—é•¿åº¦ {seq_len})")

        data_file = self.demo_dir / "demo_dataset.jsonl"

        # ç”Ÿæˆè™šæ‹Ÿå¯¹è¯æ•°æ®
        with open(data_file, 'w', encoding='utf-8') as f:
            for i in range(size):
                conversation = {
                    "conversations": [
                        {
                            "role": "user",
                            "content": f"è¿™æ˜¯æµ‹è¯•é—®é¢˜ {i}ï¼Œç”¨äºæ¼”ç¤ºé«˜æ€§èƒ½æ•°æ®åŠ è½½ç³»ç»Ÿçš„æ•ˆæœã€‚" * (seq_len // 50)
                        },
                        {
                            "role": "assistant",
                            "content": f"è¿™æ˜¯æµ‹è¯•å›ç­” {i}ï¼Œå±•ç¤ºäº†æˆ‘ä»¬çš„ä¼˜åŒ–æ•°æ®å¤„ç†æµç¨‹ã€‚" * (seq_len // 50)
                        }
                    ]
                }
                f.write(json.dumps(conversation, ensure_ascii=False) + '\n')

        print(f"âœ… æ•°æ®é›†å·²åˆ›å»º: {data_file}")
        return str(data_file)

    def demo_high_performance_data_loading(self, data_path: str):
        """æ¼”ç¤ºé«˜æ€§èƒ½æ•°æ®åŠ è½½"""
        print("\n" + "="*60)
        print("ğŸ“¦ é«˜æ€§èƒ½æ•°æ®åŠ è½½æ¼”ç¤º")
        print("="*60)

        # åˆ›å»ºè™šæ‹Ÿtokenizer
        class DemoTokenizer:
            def __init__(self):
                self.vocab_size = 10000
                self.pad_id = 0
                self.bos_id = 1
                self.eos_id = 2

            def encode(self, text, add_special_tokens=True):
                # ç®€å•çš„å­—ç¬¦çº§tokenizationæ¼”ç¤º
                tokens = [hash(char) % self.vocab_size for char in text[:100]]
                if add_special_tokens:
                    tokens = [self.bos_id] + tokens + [self.eos_id]
                return tokens

            def get_vocab_size(self):
                return self.vocab_size

        tokenizer = DemoTokenizer()

        # é…ç½®æ•°æ®åŠ è½½
        configs = [
            # åŸºç¡€é…ç½®
            DataLoadingConfig(
                data_path=data_path,
                batch_size=16,
                num_workers=0,
                enable_cache=False,
                streaming=False,
                parallel_processing=False,
                cache_dir=str(self.demo_dir / "cache_basic")
            ),
            # ä¼˜åŒ–é…ç½®
            DataLoadingConfig(
                data_path=data_path,
                batch_size=16,
                num_workers=4,
                enable_cache=True,
                streaming=True,
                parallel_processing=True,
                cache_dir=str(self.demo_dir / "cache_optimized")
            )
        ]

        results = {}

        for i, config in enumerate(configs):
            config_name = "åŸºç¡€é…ç½®" if i == 0 else "ä¼˜åŒ–é…ç½®"
            print(f"\nğŸ“Š æµ‹è¯• {config_name}:")
            print(f"   ç¼“å­˜: {'å¯ç”¨' if config.enable_cache else 'ç¦ç”¨'}")
            print(f"   æµå¼åŠ è½½: {'å¯ç”¨' if config.streaming else 'ç¦ç”¨'}")
            print(f"   å¹¶è¡Œå¤„ç†: {'å¯ç”¨' if config.parallel_processing else 'ç¦ç”¨'}")
            print(f"   Workeræ•°: {config.num_workers}")

            try:
                # åˆ›å»ºæ•°æ®é›†ï¼ˆæ¨¡æ‹Ÿï¼Œå› ä¸ºHighPerformanceDatasetéœ€è¦å®é™…æ–‡ä»¶ï¼‰
                start_time = time.time()

                # è¿™é‡Œæˆ‘ä»¬æ¨¡æ‹Ÿæ•°æ®åŠ è½½è¿‡ç¨‹
                dummy_data = []
                for j in range(100):  # æ¨¡æ‹Ÿ100ä¸ªæ ·æœ¬
                    dummy_data.append({
                        'input': f"test input {j}",
                        'output': f"test output {j}",
                        'length': 50
                    })

                # æ¨¡æ‹Ÿtokenization
                processed_data = []
                for item in dummy_data:
                    input_ids = tokenizer.encode(item['input'])
                    output_ids = tokenizer.encode(item['output'])

                    # æ„é€ åºåˆ—
                    sequence = [tokenizer.bos_id] + input_ids + output_ids + [tokenizer.eos_id]
                    if len(sequence) > config.max_length:
                        sequence = sequence[:config.max_length]

                    # å¡«å……
                    while len(sequence) < config.max_length:
                        sequence.append(tokenizer.pad_id)

                    processed_data.append(torch.tensor(sequence))

                # åˆ›å»ºDataLoader
                dataset = TensorDataset(torch.stack(processed_data))
                dataloader = DataLoader(
                    dataset,
                    batch_size=config.batch_size,
                    num_workers=config.num_workers,
                    pin_memory=torch.cuda.is_available()
                )

                # æµ‹è¯•åŠ è½½é€Ÿåº¦
                load_start = time.time()
                batch_count = 0
                for batch in dataloader:
                    batch_count += 1
                    if batch_count >= 20:  # é™åˆ¶æµ‹è¯•æ‰¹æ¬¡
                        break

                load_time = time.time() - load_start
                total_time = time.time() - start_time

                results[config_name] = {
                    'total_time': total_time,
                    'load_time': load_time,
                    'samples_per_sec': len(processed_data) / total_time
                }

                print(f"   â±ï¸  æ€»æ—¶é—´: {total_time:.2f}s")
                print(f"   ğŸ“ˆ å¤„ç†é€Ÿåº¦: {results[config_name]['samples_per_sec']:.1f} samples/sec")

            except Exception as e:
                print(f"   âŒ é”™è¯¯: {e}")
                results[config_name] = {'error': str(e)}

        # æ€§èƒ½å¯¹æ¯”
        if len(results) == 2 and all('error' not in r for r in results.values()):
            åŸºç¡€ = results["åŸºç¡€é…ç½®"]
            ä¼˜åŒ– = results["ä¼˜åŒ–é…ç½®"]

            speedup = ä¼˜åŒ–['samples_per_sec'] / åŸºç¡€['samples_per_sec']
            print(f"\nğŸ† æ€§èƒ½æå‡æ€»ç»“:")
            print(f"   åŸºç¡€é…ç½®: {åŸºç¡€['samples_per_sec']:.1f} samples/sec")
            print(f"   ä¼˜åŒ–é…ç½®: {ä¼˜åŒ–['samples_per_sec']:.1f} samples/sec")
            print(f"   åŠ é€Ÿæ¯”: {speedup:.2f}x")

        return results

    def demo_memory_optimization(self):
        """æ¼”ç¤ºå†…å­˜ä¼˜åŒ–åŠŸèƒ½"""
        print("\n" + "="*60)
        print("ğŸ§  å†…å­˜ä¼˜åŒ–æ¼”ç¤º")
        print("="*60)

        # åˆ›å»ºæµ‹è¯•æ¨¡å‹
        model = ModelFactory.create_test_transformer("small", vocab_size=10000)
        model.to(self.device)

        print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {ModelFactory.get_model_params(model):,}")

        # æµ‹è¯•ä¸åŒä¼˜åŒ–é…ç½®
        configs = [
            ("åŸºç¡€é…ç½®", MemoryConfig(
                enable_amp=False,
                gradient_accumulation_steps=1,
                enable_gradient_checkpointing=False
            )),
            ("æ··åˆç²¾åº¦", MemoryConfig(
                enable_amp=True,
                gradient_accumulation_steps=1,
                enable_gradient_checkpointing=False
            )),
            ("æ¢¯åº¦ç´¯ç§¯", MemoryConfig(
                enable_amp=True,
                gradient_accumulation_steps=4,
                enable_gradient_checkpointing=False
            )),
            ("å®Œæ•´ä¼˜åŒ–", MemoryConfig(
                enable_amp=True,
                gradient_accumulation_steps=4,
                enable_gradient_checkpointing=True,
                adaptive_batch_size=True
            ))
        ]

        results = {}

        for config_name, mem_config in configs:
            print(f"\nğŸ”¬ æµ‹è¯• {config_name}:")
            print(f"   æ··åˆç²¾åº¦: {'å¯ç”¨' if mem_config.enable_amp else 'ç¦ç”¨'}")
            print(f"   æ¢¯åº¦ç´¯ç§¯: {mem_config.gradient_accumulation_steps} æ­¥")
            print(f"   æ¢¯åº¦æ£€æŸ¥ç‚¹: {'å¯ç”¨' if mem_config.enable_gradient_checkpointing else 'ç¦ç”¨'}")

            try:
                # åˆ›å»ºä¼˜åŒ–å™¨
                optimizer = optim.AdamW(model.parameters(), lr=1e-4)
                memory_optimizer = MemoryOptimizer(model, mem_config, self.device)

                # åˆ›å»ºæµ‹è¯•æ•°æ®
                batch_size = 16
                seq_len = 512
                vocab_size = 10000

                dummy_input = torch.randint(0, vocab_size, (batch_size, seq_len), device=self.device)
                dummy_target = torch.randint(0, vocab_size, (batch_size, seq_len), device=self.device)

                # è®°å½•åˆå§‹å†…å­˜
                initial_memory = self._get_memory_usage()

                # æµ‹è¯•è®­ç»ƒæ­¥éª¤
                start_time = time.time()
                steps = 10

                for step in range(steps):
                    try:
                        with memory_optimizer.optimize_step_context(optimizer) as ctx:
                            # å‰å‘ä¼ æ’­
                            output = model(dummy_input)
                            loss = nn.CrossEntropyLoss()(
                                output.reshape(-1, vocab_size),
                                dummy_target.reshape(-1)
                            )

                            # ä¼˜åŒ–æŸå¤±å’Œåå‘ä¼ æ’­
                            optimized_loss = memory_optimizer.compute_loss(loss)
                            memory_optimizer.backward(optimized_loss)

                    except Exception as e:
                        if "out of memory" in str(e).lower():
                            print(f"   âš ï¸  OOM at step {step}")
                            break
                        else:
                            raise e

                elapsed_time = time.time() - start_time
                final_memory = self._get_memory_usage()
                memory_used = final_memory - initial_memory

                # è·å–ä¼˜åŒ–å™¨ç»Ÿè®¡
                stats = memory_optimizer.get_memory_stats()

                results[config_name] = {
                    'elapsed_time': elapsed_time,
                    'memory_used_mb': memory_used,
                    'samples_per_sec': (steps * batch_size) / elapsed_time,
                    'amp_scale': stats.get('amp_scale', 1.0),
                    'oom_count': stats.get('oom_count', 0)
                }

                print(f"   â±ï¸  è®­ç»ƒæ—¶é—´: {elapsed_time:.2f}s")
                print(f"   ğŸ’¾ å†…å­˜ä½¿ç”¨: {memory_used:.1f}MB")
                print(f"   ğŸ“ˆ è®­ç»ƒé€Ÿåº¦: {results[config_name]['samples_per_sec']:.1f} samples/sec")
                if stats.get('amp_scale'):
                    print(f"   ğŸ”¢ AMPç¼©æ”¾: {stats['amp_scale']:.0f}")

            except Exception as e:
                print(f"   âŒ é”™è¯¯: {e}")
                results[config_name] = {'error': str(e)}

        # æ€§èƒ½å¯¹æ¯”æ€»ç»“
        valid_results = {k: v for k, v in results.items() if 'error' not in v}
        if len(valid_results) >= 2:
            print(f"\nğŸ† å†…å­˜ä¼˜åŒ–æ•ˆæœæ€»ç»“:")
            baseline = valid_results.get("åŸºç¡€é…ç½®")
            optimized = valid_results.get("å®Œæ•´ä¼˜åŒ–")

            if baseline and optimized:
                speed_improvement = optimized['samples_per_sec'] / baseline['samples_per_sec']
                memory_reduction = (baseline['memory_used_mb'] - optimized['memory_used_mb']) / baseline['memory_used_mb'] * 100

                print(f"   è®­ç»ƒé€Ÿåº¦æå‡: {speed_improvement:.2f}x")
                print(f"   å†…å­˜ä½¿ç”¨å‡å°‘: {memory_reduction:.1f}%")

        return results

    def demo_training_monitoring(self):
        """æ¼”ç¤ºè®­ç»ƒç›‘æ§åŠŸèƒ½"""
        print("\n" + "="*60)
        print("ğŸ” è®­ç»ƒç›‘æ§æ¼”ç¤º")
        print("="*60)

        # åˆ›å»ºæµ‹è¯•æ¨¡å‹
        model = ModelFactory.create_test_transformer("small", vocab_size=10000)
        model.to(self.device)

        # åˆå§‹åŒ–è®­ç»ƒç›‘æ§å™¨
        monitor = TrainingMonitor(
            model=model,
            log_dir=str(self.demo_dir / "training_logs"),
            enable_tensorboard=True,
            enable_real_time_plots=False  # åœ¨æ¼”ç¤ºä¸­ç¦ç”¨å®æ—¶ç»˜å›¾
        )

        print("ğŸ“Š è®­ç»ƒç›‘æ§å™¨å·²åˆå§‹åŒ–")
        print(f"   æ—¥å¿—ç›®å½•: {self.demo_dir / 'training_logs'}")
        print(f"   TensorBoard: å¯ç”¨")

        # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
        optimizer = optim.AdamW(model.parameters(), lr=1e-4)
        batch_size = 16
        seq_len = 512
        vocab_size = 10000

        print(f"\nğŸš€ å¼€å§‹æ¨¡æ‹Ÿè®­ç»ƒ (æ‰¹å¤„ç†å¤§å°: {batch_size}, åºåˆ—é•¿åº¦: {seq_len})")

        training_metrics = []

        for epoch in range(2):
            for step in range(20):  # æ¯ä¸ªepoch 20æ­¥
                global_step = epoch * 20 + step

                # åˆ›å»ºè™šæ‹Ÿæ•°æ®
                dummy_input = torch.randint(0, vocab_size, (batch_size, seq_len), device=self.device)
                dummy_target = torch.randint(0, vocab_size, (batch_size, seq_len), device=self.device)

                # å‰å‘ä¼ æ’­
                output = model(dummy_input)
                loss = nn.CrossEntropyLoss()(
                    output.reshape(-1, vocab_size),
                    dummy_target.reshape(-1)
                )

                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                # è®¡ç®—å­¦ä¹ ç‡ï¼ˆæ¨¡æ‹Ÿè¡°å‡ï¼‰
                lr = 1e-4 * (0.95 ** (global_step // 10))
                for param_group in optimizer.param_groups:
                    param_group['lr'] = lr

                # è®°å½•ç›‘æ§æŒ‡æ ‡
                metrics = monitor.log_step(
                    step=global_step,
                    epoch=epoch,
                    loss=loss.item(),
                    learning_rate=lr,
                    batch_size=batch_size
                )

                training_metrics.append(metrics)

                # æ¯10æ­¥æ‰“å°ä¸€æ¬¡è¿›åº¦
                if global_step % 10 == 0:
                    print(f"   Step {global_step}: Loss = {loss.item():.4f}, "
                          f"LR = {lr:.2e}, "
                          f"Speed = {metrics.samples_per_sec:.1f} samples/sec")

        print(f"\nâœ… è®­ç»ƒç›‘æ§æ¼”ç¤ºå®Œæˆ!")
        print(f"   æ€»æ­¥æ•°: {len(training_metrics)}")
        print(f"   æœ€ç»ˆæŸå¤±: {training_metrics[-1].loss:.4f}")
        print(f"   å¹³å‡è®­ç»ƒé€Ÿåº¦: {np.mean([m.samples_per_sec for m in training_metrics]):.1f} samples/sec")

        # å…³é—­ç›‘æ§å™¨å¹¶ç”ŸæˆæŠ¥å‘Š
        monitor.close()

        return training_metrics

    def demo_performance_benchmark(self):
        """æ¼”ç¤ºæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("\n" + "="*60)
        print("ğŸ æ€§èƒ½åŸºå‡†æµ‹è¯•æ¼”ç¤º")
        print("="*60)

        # é…ç½®åŸºå‡†æµ‹è¯•
        config = BenchmarkConfig(
            test_batch_sizes=[8, 16, 32],
            test_sequence_lengths=[256, 512],
            test_model_sizes=["tiny", "small"],
            test_steps=10,  # å‡å°‘æ­¥æ•°åŠ å¿«æ¼”ç¤º
            output_dir=str(self.demo_dir / "benchmark_results"),
            test_training=True,
            test_inference=True,
            test_data_loading=False,  # è·³è¿‡æ•°æ®åŠ è½½æµ‹è¯•ä»¥ç®€åŒ–æ¼”ç¤º
            test_memory_optimization=True
        )

        print(f"ğŸ”§ åŸºå‡†æµ‹è¯•é…ç½®:")
        print(f"   æ‰¹å¤„ç†å¤§å°: {config.test_batch_sizes}")
        print(f"   åºåˆ—é•¿åº¦: {config.test_sequence_lengths}")
        print(f"   æ¨¡å‹å¤§å°: {config.test_model_sizes}")
        print(f"   æµ‹è¯•æ­¥æ•°: {config.test_steps}")

        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        benchmark_suite = PerformanceBenchmarkSuite(config)
        results = benchmark_suite.run_all_benchmarks()

        print(f"\nğŸ“Š åŸºå‡†æµ‹è¯•å®Œæˆ! å…±è¿è¡Œ {len(results)} ä¸ªæµ‹è¯•")

        # åˆ†æç»“æœ
        training_results = [r for r in results if r.test_name == "training_baseline"]
        if training_results:
            best_result = max(training_results, key=lambda x: x.metrics['samples_per_sec'])
            print(f"\nğŸ† æœ€ä½³è®­ç»ƒé…ç½®:")
            print(f"   æ¨¡å‹: {best_result.config['model_size']}")
            print(f"   æ‰¹å¤„ç†å¤§å°: {best_result.config['batch_size']}")
            print(f"   åºåˆ—é•¿åº¦: {best_result.config['seq_len']}")
            print(f"   ååé‡: {best_result.metrics['samples_per_sec']:.1f} samples/sec")

        inference_results = [r for r in results if r.test_name == "inference"]
        if inference_results:
            best_inference = max(inference_results, key=lambda x: x.metrics['tokens_per_sec'])
            print(f"\nğŸš€ æœ€ä½³æ¨ç†é…ç½®:")
            print(f"   æ¨¡å‹: {best_inference.config['model_size']}")
            print(f"   æ‰¹å¤„ç†å¤§å°: {best_inference.config['batch_size']}")
            print(f"   ååé‡: {best_inference.metrics['tokens_per_sec']:.0f} tokens/sec")

        return results

    def _get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡(MB)"""
        if self.device.type == 'cuda':
            return torch.cuda.memory_allocated() / 1024 / 1024
        else:
            import psutil
            return psutil.Process().memory_info().rss / 1024 / 1024

    def run_complete_demo(self):
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        print(f"ğŸ¬ å¼€å§‹MiniGPTä¼˜åŒ–å¥—ä»¶å®Œæ•´æ¼”ç¤º")
        print(f"âš™ï¸  è®¾å¤‡: {self.device}")
        print()

        try:
            # 1. åˆ›å»ºæ¼”ç¤ºæ•°æ®
            data_path = self.create_dummy_dataset(500, 256)

            # 2. é«˜æ€§èƒ½æ•°æ®åŠ è½½æ¼”ç¤º
            data_results = self.demo_high_performance_data_loading(data_path)

            # 3. å†…å­˜ä¼˜åŒ–æ¼”ç¤º
            memory_results = self.demo_memory_optimization()

            # 4. è®­ç»ƒç›‘æ§æ¼”ç¤º
            monitoring_results = self.demo_training_monitoring()

            # 5. æ€§èƒ½åŸºå‡†æµ‹è¯•æ¼”ç¤º
            benchmark_results = self.demo_performance_benchmark()

            # 6. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
            self._generate_demo_summary({
                'data_loading': data_results,
                'memory_optimization': memory_results,
                'training_monitoring': monitoring_results,
                'benchmarks': benchmark_results
            })

            print("\n" + "="*60)
            print("ğŸ‰ ä¼˜åŒ–å¥—ä»¶æ¼”ç¤ºå®Œæˆ!")
            print("="*60)
            print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {self.demo_dir}")
            print("ğŸ“Š æŸ¥çœ‹ä»¥ä¸‹æ–‡ä»¶äº†è§£è¯¦ç»†ç»“æœ:")
            print(f"   - æ¼”ç¤ºæ€»ç»“: {self.demo_dir}/demo_summary.json")
            print(f"   - è®­ç»ƒæ—¥å¿—: {self.demo_dir}/training_logs/")
            print(f"   - åŸºå‡†æµ‹è¯•: {self.demo_dir}/benchmark_results/")
            print()
            print("ğŸš€ å»ºè®®:")
            print("   1. æŸ¥çœ‹TensorBoard: tensorboard --logdir demo_results/training_logs")
            print("   2. æŸ¥çœ‹åŸºå‡†æµ‹è¯•å›¾è¡¨: demo_results/benchmark_results/*.png")
            print("   3. é˜…è¯»æ€§èƒ½åˆ†ææŠ¥å‘Š: demo_results/benchmark_results/performance_analysis.json")

        except Exception as e:
            print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

    def _generate_demo_summary(self, results: dict):
        """ç”Ÿæˆæ¼”ç¤ºæ€»ç»“æŠ¥å‘Š"""
        summary = {
            'demo_info': {
                'timestamp': time.time(),
                'device': str(self.device),
                'device_name': self._get_device_name()
            },
            'results': results,
            'key_improvements': self._extract_key_improvements(results)
        }

        summary_file = self.demo_dir / "demo_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False, default=str)

        print(f"\nğŸ“‹ æ¼”ç¤ºæ€»ç»“å·²ä¿å­˜: {summary_file}")

    def _get_device_name(self) -> str:
        """è·å–è®¾å¤‡åç§°"""
        if self.device.type == 'cuda':
            return torch.cuda.get_device_name()
        elif self.device.type == 'mps':
            return "Apple Silicon GPU (MPS)"
        else:
            return "CPU"

    def _extract_key_improvements(self, results: dict) -> dict:
        """æå–å…³é”®æ”¹è¿›æŒ‡æ ‡"""
        improvements = {}

        # æ•°æ®åŠ è½½æ”¹è¿›
        data_results = results.get('data_loading', {})
        if 'åŸºç¡€é…ç½®' in data_results and 'ä¼˜åŒ–é…ç½®' in data_results:
            baseline = data_results['åŸºç¡€é…ç½®']
            optimized = data_results['ä¼˜åŒ–é…ç½®']
            if 'samples_per_sec' in baseline and 'samples_per_sec' in optimized:
                improvements['data_loading_speedup'] = optimized['samples_per_sec'] / baseline['samples_per_sec']

        # å†…å­˜ä¼˜åŒ–æ”¹è¿›
        memory_results = results.get('memory_optimization', {})
        if 'åŸºç¡€é…ç½®' in memory_results and 'å®Œæ•´ä¼˜åŒ–' in memory_results:
            baseline = memory_results['åŸºç¡€é…ç½®']
            optimized = memory_results['å®Œæ•´ä¼˜åŒ–']
            if 'samples_per_sec' in baseline and 'samples_per_sec' in optimized:
                improvements['memory_optimization_speedup'] = optimized['samples_per_sec'] / baseline['samples_per_sec']

        return improvements


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="MiniGPTä¼˜åŒ–å¥—ä»¶æ¼”ç¤º")
    parser.add_argument("--device", type=str, default="auto",
                       choices=["auto", "cuda", "mps", "cpu"],
                       help="æŒ‡å®šä½¿ç”¨çš„è®¾å¤‡")
    parser.add_argument("--quick", action="store_true",
                       help="å¿«é€Ÿæ¼”ç¤ºæ¨¡å¼ï¼ˆå‡å°‘æµ‹è¯•è§„æ¨¡ï¼‰")

    args = parser.parse_args()

    # åˆ›å»ºå¹¶è¿è¡Œæ¼”ç¤º
    demo = OptimizationDemo(device=args.device)
    demo.run_complete_demo()


if __name__ == "__main__":
    main()