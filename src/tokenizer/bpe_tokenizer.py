"""
æ‰‹å†™å®ç°BPE (Byte Pair Encoding) Tokenizer
ç”¨äºæ–°æ‰‹æ•™å­¦ç†è§£åˆ†è¯åŸç†ï¼Œä¼˜åŒ–äº†ä¸­æ–‡å¤„ç†
"""
import json
import re
import time
import sys
import random
import os
from typing import List, Dict, Tuple, Optional
from collections import defaultdict, Counter
import pickle
from multiprocessing import Pool, cpu_count
from functools import partial


def is_chinese_char(char):
    """åˆ¤æ–­æ˜¯å¦ä¸ºä¸­æ–‡å­—ç¬¦"""
    return '\u4e00' <= char <= '\u9fff'


def print_progress_bar(current: int, total: int, prefix: str = '', suffix: str = '', 
                      decimals: int = 1, length: int = 40, fill: str = 'â–ˆ', 
                      empty: str = 'â–‘'):
    """æ‰“å°åŠ¨æ€è¿›åº¦æ¡ï¼Œä¼šè¦†ç›–ä¸Šä¸€è¡Œ"""
    percent = ("{0:." + str(decimals) + "f}").format(100 * (current / float(total)))
    filled_length = int(length * current // total)
    bar = fill * filled_length + empty * (length - filled_length)
    
    # ä½¿ç”¨ \r å›åˆ°è¡Œé¦–ï¼Œè¦†ç›–ä¸Šä¸€è¡Œå†…å®¹
    print(f'\r{prefix} |{bar}| {percent}% {suffix}', end='', flush=True)
    
    # å¦‚æœå®Œæˆï¼Œæ¢è¡Œ
    if current == total:
        print()


def show_progress_with_stats(current: int, total: int, start_time: float, 
                           prefix: str = '', extra_info: str = ''):
    """ç»Ÿä¸€çš„è¿›åº¦æ˜¾ç¤ºå‡½æ•°ï¼ŒåŒ…å«é€Ÿåº¦å’Œé¢„è®¡å®Œæˆæ—¶é—´"""
    elapsed = time.time() - start_time
    speed = current / elapsed if elapsed > 0 else 0
    eta = (total - current) / speed if speed > 0 else 0
    
    suffix = f"({current:,}/{total:,}) {speed:.0f}/ç§’"
    if eta > 0:
        suffix += f" ETA: {eta:.0f}ç§’"
    if extra_info:
        suffix += f" | {extra_info}"
    
    print_progress_bar(current, total, prefix=prefix, suffix=suffix)


class BPETokenizer:
    """BPEåˆ†è¯å™¨å®ç° - ä¼˜åŒ–äº†ä¸­æ–‡å¤„ç†
    
    BPEç®—æ³•æ ¸å¿ƒæ€æƒ³ï¼š
    1. å°†æ–‡æœ¬åˆ†å‰²æˆå­—ç¬¦
    2. ç»Ÿè®¡ç›¸é‚»å­—ç¬¦å¯¹çš„é¢‘ç‡
    3. åˆå¹¶é¢‘ç‡æœ€é«˜çš„å­—ç¬¦å¯¹
    4. é‡å¤æ­¥éª¤2-3ç›´åˆ°è¾¾åˆ°è¯æ±‡è¡¨å¤§å°
    """
    
    def __init__(self, vocab_size: int = 30000):  # å¢åŠ é»˜è®¤è¯æ±‡è¡¨å¤§å°
        self.vocab_size = vocab_size
        self.word_freqs = {}  # è¯é¢‘ç»Ÿè®¡
        self.splits = {}      # è¯çš„åˆ†å‰²ç»“æœ
        self.merges = {}      # åˆå¹¶è§„åˆ™
        self.vocab = {}       # è¯æ±‡è¡¨
        
        # ç‰¹æ®Štoken
        self.pad_token = '<PAD>'
        self.unk_token = '<UNK>'
        self.bos_token = '<BOS>'  # Begin of sequence
        self.eos_token = '<EOS>'  # End of sequence
        
        # ç‰¹æ®Štokençš„ID
        self.pad_id = 0
        self.unk_id = 1
        self.bos_id = 2
        self.eos_id = 3
        
    def pre_tokenize(self, text: str) -> List[str]:
        """é¢„åˆ†è¯ï¼šæ”¹è¿›ä¸­æ–‡å¤„ç†"""
        # é’ˆå¯¹ä¸­æ–‡ä¼˜åŒ–çš„é¢„åˆ†è¯è§„åˆ™
        words = []
        current_word = ""
        
        for char in text:
            if is_chinese_char(char):
                # ä¸­æ–‡å­—ç¬¦ä½œä¸ºç‹¬ç«‹çš„è¯
                if current_word:
                    words.append(current_word.lower())
                    current_word = ""
                words.append(char)
            elif char.isalnum():
                # è‹±æ–‡/æ•°å­—å­—ç¬¦ç´¯ç§¯
                current_word += char
            elif char.isspace():
                # ç©ºæ ¼åˆ†éš”
                if current_word:
                    words.append(current_word.lower())
                    current_word = ""
            else:
                # æ ‡ç‚¹ç¬¦å·
                if current_word:
                    words.append(current_word.lower())
                    current_word = ""
                if char.strip():  # éç©ºç™½æ ‡ç‚¹
                    words.append(char)
        
        # å¤„ç†æœ€åçš„è¯
        if current_word:
            words.append(current_word.lower())
        
        return [w for w in words if w.strip()]  # è¿‡æ»¤ç©ºè¯
    
    def compute_word_frequencies(self, texts: List[str]):
        """è®¡ç®—è¯é¢‘"""
        word_freqs = defaultdict(int)
        
        print("æ­£åœ¨è®¡ç®—è¯é¢‘...")
        start_time = time.time()
        total_texts = len(texts)
        
        for i, text in enumerate(texts):
            words = self.pre_tokenize(text)
            for word in words:
                word_freqs[word] += 1
            
            # æ˜¾ç¤ºåŠ¨æ€è¿›åº¦
            if (i + 1) % 1000 == 0 or i == total_texts - 1:
                show_progress_with_stats(i + 1, total_texts, start_time, prefix='è¯é¢‘è®¡ç®—')
        
        # é’ˆå¯¹ä¸­æ–‡ä¼˜åŒ–çš„è¿‡æ»¤ç­–ç•¥
        original_vocab_size = len(word_freqs)
        
        # ä¸­æ–‡å­—ç¬¦ä¿ç•™æ›´ä½çš„é¢‘ç‡é˜ˆå€¼
        chinese_chars = {word: freq for word, freq in word_freqs.items() 
                        if len(word) == 1 and is_chinese_char(word)}
        other_words = {word: freq for word, freq in word_freqs.items() 
                      if not (len(word) == 1 and is_chinese_char(word))}
        
        # ä¸­æ–‡å­—ç¬¦ï¼šé¢‘ç‡>=1
        # å…¶ä»–è¯ï¼šæ ¹æ®æ€»è¯æ±‡é‡åŠ¨æ€è°ƒæ•´
        min_freq_chinese = 1
        min_freq_other = 2 if len(other_words) <= 30000 else max(2, len(other_words) // 15000)
        
        filtered_word_freqs = {}
        filtered_word_freqs.update({word: freq for word, freq in chinese_chars.items() 
                                  if freq >= min_freq_chinese})
        filtered_word_freqs.update({word: freq for word, freq in other_words.items() 
                                  if freq >= min_freq_other})
        
        self.word_freqs = filtered_word_freqs
        elapsed = time.time() - start_time
        
        chinese_kept = len([w for w in filtered_word_freqs if len(w) == 1 and is_chinese_char(w)])
        print(f"âœ… è¯é¢‘è®¡ç®—å®Œæˆ! {len(filtered_word_freqs):,}è¯æ±‡ (ä¸­æ–‡:{chinese_kept:,}) è€—æ—¶:{elapsed:.1f}ç§’")
    
    def initialize_splits(self):
        """åˆå§‹åŒ–åˆ†å‰²ï¼šå°†æ¯ä¸ªè¯åˆ†å‰²æˆå­—ç¬¦"""
        splits = {}
        for word in self.word_freqs:
            # å¯¹ä¸­æ–‡å­—ç¬¦ï¼Œä¸éœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
            if len(word) == 1 and is_chinese_char(word):
                splits[word] = [word, '</w>']
            else:
                # å¯¹å…¶ä»–è¯ï¼Œåˆ†å‰²æˆå­—ç¬¦
                splits[word] = [c for c in word] + ['</w>']
        self.splits = splits
        print(f"âœ… åˆå§‹åŒ–åˆ†å‰²å®Œæˆ! {len(splits):,}ä¸ªè¯")
    
    def compute_pair_frequencies(self) -> Dict[Tuple[str, str], int]:
        """è®¡ç®—ç›¸é‚»å­—ç¬¦å¯¹çš„é¢‘ç‡"""
        pair_freqs = defaultdict(int)
        
        for word, freq in self.word_freqs.items():
            split = self.splits[word]
            
            # è®¡ç®—ç›¸é‚»å­—ç¬¦å¯¹
            for i in range(len(split) - 1):
                pair = (split[i], split[i + 1])
                pair_freqs[pair] += freq
        
        return dict(pair_freqs)
    
    def merge_vocab(self, pair: Tuple[str, str]):
        """åˆå¹¶è¯æ±‡è¡¨ä¸­çš„å­—ç¬¦å¯¹"""
        new_splits = {}
        merge_count = 0  # ç»Ÿè®¡å®é™…åˆå¹¶æ¬¡æ•°
        
        for word in self.word_freqs:
            split = self.splits[word]
            new_split = []
            i = 0
            
            while i < len(split):
                # æŸ¥æ‰¾è¦åˆå¹¶çš„å­—ç¬¦å¯¹
                if i < len(split) - 1 and (split[i], split[i + 1]) == pair:
                    # åˆå¹¶å­—ç¬¦å¯¹
                    new_split.append(split[i] + split[i + 1])
                    merge_count += 1
                    i += 2
                else:
                    new_split.append(split[i])
                    i += 1
            
            new_splits[word] = new_split
        
        self.splits = new_splits
        return merge_count  # è¿”å›å®é™…åˆå¹¶æ¬¡æ•°
    
    def train(self, texts: List[str]):
        """è®­ç»ƒBPEåˆ†è¯å™¨"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒBPEåˆ†è¯å™¨ï¼ˆä¸­æ–‡ä¼˜åŒ–ç‰ˆï¼‰...")
        
        # æ£€æŸ¥æ•°æ®é‡å¹¶è¿›è¡Œé‡‡æ ·
        original_count = len(texts)
        max_texts = 150000  # å¢åŠ æœ€å¤§æ–‡æœ¬æ•°é‡é™åˆ¶
        
        if original_count > max_texts:
            print(f"âš ï¸  æ•°æ®é‡è¿‡å¤§({original_count:,})ï¼Œé‡‡æ ·åˆ°{max_texts:,}æ¡")
            random.seed(42)  # å›ºå®šéšæœºç§å­ç¡®ä¿å¯é‡ç°
            texts = random.sample(texts, max_texts)
        
        total_start_time = time.time()
        
        # æ­¥éª¤1ï¼šè®¡ç®—è¯é¢‘
        self.compute_word_frequencies(texts)
        
        # æ­¥éª¤2ï¼šåˆå§‹åŒ–åˆ†å‰²
        self.initialize_splits()
        
        # æ­¥éª¤3ï¼šè¿­ä»£åˆå¹¶
        merges = {}
        vocab_size = self.vocab_size - 4  # å‡å»4ä¸ªç‰¹æ®Štoken
        
        chinese_words = len([w for w in self.word_freqs if len(w) == 1 and is_chinese_char(w)])
        print(f"ğŸ”„ å¼€å§‹BPEè®­ç»ƒ: ç›®æ ‡{vocab_size:,}æ¬¡åˆå¹¶, {len(self.word_freqs):,}è¯æ±‡(ä¸­æ–‡:{chinese_words:,})")
        
        merge_start_time = time.time()
        last_best_pair = None  # ä¸Šä¸€æ¬¡æœ€ä½³å­—ç¬¦å¯¹
        repeated_pair_count = 0  # ç›¸åŒå­—ç¬¦å¯¹é‡å¤æ¬¡æ•°
        no_progress_count = 0  # æ— è¿›å±•æ¬¡æ•°ï¼ˆæ²¡æœ‰å®é™…åˆå¹¶ï¼‰
        
        for i in range(vocab_size):
            # è®¡ç®—å­—ç¬¦å¯¹é¢‘ç‡
            pair_freqs = self.compute_pair_frequencies()
            
            if not pair_freqs:
                print(f"\nâš ï¸  æ²¡æœ‰æ›´å¤šçš„å­—ç¬¦å¯¹å¯ä»¥åˆå¹¶ï¼Œæå‰ç»“æŸäºç¬¬ {i + 1} æ¬¡åˆå¹¶")
                break
            
            # æ‰¾åˆ°é¢‘ç‡æœ€é«˜çš„å­—ç¬¦å¯¹
            best_pair = max(pair_freqs, key=pair_freqs.get)
            best_freq = pair_freqs[best_pair]
            
            # æ­»å¾ªç¯æ£€æµ‹é€»è¾‘
            if best_pair == last_best_pair:
                repeated_pair_count += 1
                if repeated_pair_count >= 3:  # è¿ç»­3æ¬¡ç›¸åŒå­—ç¬¦å¯¹
                    break
            else:
                repeated_pair_count = 0
            
            if best_pair in merges:
                no_progress_count += 1
                if no_progress_count >= 10:
                    break
                continue
            else:
                no_progress_count = 0
            
            # åˆå¹¶æœ€é¢‘ç¹çš„å­—ç¬¦å¯¹
            merge_count = self.merge_vocab(best_pair)
            
            # å¦‚æœæ²¡æœ‰å®é™…åˆå¹¶ä»»ä½•å†…å®¹ï¼Œè·³è¿‡
            if merge_count == 0:
                no_progress_count += 1
                if no_progress_count >= 10:
                    break
                continue
            else:
                no_progress_count = 0
            
            # è®°å½•åˆå¹¶è§„åˆ™
            merges[best_pair] = i
            last_best_pair = best_pair
            
            # æ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯100æ¬¡æˆ–é‡è¦èŠ‚ç‚¹ï¼‰
            if (i + 1) % 100 == 0 or i == vocab_size - 1:
                show_progress_with_stats(i + 1, vocab_size, merge_start_time, 
                                       prefix='BPEè®­ç»ƒ', 
                                       extra_info=f"æœ€æ–°: '{best_pair[0]}'+'{best_pair[1]}'({best_freq:,})")
        
        self.merges = merges
        
        merge_elapsed = time.time() - merge_start_time
        print(f"\nâœ… BPEåˆå¹¶å®Œæˆï¼å®é™…åˆå¹¶æ¬¡æ•°: {len(merges):,} (è€—æ—¶: {merge_elapsed:.2f}ç§’)")
        
        # æ„å»ºè¯æ±‡è¡¨
        self.build_vocab()
        
        total_elapsed = time.time() - total_start_time
        
        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦è¦†ç›–ç‡ï¼ˆåŒ…æ‹¬å¸¦</w>çš„tokenï¼‰
        chinese_tokens = len([token for token in self.vocab 
                            if (len(token) == 1 and is_chinese_char(token)) or 
                               (token.endswith('</w>') and len(token) == 5 and is_chinese_char(token[0]))])
        
        print(f"ğŸ‰ è®­ç»ƒå®Œæˆ! è¯æ±‡è¡¨:{len(self.vocab):,} ä¸­æ–‡:{chinese_tokens:,} æ€»è€—æ—¶:{total_elapsed:.1f}ç§’")
    
    def build_vocab(self):
        """æ„å»ºè¯æ±‡è¡¨"""
        vocab = {}
        
        # æ·»åŠ ç‰¹æ®Štoken
        vocab[self.pad_token] = self.pad_id
        vocab[self.unk_token] = self.unk_id
        vocab[self.bos_token] = self.bos_id
        vocab[self.eos_token] = self.eos_id
        
        # æ·»åŠ æ‰€æœ‰å­è¯
        for word in self.word_freqs:
            for token in self.splits[word]:
                if token not in vocab:
                    vocab[token] = len(vocab)
        
        self.vocab = vocab
    
    def encode_word(self, word: str) -> List[str]:
        """ç¼–ç å•ä¸ªè¯"""
        # æ£€æŸ¥æ˜¯å¦æ˜¯å•ä¸ªä¸­æ–‡å­—ç¬¦ï¼Œç›´æ¥è¿”å›å¸¦</w>çš„ç‰ˆæœ¬
        if len(word) == 1 and is_chinese_char(word):
            return [word + '</w>']
        
        # åˆå§‹åŒ–ä¸ºå­—ç¬¦åˆ—è¡¨
        tokens = [c for c in word] + ['</w>']
        
        # åº”ç”¨æ‰€æœ‰åˆå¹¶è§„åˆ™
        for pair, merge_order in sorted(self.merges.items(), key=lambda x: x[1]):
            new_tokens = []
            i = 0
            
            while i < len(tokens):
                if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == pair:
                    new_tokens.append(tokens[i] + tokens[i + 1])
                    i += 2
                else:
                    new_tokens.append(tokens[i])
                    i += 1
            
            tokens = new_tokens
        
        return tokens
    
    def encode(self, text: str, add_special_tokens: bool = True) -> List[int]:
        """ç¼–ç æ–‡æœ¬ä¸ºtoken IDåˆ—è¡¨"""
        # é¢„åˆ†è¯
        words = self.pre_tokenize(text)
        
        token_ids = []
        
        if add_special_tokens:
            token_ids.append(self.bos_id)
        
        # ç¼–ç æ¯ä¸ªè¯
        for word in words:
            try:
                tokens = self.encode_word(word)
                for token in tokens:
                    token_id = self.vocab.get(token, self.unk_id)
                    token_ids.append(token_id)
            except Exception as e:
                # å‡ºé”™æ—¶ä½¿ç”¨æœªçŸ¥token
                token_ids.append(self.unk_id)
        
        if add_special_tokens:
            token_ids.append(self.eos_id)
        
        return token_ids
    
    def decode(self, token_ids: List[int]) -> str:
        """è§£ç token IDåˆ—è¡¨ä¸ºæ–‡æœ¬"""
        # åˆ›å»ºIDåˆ°tokençš„æ˜ å°„
        id_to_token = {v: k for k, v in self.vocab.items()}
        
        tokens = []
        for token_id in token_ids:
            if token_id in id_to_token:
                token = id_to_token[token_id]
                # è·³è¿‡ç‰¹æ®Štoken
                if token not in [self.pad_token, self.bos_token, self.eos_token]:
                    tokens.append(token)
        
        # åˆå¹¶tokenså¹¶å¤„ç†è¯ç»“æŸæ ‡è®°
        text = ''.join(tokens)
        text = text.replace('</w>', ' ')
        
        return text.strip()
    
    def save(self, path: str):
        """ä¿å­˜åˆ†è¯å™¨"""
        data = {
            'vocab': self.vocab,
            'merges': self.merges,
            'vocab_size': self.vocab_size
        }
        
        with open(path, 'wb') as f:
            pickle.dump(data, f)
        
        print(f"åˆ†è¯å™¨å·²ä¿å­˜åˆ°: {path}")
    
    def load(self, path: str):
        """åŠ è½½åˆ†è¯å™¨"""
        with open(path, 'rb') as f:
            data = pickle.load(f)
        
        self.vocab = data['vocab']
        self.merges = data['merges']
        self.vocab_size = data['vocab_size']
        
        print(f"åˆ†è¯å™¨å·²åŠ è½½: {path}")
    
    def get_vocab_size(self) -> int:
        """è·å–è¯æ±‡è¡¨å¤§å°"""
        return len(self.vocab)


def train_tokenizer_from_data(data_path: str, vocab_size: int = 30000) -> BPETokenizer:  # å¢åŠ é»˜è®¤è¯æ±‡è¡¨å¤§å°
    """ä»æ•°æ®æ–‡ä»¶è®­ç»ƒåˆ†è¯å™¨"""
    print(f"ğŸ“ åŠ è½½æ•°æ®: {data_path}, ç›®æ ‡è¯æ±‡è¡¨: {vocab_size:,}")
    
    texts = []
    start_time = time.time()
    line_count = 0
    valid_count = 0
    
    with open(data_path, 'r', encoding='utf-8') as f:
        for line in f:
            line_count += 1
            line = line.strip()
            if line:
                try:
                    data = json.loads(line)
                    if 'conversations' in data:
                        for conv in data['conversations']:
                            texts.append(conv['content'])
                            valid_count += 1
                    elif 'text' in data:
                        texts.append(data['text'])
                        valid_count += 1
                except json.JSONDecodeError:
                    continue
    
    elapsed = time.time() - start_time
    
    # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦
    chinese_char_count = sum(len([c for c in text if is_chinese_char(c)]) for text in texts)
    total_char_count = sum(len(text) for text in texts)
    chinese_ratio = chinese_char_count / total_char_count if total_char_count > 0 else 0
    
    print(f"âœ… è¯»å– {len(texts):,} æ¡æ–‡æœ¬ï¼Œä¸­æ–‡å æ¯” {chinese_ratio:.1%} (è€—æ—¶: {elapsed:.2f}ç§’)")
    
    # è®­ç»ƒåˆ†è¯å™¨
    tokenizer = BPETokenizer(vocab_size=vocab_size)
    tokenizer.train(texts)
    
    return tokenizer


if __name__ == "__main__":
    # æµ‹è¯•åˆ†è¯å™¨
    sample_texts = [
        "Hello world! How are you?",
        "I am learning about BPE tokenization.",
        "This is a sample text for training.",
        "BPE stands for Byte Pair Encoding."
    ]
    
    # è®­ç»ƒåˆ†è¯å™¨
    tokenizer = BPETokenizer(vocab_size=1000)
    tokenizer.train(sample_texts)
    
    # æµ‹è¯•ç¼–ç å’Œè§£ç 
    test_text = "Hello! How are you doing?"
    token_ids = tokenizer.encode(test_text)
    decoded_text = tokenizer.decode(token_ids)
    
    print(f"åŸæ–‡: {test_text}")
    print(f"ç¼–ç : {token_ids}")
    print(f"è§£ç : {decoded_text}")
    print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.get_vocab_size()}")