"""
é€šç”¨Tokenizerç®¡ç†ç³»ç»Ÿ

å®ç°æ™ºèƒ½ç¼“å­˜æœºåˆ¶ï¼Œé¿å…é‡å¤è®­ç»ƒï¼Œæä¾›ç»Ÿä¸€APIæ¥å£
- åŸºäºæ•°æ®æ–‡ä»¶hashå’Œé…ç½®è‡ªåŠ¨åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°è®­ç»ƒ
- æ”¯æŒå¤šç§tokenizeré…ç½®çš„ç®¡ç†
- æä¾›æ ‡å‡†åŒ–çš„åŠ è½½å’Œè®­ç»ƒæ¥å£
"""

import hashlib
import json
import os
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any

from .bpe_tokenizer import BPETokenizer, train_tokenizer_from_data


@dataclass
class TokenizerConfig:
    """Tokenizeré…ç½®"""

    vocab_size: int = 30000
    tokenizer_type: str = "bpe"  # æ”¯æŒæ‰©å±•å…¶ä»–ç±»å‹
    max_samples: int = 150000  # è®­ç»ƒæ—¶æœ€å¤§æ ·æœ¬æ•°

    def __post_init__(self):
        """éªŒè¯é…ç½®"""
        if self.vocab_size <= 0:
            raise ValueError("vocab_size must be positive")
        if self.tokenizer_type not in ["bpe"]:
            raise ValueError(f"Unsupported tokenizer_type: {self.tokenizer_type}")

    def to_dict(self) -> dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)

    def get_cache_key(self) -> str:
        """è·å–ç¼“å­˜é”®"""
        return f"{self.tokenizer_type}_vocab{self.vocab_size}_max{self.max_samples}"


class TokenizerManager:
    """
    Tokenizeræ™ºèƒ½ç®¡ç†ç³»ç»Ÿ

    åŠŸèƒ½ï¼š
    - è‡ªåŠ¨ç¼“å­˜å·²è®­ç»ƒçš„tokenizer
    - åŸºäºæ•°æ®hashåˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°è®­ç»ƒ
    - æä¾›ç»Ÿä¸€çš„APIæ¥å£
    - æ”¯æŒå¤šç§é…ç½®ç®¡ç†
    """

    def __init__(self, cache_dir: str = "tokenizers"):
        """
        åˆå§‹åŒ–TokenizerManager

        Args:
            cache_dir: tokenizerç¼“å­˜ç›®å½•
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True, parents=True)

        # åˆ›å»ºå­ç›®å½•
        (self.cache_dir / "models").mkdir(exist_ok=True)
        (self.cache_dir / "metadata").mkdir(exist_ok=True)

        print(f"ğŸ”§ TokenizerManager initialized: {self.cache_dir}")

    def _get_data_hash(self, data_path: str, max_lines: int = 10000) -> str:
        """
        è®¡ç®—æ•°æ®æ–‡ä»¶çš„hashå€¼ï¼ˆé‡‡æ ·å‰Nè¡Œä»¥æé«˜æ•ˆç‡ï¼‰

        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            max_lines: æœ€å¤§é‡‡æ ·è¡Œæ•°

        Returns:
            æ•°æ®hashå€¼
        """
        hash_obj = hashlib.md5()

        try:
            with open(data_path, encoding="utf-8") as f:
                line_count = 0
                for line in f:
                    if line_count >= max_lines:
                        break
                    hash_obj.update(line.encode("utf-8"))
                    line_count += 1

            # æ·»åŠ æ–‡ä»¶å¤§å°å’Œä¿®æ”¹æ—¶é—´ä¿¡æ¯
            stat = os.stat(data_path)
            hash_obj.update(str(stat.st_size).encode())
            hash_obj.update(str(stat.st_mtime).encode())

        except Exception as e:
            print(f"âš ï¸  Warning: Error computing data hash: {e}")
            # å¦‚æœå‡ºé”™ï¼Œä½¿ç”¨æ–‡ä»¶è·¯å¾„ä½œä¸ºfallback
            hash_obj.update(data_path.encode("utf-8"))

        return hash_obj.hexdigest()[:12]  # ä½¿ç”¨å‰12ä½

    def _get_tokenizer_path(self, config: TokenizerConfig, data_hash: str) -> tuple[Path, Path]:
        """
        è·å–tokenizeræ¨¡å‹æ–‡ä»¶å’Œå…ƒæ•°æ®æ–‡ä»¶è·¯å¾„

        Args:
            config: tokenizeré…ç½®
            data_hash: æ•°æ®hashå€¼

        Returns:
            (model_path, metadata_path)
        """
        cache_key = config.get_cache_key()
        filename = f"{cache_key}_{data_hash}"

        model_path = self.cache_dir / "models" / f"{filename}.pkl"
        metadata_path = self.cache_dir / "metadata" / f"{filename}.json"

        return model_path, metadata_path

    def _save_metadata(
        self, metadata_path: Path, config: TokenizerConfig, data_path: str, data_hash: str
    ):
        """ä¿å­˜tokenizerå…ƒæ•°æ®"""
        metadata = {
            "config": config.to_dict(),
            "data_path": str(data_path),
            "data_hash": data_hash,
            "created_at": __import__("time").time(),
            "version": "1.0",
        }

        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

    def _load_metadata(self, metadata_path: Path) -> dict[str, Any] | None:
        """åŠ è½½tokenizerå…ƒæ•°æ®"""
        try:
            with open(metadata_path, encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return None

    def _is_cache_valid(
        self, model_path: Path, metadata_path: Path, config: TokenizerConfig, data_hash: str
    ) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not model_path.exists() or not metadata_path.exists():
            return False

        metadata = self._load_metadata(metadata_path)
        if not metadata:
            return False

        # æ£€æŸ¥é…ç½®æ˜¯å¦åŒ¹é…
        cached_config = TokenizerConfig(**metadata["config"])
        if cached_config != config:
            return False

        # æ£€æŸ¥æ•°æ®hashæ˜¯å¦åŒ¹é…
        if metadata.get("data_hash") != data_hash:
            return False

        return True

    def get_or_train_tokenizer(
        self,
        data_path: str,
        config: TokenizerConfig | dict[str, Any] | None = None,
        force_retrain: bool = False,
    ) -> BPETokenizer:
        """
        è·å–æˆ–è®­ç»ƒtokenizerï¼ˆæ ¸å¿ƒAPIï¼‰

        Args:
            data_path: è®­ç»ƒæ•°æ®è·¯å¾„
            config: tokenizeré…ç½®
            force_retrain: æ˜¯å¦å¼ºåˆ¶é‡æ–°è®­ç»ƒ

        Returns:
            è®­ç»ƒå¥½çš„tokenizer
        """
        # å¤„ç†é…ç½®
        if config is None:
            config = TokenizerConfig()
        elif isinstance(config, dict):
            config = TokenizerConfig(**config)

        print(f"ğŸ“ Tokenizer request: {config.get_cache_key()}")

        # è®¡ç®—æ•°æ®hash
        print("ğŸ” Computing data fingerprint...")
        data_hash = self._get_data_hash(data_path)
        print(f"   Data fingerprint: {data_hash}")

        # è·å–æ–‡ä»¶è·¯å¾„
        model_path, metadata_path = self._get_tokenizer_path(config, data_hash)

        # æ£€æŸ¥ç¼“å­˜
        if not force_retrain and self._is_cache_valid(model_path, metadata_path, config, data_hash):
            print(f"âœ… Loading cached tokenizer: {model_path.name}")
            tokenizer = BPETokenizer(vocab_size=config.vocab_size)
            tokenizer.load(str(model_path))

            # æ˜¾ç¤ºç¼“å­˜ä¿¡æ¯
            metadata = self._load_metadata(metadata_path)
            if metadata:
                created_time = metadata.get("created_at", 0)
                print(
                    f"   Cached on: {__import__('time').strftime('%Y-%m-%d %H:%M:%S', __import__('time').localtime(created_time))}"
                )
                print(f"   Vocab size: {tokenizer.get_vocab_size()}")

            return tokenizer

        # éœ€è¦é‡æ–°è®­ç»ƒ
        print(f"ğŸš€ Training new tokenizer: {config.get_cache_key()}")
        if force_retrain:
            print("   Reason: Force retrain requested")
        else:
            print("   Reason: No valid cache found")

        # è®­ç»ƒtokenizer
        tokenizer = train_tokenizer_from_data(data_path, config.vocab_size)

        # ä¿å­˜åˆ°ç¼“å­˜
        print(f"ğŸ’¾ Caching tokenizer: {model_path.name}")
        tokenizer.save(str(model_path))
        self._save_metadata(metadata_path, config, data_path, data_hash)

        print(f"âœ… Tokenizer ready: vocab_size={tokenizer.get_vocab_size()}")

        return tokenizer

    def list_cached_tokenizers(self) -> list[dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰ç¼“å­˜çš„tokenizer"""
        cached_tokenizers = []

        metadata_dir = self.cache_dir / "metadata"
        if not metadata_dir.exists():
            return cached_tokenizers

        for metadata_file in metadata_dir.glob("*.json"):
            metadata = self._load_metadata(metadata_file)
            if metadata:
                # æ£€æŸ¥å¯¹åº”çš„æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                model_file = self.cache_dir / "models" / f"{metadata_file.stem}.pkl"
                if model_file.exists():
                    cached_tokenizers.append(
                        {
                            "name": metadata_file.stem,
                            "config": metadata["config"],
                            "data_path": metadata["data_path"],
                            "created_at": metadata.get("created_at", 0),
                            "model_size": model_file.stat().st_size,
                        }
                    )

        return sorted(cached_tokenizers, key=lambda x: x["created_at"], reverse=True)

    def clean_cache(self, keep_latest: int = 5) -> int:
        """
        æ¸…ç†æ—§çš„ç¼“å­˜æ–‡ä»¶

        Args:
            keep_latest: ä¿ç•™æœ€æ–°çš„Nä¸ªtokenizer

        Returns:
            åˆ é™¤çš„æ–‡ä»¶æ•°é‡
        """
        cached = self.list_cached_tokenizers()

        if len(cached) <= keep_latest:
            print(f"ğŸ’¾ Cache clean: {len(cached)} tokenizers, nothing to clean")
            return 0

        to_delete = cached[keep_latest:]
        deleted_count = 0

        for item in to_delete:
            name = item["name"]

            # åˆ é™¤æ¨¡å‹æ–‡ä»¶
            model_file = self.cache_dir / "models" / f"{name}.pkl"
            if model_file.exists():
                model_file.unlink()
                deleted_count += 1

            # åˆ é™¤å…ƒæ•°æ®æ–‡ä»¶
            metadata_file = self.cache_dir / "metadata" / f"{name}.json"
            if metadata_file.exists():
                metadata_file.unlink()
                deleted_count += 1

        print(
            f"ğŸ§¹ Cache cleaned: removed {deleted_count} files, kept latest {keep_latest} tokenizers"
        )
        return deleted_count


# å…¨å±€tokenizerç®¡ç†å™¨å®ä¾‹
_global_manager = None


def get_global_tokenizer_manager() -> TokenizerManager:
    """è·å–å…¨å±€tokenizerç®¡ç†å™¨"""
    global _global_manager
    if _global_manager is None:
        _global_manager = TokenizerManager()
    return _global_manager


def get_tokenizer(
    data_path: str,
    vocab_size: int = 30000,
    tokenizer_type: str = "bpe",
    force_retrain: bool = False,
    cache_dir: str | None = None,
) -> BPETokenizer:
    """
    é€šç”¨tokenizerè·å–APIï¼ˆæ¨èä½¿ç”¨ï¼‰

    Args:
        data_path: è®­ç»ƒæ•°æ®è·¯å¾„
        vocab_size: è¯æ±‡è¡¨å¤§å°
        tokenizer_type: tokenizerç±»å‹
        force_retrain: æ˜¯å¦å¼ºåˆ¶é‡æ–°è®­ç»ƒ
        cache_dir: ç¼“å­˜ç›®å½•ï¼ˆNoneä½¿ç”¨å…¨å±€ç®¡ç†å™¨ï¼‰

    Returns:
        è®­ç»ƒå¥½çš„tokenizer

    Example:
        # ç®€å•ä½¿ç”¨
        tokenizer = get_tokenizer("data/train.jsonl", vocab_size=30000)

        # å¼ºåˆ¶é‡æ–°è®­ç»ƒ
        tokenizer = get_tokenizer("data/train.jsonl", force_retrain=True)
    """
    config = TokenizerConfig(vocab_size=vocab_size, tokenizer_type=tokenizer_type)

    if cache_dir is not None:
        # ä½¿ç”¨æŒ‡å®šçš„ç¼“å­˜ç›®å½•
        manager = TokenizerManager(cache_dir)
    else:
        # ä½¿ç”¨å…¨å±€ç®¡ç†å™¨
        manager = get_global_tokenizer_manager()

    return manager.get_or_train_tokenizer(data_path, config, force_retrain)


def list_tokenizers() -> list[dict[str, Any]]:
    """åˆ—å‡ºæ‰€æœ‰ç¼“å­˜çš„tokenizer"""
    manager = get_global_tokenizer_manager()
    return manager.list_cached_tokenizers()


def clean_tokenizer_cache(keep_latest: int = 5) -> int:
    """æ¸…ç†tokenizerç¼“å­˜"""
    manager = get_global_tokenizer_manager()
    return manager.clean_cache(keep_latest)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸ§ª Testing TokenizerManager...")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = [
        '{"conversations": [{"role": "user", "content": "Hello world"}, {"role": "assistant", "content": "Hi there!"}]}',
        '{"conversations": [{"role": "user", "content": "How are you?"}, {"role": "assistant", "content": "I am fine"}]}',
    ]

    test_file = "test_data.jsonl"
    with open(test_file, "w", encoding="utf-8") as f:
        for line in test_data:
            f.write(line + "\n")

    try:
        # æµ‹è¯•é€šç”¨API
        print("\n1. Testing get_tokenizer API...")
        tokenizer = get_tokenizer(test_file, vocab_size=1000)
        print(f"   Tokenizer vocab size: {tokenizer.get_vocab_size()}")

        # ç¬¬äºŒæ¬¡è°ƒç”¨åº”è¯¥ä½¿ç”¨ç¼“å­˜
        print("\n2. Testing cache functionality...")
        tokenizer2 = get_tokenizer(test_file, vocab_size=1000)
        print(f"   Tokenizer2 vocab size: {tokenizer2.get_vocab_size()}")

        # åˆ—å‡ºç¼“å­˜
        print("\n3. Listing cached tokenizers...")
        cached = list_tokenizers()
        for item in cached:
            print(f"   - {item['name']}: vocab={item['config']['vocab_size']}")

        print("\nâœ… TokenizerManager test completed!")

    finally:
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        if os.path.exists(test_file):
            os.remove(test_file)
