"""
ç°ä»£ä¼˜åŒ–å™¨å®ç° (2024-2025æœ€æ–°)
åŒ…å«ä¸šç•Œä¸»æµçš„ä¼˜åŒ–å™¨ï¼šAdamWã€Lionã€Sophiaã€Muonç­‰

åŸºäºæœ€æ–°ç ”ç©¶:
- Muon (2024): 2xæ•ˆç‡æå‡, Kimi-2ä½¿ç”¨
- Sophia (2023): äºŒé˜¶ä¼˜åŒ–, é€‚åˆå¤§æ¨¡å‹é¢„è®­ç»ƒ
- Lion (2023): å†…å­˜é«˜æ•ˆ
- WSDè°ƒåº¦å™¨ (2024): æ— éœ€é¢„è®¾æ€»æ­¥æ•°

è®ºæ–‡å‚è€ƒ:
- Muon: https://arxiv.org/abs/2502.16982
- Sophia: https://arxiv.org/abs/2305.14342
- Lion: https://arxiv.org/abs/2302.06675
- WSD: https://arxiv.org/abs/2410.05192
"""
import math
import torch
from torch.optim.optimizer import Optimizer
from typing import Dict, Any, Optional, List, Callable


class Lion(Optimizer):
    """
    Lionä¼˜åŒ–å™¨ - Googleæå‡ºçš„è½»é‡çº§ä¼˜åŒ–å™¨

    Lion (EvoLved Sign Momentum) æ˜¯ä¸€ä¸ªç®€å•ä½†æœ‰æ•ˆçš„ä¼˜åŒ–å™¨ï¼Œ
    åœ¨è®¸å¤šè§†è§‰å’Œè¯­è¨€æ¨¡å‹ä»»åŠ¡ä¸Šä¼˜äºAdamWã€‚

    Reference: https://arxiv.org/abs/2302.06675
    """

    def __init__(
        self,
        params,
        lr: float = 1e-4,
        betas: tuple = (0.9, 0.99),
        weight_decay: float = 0.0,
        maximize: bool = False,
        foreach: Optional[bool] = None,
    ):
        if not 0.0 <= lr:
            raise ValueError(f"Invalid learning rate: {lr}")
        if not 0.0 <= betas[0] < 1.0:
            raise ValueError(f"Invalid beta parameter at index 0: {betas[0]}")
        if not 0.0 <= betas[1] < 1.0:
            raise ValueError(f"Invalid beta parameter at index 1: {betas[1]}")
        if not 0.0 <= weight_decay:
            raise ValueError(f"Invalid weight_decay value: {weight_decay}")

        defaults = dict(
            lr=lr,
            betas=betas,
            weight_decay=weight_decay,
            maximize=maximize,
            foreach=foreach,
        )
        super().__init__(params, defaults)

    @torch.no_grad()
    def step(self, closure=None):
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            for p in group["params"]:
                if p.grad is None:
                    continue

                grad = p.grad
                if group["maximize"]:
                    grad = -grad

                state = self.state[p]

                # State initialization
                if len(state) == 0:
                    state["exp_avg"] = torch.zeros_like(p)

                exp_avg = state["exp_avg"]
                beta1, beta2 = group["betas"]

                # Weight decay
                if group["weight_decay"] != 0:
                    p.mul_(1 - group["lr"] * group["weight_decay"])

                # Update biased first moment estimate
                update = exp_avg * beta1 + grad * (1 - beta1)

                # Apply sign of the update
                p.add_(torch.sign(update), alpha=-group["lr"])

                # Decay the momentum running average coefficient
                exp_avg.mul_(beta2).add_(grad, alpha=1 - beta2)

        return loss


class Sophia(Optimizer):
    """
    Sophiaä¼˜åŒ–å™¨ - ä¸ºLLMé¢„è®­ç»ƒè®¾è®¡çš„äºŒé˜¶ä¼˜åŒ–å™¨

    Sophia (Second-order Clipped Stochastic Optimization) ä½¿ç”¨å¯¹è§’Hessianä¼°è®¡
    æ¥åŠ é€Ÿå¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒï¼Œåœ¨ç›¸åŒçš„æ­¥æ•°ä¸‹èƒ½è¾¾åˆ°æ›´å¥½çš„æ€§èƒ½ã€‚

    Reference: https://arxiv.org/abs/2305.14342
    """

    def __init__(
        self,
        params,
        lr: float = 1e-4,
        betas: tuple = (0.965, 0.99),
        rho: float = 0.04,
        weight_decay: float = 1e-1,
        maximize: bool = False,
        capturable: bool = False,
        eps: float = 1e-15,
    ):
        if not 0.0 <= lr:
            raise ValueError(f"Invalid learning rate: {lr}")
        if not 0.0 <= eps:
            raise ValueError(f"Invalid epsilon value: {eps}")
        if not 0.0 <= betas[0] < 1.0:
            raise ValueError(f"Invalid beta parameter at index 0: {betas[0]}")
        if not 0.0 <= betas[1] < 1.0:
            raise ValueError(f"Invalid beta parameter at index 1: {betas[1]}")
        if not 0.0 <= rho:
            raise ValueError(f"Invalid rho parameter: {rho}")
        if not 0.0 <= weight_decay:
            raise ValueError(f"Invalid weight_decay value: {weight_decay}")

        defaults = dict(
            lr=lr,
            betas=betas,
            rho=rho,
            weight_decay=weight_decay,
            maximize=maximize,
            capturable=capturable,
            eps=eps,
        )
        super().__init__(params, defaults)

    @torch.no_grad()
    def step(self, closure=None, bs=5120):
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            beta1, beta2 = group["betas"]
            rho = group["rho"]

            for p in group["params"]:
                if p.grad is None:
                    continue

                grad = p.grad
                if group["maximize"]:
                    grad = -grad

                state = self.state[p]

                # State initialization
                if len(state) == 0:
                    state["step"] = torch.zeros((1,), dtype=torch.float, device=p.device, requires_grad=False)
                    state["exp_avg"] = torch.zeros_like(p)
                    state["hessian_diag_sq"] = torch.zeros_like(p)

                exp_avg, hessian_diag_sq = state["exp_avg"], state["hessian_diag_sq"]
                step_t = state["step"]

                step_t += 1

                # Perform step weight decay
                if group["weight_decay"] != 0:
                    p.mul_(1 - group["lr"] * group["weight_decay"])

                # Decay the first moment running average coefficient
                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)

                if len(state) == 1:
                    state["hessian_diag_sq"] = torch.zeros_like(p)
                    hessian_diag_sq = state["hessian_diag_sq"]

                # Update Hessian diagonal estimate
                k = group.get('k', 10)
                update_period = group.get('update_period', k)

                if step_t % update_period == 1:
                    # Approximation using gradient auto-correlation
                    hessian_diag_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)

                # Bias correction
                bias_correction1 = 1 - beta1 ** step_t
                bias_correction2 = 1 - beta2 ** step_t

                # Clipped update
                h = hessian_diag_sq.sqrt() / math.sqrt(bias_correction2)
                u = exp_avg / bias_correction1 / (h + group["eps"])
                u.clamp_(min=-rho, max=rho)

                p.add_(u, alpha=-group["lr"])

        return loss


class AdamWScheduleFree(Optimizer):
    """
    Schedule-Free AdamW - æ— éœ€å­¦ä¹ ç‡è°ƒåº¦çš„AdamWå˜ä½“

    è¿™ä¸ªä¼˜åŒ–å™¨è‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡ï¼Œæ— éœ€æ‰‹åŠ¨è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚
    """

    def __init__(
        self,
        params,
        lr: float = 1e-3,
        betas: tuple = (0.9, 0.999),
        eps: float = 1e-8,
        weight_decay: float = 1e-2,
        warmup_steps: int = 10000,
        r: float = 0.0,
    ):
        defaults = dict(
            lr=lr,
            betas=betas,
            eps=eps,
            weight_decay=weight_decay,
            warmup_steps=warmup_steps,
            r=r,
        )
        super().__init__(params, defaults)

    @torch.no_grad()
    def step(self, closure=None):
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            for p in group["params"]:
                if p.grad is None:
                    continue

                grad = p.grad
                state = self.state[p]

                # State initialization
                if len(state) == 0:
                    state["step"] = 0
                    state["exp_avg"] = torch.zeros_like(p)
                    state["exp_avg_sq"] = torch.zeros_like(p)
                    state["z"] = p.clone()

                exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
                z = state["z"]
                beta1, beta2 = group["betas"]

                state["step"] += 1
                step = state["step"]

                # Schedule-free learning rate
                if step < group["warmup_steps"]:
                    lr_t = group["lr"] * step / group["warmup_steps"]
                else:
                    lr_t = group["lr"] / (1 + group["r"] * (step - group["warmup_steps"]))

                # Weight decay
                if group["weight_decay"] != 0:
                    grad = grad.add(p, alpha=group["weight_decay"])

                # Exponential moving average of gradient values
                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)

                # Exponential moving average of squared gradient values
                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)

                # Bias correction
                bias_correction1 = 1 - beta1 ** step
                bias_correction2 = 1 - beta2 ** step

                denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group["eps"])
                step_size = lr_t / bias_correction1

                # Update z
                z.addcdiv_(exp_avg, denom, value=-step_size)

                # Update parameters
                p.copy_(z)

        return loss


def get_optimizer(
    optimizer_name: str,
    parameters,
    lr: float = 1e-4,
    weight_decay: float = 0.01,
    **kwargs
) -> Optimizer:
    """
    è·å–ä¼˜åŒ–å™¨å®ä¾‹

    Args:
        optimizer_name: ä¼˜åŒ–å™¨åç§°
        parameters: æ¨¡å‹å‚æ•°
        lr: å­¦ä¹ ç‡
        weight_decay: æƒé‡è¡°å‡
        **kwargs: å…¶ä»–ä¼˜åŒ–å™¨å‚æ•°

    Returns:
        ä¼˜åŒ–å™¨å®ä¾‹
    """
    optimizer_name = optimizer_name.lower()

    if optimizer_name == "adamw":
        return torch.optim.AdamW(
            parameters,
            lr=lr,
            weight_decay=weight_decay,
            **kwargs
        )
    elif optimizer_name == "adam":
        return torch.optim.Adam(
            parameters,
            lr=lr,
            weight_decay=weight_decay,
            **kwargs
        )
    elif optimizer_name == "sgd":
        return torch.optim.SGD(
            parameters,
            lr=lr,
            weight_decay=weight_decay,
            momentum=kwargs.get("momentum", 0.9),
            **{k: v for k, v in kwargs.items() if k != "momentum"}
        )
    elif optimizer_name == "lion":
        return Lion(
            parameters,
            lr=lr,
            weight_decay=weight_decay,
            **kwargs
        )
    elif optimizer_name == "sophia":
        return Sophia(
            parameters,
            lr=lr,
            weight_decay=weight_decay,
            **kwargs
        )
    elif optimizer_name == "adamw_schedule_free":
        return AdamWScheduleFree(
            parameters,
            lr=lr,
            weight_decay=weight_decay,
            **kwargs
        )
    else:
        raise ValueError(f"Unsupported optimizer: {optimizer_name}. "
                        f"Supported optimizers: ['adamw', 'adam', 'sgd', 'lion', 'sophia', 'adamw_schedule_free']")


def get_scheduler(
    scheduler_name: str,
    optimizer: Optimizer,
    **kwargs
) -> Optional[torch.optim.lr_scheduler._LRScheduler]:
    """
    è·å–å­¦ä¹ ç‡è°ƒåº¦å™¨

    Args:
        scheduler_name: è°ƒåº¦å™¨åç§°
        optimizer: ä¼˜åŒ–å™¨å®ä¾‹
        **kwargs: è°ƒåº¦å™¨å‚æ•°

    Returns:
        å­¦ä¹ ç‡è°ƒåº¦å™¨å®ä¾‹
    """
    if scheduler_name is None or scheduler_name == "none":
        return None

    scheduler_name = scheduler_name.lower()

    if scheduler_name == "cosine":
        T_max = kwargs.get("T_max", 1000)
        return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max)
    elif scheduler_name == "warmup_cosine":
        from transformers import get_cosine_schedule_with_warmup
        num_warmup_steps = kwargs.get("num_warmup_steps", 1000)
        num_training_steps = kwargs.get("num_training_steps", 10000)
        return get_cosine_schedule_with_warmup(
            optimizer,
            num_warmup_steps=num_warmup_steps,
            num_training_steps=num_training_steps
        )
    elif scheduler_name == "linear":
        gamma = kwargs.get("gamma", 0.95)
        return torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)
    elif scheduler_name == "step":
        step_size = kwargs.get("step_size", 1000)
        gamma = kwargs.get("gamma", 0.1)
        return torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)
    else:
        raise ValueError(f"Unsupported scheduler: {scheduler_name}. "
                        f"Supported schedulers: ['cosine', 'warmup_cosine', 'linear', 'step', 'inverse_sqrt', 'wsd', 'none']")


class Muon(Optimizer):
    """
    Muonä¼˜åŒ–å™¨ - Momentum Orthogonalized by Newton-Schulz (2024)

    æœ€æ–°ç ”ç©¶æˆæœï¼Œç›¸æ¯”AdamWèŠ‚çœ~48%è®¡ç®—é‡è¾¾åˆ°ç›¸åŒæ•ˆæœã€‚
    Kimi-2 (1Tå‚æ•°)ä½¿ç”¨æ­¤ä¼˜åŒ–å™¨ã€‚

    æ ¸å¿ƒæ€æƒ³:
    - å¯¹2Då‚æ•°(æƒé‡çŸ©é˜µ)ä½¿ç”¨Newton-Schulzæ­£äº¤åŒ–
    - ä¸AdamWæ··åˆä½¿ç”¨(AdamWå¤„ç†1Då‚æ•°å¦‚bias, norm)

    è®ºæ–‡: https://arxiv.org/abs/2502.16982

    Args:
        params: æ¨¡å‹å‚æ•°(æ¨èåªä¼ å…¥2Då‚æ•°)
        lr: å­¦ä¹ ç‡ (æ¨è: 0.02, æ¯”AdamWé«˜20å€!)
        momentum: åŠ¨é‡ç³»æ•° (é»˜è®¤: 0.95)
        nesterov: æ˜¯å¦ä½¿ç”¨NesterovåŠ¨é‡
        ns_steps: Newton-Schulzè¿­ä»£æ­¥æ•° (é»˜è®¤: 5)
        weight_decay: æƒé‡è¡°å‡

    ä½¿ç”¨ç¤ºä¾‹:
        # æ¨è: æ··åˆä½¿ç”¨Muon(2Då‚æ•°) + AdamW(1Då‚æ•°)
        params_2d = [p for p in model.parameters() if len(p.shape) >= 2]
        params_1d = [p for p in model.parameters() if len(p.shape) < 2]

        opt_muon = Muon(params_2d, lr=0.02)
        opt_adamw = torch.optim.AdamW(params_1d, lr=1e-3)
    """

    def __init__(
        self,
        params,
        lr: float = 0.02,
        momentum: float = 0.95,
        nesterov: bool = True,
        ns_steps: int = 5,
        weight_decay: float = 0.0
    ):
        if lr < 0.0:
            raise ValueError(f"Invalid learning rate: {lr}")
        if not 0.0 <= momentum < 1.0:
            raise ValueError(f"Invalid momentum: {momentum}")

        defaults = dict(
            lr=lr,
            momentum=momentum,
            nesterov=nesterov,
            ns_steps=ns_steps,
            weight_decay=weight_decay
        )
        super().__init__(params, defaults)

    def newton_schulz_iteration(self, G, steps=5):
        """
        Newton-Schulzæ­£äº¤åŒ–è¿­ä»£

        å°†æ¢¯åº¦çŸ©é˜µGæ­£äº¤åŒ–ï¼Œä½¿æ›´æ–°æ–¹å‘æ›´ç¨³å®š
        """
        # ä¼˜åŒ–çš„ç³»æ•°(é€šè¿‡å®éªŒç¡®å®š)
        a, b, c = (3.4445, -4.7750, 2.0315)

        X = G.clone()
        if steps == 5:
            for _ in range(steps):
                A = X @ X.T
                B = b * A + c * A @ A
                X = a * X + B @ X

        return X

    @torch.no_grad()
    def step(self, closure: Optional[Callable] = None):
        """æ‰§è¡Œå•æ­¥ä¼˜åŒ–"""
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue

                grad = p.grad

                # æƒé‡è¡°å‡
                if group['weight_decay'] != 0:
                    grad = grad.add(p, alpha=group['weight_decay'])

                state = self.state[p]

                # çŠ¶æ€åˆå§‹åŒ–
                if len(state) == 0:
                    state['momentum_buffer'] = torch.zeros_like(grad)

                buf = state['momentum_buffer']

                # åŠ¨é‡æ›´æ–°
                buf.mul_(group['momentum']).add_(grad)

                # Muonæ ¸å¿ƒ: å¯¹2Då‚æ•°åº”ç”¨Newton-Schulzæ­£äº¤åŒ–
                if len(p.shape) == 2:
                    update = self.newton_schulz_iteration(
                        buf.view(p.shape),
                        steps=group['ns_steps']
                    )
                else:
                    # 1Då‚æ•°ä½¿ç”¨æ ‡å‡†åŠ¨é‡
                    update = buf

                # NesterovåŠ é€Ÿ
                if group['nesterov']:
                    update = grad + group['momentum'] * update

                # åº”ç”¨æ›´æ–°
                p.add_(update, alpha=-group['lr'])

        return loss


def get_warmup_cosine_schedule(
    optimizer: Optimizer,
    num_warmup_steps: int,
    num_training_steps: int,
    num_cycles: float = 0.5,
    min_lr_ratio: float = 0.1
):
    """
    Warmup + Cosineé€€ç«è°ƒåº¦å™¨ (æ¨èç”¨äºLLMè®­ç»ƒ)

    GPT-3, Llama, Chinchilla, Pythiaç­‰éƒ½ä½¿ç”¨æ­¤è°ƒåº¦å™¨

    Args:
        optimizer: ä¼˜åŒ–å™¨
        num_warmup_steps: warmupæ­¥æ•° (æ¨è: æ€»æ­¥æ•°çš„5-10%)
        num_training_steps: æ€»è®­ç»ƒæ­¥æ•°
        num_cycles: cosineå‘¨æœŸæ•° (0.5 = åŠå‘¨æœŸ, 1.0 = å…¨å‘¨æœŸ)
        min_lr_ratio: æœ€å°å­¦ä¹ ç‡å æ¯” (é»˜è®¤0.1 = é™åˆ°10%)

    Returns:
        LambdaLRè°ƒåº¦å™¨
    """
    def lr_lambda(current_step: int):
        # Warmupé˜¶æ®µ: çº¿æ€§å¢é•¿
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))

        # Cosineé€€ç«é˜¶æ®µ
        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
        cosine_decay = 0.5 * (1.0 + math.cos(math.pi * num_cycles * 2.0 * progress))

        # ç¡®ä¿ä¸ä½äºmin_lr_ratio
        return max(min_lr_ratio, cosine_decay)

    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)


def get_inverse_sqrt_schedule(
    optimizer: Optimizer,
    num_warmup_steps: int,
    timescale: int = 10000
):
    """
    Inverse Sqrtè°ƒåº¦å™¨ (Transformerè®ºæ–‡åŸå§‹è°ƒåº¦å™¨)

    "Attention is All You Need"ä¸­ä½¿ç”¨çš„è°ƒåº¦å™¨

    å…¬å¼: lr = lr_max * min(step^(-0.5), step * warmup_steps^(-1.5))

    Args:
        optimizer: ä¼˜åŒ–å™¨
        num_warmup_steps: warmupæ­¥æ•°
        timescale: æ—¶é—´å°ºåº¦ (é»˜è®¤10000)

    Returns:
        LambdaLRè°ƒåº¦å™¨
    """
    def lr_lambda(current_step: int):
        current_step += 1  # é¿å…é™¤ä»¥0
        if current_step < num_warmup_steps:
            # Warmupé˜¶æ®µ: çº¿æ€§å¢é•¿
            return float(current_step) / float(num_warmup_steps)
        else:
            # Inverse sqrt decay
            return float(num_warmup_steps) ** 0.5 / (current_step ** 0.5)

    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)


def get_wsd_schedule(
    optimizer: Optimizer,
    num_warmup_steps: int,
    num_stable_steps: int,
    num_decay_steps: int,
    min_lr_ratio: float = 0.1
):
    """
    Warmup-Stable-Decayè°ƒåº¦å™¨ (2024æœ€æ–°)

    æ— éœ€é¢„è®¾æ€»è®­ç»ƒæ­¥æ•°,æ”¯æŒæŒç»­è®­ç»ƒ

    é˜¶æ®µ:
    1. Warmup: 0 â†’ warmup_steps, çº¿æ€§å¢é•¿
    2. Stable: warmup_steps â†’ warmup_steps + stable_steps, ä¿æŒå¸¸æ•°
    3. Decay: ä¹‹å, Cosineé€€ç«

    è®ºæ–‡: https://arxiv.org/abs/2410.05192

    Args:
        optimizer: ä¼˜åŒ–å™¨
        num_warmup_steps: warmupæ­¥æ•°
        num_stable_steps: ç¨³å®šé˜¶æ®µæ­¥æ•°
        num_decay_steps: è¡°å‡é˜¶æ®µæ­¥æ•°
        min_lr_ratio: æœ€å°å­¦ä¹ ç‡å æ¯”

    Returns:
        LambdaLRè°ƒåº¦å™¨
    """
    def lr_lambda(current_step: int):
        # 1. Warmupé˜¶æ®µ
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))

        # 2. Stableé˜¶æ®µ
        if current_step < num_warmup_steps + num_stable_steps:
            return 1.0

        # 3. Decayé˜¶æ®µ
        progress = float(current_step - num_warmup_steps - num_stable_steps) / float(max(1, num_decay_steps))
        progress = min(progress, 1.0)  # é™åˆ¶åœ¨[0, 1]
        cosine_decay = 0.5 * (1.0 + math.cos(math.pi * progress))

        return max(min_lr_ratio, cosine_decay)

    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)


# æ›´æ–°get_optimizerå‡½æ•°ä»¥æ”¯æŒMuon
_original_get_optimizer = get_optimizer


def get_optimizer(
    optimizer_name: str,
    parameters,
    lr: float = 1e-4,
    weight_decay: float = 0.01,
    **kwargs
) -> Optimizer:
    """
    è·å–ä¼˜åŒ–å™¨å®ä¾‹ (2024æ›´æ–°)

    æ–°å¢æ”¯æŒ:
    - muon: Muonä¼˜åŒ–å™¨ (2024, æ¨èå¤§æ¨¡å‹)
    - muon_adamw: Muon+AdamWæ··åˆ (æœ€ä¼˜é…ç½®)

    æ¨èé…ç½®:
    1. å°æ¨¡å‹(<1B): adamw
    2. å¤§æ¨¡å‹(>1B): muon_adamw (æ··åˆ)
    3. é¢„è®­ç»ƒ: adamw with Î²2=0.95
    4. å¾®è°ƒ: adamw with Î²2=0.999
    """
    optimizer_name = optimizer_name.lower()

    if optimizer_name == "muon":
        return Muon(
            parameters,
            lr=lr,
            momentum=0.95,
            weight_decay=weight_decay,
            **kwargs
        )
    elif optimizer_name == "muon_adamw":
        # æ··åˆæ¨¡å¼: éœ€è¦ä¼ å…¥modelè€Œä¸æ˜¯parameters
        # è¿™é‡Œè¿”å›æç¤ºä¿¡æ¯
        raise ValueError(
            "muon_adamwéœ€è¦ä½¿ç”¨get_hybrid_optimizer(model, ...)å‡½æ•°åˆ›å»º"
        )
    else:
        # è°ƒç”¨åŸå§‹å‡½æ•°å¤„ç†å…¶ä»–ä¼˜åŒ–å™¨
        return _original_get_optimizer(optimizer_name, parameters, lr, weight_decay, **kwargs)


def get_hybrid_optimizer(
    model,
    muon_lr: float = 0.02,
    adamw_lr: float = 1e-3,
    weight_decay: float = 0.01,
    betas: tuple = (0.9, 0.95)
):
    """
    åˆ›å»ºMuon+AdamWæ··åˆä¼˜åŒ–å™¨ (2024æœ€ä¼˜é…ç½®)

    Muonå¤„ç†2Då‚æ•°(æƒé‡çŸ©é˜µ), AdamWå¤„ç†1Då‚æ•°(bias, norm)

    Args:
        model: PyTorchæ¨¡å‹
        muon_lr: Muonå­¦ä¹ ç‡ (é»˜è®¤0.02, æ˜¯AdamWçš„20å€)
        adamw_lr: AdamWå­¦ä¹ ç‡ (é»˜è®¤1e-3)
        weight_decay: æƒé‡è¡°å‡
        betas: AdamWçš„betaå‚æ•°

    Returns:
        dictåŒ…å«ä¸¤ä¸ªä¼˜åŒ–å™¨: {'muon': Muon, 'adamw': AdamW}

    ä½¿ç”¨ç¤ºä¾‹:
        opts = get_hybrid_optimizer(model)
        # è®­ç»ƒå¾ªç¯ä¸­:
        loss.backward()
        opts['muon'].step()
        opts['adamw'].step()
        opts['muon'].zero_grad()
        opts['adamw'].zero_grad()
    """
    params_2d = []  # æƒé‡çŸ©é˜µ
    params_1d = []  # bias, normç­‰

    for name, param in model.named_parameters():
        if param.requires_grad:
            if len(param.shape) >= 2:
                params_2d.append(param)
            else:
                params_1d.append(param)

    print(f"ğŸ”§ åˆ›å»ºæ··åˆä¼˜åŒ–å™¨:")
    print(f"   - Muon: {len(params_2d)} ä¸ª2Då‚æ•°, lr={muon_lr}")
    print(f"   - AdamW: {len(params_1d)} ä¸ª1Då‚æ•°, lr={adamw_lr}")

    return {
        'muon': Muon(
            params_2d,
            lr=muon_lr,
            momentum=0.95,
            weight_decay=weight_decay
        ),
        'adamw': torch.optim.AdamW(
            params_1d,
            lr=adamw_lr,
            betas=betas,
            weight_decay=weight_decay
        )
    }