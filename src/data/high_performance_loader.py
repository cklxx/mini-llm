"""
é«˜æ€§èƒ½æ•°æ®åŠ è½½ç³»ç»Ÿ
æ”¯æŒæµå¼åŠ è½½ã€æ™ºèƒ½ç¼“å­˜ã€å¹¶è¡Œå¤„ç†ã€å†…å­˜ä¼˜åŒ–
"""
import hashlib
import json
import os
import pickle
import time
from collections.abc import Iterator
from concurrent.futures import ThreadPoolExecutor
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any

import torch
from torch.utils.data import DataLoader, IterableDataset
from tqdm import tqdm


@dataclass
class DataLoadingConfig:
    """é«˜æ€§èƒ½æ•°æ®åŠ è½½é…ç½®"""
    # åŸºç¡€é…ç½®
    data_path: str
    max_length: int = 512
    batch_size: int = 32

    # æ€§èƒ½é…ç½®
    num_workers: int = 4
    prefetch_factor: int = 2
    pin_memory: bool = True

    # ç¼“å­˜é…ç½®
    enable_cache: bool = True
    cache_dir: str = "data_cache"
    force_rebuild_cache: bool = False

    # æµå¼åŠ è½½é…ç½®
    streaming: bool = True
    chunk_size: int = 10000  # æ¯ä¸ªchunkçš„æ ·æœ¬æ•°
    buffer_size: int = 50000  # å†…å­˜ä¸­æœ€å¤§æ ·æœ¬æ•°

    # é¢„å¤„ç†é…ç½®
    parallel_processing: bool = True
    max_parallel_workers: int = 8


class StreamingJsonLoader:
    """æµå¼JSONåŠ è½½å™¨ï¼Œæ”¯æŒå¤§æ–‡ä»¶é«˜æ•ˆå¤„ç†"""

    def __init__(self, file_path: str, chunk_size: int = 10000):
        self.file_path = file_path
        self.chunk_size = chunk_size
        self.file_size = os.path.getsize(file_path)

    def __iter__(self) -> Iterator[dict[str, Any]]:
        """æµå¼è¿­ä»£JSONè¡Œ"""
        with open(self.file_path, encoding='utf-8') as f:
            buffer = []
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue

                try:
                    data = json.loads(line)
                    buffer.append(data)

                    if len(buffer) >= self.chunk_size:
                        yield from buffer
                        buffer = []

                except json.JSONDecodeError as e:
                    print(f"Warning: JSON decode error at line {line_num}: {e}")
                    continue

            # å¤„ç†æœ€åçš„ç¼“å†²åŒº
            if buffer:
                yield from buffer

    def get_chunks(self) -> Iterator[list[dict[str, Any]]]:
        """ä»¥chunkå½¢å¼è¿”å›æ•°æ®"""
        buffer = []
        for item in self:
            buffer.append(item)
            if len(buffer) >= self.chunk_size:
                yield buffer
                buffer = []

        if buffer:
            yield buffer


class IntelligentDataCache:
    """æ™ºèƒ½æ•°æ®ç¼“å­˜ç³»ç»Ÿ"""

    def __init__(self, cache_dir: str = "data_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True, parents=True)

        # å…ƒæ•°æ®æ–‡ä»¶
        self.metadata_file = self.cache_dir / "cache_metadata.json"
        self.metadata = self._load_metadata()

    def _load_metadata(self) -> dict[str, Any]:
        """åŠ è½½ç¼“å­˜å…ƒæ•°æ®"""
        if self.metadata_file.exists():
            try:
                with open(self.metadata_file, encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                pass
        return {}

    def _save_metadata(self):
        """ä¿å­˜ç¼“å­˜å…ƒæ•°æ®"""
        with open(self.metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, indent=2, ensure_ascii=False)

    def _get_file_hash(self, file_path: str, sample_lines: int = 1000) -> str:
        """è®¡ç®—æ–‡ä»¶hashå€¼ï¼Œä½¿ç”¨é‡‡æ ·æé«˜æ•ˆç‡"""
        hash_obj = hashlib.md5()

        # æ·»åŠ æ–‡ä»¶åŸºæœ¬ä¿¡æ¯
        stat = os.stat(file_path)
        hash_obj.update(f"{stat.st_size}_{stat.st_mtime}".encode())

        # é‡‡æ ·æ–‡ä»¶å†…å®¹
        with open(file_path, encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i >= sample_lines:
                    break
                hash_obj.update(line.encode('utf-8'))

        return hash_obj.hexdigest()[:16]

    def _get_cache_key(self, config: DataLoadingConfig) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        file_hash = self._get_file_hash(config.data_path)
        config_str = f"{config.max_length}_{config.chunk_size}"
        return f"{file_hash}_{hashlib.md5(config_str.encode()).hexdigest()[:8]}"

    def get_cache_path(self, cache_key: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return self.cache_dir / f"{cache_key}.pkl"

    def is_cache_valid(self, config: DataLoadingConfig) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if config.force_rebuild_cache:
            return False

        cache_key = self._get_cache_key(config)
        cache_path = self.get_cache_path(cache_key)

        if not cache_path.exists():
            return False

        # æ£€æŸ¥å…ƒæ•°æ®
        if cache_key not in self.metadata:
            return False

        metadata = self.metadata[cache_key]

        # æ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦ä¿®æ”¹
        current_hash = self._get_file_hash(config.data_path)
        if metadata.get('file_hash') != current_hash:
            return False

        return True

    def load_cache(self, config: DataLoadingConfig) -> list[dict[str, Any]] | None:
        """åŠ è½½ç¼“å­˜æ•°æ®"""
        if not self.is_cache_valid(config):
            return None

        cache_key = self._get_cache_key(config)
        cache_path = self.get_cache_path(cache_key)

        try:
            print(f"ğŸ“¦ Loading cached data: {cache_path.name}")
            with open(cache_path, 'rb') as f:
                data = pickle.load(f)

            metadata = self.metadata[cache_key]
            print(f"âœ… Cache loaded: {len(data)} samples, "
                  f"created {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(metadata['created_at']))}")

            return data

        except Exception as e:
            print(f"âš ï¸  Cache loading failed: {e}")
            return None

    def save_cache(self, config: DataLoadingConfig, data: list[dict[str, Any]]):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        cache_key = self._get_cache_key(config)
        cache_path = self.get_cache_path(cache_key)

        try:
            print(f"ğŸ’¾ Saving data to cache: {cache_path.name}")
            with open(cache_path, 'wb') as f:
                pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)

            # æ›´æ–°å…ƒæ•°æ®
            self.metadata[cache_key] = {
                'file_hash': self._get_file_hash(config.data_path),
                'data_path': config.data_path,
                'max_length': config.max_length,
                'sample_count': len(data),
                'created_at': time.time(),
                'cache_size': cache_path.stat().st_size
            }
            self._save_metadata()

            print(f"âœ… Cache saved: {len(data)} samples, "
                  f"size: {cache_path.stat().st_size / 1024 / 1024:.1f}MB")

        except Exception as e:
            print(f"âŒ Cache saving failed: {e}")

    def clean_old_cache(self, keep_recent: int = 5):
        """æ¸…ç†æ—§ç¼“å­˜"""
        cache_files = list(self.cache_dir.glob("*.pkl"))

        if len(cache_files) <= keep_recent:
            return

        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
        cache_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)

        # åˆ é™¤æ—§æ–‡ä»¶
        for cache_file in cache_files[keep_recent:]:
            try:
                cache_file.unlink()
                print(f"ğŸ—‘ï¸  Removed old cache: {cache_file.name}")
            except Exception as e:
                print(f"âš ï¸  Failed to remove {cache_file.name}: {e}")

        # æ¸…ç†å…ƒæ•°æ®
        valid_keys = {f.stem for f in cache_files[:keep_recent]}
        self.metadata = {k: v for k, v in self.metadata.items() if k in valid_keys}
        self._save_metadata()


class ParallelDataProcessor:
    """å¹¶è¡Œæ•°æ®å¤„ç†å™¨"""

    def __init__(self, max_workers: int = 8):
        self.max_workers = max_workers

    def process_conversations(self, data_chunks: list[list[dict]],
                            max_length: int) -> list[dict[str, Any]]:
        """å¹¶è¡Œå¤„ç†å¯¹è¯æ•°æ®"""
        if len(data_chunks) == 1:
            # å•ä¸ªchunkï¼Œç›´æ¥å¤„ç†
            return self._process_conversation_chunk(data_chunks[0], max_length)

        # å¤šä¸ªchunkï¼Œå¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [
                executor.submit(self._process_conversation_chunk, chunk, max_length)
                for chunk in data_chunks
            ]

            results = []
            for future in tqdm(futures, desc="Processing chunks"):
                results.extend(future.result())

            return results

    def _process_conversation_chunk(self, chunk: list[dict],
                                  max_length: int) -> list[dict[str, Any]]:
        """å¤„ç†å•ä¸ªå¯¹è¯æ•°æ®chunk"""
        processed = []

        for item in chunk:
            if 'conversations' in item:
                conversations = item['conversations']
                if len(conversations) >= 2:
                    user_input = ""
                    assistant_output = ""

                    for conv in conversations:
                        if conv['role'] == 'user':
                            user_input = conv['content']
                        elif conv['role'] == 'assistant':
                            assistant_output = conv['content']

                    if user_input and assistant_output:
                        total_length = len(user_input) + len(assistant_output)
                        if total_length <= max_length:
                            processed.append({
                                'input': user_input,
                                'output': assistant_output,
                                'length': total_length,
                                'type': 'conversation'
                            })

        return processed

    def process_pretrain_texts(self, data_chunks: list[list[dict]],
                             max_length: int) -> list[str]:
        """å¹¶è¡Œå¤„ç†é¢„è®­ç»ƒæ–‡æœ¬æ•°æ®"""
        if len(data_chunks) == 1:
            return self._process_pretrain_chunk(data_chunks[0], max_length)

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [
                executor.submit(self._process_pretrain_chunk, chunk, max_length)
                for chunk in data_chunks
            ]

            results = []
            for future in tqdm(futures, desc="Processing text chunks"):
                results.extend(future.result())

            return results

    def _process_pretrain_chunk(self, chunk: list[dict],
                              max_length: int) -> list[str]:
        """å¤„ç†å•ä¸ªé¢„è®­ç»ƒæ–‡æœ¬chunk"""
        texts = []

        for item in chunk:
            if 'text' in item:
                text = item['text']
                if len(text) <= max_length:
                    texts.append(text)

        return texts


class HighPerformanceDataset(IterableDataset):
    """é«˜æ€§èƒ½å¯è¿­ä»£æ•°æ®é›†"""

    def __init__(self, config: DataLoadingConfig, tokenizer, data_type: str = "sft"):
        self.config = config
        self.tokenizer = tokenizer
        self.data_type = data_type

        # åˆå§‹åŒ–ç¼“å­˜å’Œå¤„ç†å™¨
        self.cache = IntelligentDataCache(config.cache_dir) if config.enable_cache else None
        self.processor = ParallelDataProcessor(config.max_parallel_workers)

        # åŠ è½½æˆ–å¤„ç†æ•°æ®
        self.data = self._load_or_process_data()

    def _load_or_process_data(self) -> list[dict[str, Any]]:
        """åŠ è½½æˆ–å¤„ç†æ•°æ®"""
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if self.cache and self.cache.is_cache_valid(self.config):
            cached_data = self.cache.load_cache(self.config)
            if cached_data is not None:
                return cached_data

        # å¤„ç†åŸå§‹æ•°æ®
        print(f"ğŸ”„ Processing data from: {self.config.data_path}")
        loader = StreamingJsonLoader(self.config.data_path, self.config.chunk_size)

        # æ”¶é›†æ•°æ®chunks
        data_chunks = list(loader.get_chunks())
        print(f"ğŸ“Š Loaded {len(data_chunks)} chunks for processing")

        # å¹¶è¡Œå¤„ç†
        if self.data_type == "sft":
            processed_data = self.processor.process_conversations(data_chunks, self.config.max_length)
        elif self.data_type == "pretrain":
            processed_data = self.processor.process_pretrain_texts(data_chunks, self.config.max_length)
        else:
            raise ValueError(f"Unsupported data type: {self.data_type}")

        print(f"âœ… Processed {len(processed_data)} samples")

        # ä¿å­˜åˆ°ç¼“å­˜
        if self.cache:
            self.cache.save_cache(self.config, processed_data)

        return processed_data

    def __iter__(self):
        """è¿­ä»£æ•°æ®"""
        if self.config.streaming and len(self.data) > self.config.buffer_size:
            # æµå¼æ¨¡å¼ï¼šéšæœºé‡‡æ ·æ•°æ®é¿å…å†…å­˜æº¢å‡º
            indices = torch.randperm(len(self.data))
            for i in indices:
                yield self._process_item(self.data[i])
        else:
            # æ™®é€šæ¨¡å¼ï¼šç›´æ¥è¿­ä»£
            for item in self.data:
                yield self._process_item(item)

    def _process_item(self, item: dict[str, Any]) -> dict[str, torch.Tensor]:
        """å¤„ç†å•ä¸ªæ•°æ®é¡¹"""
        if self.data_type == "sft":
            return self._process_conversation_item(item)
        elif self.data_type == "pretrain":
            return self._process_pretrain_item(item)
        else:
            raise ValueError(f"Unsupported data type: {self.data_type}")

    def _process_conversation_item(self, item: dict[str, Any]) -> dict[str, torch.Tensor]:
        """å¤„ç†å¯¹è¯æ•°æ®é¡¹"""
        input_text = item['input']
        output_text = item['output']

        # ç¼–ç æ–‡æœ¬
        input_ids = self.tokenizer.encode(input_text, add_special_tokens=False)
        output_ids = self.tokenizer.encode(output_text, add_special_tokens=False)

        # æ„é€ å®Œæ•´åºåˆ—
        full_sequence = [self.tokenizer.bos_id] + input_ids + output_ids + [self.tokenizer.eos_id]

        # æˆªæ–­å’Œå¡«å……
        if len(full_sequence) > self.config.max_length:
            full_sequence = full_sequence[:self.config.max_length]

        labels = full_sequence[1:] + [self.tokenizer.pad_id]

        # å¡«å……åˆ°å›ºå®šé•¿åº¦
        while len(full_sequence) < self.config.max_length:
            full_sequence.append(self.tokenizer.pad_id)
            labels.append(self.tokenizer.pad_id)

        return {
            'input_ids': torch.tensor(full_sequence, dtype=torch.long),
            'labels': torch.tensor(labels, dtype=torch.long),
            'attention_mask': torch.tensor([1 if x != self.tokenizer.pad_id else 0 for x in full_sequence], dtype=torch.long)
        }

    def _process_pretrain_item(self, text: str) -> torch.Tensor:
        """å¤„ç†é¢„è®­ç»ƒæ–‡æœ¬é¡¹"""
        token_ids = self.tokenizer.encode(text, add_special_tokens=True)

        # ç¡®ä¿è‡³å°‘æœ‰2ä¸ªtoken
        if len(token_ids) < 2:
            token_ids = [self.tokenizer.bos_id, self.tokenizer.eos_id]

        # æˆªæ–­æˆ–å¡«å……
        if len(token_ids) > self.config.max_length:
            token_ids = token_ids[:self.config.max_length]
        else:
            token_ids.extend([self.tokenizer.pad_id] * (self.config.max_length - len(token_ids)))

        return torch.tensor(token_ids, dtype=torch.long)

    def __len__(self):
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.data)


def create_high_performance_dataloader(
    config: DataLoadingConfig,
    tokenizer,
    data_type: str = "sft"
) -> DataLoader:
    """åˆ›å»ºé«˜æ€§èƒ½æ•°æ®åŠ è½½å™¨"""

    dataset = HighPerformanceDataset(config, tokenizer, data_type)

    # åˆ›å»ºDataLoader
    dataloader = DataLoader(
        dataset,
        batch_size=config.batch_size,
        num_workers=config.num_workers,
        prefetch_factor=config.prefetch_factor,
        pin_memory=config.pin_memory and torch.cuda.is_available(),
        drop_last=True,
        persistent_workers=config.num_workers > 0
    )

    return dataloader


def benchmark_data_loading(config: DataLoadingConfig, tokenizer, iterations: int = 100):
    """åŸºå‡†æµ‹è¯•æ•°æ®åŠ è½½æ€§èƒ½"""
    print("ğŸ”¬ Benchmarking data loading performance...")
    print(f"   Config: batch_size={config.batch_size}, num_workers={config.num_workers}")
    print(f"   Cache: {'enabled' if config.enable_cache else 'disabled'}")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = create_high_performance_dataloader(config, tokenizer, "sft")

    # é¢„çƒ­
    print("ğŸ”¥ Warming up...")
    for i, _batch in enumerate(dataloader):
        if i >= 5:
            break

    # åŸºå‡†æµ‹è¯•
    print(f"â±ï¸  Running {iterations} iterations...")
    start_time = time.time()
    samples_processed = 0

    for i, batch in enumerate(dataloader):
        if i >= iterations:
            break
        samples_processed += len(batch['input_ids'])

    end_time = time.time()
    elapsed = end_time - start_time

    # ç»“æœ
    throughput = samples_processed / elapsed
    print("ğŸ“Š Benchmark Results:")
    print(f"   Processed: {samples_processed} samples in {elapsed:.2f}s")
    print(f"   Throughput: {throughput:.1f} samples/sec")
    print(f"   Avg batch time: {elapsed / iterations * 1000:.1f}ms")

    return {
        'throughput': throughput,
        'elapsed_time': elapsed,
        'samples_processed': samples_processed,
        'avg_batch_time': elapsed / iterations
    }


if __name__ == "__main__":
    # æµ‹è¯•é«˜æ€§èƒ½æ•°æ®åŠ è½½ç³»ç»Ÿ
    print("ğŸ§ª Testing high-performance data loading system...")

    # é…ç½®
    config = DataLoadingConfig(
        data_path="data/dataset/minimind_dataset/sft_mini_512.jsonl",
        max_length=512,
        batch_size=16,
        num_workers=4,
        enable_cache=True,
        streaming=True,
        parallel_processing=True
    )

    print(f"Config: {asdict(config)}")
