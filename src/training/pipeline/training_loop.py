"""Standalone training loop logic for the MiniGPT trainer."""

from __future__ import annotations

import math
import time
from dataclasses import dataclass
from typing import Any

import torch


@dataclass
class TrainingControl:
    """Shared stop-flag between the trainer and the signal handler."""

    interrupted: bool = False


class TrainingLoopRunner:
    """Execute the gradient update loop and associated evaluations."""

    def __init__(self, config, device: str, checkpoints, mode: str):
        self.config = config
        self.device = device
        self.checkpoints = checkpoints
        self.mode = mode

    # ------------------------------------------------------------------
    def run(
        self,
        model,
        tokenizer,
        train_loader,
        val_loader,
        optimizer,
        scheduler,
        criterion,
        scaler,
        monitor,
        control: TrainingControl,
        start_step: int,
        start_time: float,
        memory_hooks=None,
        regression_suite=None,
    ) -> str:
        model.train()
        step = start_step
        accumulation_steps = self.config.gradient_accumulation_steps

        print(f"å¼€å§‹è®­ç»ƒï¼Œæœ€å¤§æ­¥æ•°: {self.config.max_steps}")
        print(
            f"Batch size: {self.config.batch_size}, æ¢¯åº¦ç´¯ç§¯: {accumulation_steps}, "
            f"æœ‰æ•ˆbatch: {self.config.batch_size * accumulation_steps}"
        )

        if memory_hooks is not None:
            memory_hooks.on_train_start()

        if self.device == "cuda":
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            print(f"ğŸ’¾ åˆå§‹GPUå†…å­˜: å·²åˆ†é…={allocated:.2f}GB, å·²ä¿ç•™={reserved:.2f}GB")

        best_val_loss = float("inf")
        no_improve_steps = 0

        for epoch in range(1000):
            epoch_loss = 0.0
            epoch_steps = 0
            optimizer.zero_grad()

            for batch_idx, batch in enumerate(train_loader):
                if control.interrupted:
                    print(f"\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­ï¼ˆæ­¥éª¤ {step}ï¼‰")
                    break
                if step >= self.config.max_steps:
                    break

                if isinstance(batch, dict):
                    seq_length = batch["input_ids"].size(1)
                else:
                    seq_length = batch.size(1)
                if seq_length < 2:
                    continue

                try:
                    loss = self._forward_backward(
                        model,
                        tokenizer,
                        batch,
                        criterion,
                        scaler,
                        accumulation_steps,
                    )
                except torch.cuda.OutOfMemoryError:
                    optimizer.zero_grad()
                    if self.device == "cuda":
                        torch.cuda.empty_cache()
                    if memory_hooks is not None:
                        memory_hooks.on_oom()
                    raise

                if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(
                    train_loader
                ):
                    step, grad_norm = self._optimizer_step(
                        model, optimizer, scheduler, scaler, step
                    )

                    actual_loss = loss.item() * accumulation_steps
                    epoch_loss += actual_loss
                    epoch_steps += 1

                    current_batch_size = (
                        batch["input_ids"].size(0) if isinstance(batch, dict) else batch.size(0)
                    )
                    monitor.log_step(
                        step=step,
                        epoch=epoch,
                        loss=actual_loss,
                        learning_rate=optimizer.param_groups[0]["lr"],
                        batch_size=current_batch_size * accumulation_steps,
                        grad_norm=grad_norm,
                    )

                    if memory_hooks is not None:
                        memory_hooks.on_step_end(step)

                    avg_loss = epoch_loss / max(epoch_steps, 1)
                    elapsed = time.time() - start_time
                    current_lr = optimizer.param_groups[0]["lr"]
                    lr_phase = "Warmup" if step < self.config.warmup_steps else "Decay"
                    lr_progress = (
                        f"{step}/{self.config.warmup_steps}"
                        if step < self.config.warmup_steps
                        else f"{step}/{self.config.max_steps}"
                    )
                    print(
                        f"Step {step:5d} | Loss: {actual_loss:.4f} | Avg: {avg_loss:.4f} | "
                        f"LR: {current_lr:.2e} ({lr_phase} {lr_progress}) | Time: {elapsed/60:.1f}min"
                    )

                    if step % 100 == 0:
                        self.checkpoints.save(model, optimizer, step, actual_loss, tokenizer)

                    if step % self.config.eval_steps == 0:
                        eval_metrics = self._evaluate(
                            model,
                            tokenizer,
                            val_loader,
                            criterion,
                            monitor,
                            step,
                            regression_suite,
                        )
                        if eval_metrics and "val_loss" in eval_metrics:
                            best_val_loss, no_improve_steps = self._maybe_update_best(
                                model,
                                optimizer,
                                tokenizer,
                                step,
                                eval_metrics["val_loss"],
                                best_val_loss,
                                no_improve_steps,
                                control,
                            )

                    if step >= self.config.max_steps:
                        break

            if step >= self.config.max_steps or control.interrupted:
                break

        if control.interrupted:
            print("\nğŸ’¾ æ­£åœ¨ä¿å­˜ä¸­æ–­checkpoint...")
            self.checkpoints.save(
                model,
                optimizer,
                step,
                epoch_loss / max(epoch_steps, 1) if epoch_steps else 0.0,
                tokenizer,
            )
            print("ğŸ’¡ å¯ä½¿ç”¨ --auto-resume ä»æ­¤å¤„æ¢å¤è®­ç»ƒ")

        final_path = self.checkpoints.save_final(model, tokenizer, step)

        if control.interrupted:
            print("\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
            print("âœ… å·²æˆåŠŸä¿å­˜ä¸­æ–­æ—¶çš„æ¨¡å‹çŠ¶æ€")
        else:
            print(f"\nğŸ‰ {self.mode}è®­ç»ƒå®Œæˆï¼")

        print(f"æ€»æ­¥æ•°: {step}")
        print(f"è®­ç»ƒæ—¶é—´: {(time.time() - start_time)/60:.1f}åˆ†é’Ÿ")
        print(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_path}")
        return final_path

    # ------------------------------------------------------------------
    def _forward_backward(self, model, tokenizer, batch, criterion, scaler, accumulation_steps):
        try:
            if isinstance(batch, dict):
                input_ids = batch["input_ids"].to(self.device, non_blocking=True)
                if "labels" in batch:
                    target_ids = batch["labels"].to(self.device, non_blocking=True)
                else:
                    target_ids = torch.cat(
                        [
                            input_ids[:, 1:],
                            torch.full(
                                (input_ids.size(0), 1),
                                tokenizer.pad_id,
                                dtype=torch.long,
                                device=self.device,
                            ),
                        ],
                        dim=1,
                    )
            else:
                batch = batch.to(self.device, non_blocking=True)
                if batch.size(1) < 2:
                    raise ValueError("æ‰¹æ¬¡åºåˆ—é•¿åº¦è¿‡çŸ­")
                input_ids = batch[:, :-1]
                target_ids = batch[:, 1:]

            if scaler is not None:
                with torch.amp.autocast("cuda"):
                    outputs = model(input_ids)
                    loss = criterion(outputs.reshape(-1, outputs.size(-1)), target_ids.reshape(-1))
                    loss = loss / accumulation_steps
                scaler.scale(loss).backward()
            else:
                outputs = model(input_ids)
                loss = criterion(outputs.reshape(-1, outputs.size(-1)), target_ids.reshape(-1))
                loss = loss / accumulation_steps
                loss.backward()

            return loss

        except torch.cuda.OutOfMemoryError:
            self._handle_oom(batch, tokenizer)
            raise

    def _optimizer_step(self, model, optimizer, scheduler, scaler, step: int) -> tuple[int, float]:
        if scaler is not None:
            scaler.unscale_(optimizer)
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
        else:
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

        scheduler.step()
        optimizer.zero_grad()

        if isinstance(grad_norm, torch.Tensor):
            grad_norm_value = float(grad_norm.detach().cpu().item())
        else:
            grad_norm_value = float(grad_norm)

        return step + 1, grad_norm_value

    def _maybe_update_best(
        self,
        model,
        optimizer,
        tokenizer,
        step: int,
        val_loss: float,
        best_val_loss: float,
        no_improve_steps: int,
        control: TrainingControl,
    ) -> tuple[float, int]:
        delta = getattr(self.config, "early_stopping_delta", 0.0)
        patience = getattr(self.config, "early_stopping_patience", 0)

        if val_loss + delta < best_val_loss:
            best_val_loss = val_loss
            no_improve_steps = 0
            self.checkpoints.save(model, optimizer, step, val_loss, tokenizer)
            print("ğŸ’¾ éªŒè¯é›†æŒ‡æ ‡æå‡ï¼Œå·²ä¿å­˜æœ€ä½³æ¨¡å‹")
        else:
            no_improve_steps += 1
            if patience > 0 and no_improve_steps >= patience:
                print(f"ğŸ›‘ è§¦å‘æ—©åœ: éªŒè¯é›†åœ¨è¿ç»­ {no_improve_steps} æ¬¡è¯„ä¼°åæœªæ”¹è¿›")
                control.interrupted = True

        return best_val_loss, no_improve_steps

    # ------------------------------------------------------------------
    def _evaluate(
        self, model, tokenizer, val_loader, criterion, monitor, step, regression_suite=None
    ):
        metrics: dict[str, Any] = {}
        if val_loader:
            model.eval()
            total_loss = 0.0
            total_tokens = 0
            pad_id = tokenizer.pad_id
            with torch.no_grad():
                for batch in val_loader:
                    if isinstance(batch, dict):
                        input_ids = batch["input_ids"].to(self.device, non_blocking=True)
                        target_ids = batch["labels"].to(self.device, non_blocking=True)
                    else:
                        batch = batch.to(self.device, non_blocking=True)
                        input_ids = batch[:, :-1]
                        target_ids = batch[:, 1:]
                    logits = model(input_ids)
                    loss = criterion(logits.reshape(-1, logits.size(-1)), target_ids.reshape(-1))
                    valid_tokens = torch.count_nonzero(target_ids != pad_id).item()
                    if valid_tokens == 0:
                        continue
                    total_loss += loss.item() * valid_tokens
                    total_tokens += valid_tokens
            avg_loss = total_loss / total_tokens if total_tokens > 0 else float("inf")
            perplexity = (
                math.exp(min(20, avg_loss))
                if avg_loss not in (float("inf"), float("nan"))
                else float("inf")
            )
            metrics = {
                "val_loss": avg_loss,
                "perplexity": perplexity,
                "val_tokens": total_tokens,
            }
            monitor.log_validation(step, avg_loss, perplexity, {"ValTokens": total_tokens})
            print(
                f"ğŸ” éªŒè¯ Step {step}: loss={avg_loss:.4f}, ppl={perplexity:.2f}, tokens={total_tokens}"
            )
            model.train()
        else:
            print("â„¹ï¸ å½“å‰æœªé…ç½®éªŒè¯é›†ï¼Œè·³è¿‡éªŒè¯æŸå¤±è®¡ç®—ï¼Œä»…è¿›è¡Œç”Ÿæˆæ£€æŸ¥")
        try:
            self._smoke_generation(model, tokenizer, step)
        except Exception as exc:
            print(f"âš ï¸  ç”ŸæˆéªŒè¯å¤±è´¥: {exc}")

        if regression_suite is not None:
            try:
                regression_suite.maybe_run(model, tokenizer, step, monitor=monitor)
            except Exception as exc:
                print(f"âš ï¸  Regression suite failed: {exc}")
        return metrics

    def _smoke_generation(
        self, model, tokenizer, step, prompt: str = "ä½ å¥½ï¼Œæˆ‘æ˜¯", max_new_tokens: int = 32
    ):
        model.eval()
        try:
            input_ids = tokenizer.encode(prompt, add_special_tokens=True)
            input_tensor = torch.tensor([input_ids], device=self.device)
            generated = input_tensor
            with torch.no_grad():
                for _ in range(max_new_tokens):
                    outputs = model(generated)
                    next_token_logits = outputs[:, -1, :]
                    next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
                    generated = torch.cat([generated, next_token], dim=1)
            decoded = tokenizer.decode(generated[0].tolist())
            print(f"ğŸ§ª Step {step} ç”Ÿæˆæ ·ä¾‹: {decoded[:200]}")
        finally:
            model.train()

    # ------------------------------------------------------------------
    def _handle_oom(self, batch, tokenizer) -> None:
        print("\nâŒ CUDA OOMé”™è¯¯!")
        if isinstance(batch, dict):
            batch_size = batch["input_ids"].size(0)
            seq_length = batch["input_ids"].size(1)
        else:
            batch_size = batch.size(0)
            seq_length = batch.size(1) if batch.dim() > 1 else 0
        print(f"   å½“å‰æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   åºåˆ—é•¿åº¦: {seq_length}")
        if self.device == "cuda":
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            print(f"   GPUå†…å­˜: å·²åˆ†é…={allocated:.2f}GB, å·²ä¿ç•™={reserved:.2f}GB")
            torch.cuda.empty_cache()
        print("\nğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
        print(f"   1. é™ä½batch_size: --batch-size {self.config.batch_size // 2}")
        print(
            f"   2. å¢åŠ æ¢¯åº¦ç´¯ç§¯: å½“å‰={self.config.gradient_accumulation_steps}, å»ºè®®={self.config.gradient_accumulation_steps * 2}"
        )
        print(f"   3. å‡å°åºåˆ—é•¿åº¦: å½“å‰max_seq_len={self.config.max_seq_len}")
        print("   4. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ (gradient checkpointing)")
        print("   5. è®¾ç½®ç¯å¢ƒå˜é‡: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True")
