"""High-level training orchestration for MiniGPT."""
from __future__ import annotations

import math
import os
import signal
import sys
import time
from datetime import datetime
from typing import Optional

import torch

from model.transformer import create_model
from training.training_monitor import TrainingMonitor

from .checkpointing import CheckpointManager
from .data_manager import DataResolver, DatasetPreparer
from .environment import TrainingEnvironment
from .tokenizer_manager import TokenizerManager
from .memory_hooks import MemoryHooks
from .regression_suite import RegressionSuite
from .training_loop import TrainingControl, TrainingLoopRunner


class MiniGPTTrainer:
    """MiniGPTè®­ç»ƒå™¨ï¼Œå°è£…æ•°æ®ã€æ¨¡å‹ä¸è®­ç»ƒæµç¨‹ã€‚"""

    def __init__(self, config, mode: str = "pretrain"):
        self.config = config
        self.mode = mode
        self.environment = TrainingEnvironment(config, mode)
        self.device = self.environment.device
        self.output_dir = self.environment.output_dir
        self.control = TrainingControl()

        self.resolver = DataResolver(config, mode)
        self.tokenizer_manager = TokenizerManager(config, mode, self.output_dir, self.resolver)
        self.checkpoints = CheckpointManager(config, mode, self.output_dir, self.device)
        self.memory_hooks = MemoryHooks(config, self.device)
        self.regression_suite = RegressionSuite(config, self.output_dir, self.device)

        print(f"=== MiniGPT {mode.upper()} è®­ç»ƒ ===")
        print(f"æ¨¡å‹é…ç½®: {config.model_size}")
        print(f"è®¾å¤‡: {self.device}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")

    # ------------------------------------------------------------------
    def setup_tokenizer(self, retrain: bool = False):
        return self.tokenizer_manager.setup(retrain=retrain)

    def setup_data_loader(self, tokenizer):
        preparer = DatasetPreparer(
            self.config,
            self.mode,
            tokenizer,
            self.resolver,
            seed=getattr(self.config, "random_seed", 42),
        )
        train_loader, val_loader, stats = preparer.build()
        self.environment.dataset_stats = stats
        self.environment.persist_dataset_stats()
        return train_loader, val_loader

    def setup_model(self, tokenizer):
        print("ğŸ§  åˆ›å»ºæ¨¡å‹...")
        model = create_model(vocab_size=tokenizer.vocab_size, model_size=self.config.model_size)
        model = model.to(self.device)

        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"æ€»å‚æ•°é‡: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        return model

    # ------------------------------------------------------------------
    def _build_scheduler(self, optimizer, start_step: int = 0):
        warmup_steps = self.config.warmup_steps
        max_steps = self.config.max_steps

        def lr_lambda(current_step: int):
            if current_step < warmup_steps:
                return float(current_step) / float(max(1, warmup_steps))
            progress = float(current_step - warmup_steps) / float(max(1, max_steps - warmup_steps))
            return max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))

        return torch.optim.lr_scheduler.LambdaLR(
            optimizer,
            lr_lambda,
            last_epoch=start_step - 1 if start_step > 0 else -1,
        )

    # ------------------------------------------------------------------
    def train(
        self,
        resume_from: Optional[str] = None,
        auto_resume: bool = False,
        retrain_tokenizer: bool = False,
    ):
        print(f"ğŸš€ å¼€å§‹{self.mode}è®­ç»ƒ...")
        signal.signal(signal.SIGINT, self._signal_handler)
        print("ğŸ’¡ æŒ‰ Ctrl+C å¯ä¼˜é›…åœ°åœæ­¢è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹")

        start_time = time.time()
        tokenizer = self.setup_tokenizer(retrain=retrain_tokenizer)
        train_loader, val_loader = self.setup_data_loader(tokenizer)
        model = self.setup_model(tokenizer)

        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
            betas=(self.config.beta1, self.config.beta2),
            eps=self.config.eps,
        )

        criterion = torch.nn.CrossEntropyLoss(
            ignore_index=tokenizer.pad_id,
            label_smoothing=getattr(self.config, "label_smoothing", 0.0),
        )

        scaler = None
        if self.config.mixed_precision and self.device == "cuda":
            scaler = torch.cuda.amp.GradScaler()
            print("âœ… å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (FP16)")

        if self.config.gradient_checkpointing and hasattr(model, "gradient_checkpointing_enable"):
            model.gradient_checkpointing_enable()
            print("âœ… å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")

        start_step, _ = self.checkpoints.resume(
            model,
            optimizer,
            resume_from=resume_from,
            auto_resume=auto_resume,
        )

        # Set initial_lr for scheduler (required when resuming from checkpoint)
        for param_group in optimizer.param_groups:
            if 'initial_lr' not in param_group:
                param_group['initial_lr'] = param_group['lr']

        scheduler = self._build_scheduler(optimizer, start_step=start_step)
        self._log_scheduler_state(optimizer, start_step)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        tensorboard_dir = os.path.join(
            self.config.tensorboard_dir,
            f"{self.mode}_{self.config.model_size}_{timestamp}",
        )
        monitor = TrainingMonitor(
            model=model,
            log_dir=tensorboard_dir,
            enable_tensorboard=self.config.enable_tensorboard,
            enable_real_time_plots=False,
            lightweight_mode=True,
            log_interval=10,
        )

        if self.device == "cuda":
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
            print("ğŸ§¹ GPUç¼“å­˜å·²æ¸…ç†")

        runner = TrainingLoopRunner(self.config, self.device, self.checkpoints, self.mode)
        final_path = runner.run(
            model,
            tokenizer,
            train_loader,
            val_loader,
            optimizer,
            scheduler,
            criterion,
            scaler,
            monitor,
            self.control,
            start_step,
            start_time,
            memory_hooks=self.memory_hooks,
            regression_suite=self.regression_suite,
        )

        monitor.close()
        print(f"ğŸ“Š TensorBoardæ—¥å¿—: {tensorboard_dir}")
        print(f"ğŸ’¡ æŸ¥çœ‹è®­ç»ƒè¿‡ç¨‹: tensorboard --logdir={tensorboard_dir}")
        return final_path

    # ------------------------------------------------------------------
    def _log_scheduler_state(self, optimizer, start_step: int) -> None:
        if start_step > 0:
            current_lr = optimizer.param_groups[0]["lr"]
            if start_step >= self.config.warmup_steps:
                phase = "Cosine Decay"
                progress = (start_step - self.config.warmup_steps) / (
                    self.config.max_steps - self.config.warmup_steps
                ) * 100
            else:
                phase = "Warmup"
                progress = start_step / self.config.warmup_steps * 100
            print(f"ğŸ“Š å­¦ä¹ ç‡è°ƒåº¦å™¨å·²æ¢å¤åˆ°ç¬¬ {start_step} æ­¥")
            print(f"   å½“å‰é˜¶æ®µ: {phase} (å·²å®Œæˆ{progress:.1f}%)")
            print(f"   å½“å‰å­¦ä¹ ç‡: {current_lr:.2e}")
        else:
            warmup_ratio = self.config.warmup_steps / self.config.max_steps * 100
            print(f"âœ… å­¦ä¹ ç‡è°ƒåº¦å™¨: Warmup({self.config.warmup_steps}æ­¥, {warmup_ratio:.1f}%) + Cosine Decay")
            print(
                f"   åˆå§‹LR: 0 -> å³°å€¼LR: {self.config.learning_rate:.2e} -> "
                f"æœ€ä½LR: {self.config.learning_rate * 0.1:.2e}"
            )

    def _signal_handler(self, signum, frame):
        print("\n\nâš ï¸  æ”¶åˆ°ä¸­æ–­ä¿¡å· (Ctrl+C)")
        if not self.control.interrupted:
            self.control.interrupted = True
            print("ğŸ”„ æ­£åœ¨ä¼˜é›…åœ°åœæ­¢è®­ç»ƒ...")
            print("ğŸ’¾ å°†ä¿å­˜å½“å‰æ¨¡å‹çŠ¶æ€...")
            print("   (å†æ¬¡æŒ‰ Ctrl+C å¯å¼ºåˆ¶é€€å‡º)")
        else:
            print("âš¡ å¼ºåˆ¶é€€å‡ºï¼")
            sys.exit(1)
