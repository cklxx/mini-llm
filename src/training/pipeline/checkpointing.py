"""Checkpoint helpers for the training pipeline."""
from __future__ import annotations

import glob
import os
from typing import Optional, Tuple

import torch


class CheckpointManager:
    """Persist and restore checkpoints with cleanup helpers."""

    def __init__(self, config, mode: str, output_dir: str, device: str):
        self.config = config
        self.mode = mode
        self.output_dir = output_dir
        self.device = device

    # ------------------------------------------------------------------
    def find_latest(self) -> Optional[str]:
        pattern = os.path.join(self.output_dir, "checkpoint_step_*.pt")
        checkpoints = glob.glob(pattern)
        if not checkpoints:
            return None
        checkpoints.sort(key=os.path.getmtime, reverse=True)
        return checkpoints[0]

    def find_pretrain_source(self) -> Optional[str]:
        pretrain_dir = os.path.join(self.config.checkpoint_dir, f"pretrain_{self.config.model_size}")
        search_patterns = [
            os.path.join(pretrain_dir, "checkpoint_step_*.pt"),
            os.path.join(pretrain_dir, "final_model.pt"),
        ]
        for pattern in search_patterns:
            matches = glob.glob(pattern)
            if not matches:
                continue
            matches.sort(key=os.path.getmtime, reverse=True)
            print(f"   âœ… æ‰¾åˆ° pretrain checkpoint: {matches[0]}")
            return matches[0]
        return None

    # ------------------------------------------------------------------
    def resume(
        self,
        model,
        optimizer,
        *,
        resume_from: Optional[str] = None,
        auto_resume: bool = False,
    ) -> Tuple[int, bool]:
        start_step = 0
        checkpoint_loaded = False

        if auto_resume:
            latest_checkpoint = self.find_latest()
            if latest_checkpoint:
                print(f"ðŸ” æ‰¾åˆ°checkpoint: {latest_checkpoint}")
                start_step = self._load_checkpoint(latest_checkpoint, model, optimizer)
                checkpoint_loaded = True
            else:
                print("â„¹ï¸  æœªæ‰¾åˆ°å½“å‰æ¨¡å¼çš„checkpoint")
        elif resume_from:
            if os.path.exists(resume_from):
                start_step = self._load_checkpoint(resume_from, model, optimizer)
                checkpoint_loaded = True
            else:
                print(f"âš ï¸  Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {resume_from}")

        if not checkpoint_loaded and self.mode in {"sft", "dpo", "rlhf"}:
            pretrain_model_path = self.find_pretrain_source()
            if pretrain_model_path:
                print(f"\nðŸŽ¯ {self.mode.upper()} æ¨¡å¼ï¼šä»Ž pretrain checkpoint åŠ è½½åˆå§‹æƒé‡")
                try:
                    self._load_model_weights(pretrain_model_path, model)
                    checkpoint_loaded = True
                    print("âœ… æˆåŠŸåŠ è½½ pretrain æ¨¡åž‹æƒé‡")
                except Exception as exc:
                    print(f"âš ï¸  åŠ è½½ pretrain æƒé‡å¤±è´¥: {exc}")
                    print("   å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡åž‹")
            else:
                print(f"\nâš ï¸  æœªæ‰¾åˆ° pretrain æ¨¡åž‹: {os.path.join(self.config.checkpoint_dir, f'pretrain_{self.config.model_size}')}")
                print(
                    f"   å»ºè®®å…ˆè¿è¡Œ pretrain æ¨¡å¼è®­ç»ƒåŸºç¡€æ¨¡åž‹ï¼š\n"
                    f"   uv run python scripts/train.py --mode pretrain --config {self.config.model_size}"
                )
                print(f"   çŽ°åœ¨å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡åž‹è¿›è¡Œ {self.mode} è®­ç»ƒ")
        elif not checkpoint_loaded and self.mode == "pretrain":
            print("\nðŸ“š Pretrain æ¨¡å¼ï¼šä»Žéšæœºåˆå§‹åŒ–å¼€å§‹è®­ç»ƒ")

        return start_step, checkpoint_loaded

    # ------------------------------------------------------------------
    def load(self, path: str, model, optimizer) -> int:
        return self._load_checkpoint(path, model, optimizer)

    def save(self, model, optimizer, step: int, loss: float) -> str:
        self._remove_old_checkpoints()
        checkpoint_path = os.path.join(self.output_dir, f"checkpoint_step_{step}.pt")
        torch.save(
            {
                "step": step,
                "model_state_dict": model.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "loss": loss,
                "config": self.config,
                "mode": self.mode,
            },
            checkpoint_path,
        )
        print(f"ðŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
        return checkpoint_path

    def save_final(self, model, tokenizer, step: int) -> str:
        final_path = os.path.join(self.output_dir, "final_model.pt")
        torch.save(
            {
                "model_state_dict": model.state_dict(),
                "tokenizer_vocab_size": tokenizer.vocab_size,
                "config": self.config,
                "mode": self.mode,
                "step": step,
            },
            final_path,
        )
        return final_path

    # ------------------------------------------------------------------
    def _remove_old_checkpoints(self) -> None:
        pattern = os.path.join(self.output_dir, "checkpoint_step_*.pt")
        for old_ckpt in glob.glob(pattern):
            try:
                os.remove(old_ckpt)
                print(f"ðŸ—‘ï¸  åˆ é™¤æ—§checkpoint: {os.path.basename(old_ckpt)}")
            except Exception as exc:
                print(f"âš ï¸  åˆ é™¤æ—§checkpointå¤±è´¥: {exc}")

    def _load_checkpoint(self, checkpoint_path, model, optimizer) -> int:
        print(f"ðŸ”„ æ­£åœ¨åŠ è½½checkpoint: {checkpoint_path}")
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
            if "model_state_dict" in checkpoint:
                model.load_state_dict(checkpoint["model_state_dict"])
                print("âœ… æ¨¡åž‹æƒé‡å·²åŠ è½½")
            else:
                model.load_state_dict(checkpoint)
                print("âœ… æ¨¡åž‹æƒé‡å·²åŠ è½½ï¼ˆæ—§æ ¼å¼ï¼‰")
            if "optimizer_state_dict" in checkpoint and optimizer is not None:
                try:
                    optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
                    # Set initial_lr for each param group (required for LR schedulers when resuming)
                    for group in optimizer.param_groups:
                        if 'initial_lr' not in group:
                            group['initial_lr'] = group['lr']
                    print("âœ… ä¼˜åŒ–å™¨çŠ¶æ€å·²åŠ è½½")
                except Exception as exc:
                    print(f"âš ï¸  ä¼˜åŒ–å™¨çŠ¶æ€åŠ è½½å¤±è´¥: {exc}")
                    print("   å°†ä½¿ç”¨æ–°çš„ä¼˜åŒ–å™¨çŠ¶æ€")
            start_step = checkpoint.get("step", checkpoint.get("global_step", 0))
            if start_step > 0:
                print(f"âœ… å°†ä»Žç¬¬ {start_step} æ­¥ç»§ç»­è®­ç»ƒ")
            if "loss" in checkpoint:
                print(f"ðŸ“Š CheckpointæŸå¤±: {checkpoint['loss']:.4f}")
            if "mode" in checkpoint:
                print(f"ðŸ“ è®­ç»ƒæ¨¡å¼: {checkpoint['mode']}")
            if "config" in checkpoint:
                print("â„¹ï¸  CheckpointåŒ…å«è®­ç»ƒé…ç½®ï¼Œå¯ç”¨äºŽæ¢å¤å®žéªŒçŽ¯å¢ƒ")
            else:
                print("â„¹ï¸  Checkpointä¸åŒ…å«è®­ç»ƒé…ç½®ï¼Œè¯·ç¡®ä¿å½“å‰é…ç½®ä¸ŽåŽŸè®­ç»ƒä¸€è‡´")
            return start_step
        except Exception as exc:
            print(f"âŒ åŠ è½½checkpointå¤±è´¥: {exc}")
            print("âš ï¸  å°†ä»Žå¤´å¼€å§‹è®­ç»ƒ")
            return 0

    def _load_model_weights(self, checkpoint_path: str, model) -> None:
        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
        if "model_state_dict" in checkpoint:
            model.load_state_dict(checkpoint["model_state_dict"])
        else:
            model.load_state_dict(checkpoint)
