"""
ç»¼åˆè®­ç»ƒç›‘æ§å’Œå¯è§†åŒ–ç³»ç»Ÿ
æä¾›å®æ—¶è®­ç»ƒæŒ‡æ ‡ç›‘æ§ã€æ€§èƒ½åˆ†æã€å¼‚å¸¸æ£€æµ‹å’Œå¯è§†åŒ–ä»ªè¡¨æ¿
"""
import os
import time
import json
import psutil
import threading
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from matplotlib.figure import Figure
import seaborn as sns

import torch
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter


@dataclass
class TrainingMetrics:
    """è®­ç»ƒæŒ‡æ ‡æ•°æ®ç»“æ„"""
    step: int
    epoch: int
    loss: float
    learning_rate: float
    grad_norm: float
    param_norm: float
    timestamp: float

    # æ€§èƒ½æŒ‡æ ‡
    samples_per_sec: float = 0.0
    gpu_memory_used: float = 0.0
    cpu_usage: float = 0.0
    ram_usage: float = 0.0

    # æ¨¡å‹å¥åº·æŒ‡æ ‡
    weight_update_ratio: float = 0.0
    activation_mean: float = 0.0
    activation_std: float = 0.0

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


class SystemMonitor:
    """ç³»ç»Ÿæ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available()
                                 else 'mps' if torch.backends.mps.is_available()
                                 else 'cpu')

    def get_gpu_memory_info(self) -> Tuple[float, float]:
        """è·å–GPUå†…å­˜ä¿¡æ¯ (used_gb, total_gb)"""
        if self.device.type == 'cuda':
            return (
                torch.cuda.memory_allocated() / 1024**3,
                torch.cuda.max_memory_allocated() / 1024**3
            )
        elif self.device.type == 'mps':
            # MPSå†…å­˜ç›‘æ§ï¼ˆè¿‘ä¼¼ï¼‰
            return (psutil.virtual_memory().used / 1024**3 * 0.3,
                   psutil.virtual_memory().total / 1024**3 * 0.3)
        else:
            return 0.0, 0.0

    def get_cpu_info(self) -> Tuple[float, float]:
        """è·å–CPUä¿¡æ¯ (usage_percent, frequency_ghz)"""
        return (
            psutil.cpu_percent(interval=0.1),
            psutil.cpu_freq().current / 1000 if psutil.cpu_freq() else 0.0
        )

    def get_memory_info(self) -> Tuple[float, float]:
        """è·å–RAMä¿¡æ¯ (used_gb, total_gb)"""
        mem = psutil.virtual_memory()
        return (mem.used / 1024**3, mem.total / 1024**3)

    def get_disk_info(self) -> Tuple[float, float]:
        """è·å–ç£ç›˜ä¿¡æ¯ (used_gb, total_gb)"""
        disk = psutil.disk_usage('/')
        return (disk.used / 1024**3, disk.total / 1024**3)


class ModelHealthMonitor:
    """æ¨¡å‹å¥åº·ç›‘æ§å™¨"""

    def __init__(self, model: nn.Module, lightweight_mode: bool = False):
        self.model = model
        self.lightweight_mode = lightweight_mode
        self.prev_params = None
        self.grad_history = deque(maxlen=100)
        self.param_history = deque(maxlen=100)

    def compute_gradient_norm(self) -> float:
        """è®¡ç®—æ¢¯åº¦èŒƒæ•°"""
        total_norm = 0.0
        param_count = 0

        for p in self.model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
                param_count += 1

        return np.sqrt(total_norm) if param_count > 0 else 0.0

    def compute_parameter_norm(self) -> float:
        """è®¡ç®—å‚æ•°èŒƒæ•°"""
        total_norm = 0.0

        for p in self.model.parameters():
            param_norm = p.data.norm(2)
            total_norm += param_norm.item() ** 2

        return np.sqrt(total_norm)

    def compute_weight_update_ratio(self) -> float:
        """è®¡ç®—æƒé‡æ›´æ–°æ¯”ä¾‹"""
        # è½»é‡çº§æ¨¡å¼ä¸‹è·³è¿‡è¿™ä¸ªè€—æ—¶æ“ä½œ
        if self.lightweight_mode:
            return 0.0
            
        if self.prev_params is None:
            self.prev_params = {name: param.clone() for name, param in self.model.named_parameters()}
            return 0.0

        update_norms = []
        param_norms = []

        for name, param in self.model.named_parameters():
            if name in self.prev_params:
                update = param.data - self.prev_params[name]
                update_norm = update.norm().item()
                param_norm = param.data.norm().item()

                if param_norm > 0:
                    update_norms.append(update_norm)
                    param_norms.append(param_norm)

                # æ›´æ–°å†å²å‚æ•°
                self.prev_params[name] = param.clone()

        if len(update_norms) > 0:
            return np.mean(update_norms) / np.mean(param_norms)
        else:
            return 0.0

    def detect_gradient_anomaly(self, grad_norm: float, step: int = 0) -> Dict[str, Any]:
        """æ£€æµ‹æ¢¯åº¦å¼‚å¸¸

        Args:
            grad_norm: å½“å‰æ¢¯åº¦èŒƒæ•°
            step: å½“å‰è®­ç»ƒæ­¥æ•°ï¼ˆç”¨äºåŠ¨æ€è°ƒæ•´é˜ˆå€¼ï¼‰
        """
        self.grad_history.append(grad_norm)

        if len(self.grad_history) < 10:
            return {'status': 'normal', 'reason': 'insufficient_data'}

        recent_grads = list(self.grad_history)[-10:]
        mean_grad = np.mean(recent_grads)
        std_grad = np.std(recent_grads)

        anomaly_info = {'status': 'normal', 'mean': mean_grad, 'std': std_grad}

        # æ¢¯åº¦çˆ†ç‚¸æ£€æµ‹
        if grad_norm > mean_grad + 3 * std_grad and grad_norm > 10.0:
            anomaly_info.update({
                'status': 'gradient_explosion',
                'current': grad_norm,
                'threshold': mean_grad + 3 * std_grad
            })

        # æ¢¯åº¦æ¶ˆå¤±æ£€æµ‹ - åŠ¨æ€é˜ˆå€¼ï¼Œè®­ç»ƒåˆæœŸæ›´å®½æ¾
        # Step 100ä¹‹å‰: 1e-10 (å‡ ä¹ä¸è§¦å‘)
        # Step 100-1000: 1e-6 (å®½æ¾ï¼Œé€‚åº”é˜¶æ®µ)
        # Step 1000ä¹‹å: 1e-7 (æ­£å¸¸è®­ç»ƒä¸­åˆç†çš„æ¢¯åº¦èŒƒå›´)
        if step < 100:
            vanishing_threshold = 1e-10
        elif step < 1000:
            vanishing_threshold = 1e-6
        else:
            vanishing_threshold = 1e-7
            
        if grad_norm < vanishing_threshold:
            anomaly_info.update({
                'status': 'gradient_vanishing',
                'current': grad_norm,
                'threshold': vanishing_threshold
            })

        return anomaly_info

    def get_activation_stats(self, activations: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """è·å–æ¿€æ´»å€¼ç»Ÿè®¡"""
        stats = {}

        for name, tensor in activations.items():
            if tensor.numel() > 0:
                stats[f'{name}_mean'] = tensor.mean().item()
                stats[f'{name}_std'] = tensor.std().item()
                stats[f'{name}_max'] = tensor.max().item()
                stats[f'{name}_min'] = tensor.min().item()

        return stats


class RealTimeVisualizer:
    """å®æ—¶å¯è§†åŒ–å™¨"""

    def __init__(self, save_dir: str = "training_plots", max_points: int = 1000):
        self.save_dir = save_dir
        self.max_points = max_points
        os.makedirs(save_dir, exist_ok=True)

        # æ•°æ®å­˜å‚¨
        self.metrics_history = deque(maxlen=max_points)

        # è®¾ç½®ç»˜å›¾æ ·å¼
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")

        # åˆ›å»ºå›¾å½¢
        self.fig = None
        self.axes = None
        self.animation = None

    def start_real_time_plot(self):
        """å¯åŠ¨å®æ—¶ç»˜å›¾"""
        self.fig, self.axes = plt.subplots(2, 3, figsize=(15, 10))
        self.fig.suptitle('Training Monitor Dashboard', fontsize=16)

        # è®¾ç½®å­å›¾æ ‡é¢˜
        titles = [
            'Training Loss', 'Learning Rate', 'Gradient Norm',
            'GPU Memory Usage', 'CPU Usage', 'Training Speed'
        ]

        for ax, title in zip(self.axes.flat, titles):
            ax.set_title(title)
            ax.grid(True, alpha=0.3)

        # å¯åŠ¨åŠ¨ç”»
        self.animation = animation.FuncAnimation(
            self.fig, self._update_plots, interval=1000, blit=False
        )

        plt.tight_layout()
        return self.fig

    def _update_plots(self, frame):
        """æ›´æ–°ç»˜å›¾"""
        if len(self.metrics_history) < 2:
            return

        # æå–æ•°æ®
        steps = [m.step for m in self.metrics_history]
        losses = [m.loss for m in self.metrics_history]
        lrs = [m.learning_rate for m in self.metrics_history]
        grad_norms = [m.grad_norm for m in self.metrics_history]
        gpu_memory = [m.gpu_memory_used for m in self.metrics_history]
        cpu_usage = [m.cpu_usage for m in self.metrics_history]
        speeds = [m.samples_per_sec for m in self.metrics_history]

        # æ¸…é™¤å¹¶é‡ç»˜
        for ax in self.axes.flat:
            ax.clear()

        # ç»˜åˆ¶å„ä¸ªæŒ‡æ ‡
        plots_data = [
            (losses, 'Loss', 'red'),
            (lrs, 'Learning Rate', 'blue'),
            (grad_norms, 'Gradient Norm', 'green'),
            (gpu_memory, 'GPU Memory (GB)', 'orange'),
            (cpu_usage, 'CPU Usage (%)', 'purple'),
            (speeds, 'Samples/sec', 'brown')
        ]

        for ax, (data, ylabel, color) in zip(self.axes.flat, plots_data):
            if len(data) > 1:
                ax.plot(steps, data, color=color, linewidth=2)
                ax.set_ylabel(ylabel)
                ax.set_xlabel('Step')
                ax.grid(True, alpha=0.3)

                # æ·»åŠ æœ€æ–°å€¼æ ‡æ³¨
                if data:
                    ax.annotate(f'{data[-1]:.3f}',
                              xy=(steps[-1], data[-1]),
                              xytext=(10, 10), textcoords='offset points',
                              bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.3))

        plt.tight_layout()

    def add_metrics(self, metrics: TrainingMetrics):
        """æ·»åŠ æ–°çš„æŒ‡æ ‡æ•°æ®"""
        self.metrics_history.append(metrics)

    def save_plots(self, prefix: str = "training_plots"):
        """ä¿å­˜å›¾ç‰‡"""
        if self.fig is not None:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filepath = os.path.join(self.save_dir, f"{prefix}_{timestamp}.png")
            self.fig.savefig(filepath, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š Plots saved to: {filepath}")

    def generate_summary_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆè®­ç»ƒæ€»ç»“æŠ¥å‘Š"""
        if len(self.metrics_history) < 10:
            return {}

        metrics_list = list(self.metrics_history)

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        losses = [m.loss for m in metrics_list]
        speeds = [m.samples_per_sec for m in metrics_list if m.samples_per_sec > 0]
        grad_norms = [m.grad_norm for m in metrics_list]

        report = {
            'training_summary': {
                'total_steps': len(metrics_list),
                'training_time_hours': (metrics_list[-1].timestamp - metrics_list[0].timestamp) / 3600,
                'final_loss': losses[-1],
                'min_loss': min(losses),
                'loss_reduction': (losses[0] - losses[-1]) / losses[0] * 100,
            },
            'performance_summary': {
                'avg_speed_samples_per_sec': np.mean(speeds) if speeds else 0,
                'max_speed_samples_per_sec': max(speeds) if speeds else 0,
                'avg_gradient_norm': np.mean(grad_norms),
                'max_gradient_norm': max(grad_norms),
            },
            'system_summary': {
                'avg_gpu_memory_gb': np.mean([m.gpu_memory_used for m in metrics_list]),
                'max_gpu_memory_gb': max([m.gpu_memory_used for m in metrics_list]),
                'avg_cpu_usage_percent': np.mean([m.cpu_usage for m in metrics_list]),
            }
        }

        return report


class TrainingMonitor:
    """ç»¼åˆè®­ç»ƒç›‘æ§å™¨"""

    def __init__(self, model: nn.Module, log_dir: str = "training_logs",
                 enable_tensorboard: bool = True, enable_real_time_plots: bool = False,
                 lightweight_mode: bool = False, log_interval: int = 1):
        self.model = model
        self.log_dir = log_dir
        os.makedirs(log_dir, exist_ok=True)

        # è½»é‡çº§æ¨¡å¼é…ç½®
        self.lightweight_mode = lightweight_mode
        self.log_interval = log_interval if lightweight_mode else 1
        
        # åˆå§‹åŒ–å„ä¸ªç›‘æ§ç»„ä»¶
        self.system_monitor = SystemMonitor()
        self.health_monitor = ModelHealthMonitor(model, lightweight_mode=lightweight_mode)
        self.visualizer = RealTimeVisualizer(os.path.join(log_dir, "plots"))

        # TensorBoard - æ”¯æŒè‡ªå®šä¹‰flushé—´éš”
        if enable_tensorboard:
            flush_secs = 30
            if hasattr(model, 'config'):
                flush_secs = getattr(model.config, 'tensorboard_flush_secs', 30)
            self.tensorboard_writer = SummaryWriter(log_dir, flush_secs=flush_secs)
        else:
            self.tensorboard_writer = None

        # å®æ—¶ç»˜å›¾ï¼ˆè½»é‡çº§æ¨¡å¼ä¸‹ç¦ç”¨ï¼‰
        self.enable_real_time_plots = enable_real_time_plots and not lightweight_mode
        if self.enable_real_time_plots:
            self.plot_thread = None
            self._start_real_time_plotting()

        # æ€§èƒ½ç»Ÿè®¡
        self.step_start_time = time.time()
        self.samples_processed = 0

        # å¼‚å¸¸æ£€æµ‹å†å²
        self.anomaly_history = []

        print(f"ğŸ” TrainingMonitor initialized:")
        print(f"   Log directory: {log_dir}")
        print(f"   TensorBoard: {'enabled' if enable_tensorboard else 'disabled'}")
        print(f"   Real-time plots: {'enabled' if self.enable_real_time_plots else 'disabled'}")
        print(f"   Lightweight mode: {'enabled' if lightweight_mode else 'disabled'}")
        if lightweight_mode:
            print(f"   Log interval: every {log_interval} steps")

    def _start_real_time_plotting(self):
        """å¯åŠ¨å®æ—¶ç»˜å›¾çº¿ç¨‹"""
        if self.enable_real_time_plots:
            def plot_worker():
                fig = self.visualizer.start_real_time_plot()
                plt.show()

            self.plot_thread = threading.Thread(target=plot_worker, daemon=True)
            self.plot_thread.start()

    def log_step(self, step: int, epoch: int, loss: float, learning_rate: float,
                batch_size: int = 1) -> Optional[TrainingMetrics]:
        """è®°å½•è®­ç»ƒæ­¥éª¤"""
        # è½»é‡çº§æ¨¡å¼ä¸‹ï¼Œåªåœ¨æŒ‡å®šé—´éš”è®°å½•è¯¦ç»†æŒ‡æ ‡
        should_log_full = (step % self.log_interval == 0)
        
        current_time = time.time()

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        step_duration = current_time - self.step_start_time
        samples_per_sec = batch_size / step_duration if step_duration > 0 else 0

        # è·å–ç³»ç»Ÿä¿¡æ¯ï¼ˆè½»é‡çº§ï¼šé™ä½é¢‘ç‡ï¼‰
        if should_log_full:
            gpu_memory_used, _ = self.system_monitor.get_gpu_memory_info()
            cpu_usage, _ = self.system_monitor.get_cpu_info()
            ram_used, _ = self.system_monitor.get_memory_info()
        else:
            gpu_memory_used = cpu_usage = ram_used = 0.0

        # è·å–æ¨¡å‹å¥åº·æŒ‡æ ‡
        grad_norm = self.health_monitor.compute_gradient_norm()
        param_norm = self.health_monitor.compute_parameter_norm()
        
        # æƒé‡æ›´æ–°æ¯”ä¾‹ï¼ˆæœ€è€—æ—¶ï¼Œè½»é‡çº§æ¨¡å¼ä¸‹è·³è¿‡ï¼‰
        if should_log_full:
            weight_update_ratio = self.health_monitor.compute_weight_update_ratio()
        else:
            weight_update_ratio = 0.0

        # æ£€æµ‹å¼‚å¸¸ï¼ˆå§‹ç»ˆæ£€æŸ¥ï¼Œå› ä¸ºå¾ˆé‡è¦ï¼‰
        anomaly_info = self.health_monitor.detect_gradient_anomaly(grad_norm, step)
        if anomaly_info['status'] != 'normal':
            self.anomaly_history.append({
                'step': step,
                'timestamp': current_time,
                'anomaly': anomaly_info
            })
            print(f"âš ï¸  Anomaly detected at step {step}: {anomaly_info['status']}")

        # åˆ›å»ºæŒ‡æ ‡å¯¹è±¡
        metrics = TrainingMetrics(
            step=step,
            epoch=epoch,
            loss=loss,
            learning_rate=learning_rate,
            grad_norm=grad_norm,
            param_norm=param_norm,
            timestamp=current_time,
            samples_per_sec=samples_per_sec,
            gpu_memory_used=gpu_memory_used,
            cpu_usage=cpu_usage,
            ram_usage=ram_used,
            weight_update_ratio=weight_update_ratio
        )

        # è®°å½•åˆ°å„ä¸ªç³»ç»Ÿï¼ˆåªåœ¨å®Œæ•´è®°å½•æ—¶å†™å…¥è¯¦ç»†ä¿¡æ¯ï¼‰
        if should_log_full:
            self._log_to_tensorboard(metrics)
            self._log_to_console(metrics, step % 100 == 0)  # æ¯100æ­¥æ‰“å°ä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
            self.visualizer.add_metrics(metrics)
        else:
            # è½»é‡çº§ï¼šåªè®°å½•å…³é”®æŒ‡æ ‡åˆ° TensorBoard
            if self.tensorboard_writer:
                self.tensorboard_writer.add_scalar('Training/Loss', loss, step)
                self.tensorboard_writer.add_scalar('Training/LearningRate', learning_rate, step)

        # é‡ç½®è®¡æ—¶å™¨
        self.step_start_time = current_time

        return metrics if should_log_full else None

    def _log_to_tensorboard(self, metrics: TrainingMetrics):
        """è®°å½•åˆ°TensorBoard"""
        if self.tensorboard_writer is None:
            return

        writer = self.tensorboard_writer
        step = metrics.step

        # è®­ç»ƒæŒ‡æ ‡
        writer.add_scalar('Training/Loss', metrics.loss, step)
        writer.add_scalar('Training/LearningRate', metrics.learning_rate, step)
        writer.add_scalar('Training/GradientNorm', metrics.grad_norm, step)
        writer.add_scalar('Training/ParameterNorm', metrics.param_norm, step)
        writer.add_scalar('Training/WeightUpdateRatio', metrics.weight_update_ratio, step)

        # æ€§èƒ½æŒ‡æ ‡
        writer.add_scalar('Performance/SamplesPerSec', metrics.samples_per_sec, step)
        writer.add_scalar('Performance/GPUMemoryGB', metrics.gpu_memory_used, step)
        writer.add_scalar('Performance/CPUUsagePercent', metrics.cpu_usage, step)
        writer.add_scalar('Performance/RAMUsageGB', metrics.ram_usage, step)

    def _log_to_console(self, metrics: TrainingMetrics, verbose: bool = False):
        """è®°å½•åˆ°æ§åˆ¶å°"""
        if verbose:
            print(f"\nğŸ“Š Step {metrics.step} (Epoch {metrics.epoch}) Metrics:")
            print(f"   Loss: {metrics.loss:.4f}")
            print(f"   LR: {metrics.learning_rate:.2e}")
            print(f"   Grad Norm: {metrics.grad_norm:.4f}")
            print(f"   Speed: {metrics.samples_per_sec:.1f} samples/sec")
            print(f"   GPU Memory: {metrics.gpu_memory_used:.1f}GB")
            print(f"   CPU: {metrics.cpu_usage:.1f}%")

            if metrics.weight_update_ratio > 0:
                print(f"   Weight Update Ratio: {metrics.weight_update_ratio:.2e}")

    def save_training_summary(self):
        """ä¿å­˜è®­ç»ƒæ€»ç»“"""
        summary = self.visualizer.generate_summary_report()

        if summary:
            summary_file = os.path.join(self.log_dir, "training_summary.json")
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)

            print(f"ğŸ“‹ Training summary saved to: {summary_file}")

            # æ‰“å°å…³é”®ç»Ÿè®¡ä¿¡æ¯
            print("\nğŸ“ˆ Training Summary:")
            training_summary = summary.get('training_summary', {})
            performance_summary = summary.get('performance_summary', {})

            print(f"   Total Steps: {training_summary.get('total_steps', 0)}")
            print(f"   Training Time: {training_summary.get('training_time_hours', 0):.2f} hours")
            print(f"   Final Loss: {training_summary.get('final_loss', 0):.4f}")
            print(f"   Loss Reduction: {training_summary.get('loss_reduction', 0):.1f}%")
            print(f"   Avg Speed: {performance_summary.get('avg_speed_samples_per_sec', 0):.1f} samples/sec")

    def close(self):
        """å…³é—­ç›‘æ§å™¨"""
        if self.tensorboard_writer:
            self.tensorboard_writer.close()

        self.visualizer.save_plots()
        self.save_training_summary()

        print("ğŸ” TrainingMonitor closed successfully")


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•æ¨¡å‹
    test_model = nn.Sequential(
        nn.Linear(100, 50),
        nn.ReLU(),
        nn.Linear(50, 10)
    )

    # åˆå§‹åŒ–ç›‘æ§å™¨
    monitor = TrainingMonitor(
        test_model,
        log_dir="test_logs",
        enable_tensorboard=True,
        enable_real_time_plots=False  # åœ¨æµ‹è¯•ä¸­ç¦ç”¨å®æ—¶ç»˜å›¾
    )

    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    print("ğŸ§ª Simulating training process...")

    for step in range(100):
        # æ¨¡æ‹Ÿä¸€æ¬¡å‰å‘ä¼ æ’­
        x = torch.randn(32, 100)
        y = test_model(x)
        loss = y.sum()

        # æ¨¡æ‹Ÿåå‘ä¼ æ’­
        loss.backward()

        # è®°å½•æŒ‡æ ‡
        metrics = monitor.log_step(
            step=step,
            epoch=step // 20,
            loss=loss.item(),
            learning_rate=0.001 * (0.95 ** (step // 10)),
            batch_size=32
        )

        # æ¸…é™¤æ¢¯åº¦
        test_model.zero_grad()

        time.sleep(0.01)  # æ¨¡æ‹Ÿè®­ç»ƒæ—¶é—´

    # å…³é—­ç›‘æ§å™¨
    monitor.close()
    print("âœ… Test completed successfully!")