"""Language modeling dataset utilities."""

from __future__ import annotations

import multiprocessing as mp
import time
from functools import partial
from typing import Sequence

import numpy as np
import torch
from torch.utils.data import Dataset


def _encode_text_worker(text: str, tokenizer, max_length: int) -> np.ndarray:
    """å¤šè¿›ç¨‹workerå‡½æ•°ï¼šç¼–ç å•ä¸ªæ–‡æœ¬"""
    token_ids = tokenizer.encode(text, add_special_tokens=True)
    if len(token_ids) < 2:
        token_ids = [tokenizer.bos_id, tokenizer.eos_id]

    if len(token_ids) > max_length:
        token_ids = token_ids[:max_length]
    else:
        needed_padding = max(0, max_length - len(token_ids))
        if needed_padding:
            token_ids.extend([tokenizer.pad_id] * needed_padding)

    if len(token_ids) < 2:
        token_ids = [
            tokenizer.bos_id,
            tokenizer.eos_id,
            *([tokenizer.pad_id] * max(0, max_length - 2)),
        ]

    return np.asarray(token_ids, dtype=np.int64)


class LanguageModelingDataset(Dataset):
    """Dataset that packs plain text into tokenized language modeling samples."""

    def __init__(
        self,
        texts: Sequence[str],
        tokenizer,
        max_length: int = 512,
        *,
        pretokenize: bool = True,
    ):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.pretokenize = pretokenize

        if pretokenize:
            self._tokens = self._pretokenize_texts(list(texts))
            self.texts: list[str] | None = None
        else:
            self.texts = list(texts)
            self._tokens = None
        total = len(self._tokens) if self._tokens is not None else len(self.texts)
        print(f"ğŸ“Š æ•°æ®é›†åˆå§‹åŒ–: {total} æ¡æ–‡æœ¬, æœ€å¤§é•¿åº¦: {max_length}")

    def __len__(self) -> int:
        if self._tokens is not None:
            return self._tokens.shape[0]
        return len(self.texts) if self.texts is not None else 0

    def __getitem__(self, idx: int) -> torch.Tensor:
        if self._tokens is not None:
            return torch.from_numpy(self._tokens[idx])
        return self._encode_to_tensor(self.texts[idx], idx)

    # ------------------------------------------------------------------
    def _pretokenize_texts(self, texts: list[str]) -> np.ndarray:
        total = len(texts)
        tokens = np.empty((total, self.max_length), dtype=np.int64)
        start_time = time.time()

        # ä½¿ç”¨å¤šè¿›ç¨‹åŠ é€Ÿé¢„ç¼–ç 
        num_workers = min(mp.cpu_count(), 16)  # æœ€å¤šä½¿ç”¨16ä¸ªè¿›ç¨‹

        if total < 1000 or num_workers <= 1:
            # æ•°æ®é‡å°æˆ–å•æ ¸ï¼Œä½¿ç”¨å•çº¿ç¨‹
            for idx, text in enumerate(texts):
                tokens[idx] = self._encode_numpy(text)
                if (idx + 1) % 5000 == 0 or idx == total - 1:
                    elapsed = time.time() - start_time
                    speed = (idx + 1) / elapsed if elapsed > 0 else 0.0
                    print(f"  ğŸ”„ é¢„ç¼–ç  {idx + 1:,}/{total:,} æ ·æœ¬ (é€Ÿåº¦ {speed:.1f}/s)")
        else:
            # æ•°æ®é‡å¤§ï¼Œä½¿ç”¨å¤šè¿›ç¨‹
            print(f"  ğŸš€ ä½¿ç”¨ {num_workers} ä¸ªè¿›ç¨‹å¹¶è¡Œé¢„ç¼–ç ...")

            # åˆ›å»ºç¼–ç å‡½æ•°
            encode_func = partial(
                _encode_text_worker,
                tokenizer=self.tokenizer,
                max_length=self.max_length
            )

            # åˆ†æ‰¹å¤„ç†å¹¶æ˜¾ç¤ºè¿›åº¦
            chunk_size = max(100, total // (num_workers * 10))
            with mp.Pool(processes=num_workers) as pool:
                results = []
                for i, encoded in enumerate(pool.imap(encode_func, texts, chunksize=chunk_size)):
                    tokens[i] = encoded
                    if (i + 1) % 5000 == 0 or i == total - 1:
                        elapsed = time.time() - start_time
                        speed = (i + 1) / elapsed if elapsed > 0 else 0.0
                        print(f"  ğŸ”„ é¢„ç¼–ç  {i + 1:,}/{total:,} æ ·æœ¬ (é€Ÿåº¦ {speed:.1f}/s)")

        elapsed = time.time() - start_time
        avg_speed = total / elapsed if elapsed > 0 else 0.0
        print(f"  âœ… é¢„ç¼–ç å®Œæˆ: {total:,} æ ·æœ¬, è€—æ—¶ {elapsed:.1f}s, å¹³å‡é€Ÿåº¦ {avg_speed:.1f}/s")

        return tokens

    def _encode_numpy(self, text: str) -> np.ndarray:
        token_ids = self.tokenizer.encode(text, add_special_tokens=True)
        if len(token_ids) < 2:
            token_ids = [self.tokenizer.bos_id, self.tokenizer.eos_id]

        if len(token_ids) > self.max_length:
            token_ids = token_ids[: self.max_length]
        else:
            needed_padding = max(0, self.max_length - len(token_ids))
            if needed_padding:
                token_ids.extend([self.tokenizer.pad_id] * needed_padding)

        if len(token_ids) < 2:
            token_ids = [
                self.tokenizer.bos_id,
                self.tokenizer.eos_id,
                *([self.tokenizer.pad_id] * max(0, self.max_length - 2)),
            ]

        return np.asarray(token_ids, dtype=np.int64)

    def _encode_to_tensor(self, text: str, idx: int) -> torch.Tensor:
        try:
            return torch.from_numpy(self._encode_numpy(text))
        except Exception as exc:  # pragma: no cover - defensive logging
            print(f"âŒ å¤„ç†æ•°æ®é¡¹ {idx} æ—¶å‘ç”Ÿé”™è¯¯: {exc}")
            if text:
                print(f"âŒ é”™è¯¯æ–‡æœ¬é•¿åº¦: {len(text)}")
                print(f"âŒ é”™è¯¯æ–‡æœ¬é¢„è§ˆ: {text[:200]}...")
            import traceback

            traceback.print_exc()
            default_tokens = [
                self.tokenizer.bos_id,
                self.tokenizer.eos_id,
                *([self.tokenizer.pad_id] * max(0, self.max_length - 2)),
            ]
            return torch.tensor(default_tokens, dtype=torch.long)
