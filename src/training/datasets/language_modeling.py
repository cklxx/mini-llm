"""Language modeling dataset utilities."""

from __future__ import annotations

import concurrent.futures
import hashlib
import os
import pickle
import time
import tempfile
from multiprocessing import get_context
from typing import Any, Sequence

import numpy as np
import torch
from torch.utils.data import Dataset


_WORKER_TOKENIZER: Any | None = None
_WORKER_MAX_LENGTH: int | None = None


def _init_pretokenize_worker(tokenizer_bytes: bytes, max_length: int) -> None:
    """Initialise worker local tokenizer state."""

    global _WORKER_TOKENIZER, _WORKER_MAX_LENGTH
    _WORKER_TOKENIZER = pickle.loads(tokenizer_bytes)
    _WORKER_MAX_LENGTH = max_length


def _worker_encode(index_and_text: tuple[int, str]) -> tuple[int, np.ndarray]:
    """Encode text inside a worker process."""

    global _WORKER_TOKENIZER, _WORKER_MAX_LENGTH
    if _WORKER_TOKENIZER is None or _WORKER_MAX_LENGTH is None:
        raise RuntimeError("é¢„ç¼–ç workeræœªæ­£ç¡®åˆå§‹åŒ–tokenizerçŠ¶æ€")

    idx, text = index_and_text
    token_ids = _encode_with_tokenizer(_WORKER_TOKENIZER, text, _WORKER_MAX_LENGTH)
    return idx, np.asarray(token_ids, dtype=np.int64)


def _encode_with_tokenizer(tokenizer, text: str, max_length: int) -> list[int]:
    """Encode text using the provided tokenizer and max length constraints."""

    token_ids = tokenizer.encode(text, add_special_tokens=True)
    if len(token_ids) < 2:
        token_ids = [tokenizer.bos_id, tokenizer.eos_id]

    if len(token_ids) > max_length:
        token_ids = token_ids[:max_length]
    else:
        needed_padding = max(0, max_length - len(token_ids))
        if needed_padding:
            token_ids.extend([tokenizer.pad_id] * needed_padding)

    if len(token_ids) < 2:
        token_ids = [
            tokenizer.bos_id,
            tokenizer.eos_id,
            *([tokenizer.pad_id] * max(0, max_length - 2)),
        ]

    return token_ids


class LanguageModelingDataset(Dataset):
    """Dataset that packs plain text into tokenized language modeling samples."""

    def __init__(
        self,
        texts: Sequence[str],
        tokenizer,
        max_length: int = 512,
        *,
        pretokenize: bool = True,
        pretokenize_workers: int | None = None,
    ):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.pretokenize = pretokenize
        self._explicit_worker_count = pretokenize_workers

        if pretokenize:
            text_list = list(texts)
            cache_path = self._build_cache_path(text_list)
            cached_tokens = self._load_cached_tokens(cache_path, len(text_list))
            if cached_tokens is not None:
                self._tokens = cached_tokens
            else:
                self._tokens = self._pretokenize_texts(text_list)
                self._save_tokens_to_cache(cache_path, self._tokens)
            self.texts: list[str] | None = None
        else:
            self.texts = list(texts)
            self._tokens = None
        total = len(self._tokens) if self._tokens is not None else len(self.texts)
        print(f"ğŸ“Š æ•°æ®é›†åˆå§‹åŒ–: {total} æ¡æ–‡æœ¬, æœ€å¤§é•¿åº¦: {max_length}")

    def __len__(self) -> int:
        if self._tokens is not None:
            return self._tokens.shape[0]
        return len(self.texts) if self.texts is not None else 0

    def __getitem__(self, idx: int) -> torch.Tensor:
        if self._tokens is not None:
            return torch.from_numpy(self._tokens[idx])
        return self._encode_to_tensor(self.texts[idx], idx)

    # ------------------------------------------------------------------
    def _resolve_worker_count(self, total: int) -> int:
        if total <= 1:
            return 1

        if self._explicit_worker_count is not None:
            requested = max(1, self._explicit_worker_count)
        else:
            env_value = os.environ.get("MINIGPT_PRETOKENIZE_WORKERS")
            if env_value is not None:
                try:
                    requested = max(1, int(env_value))
                except ValueError:
                    requested = 1
            else:
                requested = min(16, os.cpu_count() or 1)

        return min(requested, total)

    def _pretokenize_texts(self, texts: list[str]) -> np.ndarray:
        total = len(texts)
        tokens = np.empty((total, self.max_length), dtype=np.int64)
        start_time = time.time()

        worker_count = self._resolve_worker_count(total)
        print(
            f"  ğŸ”„ å¼€å§‹é¢„ç¼–ç  {total:,} ä¸ªæ ·æœ¬..."
            + (f" (å¹¶è¡Œ {worker_count} workers)" if worker_count > 1 else "")
        )

        if worker_count <= 1:
            self._pretokenize_texts_single(texts, tokens, start_time)
        else:
            try:
                serialized_tokenizer = pickle.dumps(self.tokenizer)
            except Exception as exc:  # pragma: no cover - defensive fallback
                print(f"  âš ï¸ æ— æ³•åºåˆ—åŒ–tokenizerä»¥è¿›è¡Œå¹¶è¡Œé¢„ç¼–ç : {exc}ï¼Œå›é€€åˆ°å•çº¿ç¨‹æ¨¡å¼ã€‚")
                self._pretokenize_texts_single(texts, tokens, start_time)
            else:
                mp_ctx = get_context("spawn")
                with concurrent.futures.ProcessPoolExecutor(
                    max_workers=worker_count,
                    mp_context=mp_ctx,
                    initializer=_init_pretokenize_worker,
                    initargs=(serialized_tokenizer, self.max_length),
                ) as executor:
                    # ``executor.map`` streams results back in submission order without
                    # building an unbounded future queue.  ``chunksize`` keeps each worker
                    # busy while avoiding the enormous memory spike that ``submit``ing the
                    # entire corpus at once would trigger when datasets contain millions of
                    # samples.
                    chunk_hint = max(1, min(64, total // max(worker_count, 1)))
                    result_iter = executor.map(
                        _worker_encode,
                        enumerate(texts),
                        chunksize=chunk_hint,
                    )
                    for completed, (idx, encoded) in enumerate(result_iter, start=1):
                        tokens[idx] = encoded
                        if completed % 5000 == 0 or completed == total:
                            elapsed = time.time() - start_time
                            speed = completed / elapsed if elapsed > 0 else 0.0
                            eta = (total - completed) / speed if speed > 0 else 0
                            print(
                                f"  ğŸ”„ é¢„ç¼–ç  {completed:,}/{total:,} æ ·æœ¬ (é€Ÿåº¦ {speed:.1f}/s, é¢„è®¡å‰©ä½™ {eta/60:.1f}åˆ†é’Ÿ)"
                            )

        elapsed = time.time() - start_time
        avg_speed = total / elapsed if elapsed > 0 else 0.0
        print(f"  âœ… é¢„ç¼–ç å®Œæˆ: {total:,} æ ·æœ¬, è€—æ—¶ {elapsed:.1f}s, å¹³å‡é€Ÿåº¦ {avg_speed:.1f}/s")

        return tokens

    def _pretokenize_texts_single(
        self, texts: list[str], tokens: np.ndarray, start_time: float
    ) -> np.ndarray:
        total = len(texts)
        for idx, text in enumerate(texts):
            tokens[idx] = self._encode_numpy(text)
            if (idx + 1) % 5000 == 0 or idx == total - 1:
                elapsed = time.time() - start_time
                speed = (idx + 1) / elapsed if elapsed > 0 else 0.0
                eta = (total - idx - 1) / speed if speed > 0 else 0
                print(
                    f"  ğŸ”„ é¢„ç¼–ç  {idx + 1:,}/{total:,} æ ·æœ¬ (é€Ÿåº¦ {speed:.1f}/s, é¢„è®¡å‰©ä½™ {eta/60:.1f}åˆ†é’Ÿ)"
                )
        return tokens

    def _encode_numpy(self, text: str) -> np.ndarray:
        token_ids = _encode_with_tokenizer(self.tokenizer, text, self.max_length)
        return np.asarray(token_ids, dtype=np.int64)

    def _encode_to_tensor(self, text: str, idx: int) -> torch.Tensor:
        try:
            return torch.from_numpy(self._encode_numpy(text))
        except Exception as exc:  # pragma: no cover - defensive logging
            print(f"âŒ å¤„ç†æ•°æ®é¡¹ {idx} æ—¶å‘ç”Ÿé”™è¯¯: {exc}")
            if text:
                print(f"âŒ é”™è¯¯æ–‡æœ¬é•¿åº¦: {len(text)}")
                print(f"âŒ é”™è¯¯æ–‡æœ¬é¢„è§ˆ: {text[:200]}...")
            import traceback

            traceback.print_exc()
            default_tokens = [
                self.tokenizer.bos_id,
                self.tokenizer.eos_id,
                *([self.tokenizer.pad_id] * max(0, self.max_length - 2)),
            ]
            return torch.tensor(default_tokens, dtype=torch.long)

    # ------------------------------------------------------------------
    def _build_cache_path(self, texts: Sequence[str]) -> str | None:
        cache_dir = os.environ.get("MINILLM_CACHE_DIR")
        if not cache_dir:
            home_dir = os.path.expanduser("~")
            if not home_dir or home_dir == "~":
                return None
            cache_dir = os.path.join(home_dir, ".cache", "mini-llm")

        cache_dir = os.path.join(cache_dir, "language_modeling")
        try:
            os.makedirs(cache_dir, exist_ok=True)
        except OSError as exc:  # pragma: no cover - filesystem errors are environment specific
            print(f"  âš ï¸ æ— æ³•åˆ›å»ºç¼“å­˜ç›®å½• {cache_dir}: {exc}ï¼Œè·³è¿‡ç¼“å­˜ã€‚")
            return None

        hasher = hashlib.sha256()
        try:
            tokenizer_bytes = pickle.dumps(self.tokenizer)
        except Exception:  # pragma: no cover - pickle failure is rare
            tokenizer_bytes = repr(self.tokenizer).encode("utf-8", "ignore")
        hasher.update(hashlib.sha256(tokenizer_bytes).digest())
        hasher.update(str(self.max_length).encode("utf-8"))
        hasher.update(len(texts).to_bytes(8, "little"))
        for text in texts:
            encoded = text.encode("utf-8", "ignore")
            hasher.update(len(encoded).to_bytes(8, "little"))
            hasher.update(hashlib.sha1(encoded).digest())

        cache_key = hasher.hexdigest()
        return os.path.join(cache_dir, f"{cache_key}.npy")

    def _load_cached_tokens(
        self, cache_path: str | None, expected_rows: int
    ) -> np.ndarray | None:
        if not cache_path or not os.path.exists(cache_path):
            return None

        try:
            cached = np.load(cache_path, allow_pickle=False)
        except Exception as exc:  # pragma: no cover - depends on external state
            print(f"  âš ï¸ æ— æ³•ä»ç¼“å­˜åŠ è½½ {cache_path}: {exc}ï¼Œé‡æ–°é¢„ç¼–ç ã€‚")
            return None

        if not isinstance(cached, np.ndarray):
            print(f"  âš ï¸ ç¼“å­˜æ–‡ä»¶ {cache_path} éæ•°ç»„æ ¼å¼ï¼Œé‡æ–°é¢„ç¼–ç ã€‚")
            return None

        if cached.dtype != np.int64:
            print(f"  âš ï¸ ç¼“å­˜æ–‡ä»¶ {cache_path} dtype ä¸åŒ¹é…ï¼Œé‡æ–°é¢„ç¼–ç ã€‚")
            return None

        if cached.ndim != 2 or cached.shape[1] != self.max_length:
            print(f"  âš ï¸ ç¼“å­˜æ–‡ä»¶ {cache_path} å½¢çŠ¶ä¸åŒ¹é…ï¼Œé‡æ–°é¢„ç¼–ç ã€‚")
            return None

        if cached.shape[0] != expected_rows:
            print(f"  âš ï¸ ç¼“å­˜æ–‡ä»¶ {cache_path} è¡Œæ•°ä¸ä¸€è‡´ï¼Œé‡æ–°é¢„ç¼–ç ã€‚")
            return None

        print(f"  ğŸ’¾ ä»ç¼“å­˜åŠ è½½é¢„ç¼–ç æ•°æ®: {cache_path}")
        return cached

    def _save_tokens_to_cache(
        self, cache_path: str | None, tokens: np.ndarray
    ) -> None:
        if cache_path is None:
            return

        cache_dir = os.path.dirname(cache_path)
        try:
            os.makedirs(cache_dir, exist_ok=True)
        except OSError as exc:  # pragma: no cover - filesystem errors
            print(f"  âš ï¸ æ— æ³•åˆ›å»ºç¼“å­˜ç›®å½• {cache_dir}: {exc}ï¼Œè·³è¿‡ç¼“å­˜ã€‚")
            return

        tmp_path = None
        try:
            fd, tmp_path = tempfile.mkstemp(prefix="cache-", suffix=".npy", dir=cache_dir)
            with os.fdopen(fd, "wb") as tmp_file:
                np.save(tmp_file, tokens, allow_pickle=False)
            os.replace(tmp_path, cache_path)
        except Exception as exc:  # pragma: no cover - depends on filesystem state
            print(f"  âš ï¸ å†™å…¥ç¼“å­˜å¤±è´¥ {cache_path}: {exc}ï¼Œè·³è¿‡ç¼“å­˜ã€‚")
            if tmp_path and os.path.exists(tmp_path):
                try:
                    os.remove(tmp_path)
                except OSError:
                    pass
        else:
            print(f"  ğŸ’¾ å·²ç¼“å­˜é¢„ç¼–ç ç»“æœ: {cache_path}")

