"""Language modeling dataset utilities."""

from __future__ import annotations

import time
from typing import Sequence

import numpy as np
import torch
from torch.utils.data import Dataset


class LanguageModelingDataset(Dataset):
    """Dataset that packs plain text into tokenized language modeling samples."""

    def __init__(
        self,
        texts: Sequence[str],
        tokenizer,
        max_length: int = 512,
        *,
        pretokenize: bool = True,
    ):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.pretokenize = pretokenize

        if pretokenize:
            self._tokens = self._pretokenize_texts(list(texts))
            self.texts: list[str] | None = None
        else:
            self.texts = list(texts)
            self._tokens = None
        total = len(self._tokens) if self._tokens is not None else len(self.texts)
        print(f"ğŸ“Š æ•°æ®é›†åˆå§‹åŒ–: {total} æ¡æ–‡æœ¬, æœ€å¤§é•¿åº¦: {max_length}")

    def __len__(self) -> int:
        if self._tokens is not None:
            return self._tokens.shape[0]
        return len(self.texts) if self.texts is not None else 0

    def __getitem__(self, idx: int) -> torch.Tensor:
        if self._tokens is not None:
            return torch.from_numpy(self._tokens[idx])
        return self._encode_to_tensor(self.texts[idx], idx)

    # ------------------------------------------------------------------
    def _pretokenize_texts(self, texts: list[str]) -> np.ndarray:
        total = len(texts)
        tokens = np.empty((total, self.max_length), dtype=np.int64)
        start_time = time.time()

        print(f"  ğŸ”„ å¼€å§‹é¢„ç¼–ç  {total:,} ä¸ªæ ·æœ¬...")

        # å•çº¿ç¨‹ç¼–ç ï¼ˆç®€å•ç¨³å®šï¼Œé¿å…multiprocessingé—®é¢˜ï¼‰
        # tokenizeré€šå¸¸æ˜¯C++å®ç°ï¼Œé€Ÿåº¦å·²ç»å¾ˆå¿«
        for idx, text in enumerate(texts):
            tokens[idx] = self._encode_numpy(text)

            # å®šæœŸæ˜¾ç¤ºè¿›åº¦
            if (idx + 1) % 5000 == 0 or idx == total - 1:
                elapsed = time.time() - start_time
                speed = (idx + 1) / elapsed if elapsed > 0 else 0.0
                eta = (total - idx - 1) / speed if speed > 0 else 0
                print(f"  ğŸ”„ é¢„ç¼–ç  {idx + 1:,}/{total:,} æ ·æœ¬ (é€Ÿåº¦ {speed:.1f}/s, é¢„è®¡å‰©ä½™ {eta/60:.1f}åˆ†é’Ÿ)")

        elapsed = time.time() - start_time
        avg_speed = total / elapsed if elapsed > 0 else 0.0
        print(f"  âœ… é¢„ç¼–ç å®Œæˆ: {total:,} æ ·æœ¬, è€—æ—¶ {elapsed:.1f}s, å¹³å‡é€Ÿåº¦ {avg_speed:.1f}/s")

        return tokens

    def _encode_numpy(self, text: str) -> np.ndarray:
        token_ids = self.tokenizer.encode(text, add_special_tokens=True)
        if len(token_ids) < 2:
            token_ids = [self.tokenizer.bos_id, self.tokenizer.eos_id]

        if len(token_ids) > self.max_length:
            token_ids = token_ids[: self.max_length]
        else:
            needed_padding = max(0, self.max_length - len(token_ids))
            if needed_padding:
                token_ids.extend([self.tokenizer.pad_id] * needed_padding)

        if len(token_ids) < 2:
            token_ids = [
                self.tokenizer.bos_id,
                self.tokenizer.eos_id,
                *([self.tokenizer.pad_id] * max(0, self.max_length - 2)),
            ]

        return np.asarray(token_ids, dtype=np.int64)

    def _encode_to_tensor(self, text: str, idx: int) -> torch.Tensor:
        try:
            return torch.from_numpy(self._encode_numpy(text))
        except Exception as exc:  # pragma: no cover - defensive logging
            print(f"âŒ å¤„ç†æ•°æ®é¡¹ {idx} æ—¶å‘ç”Ÿé”™è¯¯: {exc}")
            if text:
                print(f"âŒ é”™è¯¯æ–‡æœ¬é•¿åº¦: {len(text)}")
                print(f"âŒ é”™è¯¯æ–‡æœ¬é¢„è§ˆ: {text[:200]}...")
            import traceback

            traceback.print_exc()
            default_tokens = [
                self.tokenizer.bos_id,
                self.tokenizer.eos_id,
                *([self.tokenizer.pad_id] * max(0, self.max_length - 2)),
            ]
            return torch.tensor(default_tokens, dtype=torch.long)
