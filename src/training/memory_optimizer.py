"""
å†…å­˜ä¼˜åŒ–ç³»ç»Ÿ
åŒ…å«æ··åˆç²¾åº¦è®­ç»ƒã€æ¢¯åº¦ç´¯ç§¯ã€å†…å­˜ç®¡ç†å’Œä¼˜åŒ–ç­–ç•¥
"""
import os
import gc
import psutil
import warnings
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass
from contextlib import contextmanager

import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler
import torch.distributed as dist


@dataclass
class MemoryConfig:
    """å†…å­˜ä¼˜åŒ–é…ç½®"""
    # æ··åˆç²¾åº¦é…ç½®
    enable_amp: bool = True
    amp_dtype: torch.dtype = torch.float16
    loss_scale_window: int = 2000
    init_scale: float = 65536.0

    # æ¢¯åº¦ç´¯ç§¯é…ç½®
    gradient_accumulation_steps: int = 1
    max_grad_norm: float = 1.0
    adaptive_grad_clip: bool = True

    # å†…å­˜ç®¡ç†é…ç½®
    enable_memory_efficient_attention: bool = True
    enable_gradient_checkpointing: bool = False
    clear_cache_frequency: int = 100

    # åŠ¨æ€æ‰¹å¤„ç†é…ç½®
    adaptive_batch_size: bool = False
    min_batch_size: int = 1
    max_batch_size: int = 64
    oom_batch_size_reduction: float = 0.75

    # å†…å­˜ç›‘æ§é…ç½®
    memory_fraction_threshold: float = 0.9
    enable_memory_profiling: bool = False


class MemoryMonitor:
    """å†…å­˜ç›‘æ§å™¨"""

    def __init__(self, device: torch.device):
        self.device = device
        self.memory_history = []

    def get_memory_info(self) -> Dict[str, float]:
        """è·å–è¯¦ç»†å†…å­˜ä¿¡æ¯"""
        info = {}

        if self.device.type == 'cuda':
            # CUDAå†…å­˜ä¿¡æ¯
            info.update({
                'gpu_allocated_gb': torch.cuda.memory_allocated(self.device) / 1024**3,
                'gpu_reserved_gb': torch.cuda.memory_reserved(self.device) / 1024**3,
                'gpu_max_allocated_gb': torch.cuda.max_memory_allocated(self.device) / 1024**3,
                'gpu_max_reserved_gb': torch.cuda.max_memory_reserved(self.device) / 1024**3,
            })

            # è®¡ç®—GPUå†…å­˜ä½¿ç”¨ç‡
            if torch.cuda.is_available():
                total_memory = torch.cuda.get_device_properties(self.device).total_memory
                info['gpu_total_gb'] = total_memory / 1024**3
                info['gpu_usage_percent'] = info['gpu_allocated_gb'] / info['gpu_total_gb'] * 100

        elif self.device.type == 'mps':
            # MPSå†…å­˜ä¿¡æ¯ï¼ˆè¿‘ä¼¼ï¼‰
            current_alloc = torch.mps.current_allocated_memory() / 1024**3 if hasattr(torch.mps, 'current_allocated_memory') else 0
            info.update({
                'mps_allocated_gb': current_alloc,
                'mps_usage_percent': min(current_alloc / 16 * 100, 100)  # å‡è®¾16GBç»Ÿä¸€å†…å­˜
            })

        # CPUå†…å­˜ä¿¡æ¯
        memory = psutil.virtual_memory()
        info.update({
            'ram_used_gb': memory.used / 1024**3,
            'ram_total_gb': memory.total / 1024**3,
            'ram_usage_percent': memory.percent,
            'ram_available_gb': memory.available / 1024**3
        })

        self.memory_history.append(info)
        return info

    def check_memory_pressure(self, threshold: float = 0.9) -> bool:
        """æ£€æŸ¥å†…å­˜å‹åŠ›"""
        info = self.get_memory_info()

        # æ£€æŸ¥GPUå†…å­˜
        if 'gpu_usage_percent' in info:
            if info['gpu_usage_percent'] / 100 > threshold:
                return True

        # æ£€æŸ¥MPSå†…å­˜
        if 'mps_usage_percent' in info:
            if info['mps_usage_percent'] / 100 > threshold:
                return True

        # æ£€æŸ¥RAM
        if info['ram_usage_percent'] / 100 > threshold:
            return True

        return False

    def force_cleanup(self):
        """å¼ºåˆ¶æ¸…ç†å†…å­˜"""
        # Pythonåƒåœ¾å›æ”¶
        gc.collect()

        # PyTorchå†…å­˜æ¸…ç†
        if self.device.type == 'cuda':
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats(self.device)
        elif self.device.type == 'mps':
            if hasattr(torch.mps, 'empty_cache'):
                torch.mps.empty_cache()

    def get_memory_summary(self) -> str:
        """è·å–å†…å­˜ä½¿ç”¨æ‘˜è¦"""
        info = self.get_memory_info()
        lines = ["Memory Usage Summary:"]

        if 'gpu_allocated_gb' in info:
            lines.append(f"  GPU: {info['gpu_allocated_gb']:.2f}GB/{info.get('gpu_total_gb', 0):.2f}GB "
                        f"({info.get('gpu_usage_percent', 0):.1f}%)")

        if 'mps_allocated_gb' in info:
            lines.append(f"  MPS: {info['mps_allocated_gb']:.2f}GB "
                        f"({info.get('mps_usage_percent', 0):.1f}%)")

        lines.append(f"  RAM: {info['ram_used_gb']:.2f}GB/{info['ram_total_gb']:.2f}GB "
                    f"({info['ram_usage_percent']:.1f}%)")

        return "\n".join(lines)


class MixedPrecisionManager:
    """æ··åˆç²¾åº¦è®­ç»ƒç®¡ç†å™¨"""

    def __init__(self, config: MemoryConfig, device: torch.device):
        self.config = config
        self.device = device
        self.enabled = config.enable_amp and self._check_amp_support()

        if self.enabled:
            self.scaler = GradScaler(
                init_scale=config.init_scale,
                growth_factor=2.0,
                backoff_factor=0.5,
                growth_interval=config.loss_scale_window
            )
            print(f"âœ… Mixed precision training enabled (dtype: {config.amp_dtype})")
        else:
            self.scaler = None
            print("âŒ Mixed precision training disabled or not supported")

    def _check_amp_support(self) -> bool:
        """æ£€æŸ¥è®¾å¤‡æ˜¯å¦æ”¯æŒè‡ªåŠ¨æ··åˆç²¾åº¦"""
        if self.device.type == 'cuda':
            # æ£€æŸ¥CUDAç‰ˆæœ¬å’Œè®¾å¤‡æ”¯æŒ
            return torch.cuda.is_available() and torch.cuda.amp.autocast
        elif self.device.type == 'cpu':
            # CPUæ”¯æŒAMPï¼ˆPyTorch 1.10+ï¼‰
            return hasattr(torch.cpu.amp, 'autocast')
        else:
            # MPSæš‚ä¸æ”¯æŒAMP
            return False

    @contextmanager
    def autocast_context(self):
        """è‡ªåŠ¨æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        if self.enabled:
            if self.device.type == 'cuda':
                with autocast(dtype=self.config.amp_dtype):
                    yield
            elif self.device.type == 'cpu':
                with torch.cpu.amp.autocast(dtype=self.config.amp_dtype):
                    yield
            else:
                yield
        else:
            yield

    def scale_loss(self, loss: torch.Tensor) -> torch.Tensor:
        """ç¼©æ”¾æŸå¤±"""
        if self.enabled and self.scaler:
            return self.scaler.scale(loss)
        return loss

    def step_optimizer(self, optimizer: optim.Optimizer) -> bool:
        """æ‰§è¡Œä¼˜åŒ–å™¨æ­¥éª¤"""
        if self.enabled and self.scaler:
            self.scaler.step(optimizer)
            self.scaler.update()
            return True
        else:
            optimizer.step()
            return True

    def backward(self, loss: torch.Tensor):
        """åå‘ä¼ æ’­"""
        if self.enabled and self.scaler:
            self.scaler.scale(loss).backward()
        else:
            loss.backward()

    def get_scale(self) -> float:
        """è·å–å½“å‰ç¼©æ”¾å› å­"""
        if self.enabled and self.scaler:
            return self.scaler.get_scale()
        return 1.0


class GradientAccumulator:
    """æ¢¯åº¦ç´¯ç§¯ç®¡ç†å™¨"""

    def __init__(self, config: MemoryConfig, model: nn.Module):
        self.config = config
        self.model = model
        self.accumulation_steps = config.gradient_accumulation_steps
        self.max_grad_norm = config.max_grad_norm
        self.adaptive_clip = config.adaptive_grad_clip

        self.step_count = 0
        self.gradient_norms = []

        print(f"ğŸ”„ Gradient accumulation: {self.accumulation_steps} steps")

    def should_update(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ›´æ–°å‚æ•°"""
        return (self.step_count + 1) % self.accumulation_steps == 0

    def accumulate_gradients(self, loss: torch.Tensor) -> torch.Tensor:
        """ç´¯ç§¯æ¢¯åº¦"""
        # æŒ‰ç´¯ç§¯æ­¥æ•°å½’ä¸€åŒ–æŸå¤±
        normalized_loss = loss / self.accumulation_steps
        return normalized_loss

    def clip_gradients(self) -> float:
        """æ¢¯åº¦è£å‰ª"""
        if self.max_grad_norm <= 0:
            return 0.0

        # è®¡ç®—æ¢¯åº¦èŒƒæ•°
        if self.adaptive_clip and len(self.gradient_norms) > 10:
            # è‡ªé€‚åº”æ¢¯åº¦è£å‰ª
            recent_norms = self.gradient_norms[-10:]
            adaptive_norm = np.mean(recent_norms) + 2 * np.std(recent_norms)
            clip_norm = min(self.max_grad_norm, adaptive_norm)
        else:
            clip_norm = self.max_grad_norm

        # æ‰§è¡Œæ¢¯åº¦è£å‰ª
        grad_norm = torch.nn.utils.clip_grad_norm_(
            self.model.parameters(),
            max_norm=clip_norm
        ).item()

        self.gradient_norms.append(grad_norm)
        if len(self.gradient_norms) > 100:
            self.gradient_norms = self.gradient_norms[-100:]

        return grad_norm

    def reset_gradients(self):
        """é‡ç½®æ¢¯åº¦"""
        self.model.zero_grad()

    def step(self):
        """æ‰§è¡Œä¸€æ­¥ç´¯ç§¯"""
        self.step_count += 1


class DynamicBatchSizer:
    """åŠ¨æ€æ‰¹å¤„ç†å¤§å°ç®¡ç†å™¨"""

    def __init__(self, config: MemoryConfig, memory_monitor: MemoryMonitor):
        self.config = config
        self.memory_monitor = memory_monitor
        self.enabled = config.adaptive_batch_size

        self.current_batch_size = config.max_batch_size if config.adaptive_batch_size else None
        self.min_batch_size = config.min_batch_size
        self.max_batch_size = config.max_batch_size
        self.reduction_factor = config.oom_batch_size_reduction

        self.oom_count = 0
        self.last_successful_batch_size = None

        if self.enabled:
            print(f"ğŸ”„ Dynamic batch sizing enabled: {self.min_batch_size}-{self.max_batch_size}")

    def handle_oom(self) -> Optional[int]:
        """å¤„ç†OOMé”™è¯¯ï¼Œè¿”å›æ–°çš„æ‰¹å¤„ç†å¤§å°"""
        if not self.enabled or self.current_batch_size is None:
            return None

        self.oom_count += 1
        new_batch_size = max(
            self.min_batch_size,
            int(self.current_batch_size * self.reduction_factor)
        )

        print(f"âš ï¸  OOM detected! Reducing batch size: {self.current_batch_size} -> {new_batch_size}")

        # å¼ºåˆ¶æ¸…ç†å†…å­˜
        self.memory_monitor.force_cleanup()

        self.current_batch_size = new_batch_size
        return new_batch_size

    def try_increase_batch_size(self) -> Optional[int]:
        """å°è¯•å¢åŠ æ‰¹å¤„ç†å¤§å°"""
        if not self.enabled or self.current_batch_size is None:
            return None

        # æ£€æŸ¥å†…å­˜å‹åŠ›
        if self.memory_monitor.check_memory_pressure(0.7):
            return None

        # å°è¯•å¢åŠ æ‰¹å¤„ç†å¤§å°
        new_batch_size = min(
            self.max_batch_size,
            int(self.current_batch_size * 1.25)
        )

        if new_batch_size > self.current_batch_size:
            print(f"ğŸ“ˆ Increasing batch size: {self.current_batch_size} -> {new_batch_size}")
            self.current_batch_size = new_batch_size
            return new_batch_size

        return None

    def get_current_batch_size(self) -> Optional[int]:
        """è·å–å½“å‰æ‰¹å¤„ç†å¤§å°"""
        return self.current_batch_size


class MemoryOptimizer:
    """ç»¼åˆå†…å­˜ä¼˜åŒ–ç®¡ç†å™¨"""

    def __init__(self, model: nn.Module, config: MemoryConfig, device: torch.device):
        self.model = model
        self.config = config
        self.device = device

        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.memory_monitor = MemoryMonitor(device)
        self.mixed_precision = MixedPrecisionManager(config, device)
        self.gradient_accumulator = GradientAccumulator(config, model)
        self.batch_sizer = DynamicBatchSizer(config, self.memory_monitor)

        # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        if config.enable_gradient_checkpointing:
            self._enable_gradient_checkpointing()

        # å†…å­˜æ¸…ç†è®¡æ•°å™¨
        self.step_count = 0

        print("ğŸš€ MemoryOptimizer initialized successfully")
        print(self.memory_monitor.get_memory_summary())

    def _enable_gradient_checkpointing(self):
        """å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹"""
        def apply_gradient_checkpointing(module):
            if hasattr(module, 'gradient_checkpointing'):
                module.gradient_checkpointing = True
            for child in module.children():
                apply_gradient_checkpointing(child)

        apply_gradient_checkpointing(self.model)
        print("âœ… Gradient checkpointing enabled")

    @contextmanager
    def optimize_step_context(self, optimizer: optim.Optimizer):
        """ä¼˜åŒ–æ­¥éª¤ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        try:
            # å†…å­˜ç›‘æ§
            if self.step_count % 50 == 0:
                print(self.memory_monitor.get_memory_summary())

            # æ£€æŸ¥å†…å­˜å‹åŠ›
            if self.memory_monitor.check_memory_pressure(self.config.memory_fraction_threshold):
                print("âš ï¸  High memory pressure detected, forcing cleanup")
                self.memory_monitor.force_cleanup()

            # æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡
            with self.mixed_precision.autocast_context():
                yield {
                    'should_update': self.gradient_accumulator.should_update(),
                    'current_batch_size': self.batch_sizer.get_current_batch_size()
                }

            # æ¢¯åº¦å¤„ç†
            if self.gradient_accumulator.should_update():
                # æ¢¯åº¦è£å‰ª
                grad_norm = self.gradient_accumulator.clip_gradients()

                # ä¼˜åŒ–å™¨æ­¥éª¤
                self.mixed_precision.step_optimizer(optimizer)

                # é‡ç½®æ¢¯åº¦
                self.gradient_accumulator.reset_gradients()

                return grad_norm

            # ç´¯ç§¯æ­¥éª¤
            self.gradient_accumulator.step()

        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                # å¤„ç†OOM
                new_batch_size = self.batch_sizer.handle_oom()
                if new_batch_size:
                    raise MemoryError(f"OOM handled, try batch_size={new_batch_size}")
                else:
                    raise e
            else:
                raise e

        finally:
            # å®šæœŸæ¸…ç†
            self.step_count += 1
            if self.step_count % self.config.clear_cache_frequency == 0:
                self.memory_monitor.force_cleanup()

    def compute_loss(self, loss: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—æŸå¤±ï¼ˆå¸¦æ¢¯åº¦ç´¯ç§¯ï¼‰"""
        return self.gradient_accumulator.accumulate_gradients(loss)

    def backward(self, loss: torch.Tensor):
        """åå‘ä¼ æ’­"""
        self.mixed_precision.backward(loss)

    def get_memory_stats(self) -> Dict[str, Any]:
        """è·å–å†…å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'memory_info': self.memory_monitor.get_memory_info(),
            'amp_scale': self.mixed_precision.get_scale(),
            'gradient_accumulation_steps': self.gradient_accumulator.accumulation_steps,
            'current_batch_size': self.batch_sizer.get_current_batch_size(),
            'oom_count': self.batch_sizer.oom_count
        }

    def optimize_model_for_inference(self):
        """ä¸ºæ¨ç†ä¼˜åŒ–æ¨¡å‹"""
        print("ğŸ”§ Optimizing model for inference...")

        # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
        self.model.eval()

        # ç¦ç”¨æ¢¯åº¦è®¡ç®—
        for param in self.model.parameters():
            param.requires_grad = False

        # æ¸…ç†å†…å­˜
        self.memory_monitor.force_cleanup()

        # å¯é€‰ï¼šæ¨¡å‹é‡åŒ–ï¼ˆå¦‚æœæ”¯æŒï¼‰
        if hasattr(torch.quantization, 'quantize_dynamic'):
            # åŠ¨æ€é‡åŒ–
            try:
                quantized_model = torch.quantization.quantize_dynamic(
                    self.model, {nn.Linear}, dtype=torch.qint8
                )
                print("âœ… Model quantization applied")
                return quantized_model
            except Exception as e:
                print(f"âš ï¸  Quantization failed: {e}")
                return self.model

        return self.model

    def save_memory_profile(self, filepath: str):
        """ä¿å­˜å†…å­˜ä½¿ç”¨åˆ†æ"""
        if not self.config.enable_memory_profiling:
            return

        profile_data = {
            'config': {
                'enable_amp': self.config.enable_amp,
                'gradient_accumulation_steps': self.config.gradient_accumulation_steps,
                'enable_gradient_checkpointing': self.config.enable_gradient_checkpointing
            },
            'memory_history': self.memory_monitor.memory_history,
            'final_stats': self.get_memory_stats()
        }

        import json
        with open(filepath, 'w') as f:
            json.dump(profile_data, f, indent=2)

        print(f"ğŸ’¾ Memory profile saved to: {filepath}")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    test_model = nn.Sequential(
        nn.Linear(1000, 500),
        nn.ReLU(),
        nn.Linear(500, 100),
        nn.ReLU(),
        nn.Linear(100, 10)
    )

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    test_model.to(device)

    # é…ç½®å†…å­˜ä¼˜åŒ–
    config = MemoryConfig(
        enable_amp=True,
        gradient_accumulation_steps=4,
        adaptive_batch_size=True,
        min_batch_size=8,
        max_batch_size=64,
        enable_gradient_checkpointing=True
    )

    # åˆå§‹åŒ–å†…å­˜ä¼˜åŒ–å™¨
    optimizer_params = optim.AdamW(test_model.parameters(), lr=1e-3)
    memory_optimizer = MemoryOptimizer(test_model, config, device)

    print("ğŸ§ª Testing memory optimization...")

    # æ¨¡æ‹Ÿè®­ç»ƒå¾ªç¯
    for step in range(20):
        try:
            with memory_optimizer.optimize_step_context(optimizer_params) as ctx:
                # æ¨¡æ‹Ÿæ‰¹å¤„ç†æ•°æ®
                batch_size = ctx.get('current_batch_size', 32)
                x = torch.randn(batch_size, 1000, device=device)

                # å‰å‘ä¼ æ’­
                y = test_model(x)
                loss = y.sum()

                # è®¡ç®—ä¼˜åŒ–åçš„æŸå¤±
                optimized_loss = memory_optimizer.compute_loss(loss)

                # åå‘ä¼ æ’­
                memory_optimizer.backward(optimized_loss)

                if ctx['should_update']:
                    print(f"Step {step}: Updated parameters, loss = {loss.item():.4f}")

        except MemoryError as e:
            print(f"Handled memory error: {e}")
            continue

    # è·å–æœ€ç»ˆç»Ÿè®¡
    stats = memory_optimizer.get_memory_stats()
    print(f"\nğŸ“Š Final memory stats: {stats}")

    print("âœ… Memory optimization test completed!")